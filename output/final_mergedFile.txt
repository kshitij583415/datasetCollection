Consultative Committee for Space Data Systems and reflects the consensus of Consultative Committee for Space Data Systems . Washington DC USA protocols recommended by CCSDS. space link is communications link between spacecraft and its associated ground system or between two spacecraft. space communications protocol is communications protocol designed to be used over space link or in network that contains one or multiple space links. Through the process of normal evolution it is expected that expansion deletion or document management and change control procedures which are defined in Organization and Processes for the Consultative Committee for Space Data Systems . Canadian Space Agency Canada. Centre National dEtudes Spatiales France. China National Space Administration Peoples Republic of China. Deutsches Zentrum für Luft und Raumfahrt Germany. Federal Space Agency Russian Federation. Instituto Nacional de Pesquisas Espaciais Brazil. Japan Aerospace Exploration Agency Japan. National Aeronautics and Space Administration USA. Belgian Science Policy Office Belgium. Central Research Institute of Machine Building Russian Federation. China Satellite Launch and Tracking Control General Beijing Institute of Tracking and China Academy of Space Technology China. Commonwealth Scientific and Industrial Research Organization Australia. Danish National Space Center Denmark. Departamento de Ciência Tecnologia Aeroespacial Brazil. Electronics and Telecommunications Research Institute Korea. European Organization for the Exploitation of Meteorological Satellites Europe. European Telecommunications Satellite Organization Europe. GeoInformatics and Space Technology Development Agency Thailand. Indian Space Research Organization India. Institute of Space Research Russian Federation. Ministry of Communications Israel. Mohammed Bin Rashid Space Centre United Arab Emirates. National Institute of Information and Communications Technology Japan. National Oceanic and Atmospheric Administration USA. National Space Agency of the Republic of Kazakhstan Kazakhstan. National Space Organization Chinese Taipei. Netherlands Space Office The Netherlands. Research Institute for Particle Nuclear Physics Hungary. Scientific and Technological Research Council of Turkey Turkey. South African National Space Agency Republic of South Africa. Space and Upper Atmosphere Research Commission Pakistan. Encapsulation Packet Protocol Is Used for EndtoEnd Forwarding . 43 43 Protocol Configuration in Space Data System When Space Packet or Encapsulation Packet Protocol Is Used for EndtoEnd Forwarding . 43 44 Protocol Configuration on Space Link When IP over CCSDS Is Used for EndtoEnd Routing . 44 45 Protocol Configuration in Space Data System When IP over CCSDS is Used for EndtoEnd Routing . 45 46 Protocol Configuration in Space Data System When BP Is Used for EndtoEnd Data Routing . 46 31 Identifiers of Space Data Link Protocols . 35 33 Functions of Synchronization and Channel Coding Standards . 38 communications protocols recommended by CCSDS and to show how these protocols are used in space mission data systems. The focus is on the recommendations within the Space Link Services area of CCSDS while protocols from other areas are covered to lesser space link is communications link between spacecraft and its associated ground system or between two spacecraft. space communications protocol is communications protocol designed to be used over space link or in network that contains one or multiple space does not contain the specification or rationale of each protocol. The specification of space communications protocol developed by CCSDS is contained in CCSDS Blue Book and its rationale is described in CCSDS Green Book that accompanies the Blue Book. space data systems annex lists acronyms and abbreviations used within this document. Most of the CCSDS space communications protocols are defined using the style established by the Open Systems Interconnection Basic Reference Model . This model provides common framework for the development of standards in the field of systems interconnection. It defines concepts and terms associated with layered architecture and introduces seven specific layers. The concepts and terms defined in this model are extensively used in the Blue Books that define CCSDS space communications protocols. If the reader is not familiar with this model an excellent introduction can be found in Transport Layer. forwarding The act of transferring data from its source towards its destination which may octet An 8bit word. Physical Channel stream of bits transferred over space link in single routing The process of selecting paths from origins to destinations in network. space link communications link between spacecraft and its associated ground system or between two spacecraft. space link consists of one or more Physical Channels in one or both directions. space communications protocol communications protocol designed to be used over space link or in network that contains one or multiple space links. document are encouraged to investigate the possibility of applying the most recent editions of Organization and Processes for the Consultative Committee for Space Data Systems. CCSDS April 2014. Information TechnologyOpen Systems InterconnectionBasic Reference Model The Basic Model. 2nd ed. International Standard ISOIEC 749811994. Geneva ISO 1994. Andrew . Tanenbaum and David . Wetherall. Computer Networks. 5th ed. Boston Space Communications Protocol Specification Transport Protocol CCSDS 211.0B6. Washington . Washington . CCSDS October 2019. Data System Standards CCSDS 211.1B4. Washington . CCSDS Information TechnologyOpen Systems InterconnectionBasic Reference Model Conventions for the Definition of OSI Services. International Standard ISOIEC 2460. Reston Virginia ISOC December 1998. Postel. Transmission Control Protocol. STD 7. Reston Virginia ISOC September . Postel. User Datagram Protocol. STD 6. Reston Virginia ISOC August 1980. Postel and . Reynolds. File Transfer Protocol . STD 9. Reston Virginia ISOC October 1985. Kent and . Seo. Security Architecture for the Internet Protocol. RFC 4301. Reston Data System Standards CCSDS 350.0G3. Washington . CCSDS Recommendation for Space Data System Standards CCSDS 911.1B4. Washington . CCSDS August 2016. Recommendation for Space Data System Standards CCSDS 911.2B3. Recommendation for Space Data System Standards CCSDS 912.1B4. Washington . CCSDS August 2016. Recommendation for Space Data System Standards CCSDS 912.3B3S. Mission OperationsMAL Space Packet Transport Binding and Binary Encoding. Flexible Advanced Coding and Modulation Scheme for High Rate Telemetry Washington . CCSDS November 2017. Data System Practices CCSDS 351.0M1. Washington . Data System Standards CCSDS 734.1B1. Washington . CCSDS Recommendation for Space Data System Practices CCSDS 901.1M 1. Washington . CCSDS May 2015. Spectral Preprocessing Transform for Multispectral and Hyperspectral Image Traditionally telemetry transmitted from the spacecraft was formatted with Time Division Multiplexing scheme in which data items were multiplexed into continuous stream of fixedlength frames based on predefined multiplexing rule. To design and implement data system for spacecraft each project was forced to develop custom system used by that project alone with the exception of the ground tracking network because of the lack of established standards in this field. The advent of microprocessorbased spacecraft instruments and subsystems however enabled telemetry systems to become more flexible and have greater throughput so that data processed by onboard software could be transmitted efficiently. In the early 1980s CCSDS developed an international standard for Packet Telemetry protocol capable of sending processed telemetry efficiently using variablelength data unit called the Source Packet. Source Packets generated by various instruments and subsystems on spacecraft are transmitted to the ground in stream of continuous fixedlength Transfer Frames. This standard has been used by many space projects enabling them to share onboard and ground data processing equipment. Based on similar concept another international standard on Telecommand was developed by CCSDS shortly after Packet Telemetry for sending commands to spacecraft with data unit known as the TC Packet. TC Packets destined for various instruments and subsystems on spacecraft are transmitted from the ground in stream of sporadic variablelength Transfer Frames. In the late 1980s CCSDS extended the above standards to meet the requirements of the Advanced Orbiting Systems such as the International Space Station and came up with third standard known as AOS. The AOS standard added to the Packet Telemetry standard services for transmitting various types of online data and it may be used on both spacetoground and groundtospace links. The AOS uses the same packet structure as the Packet Telemetry standard but the frame format is slightly These three standards were later restructured by CCSDS in order to define the protocols in more structured and unified way and the following standards replaced the original standards As an international standard for the Radio Frequency signal between spacecraft and ground station CCSDS developed standard called Radio Frequency and Modulation Systems . This standard specifies the characteristics of the RF signal used to carry Packets and Frames. In the 1990s CCSDS developed another set of protocols collectively known as Space Communications Protocol Specifications which include SCPS Network Protocol SCPS Security Protocol SCPS Transport Protocol and SCPS File Protocol . The SCPS protocols are generally based on Internet protocols but modifications and extensions to the Internet protocols are incorporated in the design of the SCPS protocols to meet the specific needs of space missions. CCSDS has retired all of the SCPS protocols with the exception of SCPSTP. In response to the needs of space missions to transfer files to and from an onboard mass memory CCSDS has developed protocol called the CCSDS File Delivery Protocol . This protocol provides the capability to transfer files reliably and efficiently over an unreliable protocol . In July 1998 following the successes with relay experiments for Mars spacecraft NASA began investigating the design for standard protocol that can provide Internetlike services to spacecraft that may be in deepspace andor only intermittentlyconnected to Earth. team of researchers was formed that included Dr. Vint Cerf coauthor of the provides generalpurpose NetworkTransportLayer service that is logically similar to what TCPIP provides for the terrestrial Internet but suitable for use in the space environment. In addition to the basic storeandforward internetworking service DTN also provides efficient reliability security inorder delivery duplicate suppression class of service remote management DVRlike streaming service rate buffering and data accounting all over possibly asymmetric and timedisjoint paths. Multiple applications including file transfer messaging and streaming audiovideo can all be implemented on top of DTN and leverage its services to reduce risk cost and complexity. CCSDS has other specifications that individually implement some aspects of the network and transportlayer services that DTN provides but none of them provide the flexibility or automated data transfer that DTN does. In the area of data compression CCSDS has developed standards for generalpurpose lossless data compression compressors specialized for compression of two dimensional image data and threedimensional requirement for onboard memory station contact time and data archival volume. The first standard provides lossless compression guaranteeing full reconstruction of the original data without incurring any distortion in the process. The other standards can be used to provide lossless or lossynearlossless compression in which case quantization or other approximations used in the compression process may result in the inability to reproduce the original data set without some distortion but in return for higher compression ratios. Recently CCSDS has developed protocol called Proximity1 Space Link Protocol links are defined to be short range bidirectional fixed or mobile radio links generally used to communicate among fixed probes landers rovers orbiting constellations and orbiting relays. This protocol defines data link protocol coding and synchronization methods and RF and modulation characteristics . In addition CCSDS in 2018 released the Unified Space Data Link Protocol . This protocol has been designed to meet the requirements of space missions for efficient transfer of space application data of various types and characteristics over space toground groundtospace or spacetospace communications links. It is envisioned that USLP will be used as the data link layer protocol for all future robotic and crewed space Security is of great concern to many space missions. CCSDS has published several documents including The Application of CCSDS Protocols to Secure Systems Security Architecture for Space Data Systems CCSDS Cryptographic Algorithms the Space Data Link Security protocol missions that wish to use the CCSDS space communications protocols for spacecraft control and data handling but also require level of security or data protection. communications protocol is usually associated with one of the seven layers defined in the OSI Basic Reference Model . Although some space communications protocols the space communications protocols. The space communications protocols are defined for the following five layers of the ISO Application Layer. As in most terrestrial networks protocols of the Session and Presentation Layers of the OSI model are rarely used over space links. as well as the Physical and Data Link Layers. Either Encapsulation Packet Protocol or Space Packet Protocol CCSDS does not formally define Application Program Interfaces for the space communications protocols but most CCSDS standards provide abstract service definitions in the form of primitives following the conventions established by ISO . primitive is an abstract representation of the services provided by the protocol layer but it does not depend on any implementation technology. This abstract specification may be used as reference for developing an API. CCSDS has an omnibus standard for the Physical Layer called the Radio Frequency and Modulation Systems to be used for space links between spacecraft and ground stations. The Proximity1 Space Link Protocol also contains recommendations for the Physical Layer of proximity space links . CCSDS defines two Sublayers in the Data Link Layer of the OSI Model the Data Link Protocol Sublayer and the Synchronization and Channel Coding Sublayer. The Data Link Protocol Sublayer specifies methods of transferring data units provided by the higher layer over pointtopoint space link using data units known as Transfer Frames. The Synchronization and Channel Coding Sublayer specifies methods of synchronization and channel coding for transferring Transfer Frames over space link. CCSDS has developed several protocols for the Data Link Protocol Sublayer of the Data Link AOS Space Data Link Protocol Proximity1 Space Link ProtocolData Link Layer USLP . The above protocols provide the capability to send data over single space link. TM TC AOS and USLP have provisions for inserting secured user data into frame using the SDLS Protocol and the associated SDLS Extended Procedures . SDLS protocol can provide security services such as authentication and confidentiality for TM Transfer Frames AOS Transfer Frames TC Transfer Frames or USLP Transfer Frames. It should be noted that the use of the SDLS function within these protocols is optional. The SDLS Extended Procedures provides Key and Security Associations management services needed to operate an SDLS secured space link. CCSDS has developed three standards for the Synchronization and Channel Coding Sublayer of the Data Link Layer TM Synchronization and Channel Coding TC Synchronization and Channel Coding Proximity1 Space Link ProtocolCoding and Synchronization Layer Flexible Advanced Coding and Modulation Scheme for High Rate Telemetry Applications or SCCC CCSDS Space Link Protocols over ETSI DVBS2 Standard or DVBS2. TM Synchronization and Channel Coding is used with the TM or AOS Space Data Link Protocol or USLP TC Synchronization and Channel Coding is used with the TC Space Data Link Protocol or USLP and the Proximity1 Space Link ProtocolCoding and Synchronization Layer is used with the Proximity1 Space Link Protocol or USLP. The TM TC and AOS Space Data Link Protocols the Proximity1 Space Link Protocol and USLP are called the Space Data Link Protocols in this document. Licklider Transmission Protocol provides optional reliability mechanisms on top of an underlying communication service. From the point of view of protocols above LTP the service LTP provides is optionally reliable delivery of layer Protocol Data Units across link. Layer PDUs are encapsulated within LTP blocks which are segmented for transmission over data link protocols typically each LTP segment is encapsulated within single linklayer protocol data unit that is an Encapsulation Packet. It should be noted that LTP segments may also be encapsulated within Space Packets. The limited size of the Space Packet does not in itself argue against the use of Space Packets to carry DTN traffic because large bundles encapsulated within large LTP blocks will in most cases be segmented into smaller LTP segments for transmission. However the reduced overhead of the Encapsulation Packet makes it more bandwidthefficient alternative to the Space Packet. For more information see reference . CCSDSrecognized Internet datagrams can also be transferred by CCSDS Space Data Link Protocols over space link multiplexed or notmultiplexed using the shim protocol IP over CCSDS . Space communications protocols of the Network Layer provide the function of routing or forwarding higherlayer data through the entire data system that includes both onboard and ground subnetworks. CCSDS recognizes two standards for interfacing at the Network Layer CCSDSrecognized Internet Protocol datagrams Delay Tolerant Networking is an architecture that provides automated network communications much as the Internet architecture does but it does so over networks intermittent connectivity variable delays which may be large and irregular asymmetric and simplex links. One core element of DTN is the Bundle Protocol which serves as the networklayer protocol in delaytolerant network. BP provides endtoend network services operating above the data transport services provided by links or networks accessed via Convergence Layer Adapters 1 and forming storeandforward network BPbased network is an overlay network that may span multiple networks just as the Internet is an overlay network that spans multiple subnets or local area networks. The Bundle Protocol uses the native local protocols for communications within given network. The interface between the Bundle Protocol and specific lowerlayer reference model the Bundle Protocol and several optional convergence layer adapters. On the right two CLAs running above transport protocol are shown. On the left the two CLAs running over LTP and the CLA running on EPPSPP are shown. Space communications protocols of the Transport Layer provide users with endtoend transport services. CCSDS has developed SCPSTP for the Transport Layer. PDUs of Transport Layer protocol are usually transferred with protocol of the Network Layer over space link but they can be transferred directly by Space Data Link. Transport protocols used in the Internet can also be used on top of IP datagrams over CCSDS space links reference . IPSec may be used with Transport protocol of the Internet suite to provide end toend data protection capability. 1 convergence layer adapter CLA Adapter that sends and receives bundles on behalf of the Bundle Protocol Adapter that is the Node component that offers the BP services and executes the procedures of the Bundle Protocol. Space communications protocols of the Application Layer provide users with endtoend application services such as file transfer and data compression. CCSDS has developed several protocols for the Application Layer Asynchronous Messaging Service Space Packet Protocol Message Abstraction Layer Space Packet Transport Binding and Binary Encoding . AMS is an application layer protocol for endtoend mission data system message transfer. CFDP provides the functionality of the Application Layer . The CFDP StoreandForward Overlay procedures provide application specific transfer of data across multiple linklayer hops. Each project may also elect to use applicationspecific protocols not recommended by CCSDS to fulfill their mission requirements in the Application Layer over CCSDS space communications protocols. PDUs of an Application Layer protocol are usually transferred with protocol of the Transport Layer over space link but they can be transferred directly with protocol of the For the Space Packet Protocol PDUs are generated and consumed by application processes that are on spacecraft or on the ground. CCSDS Encapsulation Packet Protocol allows encapsulation of PDUs of CCSDSrecognized protocols as defined in SANA registry into Encapsulation Packets . These packets can then be transferred over space link using the VCMAP Packet Service provided by CCSDS Space Data Link Protocols. Applications protocols used in the Internet can also be used on top of SCPSTP TCP and UDP over space links. The CCSDS Recommended Standard for Radio Frequency and Modulation Systems recommends the characteristics of the RF and modulation systems used for communications over space links between spacecraft and ground stations. The Proximity1 Space Link ProtocolPhysical Layer also contains recommendations for the Physical Layer of proximity space links. CCSDS has developed four protocols for the Data Link Protocol Sublayer of the Data Link USLP Data Link Layer Data Link Protocol Sublayer portion of Proximity1 Space Link Protocol These protocols provide the capability to transfer various types of data on space links but their principal function is to transfer variablelength data units known as packets. Number recognized by CCSDS. These numbers are contained in SANA Space Data Link Protocols directly but CCSDS has another mechanism to transfer PDUs of CCSDS and nonCCSDS protocols using the Encapsulation Packet Protocol defined in reference . With this service packets are transferred by the Space Data Link Protocols encapsulated in either Space Packets defined in reference or Encapsulation Packets defined The TM Space Data Link Protocol is usually used for sending telemetry from spacecraft to ground station . The TC Space Data Link Protocol is usually used for sending commands from ground station to spacecraft . The AOS Space Data Link Protocol may be used on return link alone or on both forward and return links if there is need for twoway higher speed communications between spacecraft and the ground. The Proximity1 Space Link Protocol is to be used over proximity space links for which proximity space links are defined to be short range bidirectional fixed or mobile radio links generally used to communicate among fixed probes landers rovers orbiting constellations and orbiting relays. The Unified Space Link Protocol can be used over space toground groundtospace or spacetospace communications links by space missions. It is envisioned that USLP may be used by future space missions in lieu of TM TC AOS and The protocol data units used by the Space Data Link Protocols are called Transfer Frames. TM AOS and USLP use fixedlength Transfer Frames to facilitate robust synchronization procedures over noisy link TC Proximity1 and USLP use variablelength Transfer Frames to facilitate reception of variablelength messages with various latency requirements for telecommand. key feature of all the Space Data Link Protocols is the concept of Virtual Channels. The Virtual Channel facility allows one Physical Channel to be shared among multiple higherlayer data streams each of which may have different service requirements. single Physical Channel may therefore be divided into several separate logical data channels each known as Virtual Channel . Each Transfer Frame transferred over Physical Channel belongs to one of the Virtual Channels of the Physical Channel. All Transfer Frames with the same Master Channel ID that is Transfer Frame Channel . Master Channel consists of one or more VCs. In most cases Physical Channel carries only Transfer Frames of single MCID and the MC will be identical with the Physical Channel. However Physical Channel may carry Transfer Frames with multiple MCIDs . In such case the Physical Channel consists of multiple MCs. Both the TC Space Data Link Protocol and USLP have function for retransmitting lost or corrupted data to ensure delivery of data in sequence without gaps or duplication over space link. This function is provided by retransmission control mechanism called the Communications Operation Procedure1 which is defined in separate document . Both the Proximity1 Space Link Protocol and USLP also have similar function called COPP which is defined in the Data Link Layer Recommended Standard . Neither the TM Space Data Link Protocol nor the AOS Space Data Link Protocol has such function so retransmission must be done by higherlayer protocol if complete delivery of data is required. The TM and AOS Space Data Link Protocols along with USLP can be used together with the TM Synchronization and Channel Coding Recommended Standard . The TC Space Data Link Protocol along with USLP can be used together with the TC Synchronization and Channel Coding Recommended Standard . The TM TC AOS Space Data Link Protocols and USLP can be used on top of the Recommended Standard for Radio Frequency and Modulation Systems . Proximity1 Space Link ProtocolData Link Layer as well as USLP can be used together with the Proximity1 Space Link ProtocolCoding and Synchronization Sublayer and on top of the Proximity1 Space Link ProtocolPhysical Layer . is contained in reference . Similarly for Proximity1 Space Link Protocol that information is contained in reference and for the USLP in reference . The Space Data Link Protocols provide link identifiers to identify data streams. The identifier SANA is the registrar for all protocol registries created under CCSDS. SANA replaces the retired Space Link Identifiers Blue Book. The CCSDS Global Spacecraft Identification Field Code Assignment Control Procedures Magenta Book contains the procedures governing requesting assigning and relinquishing CCSDS SCID field codes. The TM TC AOS and USLP Space Data Link Protocols have the following three identifiers TFVN SCID and the Virtual Channel Identifier . The TFVN is used to distinguish among different Transfer Frames. However different Transfer Frames must not be multiplexed on Physical Channel. The concatenation of TFVN and SCID is known as an MCID which is used for identifying spacecraft associated with space link. All Transfer Frames with the same MCID on Physical Channel constitute an MC. Master Channel consists of one or more Virtual Channels each of which is identified with VCID. In most cases Physical Channel carries only Transfer Frames of single MCID and the Master Channel will be identical with the Physical Channel. However Physical Channel may carry Transfer Frames with multiple MCIDs . In such case the Physical Channel consists of multiple Master Channels. Physical Channel is identified with Physical Channel Name which is set by management and not included in Both the TC Space Data Link Protocol and USLP uses an optional identifier called the Multiplexer Access Point Identifier that is used to create multiple streams of data within Virtual Channel. All the Transfer Frames on Virtual Channel with the same MAP ID constitute MAP Channel. If the MAP ID is used Virtual Channel consists of one or multiple The Proximity1 Space Link ProtocolData Link Layer uses triad of multiplexing capabilities which is incorporated for specific functionality within the link. The SCID identifies the source or destination of Transfer Frames transported in the link connection based upon the SourceorDestination Identifier. The Physical Channel Identifier provides up to two independently multiplexed channels. The Port ID provides the means to route user data internally to specific logical ports such as applications or transport processes or to physical ports such as onboard buses or physical connections . IDs are maintained by the SANA registries . The Space Data Link Protocols provide several services to transfer variety of data on space link. The most important service is service to transfer variablelength data units known as packets . In addition to this service the Space Data Link Protocols provide services to transfer fixed or variable length data units with private formats short fixedlength data units for Link Protocols categorized by the types of data transferred by the services. For complete NOTE The Proximity1 Space Link ProtocolData Link Layer is not included in this table because no formal service definition is given in the Recommended Standard Packets directly transferred by the Space Data Link Protocols must have Packet in reference . Other PDUs of CCSDS recognized protocols can be transferred using the Encapsulation Packet Protocol defined in reference . OCF Operational Control Field. The standards of the Synchronization and Channel Coding Sublayer provide some additional functions necessary for transferring Transfer Frames over space links. These functions are delimitingsynchronizing Transfer Frames errorcorrection codingdecoding and bit transition generationremoval. CCSDS has five standards for Synchronization and Channel Coding set of three TM specifications 1 TM Synchronization and Channel Coding 2 Flexible Advanced Coding and Modulation Scheme for High Rate Telemetry Applications or SCCC 3 CCSDS Space Link Protocols over ETSI DVBS2 Standard or TC Synchronization and Channel Coding The three TM specifications define alternative synchronization and channel coding schemes used with the TM AOS or USLP Space Data Link Protocol. TC Synchronization and Channel Coding defines synchronization and channel coding schemes used with the TC or USLP Space Data Link Protocol. Proximity1 Space Link ProtocolCoding and Synchronization Sublayer defines synchronization and channel coding schemes used with both the Proximity1 Space Link Protocol or USLP. Options a2 and a3 are recommended only for high rate downlink. The various coding specifications offer to the Data Link Protocol sublayer two types of services at the sending end for fixedlength frames provided periodically to the lower layer at sending end for variablelength frames provided intermittently to the lower layer at sending end type of service can present two different qualities of service at the receiving end in the table denotes an optional function. When box of the table shows several options only one option can be applied at When only an Error Correction scheme is mentioned it means that the scheme is also capable of validating the frame that is declaring it erroneous or error free. In other cases for TMTCAOSUSLP Frame Error Control Field is used for error detection while Proximity1 uses Cyclic Redundancy Code attached to the frame . The Frame Error Control Field is defined in the Recommended Standards on the TMTCAOSUSLP Space Data Link Protocols and not in the Recommended Standards on Synchronization and Channel Coding. The cyclic pseudonoise sequence used by TM Synchronization and Channel Coding differs from that one used for both TC Synchronization and Channel Coding and Proximity1 Space Link ProtocolCoding and Synchronization Layer. Summaries of concept and rationale for TM Synchronization and Channel Coding TC Synchronization and Channel Coding and Proximity1 Space Link ProtocolCoding and Data Forwarding differs greatly from data routing defined in reference as the process of selecting paths from origins to destinations in network. Here the concept of an endpoint is global over series of open and extensible subnetworks. Whenever routing is done across multiple subnetworks network routing protocol is required which is not in the purview of SPP. It is essential that when one plans to route data over an open network composed of multiple subnetworks one uses network protocol. By using the Encapsulation Packet Protocol as shim other CCSDSrecognized Network units of network protocols can also be transferred using the Space Packet Protocol done in missionspecific manner using APID as set by management. Over space link protocol data units of the network protocols are transferred within the Space Data Link Protocols. In particular the Space Data Link Protocols have the capability to carry several protocol data units of the Internet Protocol multiplexed or notmultiplexed within the Encapsulation packet. IP over CCSDS specifies how CCSDSrecognized IP datagrams are transferred over the link. An End System Identifier as used by IP and BP unambiguously identifies single end system or group of end systems. If it is necessary to identify both the source and destination when using End System Identifiers pair of End System Identifiers must be specified in the IP or BP PDUs and they are used by the IP or BP routing nodes to perform routing decisions at each step along the endtoend path. As already mentioned CCSDS Encapsulation Packet Protocol allows the use of other CCSDS recognized Network Protocols within their own end system identification notations. perform interoperable internetworking in space in either disrupted or delayed endtoend communication environments. CCSDS has developed SCPSTP for the Transport Layer. CFDP also provides the functionality of the Transport Layer but it provides some functions of the Application Layer as well. SCPSTP supports endtoend communications between applications and is designed to meet the needs of broad range of space missions. It defines extensions to TCP and incorporates UDP by reference. It may be used on top of the Space Packet Encapsulation Packet or IP CFDP provides the functionality of the Application Layer but it also provides functions of the Transport Layer. Transport protocols used in the Internet can also be used on top of the Encapsulation packet or IP over CCSDS space links. IPSec can be used with the Internet Protocol suite to provide endtoend data protection capability. CCSDS has developed several protocols for the Application Layer Image Data Compression LowComplexity Lossless and NearLossless Multispectral and Hyperspectral Image Spectral Preprocessing Transform for Multispectral Hyperspectral Image Space Packet Protocol . AMS implements an interoperable protocol under which the mission modules may be designed without explicit awareness of which other modules are currently operating or of where they are deployed. CFDP is designed to meet the needs of space missions to transfer files. It is file transfer protocol but it also provides services typically found in the Transport Layer that is complete inorder and without duplicate data delivery. It can be used on top of any protocol of the Network Layer on top of the Space Packet Protocol or Encapsulation Packet Protocol or directly on top of the CCSDS Space Data Link Protocols if Virtual Channel MAP or Port is assigned to CFDP. In some circumstances it can be used on top of UDP TCP or SCPSTP. Alternatively CFDP can be used in Unacknowledged Mode on top of DTN exploit the threedimensional structure of such images to provide effective compression. Reference specifies compression approach capable of providing lossless compression as well as nearlossless compression in which case the error in the reconstruted image is guaranteed to meet userspecified maximum allowed error. Reference defines spectral preprocessing transform to be used in conjunction with the twodimensional image compressor specified in reference . Applications protocols used in the Internet can be used over TCP or UDP as long as the underlying links are sufficiently short and continuously available. Typically an application is written to use the reliable streamoriented service of TCP or the unreliable datagram service of UDP but not both. Some exceptions to this exist however in which applications are written to operate over either service. Each project may elect to use applicationspecific protocols not recommended by CCSDS to fulfill their mission requirements in the Application Layer over CCSDS space communications protocols. Over space link protocol data units of the network protocols are encapsulated into Encapsulation Packets via the Encapsulation Protocol and then transferred by one of the the Space Data Link Protocols. The Space Packet Protocol was developed to transfer data from source on spacecraft to one or multiple destinations on the ground or on other spacecraft or from source on the ground to one or multiple destinations on one or multiple spacecraft. When protocol data units of this protocol traverse the data system of space mission the Application Identifier that is part of each packet is used for determining the managed data path that packet 2 bundle protocol data unit of the DTN Bundle Protocol. will take. All decisions about how packets are to be handled and forwarded based on this APID are set by management agreement and are not formal part of the protocol. There should be no expectation of interoperable handling of APIDs and managed data paths in cross support situation unless agreements have been clearly defined as to how such forwarding is to be done. Cross support is defined in reference and interested readers should refer to it for further details. The Space Packet Protocol provides the capability to transfer space application data over managed data path that involves groundtospace or spacetospace communications link. However unlike DTN no provisions are made in SPP for addressing the scheduled nature of connectivity between any of the end points or intermediate links. By encapsulating the PDUs of and or IP one for one into an Encapsulation Packet these Network Protocols can be used over CCSDS space links. The protocol data units of the Space Packet Protocol are called Space Packets while the protocol data units of IP are called IP datagrams. SPP and IP do not provide any Quality of Service mechanisms for reliable delivery inorder delivery or duplicate suppression. If these functions are required they must be implemented by higherlayer protocol. The Space Data Link Protocols have the capability to carry several protocol data units of the Internet Protocol multiplexed or not multiplexed within the Encapsulation packet. IP over CCSDS specifies how CCSDSrecognized IP datagrams are transferred over are used in space data systems. combinations of protocols that can be used in space data systems but it is not the intention of selected to illustrate the basic functionality of the space communications protocols. of protocols used over space link . space data system consists of one or more onboard subnetworks one or more space links communications protocols are used in an endtoend space data system. It will be shown that some space communications protocols are used for endtoend communications between onboard and groundend systems and some space communications protocols are used only for communications over the space link. protocol used for endtoend routing or forwarding. In space data system user data traverse subnetworks . One of the protocols used in space data system provides the capability of routing user data from source to destination through these subnetworks. and forwarding in 1.3.2. SPP is used for endtoend data forwarding in closed subnetwork within an ABA IP over CCSDS over the Encapsulation Packet In this example the Space Packet is used for endtoend forwarding. The Space Packet Protocol was designed by CCSDS to meet the requirements of space missions for efficient transfer of processed data over space links. This configuration is suited to space missions that require the simple APID source or destination labeling and forwarding capabilities provided by the Space Packet Protocol. an example of protocol configuration in an endtoend space data system. At each intermediate system some mechanism not specified in SPP examines the APID and forwards the data to the next node that it has been instructed to use. There is no endpoint address and there is no specified mechanism for doing this interoperably. It is done by management and external agreement between user and service provider. When the Space Packet Protocol is used for endtoend forwarding in the ground subnetwork Space Packets are usually transferred with Space Link Extension defined in reference as the process of selecting managed data paths from origins to destinations in network. Here the concept of an endpoint is global over series of open and extensible subnetworks. Whenever we route across multiple subnetworks network routing protocol is required which is not in the purview of SPP. Encapsulation Packet Protocol Is Used for EndtoEnd Forwarding Encapsulation Packet Protocol Is Used for EndtoEnd Forwarding In the fourth example one of the CCSDS recognized IP datagrams defined in SANA is used for endtoend routing. This configuration is suited to space missions that require integration of their space segments into the Internet when endtoend internetworking is required and when connectivity and Round Trip Light Time is suitable to support this approach. an example of protocol configuration in an endtoend space data system. Protocol data units of IP are transferred by Space Data Link Protocols using the IP over CCSDS protocol in order for the Space Data Link Protocols to process IP datagrams In this example it is assumed that the Internet is directly extended into the space segment. Most Internet endtoend protocols and SCPSTP can be used on top of IP. SCPSTP can be At each intermediate system in an IP deployment routing mechanism specified in the IP protocol examines the destination address and makes routing decision that sends the data to the next node in the route. The endpoint address is explicit and all of the mechanisms for doing this interoperably are fully specified. The Encapsulation Packet Protocol provides the shim to insert the IP datagrams into CCSDS space link and to extract it at the other end. BP FOR ENDTOEND DATA ROUTING of messages that support DTN. BP provides endtoend network services operating above the data transport services provided by links or networks accessed via CLAs and The Bundle Protocol uses the native local protocols for communications within given network. The interface between the Bundle Protocol and specific lowerlayer protocol suite is Bundle Protocol and CLA running above transport protocol on the left and running directly over Data Link Layer on the right. The CL on the right could for example be the interface to the Licklider Transmission Protocol with the Link B1 representing LTP running over one of the CCSDS Data Link Layer protocols. Alternatively BP could be used to connect together two internets that may exist such as an onorbit network and ground network. Digital Video Broadcasting Satellite Second Generation Space Communications Protocol Standards File Protocol Space Communications Protocol Standards Network Protocol Space Communications Protocol Standards Security Protocol Space Communications Protocol Standards Transport Protocol

Consultative Committee for Space Data Systems and represents the consensus review and authorization of CCSDS documents is detailed in Organization and Processes for the Consultative Committee for Space Data Systems and the record of Agency participation in the authorization of this document can be obtained from the CCSDS The Consultative Committee for Space Data Systems is an organization officially established by the management of its members. The Committee meets periodically to address voluntary the results of Committee actions are termed Recommended Standards and are not considered binding on any Agency. members. Endorsement of this Recommendation is entirely voluntary. Endorsement however indicates the following understandings Whenever member establishes CCSDSrelated standard this standard will be in accord with the relevant Recommended Standard. Establishing such standard does not preclude other provisions which member may develop. Whenever member establishes CCSDSrelated standard that member will provide other CCSDS members with the following information The standard itself. The anticipated duration of operational service. Specific service arrangements shall be made via memoranda of agreement. Neither this Recommended Standard nor any ensuing standard is substitute for memorandum of agreement. reviewed by the CCSDS to determine whether it should remain in effect without change be changed to reflect the impact of new technologies new requirements or new directions or be retired or canceled. CCSDSrelated member standards and implementations are not negated or deemed to be nonCCSDS compatible. It is the responsibility of each member to determine when such standards or implementations are to be modified. Each member is however strongly encouraged to direct planning for its new standards and implementations towards the later This document describes protocol for applying security services to the CCSDS Space Data Link Protocols used by space missions over space link. Attention is drawn to the possibility that some of the elements of this document may be the from the patent holder agreement that all licensing policies are reasonable and non discriminatory. However CCSDS does not have patent law staff and CCSDS shall not be held responsible for identifying any or all such patent rights. Through the process of normal evolution it is expected that expansion deletion or modification of this document may occur. This Recommended Standard is therefore subject to CCSDS document management and change control procedures which are defined in Organization and Processes for the Consultative Committee for Space Data Systems Canadian Space Agency Canada. Centre National dEtudes Spatiales France. China National Space Administration Peoples Republic of China. Deutsches Zentrum für Luft und Raumfahrt Germany. Federal Space Agency Russian Federation. Instituto Nacional de Pesquisas Espaciais Brazil. Japan Aerospace Exploration Agency Japan. National Aeronautics and Space Administration USA. Belgian Science Policy Office Belgium. Central Research Institute of Machine Building Russian Federation. China Satellite Launch and Tracking Control General Beijing Institute of Tracking and China Academy of Space Technology China. Commonwealth Scientific and Industrial Research Organization Australia. Danish National Space Center Denmark. Departamento de Ciência Tecnologia Aeroespacial Brazil. Electronics and Telecommunications Research Institute Korea. European Organization for the Exploitation of Meteorological Satellites Europe. European Telecommunications Satellite Organization Europe. GeoInformatics and Space Technology Development Agency Thailand. Indian Space Research Organization India. Institute of Space Research Russian Federation. Ministry of Communications Israel. Mohammed Bin Rashid Space Centre United Arab Emirates. National Institute of Information and Communications Technology Japan. National Oceanic and Atmospheric Administration USA. National Space Agency of the Republic of Kazakhstan Kazakhstan. National Space Organization Chinese Taipei. Netherlands Space Office The Netherlands. Research Institute for Particle Nuclear Physics Hungary. Scientific and Technological Research Council of Turkey Turkey. South African National Space Agency Republic of South Africa. Space and Upper Atmosphere Research Commission Pakistan. The purpose of this Recommended Standard is to specify the Space Data Link Security Protocol for CCSDS data links. This protocol provides Telemetry Telecommand Advanced Orbiting Systems and Unified Space Data Link Protocols authentication andor data confidentiality at the Data Link Layer. This Recommended Standard defines the Security Protocol in terms of the protocol data units employed by the service provider and the procedures performed by the service provider. individual implementations or products the implementation of service interfaces within real systems the methods or technologies required to perform the procedures or algorithm with the Security Protocol. Reference provides listing of algorithms recommended by CCSDS any organization should conduct risk assessment before choosing to substitute other algorithms. Annex defines baseline implementations suitable for large range of space missions. To manage the Security Protocol over space link set of procedures has been specified the Space Data Link Security Protocol Extended Procedures . This Recommended Standard applies to the creation of Agency standards and for secure data communications over space links between CCSDS Agencies in crosssupport situations. The Recommended Standard includes comprehensive specification of the service for inter Agency cross support. It is neither specification of nor design for real systems that may be implemented for existing or future missions. The Recommended Standard specified in this document is to be invoked through the normal standards programs of each CCSDS Agency and is applicable to those missions for which interoperability and cross support based on capabilities described in this Recommended Recommended Standard they must be implemented when this document is used as basis for interoperability and cross support. Where options are allowed or implied implementation of these options is subject to specific bilateral cross support agreements The goals of this Recommended Standard are to provide standard method of applying security at the Data Link Layer independent of the underlying cryptographic algorithms employed by any particular space mission preserve compatibility with existing CCSDS Space Data Link Protocol Transfer when appropriate legacy frame processing infrastructure may continue to be used without modification preserve compatibility with the CCSDS Space Link Extension forward and return services and facilitate the development of common commercial implementations to improve interoperability across agencies. More discussion of the Security Protocols goals and design choices including its interaction with other CCSDS services may be found in reference . Annex provides Protocol Implementation Conformance Statement proforma for the Security Protocol. considerations related to this Recommended Standard. Annex defines baseline implementations suitable for large range of space This Recommended Standard makes use of number of terms defined in reference . For the purposes of this Recommended Standard the following definitions also apply. Payload Data input to be processed by Security Protocol function. ApplySecurity Payload Payload to the ApplySecurity function. ProcessSecurity Payload Payload to the ProcessSecurity function. Authentication Payload Part of the Transfer Frame to be authenticated. The following conventions apply for the normative specifications in this Recommended Standard the words shall and must imply binding and verifiable specification the word should implies an optional but desirable specification the word may implies an optional specification the words is are and will imply statements of fact. NOTE These conventions do not imply constraints on diction in text that is clearly informative in nature. CCSDS RECOMMENDED STANDARD FOR SPACE DATA LINK SECURITY encouraged to investigate the possibility of applying the most recent editions of the The SDLS Protocol is data processing method for space missions that need to apply Link Protocols over space link. The Security Protocol is provided only at the Data Link is not applicable for use with the Proximity1 Space Data Link Protocol. The purpose of the Security Protocol is to provide secure standard method with associated data structures for performing security functions on octetaligned user data within Space Data Link Protocol Transfer Frames over space link. The maximum length of input data delimiting the protected data and conveying the necessary cryptographic parameters within the Transfer Frame Data Field allowed by the underlying Space Data Link Protocol. The Security Protocol preserves the quality of service that is provided by the Space Data Link Protocol. The Security Protocol is scalable to operate across any number of Virtual Channels supported by the Space Data Link Protocols. The use and sizes of Security Access Point are managed parameters that remain constant for given mission. To operate the Security Protocol over space link set of procedures is specified in Space Data Link Security ProtocolExtended Procedures . Those extended procedures define key management security association management and SDLS monitoring and control services procedures and associated protocol data units for those three services interfaces with the SDLS and Space Data Link protocols. Two sublayers of the Data Link Layer are defined for CCSDS space link protocols as shown in reference . Each of the four supported Space Data Link Protocols Telemetry Telecommand Advanced Orbiting Systems and Unified Space Data Link Protocol correspond to the Data Link Protocol Sublayer. Operation of the Security Protocol is unaffected by the Synchronization and Channel Coding Sublayer. frame data supplied by higher layers. The detailed structure of the TM TC AOS and USLP The relationship of the Security Protocols functions to the TM Protocol is shown in NOTE TM services not supported by the security protocol are shown greyedout and italicized Almostcomplete transfer frames are frames without the following fields Master Frame Count Operational Control Field Frame Error Control Field The Security Protocol provides all its functions for the data in the Transfer Frame Data Field of TM Transfer Frame. It therefore provides full protection for the service data of the following TM Services the Virtual Channel Packet Service and the Virtual Channel Access The Security Protocol provides authentication for some fields in the Transfer Frame Primary encryption for these fields. The Security Protocol can provide authentication protection for The Security Protocol provides no protection for data of the other TM Services that use auxiliary data fields in TM Transfer Frame the Virtual Channel Operational Control Field the Master Channel Operational Control Field Service. The Security Protocol also provides no protection for the frames supplied to the TM Protocol by external sources on the following services the Virtual Channel Frame Service and the Master Channel Frame Service. The relationship of the Security Protocols functions to the TC Protocol is shown in NOTE TC services not supported by the security protocol are shown greyedout and italicized Almostcomplete transfer frames are frames without the following fields Frame Error Control Field The Security Protocol provides all its functions for the data in the Transfer Frame Data Field of TC Transfer Frame. It therefore provides full protection for the service data of the following TC Services the Multiplexer Access Point Packet Service the MAP Access Service the Virtual Channel Packet Service and the Virtual Channel Access The Security Protocol provides authentication for some fields in the Transfer Frame Primary There are no auxiliary data fields in TC Transfer Frame. The Security Protocol provides no protection for the control frames generated for the Communications Operation Procedure provides no protection for the frames supplied to the TC Protocol by external sources on the following services the VCF Service and the MCF Service. The relationship of the Security Protocols functions to the AOS Protocol is shown in NOTE AOS services not supported by the security protocol are shown greyedout and italicized Almostcomplete transfer frames are frames without the following fields Operational Control Field Frame Error Control Field The Security Protocol provides all its functions for the data in the Transfer Frame Data Field of an AOS Transfer Frame. It therefore provides full protection for the service data of the following AOS Services the VCP Service the Bitstream Service and the VCA Service. The Security Protocol provides authentication for some fields in the Transfer Frame Primary The Security Protocol provides no protection for data of the AOS Services that use auxiliary data fields in an AOS Transfer Frame the VC_OCF Service and the Insert Service. The Security Protocol also provides no protection for the frames supplied to the AOS Protocol by external sources on the following services the VCF Service and the MCF Service. NOTE USLP services not supported by the security protocol are shown greyedout and italicized Almostcomplete transfer frames are frames without the following fields Operational Control Field Frame Error Control Field The Security Protocol provides all its functions for the data in the Transfer Frame Data Field of USLP Transfer Frame. It therefore provides full protection for the service data of the following USLP Services the MAPP Service the MAP Octet Stream Service and the MAPA Service. The Security Protocol provides authentication for some fields in the Transfer Frame Primary The Security Protocol provides no protection for data of the USLP Services that use auxiliary data fields in USLP Transfer Frame the MC_OCF Service and the Insert Service. The Security Protocol also provides no protection for the frames supplied to USLP by external sources on the following services the VCF Service and the MCF Service. The Security Protocol provides no protection for the control frames generated for the COPs The Security Protocol provides security associations for defining the cryptographic communications parameters to be used by both the sending and receiving ends of communications session and for maintaining state information for the duration of the session. Security Association defines simplex stateful cryptographic session for providing authentication data integrity replay protection andor data confidentiality. All Transfer Frames that share the same SA on physical channel constitute Secure Channel. Secure Channel consists of one or more Global Virtual Channels or Global MAP IDs assigned to an SA at the time of its creation. applicable to Transfer Frame. All Transfer Frames having the same SPI on physical that stores all of the managed information required by each of the SAs on physical channel. When an SA is created one of the following cryptographic functions is selected to be applied on specified fields for all Transfer Frames using that SA authenticated encryption. Once an SA is created the authentication andor encryption algorithms specified along with their modes of operation are fixed and cannot be changed for the duration of the SA. initialization vector antireplay sequence number length of any block padding used the Security Trailer carries Message Authentication Code . The detailed structure of the TM TC AOS and USLP Transfer Frames with the Security are fixed for the duration of that SA. Both the sender and the receiver must create an SA associate it with cryptographic key and activate it before the SA may be used to secure Transfer Frames on channel. SAs may be statically preloaded prior to the start of mission. SAs may also be created dynamically as needed even while other existing SAs are active. The mechanism for switching from one active SA to another is an Application Layer function. NOTE Overtheair negotiation of SA parameters is Application The Security Protocol provides for the use of authentication algorithms to ensure the integrity of transmitted data and the authenticity of the data source. The Security Protocol also provides for the use of sequence numbering to detect the unauthorized replay of previously transmitted data. When the Security Protocol is used for authentication MAC is computed over the specified security protocol and the Frame Data Field. An SA providing authentication also manages an authentication bit mask for that SA enabling the sender and receiver to mask out optional Operational Control Field optional Error Control Field and the MAC field itself within the Security Trailer. Transfer Frame fields always included for MAC computation are the Virtual Channel ID NOTE The channel coding synchronization marker prepended to Transfer Frame prior to transmission is always excluded from MAC computation. When the Security Protocol is used for authentication sequence number is also transmitted in the Transfer Frame. As part of an SA providing authentication both the sender and receiver manage the following information sequence number value sequence number window for comparison by the receiver The sender increments its managed sequence number by one with each transmitted frame belonging to that SA. With each valid received frame belonging to that SA the receiver will replace its stored sequence number with the received value on the condition that the received sequence number is higher than the stored sequence number. Additionally if the received Sequence Number differs from the expected value by more than defined positive value called the Sequence Number Window the receiver discards the frame and neither replaces nor increments its stored sequence number. NOTE The interpretation of sequence number rollover is missionspecific. The sequence number window is fixed positive delta value specified in the SA for the receiver to use in comparing the sequence number received to the expected value. received frame whose sequence number falls outside this window is discarded. The size of the selected window accounts for predicted delays and gaps in RF transmission. CCSDS RECOMMENDED STANDARD FOR SPACE DATA LINK SECURITY The Sequence Number can be located in the Sequence Number field of the Security For systems that implement authenticated encryption using simple incrementing counter as an initialization vector octets in length. The Security Protocol provides for the use of encryption algorithms to ensure the confidentiality of transmitted data. When the Security Protocol is used for encryption the data area of the frame vector is often used as an input to the encryption process. Depending upon the cryptographic algorithm and mode used additional fill data may be needed to pad any undersized blocks. NOTE Encryption used without authentication can provide false sense of security depending upon the specific implementation. Selection of security services should be done carefully after considering missionspecific threat and risk analysis. The Security Protocol provides for the use of authentication and encryption as one combined procedure. When the Security Protocol is used for authenticated encryption the frame data supplied by the user is first encrypted as described in 2.3.3 current antireplay sequence number is applied to the Transfer Frame and lastly MAC is computed over the resultant Transfer The services that the Security Protocol provides to the Space Data Link Protocols are defined as functions. The ApplySecurity Function is defined for the sending end of physical channel and the ProcessSecurity Function is defined for the receiving end. The definitions of the functions are independent of specific implementation approaches. The parameters of the functions are specified in an abstract sense and specify the information passed in either direction between the Space Data Link Protocol entity that calls the function and the Security Protocol entity that executes the function. The way in which specific implementation makes this information available is not constrained by this specification. In parameters on the function interface data structure that is in use on the physical The input parameters of the function include the ApplySecurity Payload containing the partially formatted frame and the identifiers of the Virtual Channel and the MAP channel . When the function is called the Security Protocol applies encryption andor authentication to the data supplied in the ApplySecurity Payload. In any given call to the ApplySecurity Function the processing depends on the settings for the Security Association of the applicable Virtual Channel or MAP. When the ApplySecurity Function has completed the processing it returns the resulting data to the caller in the return parameter the ApplySecurity Return. The ApplySecurity Function applies security processing to partially formatted Transfer Frame of the Space Data Link Protocol used on the physical channel. CCSDS RECOMMENDED STANDARD FOR SPACE DATA LINK SECURITY The input parameter provided by the Space Data Link Protocol consists of an ApplySecurity USLP ApplySecurity Payload. The TM ApplySecurity Payload shall consist of the portion of the TM Transfer Frame parameter shall contain the ID of the Global Frame contained in the ApplySecurity Payload. NOTE The GVCID consists of Master Channel ID and Virtual Channel ID. The GMAP Identifier parameter shall contain the ID of the GMAP that is in use on the physical channel. The input parameters include the ProcessSecurity Payload containing the frame and the identifiers of the Virtual Channel and the MAP channel . When the function is called the Security Protocol always applies verification and may apply decryption to the data supplied in the ProcessSecurity Payload. In any given call to the ProcessSecurity Function the processing depends on the settings for the Security Association of the applicable Virtual Channel or MAP. When the ProcessSecurity Function has completed the processing it returns the results to the The ProcessSecurity Function applies security processing to Transfer Frame of the Space Data Link Protocol used on the physical channel. The input parameter provided by the Space Data Link Protocol consists of ProcessSecurity USLP ProcessSecurity Payload. The TM ProcessSecurity Payload shall consist of the portion of the TM Transfer Frame . The length of any Transfer Frame transferred on physical channel is constant and is established by management. The TC ProcessSecurity Payload shall consist of the portion of the TC Transfer Frame . The length of any Transfer Frame transferred on physical channel is constant and is established by management. The USLP ProcessSecurity Payload shall consist of the portion of the USLP Transfer Frame of the Security Trailer if present or the last octet of the Transfer Frame Data Field if the Security Trailer is not present. NOTE The USLP Transfer Frame is the variable or fixedlength protocol data unit of USLP. and of the partial Transfer Frame contained in the ProcessSecurity Payload. NOTE The GVCID consists of Master Channel ID and Virtual Channel ID. the partial TC or USLP Transfer Frame contained in the TC or USLP ProcessSecurity The GMAP_ID consists of GVCID and TC or USLP MAP ID that indicates MAP Channel within the Virtual Channel specified by GVCID. The GMAP_ID is applicable only if the ProcessSecurity Payload is TC or USLP ProcessSecurity Payload and the Virtual Channel specified by the GVCID is using no failures were detected or the ProcessSecurity function has detected failure. NOTE In addition to authentication failures the ProcessSecurity function can detect additional failures such as an invalid Security Association identification in the has performed successful authentication or the SA does not include padding error. The ProcessSecurity Return shall consist of the portion of the Transfer Frame corresponding ending at the last octet of the Transfer Frame Data Field. When the ProcessSecurity function has finished processing the frame that was input in the ProcessSecurity Payload parameter it returns part of the processed frame in the ProcessSecurity Return parameter. If the function has performed decryption then the ProcessSecurity Return contains the decrypted data. If the SA does not include encryption then the ProcessSecurity function does not perform decryption. Also the ProcessSecurity function does not perform decryption The Security Association Management Service establishes the context of an SA for particular Global Virtual Channel andor MAP ID. This Recommended Standard specifies only the service parameters contained in the Security Association data base. Implementation of the services necessary to manage the parameters contained in the SA data base is missionspecific function. Service directives for managing the SA parameters inline are specified in the CCSDS SDLS Extended Procedures Recommended Standard . and applicable to the SA. The GVCID consists of Master Channel ID and Virtual Channel ID. If the TC Space Data Link Protocol is used on the physical channel single Global Virtual Channel is applicable to the SA . If USLP and COP are used on the physical channel single Global Virtual Channel applicable to the SA. NOTE The GMAP_ID consists of GVCID and TC or USLP MAP ID that indicates MAP Channel within the Virtual Channel specified by GVCID. The GMAP_ID is applicable only if the TC Space Data Link Protocol or USLP is used on the NOTE Each SA on physical channel is identified by unique SPI. The SA_service_type parameter shall indicate the cryptographic function specified for the SA one of authentication encryption or authenticated encryption. The SA_length_SN parameter shall indicate the length of the Sequence Number field in the The SA_length_IV parameter shall indicate the length of the Initialization Vector field in the The SA_length_PL parameter shall indicate the length of the Pad Length field in the Security The SA_length_MAC parameter shall indicate the length of the MAC field in the Security parameter is Authentication or Authenticated Encryption. The SA_authentication_algorithm parameter shall indicate the applicable authentication The SA_authentication_key parameter shall indicate the value of provided authentication The SA_authentication_mask parameter shall indicate the value of provided bit mask that is applied against the Transfer Frame in bitwiseAND operation to generate an Authentication Payload. The SA_sequence_number parameter shall indicate the present value of managed anti replay sequence number. The SA_sequence_window parameter shall indicate the amount of deviation the receiving end will accept between the expected antireplay sequence number and the sequence number in the received frame. parameter is Encryption or Authenticated Encryption. The SA_encryption_algorithm parameter shall indicate the applicable encryption algorithm and mode of operation. The SA_encryption_key parameter shall indicate the value of provided encryption key or The SA_initialization_vector parameter shall indicate the present value of managed This Recommended Standard specifies only the service parameters contained in the Security Association data base and does not define specific management services or data structures for implementation. Service directives for managing the SA parameters inline are specified in the CCSDS SDLS Extended Procedures Recommended Standard which may be used to provide key management SA management SDLS monitoring and control services procedures and associated protocol data units for those services and interfaces remain constant throughout mission. authentication encryption or authenticated encryption is applied on that Virtual Channel or MAP. fields positioned contiguously in the following sequence Initialization Vector Sequence Number Pad Length . The receiver will determine the presence and length of optional fields in the Security CCSDS for future use. CCSDS RECOMMENDED STANDARD FOR SPACE DATA LINK SECURITY upon portion of it consisting of an integral number of octets. shall be zero octets in length. selected for an SA shall contain the antireplay sequence number consisting of an integral number of octets. NOTE For systems that implement authenticated encryption using simple incrementing counter as an initialization vector . initialize common SA data base containing all the parameters of the SAs to be used on the be maintained during operation. NOTE Initialization modification and maintenance procedures for those SA data bases are not part of this Security Protocol but are specified in the CCSDS SDLS Extended Procedures Recommended Standard . CCSDS RECOMMENDED STANDARD FOR SPACE DATA LINK SECURITY In order to use an SA to secure Transfer Frames on channel each end associate it with cryptographic key associate it with the Global Virtual Channel or GMAP_IDs with which it is to be It is expected that some missions will choose to define SAs statically and preloadpre activate them prior to the start of the mission. Specifying the successful implementation of cryptographic key management is beyond the scope of this document. Every SA shall specify one or more Global Virtual Channels or GMAP_IDs with which the SA is to be used. The GVCID consists of Master Channel ID and Virtual Channel ID. The GMAP_ID parameter is applicable only if USLP is used on the physical channel or if the TC Space Data Link Protocol is used on the physical channel and Segment At the sending end only one SA at time shall be used for transferring frames over particular Global Virtual Channel or GMAP_ID. SAs shall not be created for use with Virtual Channels carrying Only Idle Data Every SA shall be associated with an SPI. The SPI is transmitted value that uniquely identifies the SA applicable to Transfer Frame. All Transfer Frames having the same SPI on Master Channel share single SA. Every SA shall specify one and only one of the following cryptographic functions to perform authenticated encryption. NOTE It is possible to create clear mode SA using one of the defined service types by specifying the algorithm as noop function . Such an SA might be used for example during development testing of other aspects of data link processing before cryptographic capabilities are field lengths are kept constant across all supported configurations. For security reasons the use of such an SA is not recommended in normal operation. length of MAC field in Security Trailer. Every SA providing authentication shall specify the following authentication algorithm and mode of operation Every SA providing authentication shall initialize its authentication bit mask as follows the mask to be applied shall be greater or equal in length to the data extending from Frame Data Field immediately preceding the MAC field in the Security Trailer NOTE For variablelength TC or USLP Transfer Frames accounting for the largest expected frame data field will result in mask suitable for all Transfer Frames. the mask bits corresponding to the Virtual Channel ID field of the Transfer Frame the mask bits corresponding to the MAP ID field of the Transfer Frame the mask bits corresponding to the Master Channel Frame Count field of excluded from the authenticated data field shall contain all zeros the mask bits corresponding to the Insert Zone shall contain all zeros corresponding to the Initialization Vector field shall contain all ones the mask bits corresponding to the Frame Data Field shall contain all ones all zeros unless otherwise specified according to mission requirements. Missions desiring to authenticate other fields above. Possible security concerns affecting the selection of an authentication mask Every SA providing encryption shall specify the following NOTE The chosen algorithm and mode also imply other attributes such as the required block size and the corresponding need to pad undersized data managed initialization vector. Every SA providing authenticated encryption shall specify everything required in both the Security Association as follows the encryption operations specified in 4.2.3.3 shall be applied to the partial frame the ApplySecurity Return parameter. CCSDS RECOMMENDED STANDARD FOR SPACE DATA LINK SECURITY the authentication operations specified in 4.2.3.4 shall be applied to the partial frame Trailer shall be returned in the ApplySecurity Return parameter. If the cryptographic algorithm requires both plaintext and Additional Authenticated Data as separate inputs then 1 the plaintext shall be the Transfer Frame Data Field and 2 the AAD shall be the portion from the first octet of the Authentication Payload to the octet immediately preceding the Transfer Frame Data Field NOTE This definitional distinction is common to class of cryptographic algorithms known as Authenticated Encryption with Associated Data algorithms. the encryption operations specified in 4.2.3.3 shall be applied to the partial frame contained in the ApplySecurity Payload parameter the authentication operations specified in 4.2.3.4 shall be applied to the partial frame Trailer shall be returned in the ApplySecurity Return parameter. If encryption is selected for an SA then for each transmitted frame belonging to that SA the encrypt the Transfer Frame Data Field if the algorithm and mode selected for the SA require the use of fill padding place the If authentication is selected for an SA then for each transmitted frame belonging to that SA the sender shall increment the SAs managed sequence number by one place the managed sequence number in the Sequence Number field of the Security NOTE The interpretation of sequence number rollover is mission discussed in reference . apply the SAs authentication bit mask in bitwiseAND operation against the partial frame thus resulting in the Authentication Payload NOTE The partial frame supplied in the ApplySecurity Payload consists of the Transfer Frame Data Field. The result is used for the masking operation. compute MAC over the Authentication Payload truncate the leastsignificant bits of the computed MAC such that the result is of identical length to the MAC field in the Security Trailer place the computed MAC in the Security Trailer if the algorithm and mode selected for the SA require the use of fill padding place the entity calls the ProcessSecurity function for frame. abstract sense and are not intended to imply any particular implementation approach for the handling of frames or for the transfer of frame data between the Space Data Link Protocol entity and the Security Protocol entity. Association that applies to the frame as specified in 4.2.4.3. ProcessSecurity function shall depend on the Security Type of the Security Association as the authentication operations specified in 4.2.4.4 shall be applied to the partial frame contained in the ProcessSecurity Payload parameter ProcessSecurity function shall exit if an authentication failure is detected the Transfer Frame Data Field in the ProcessSecurity Return parameter and success function shall exit if no authentication failure is detected. If the cryptographic algorithm requires both plaintext and AAD as separate inputs 1 the plaintext shall be the Transfer Frame Data Field and 2 the AAD shall be the portion from the first octet of the Authentication Payload to the octet immediately preceding the Transfer Frame Data Field NOTE This definitional distinction is common to class of cryptographic algorithms known as AEAD algorithms. the authentication operations specified in 4.2.4.4 shall be applied to the partial frame contained in the ProcessSecurity Payload parameter ProcessSecurity function shall exit if an authentication failure was detected the encryption operations specified in 4.2.4.5 shall be applied to the partial frame contained in the ProcessSecurity Payload parameter the decrypted Transfer Frame Data Field in the ProcessSecurity Return parameter and ProcessSecurity function shall exit. the encryption operations specified in 4.2.4.5 shall be applied to the partial frame contained in the ProcessSecurity Payload parameter the decrypted Transfer Frame Data Field in the ProcessSecurity Return parameter and For all frames received over Global Virtual Channel the receiver shall associated with that Global Virtual Channel andor GMAP_ID SA verification and discard those frames. NOTE Discarded frames can be archived for forensic investigation if desired. If authentication is selected for an SA then for each received frame belonging to that SA the apply the SAs authentication bit mask in bitwiseAND operation against the portion of the partial Transfer Frame in the ProcessSecurity Payload parameter of the Transfer Frame Data Field immediately preceding the MAC field in the Security Trailer thus resulting in the Authentication Payload compute MAC over the Authentication Payload truncate the leastsignificant bits of the computed MAC such that the result is of identical length to the MAC field in the Security Trailer verify that the computed MAC matches the MAC received in the Security Trailer NOTE Discarded frames can be archived for forensic investigation if desired. extract the received sequence number from either the Sequence Number field or the compare the received sequence number to the managed sequence number number is lower or equal to the managed sequence number and discard those frames number is larger than the managed sequence number by value greater than the window defined for that SA and discard those frames NOTE Discarded frames can be archived for forensic investigation if desired. CCSDS RECOMMENDED STANDARD FOR SPACE DATA LINK SECURITY only upon receipt of frames that pass the verification operations ai above replace the managed sequence number with the received sequence number NOTE The interpretation of sequence number rollover is mission discussed in reference . if specified for that SA extract the count of fill bytes used from the Pad Field to be returned. If encryption is selected for an SA then for each received frame belonging to that SA the decrypt the Transfer Frame Data Field if specified for that SA extract the count of fill bytes used from the Pad The following restrictions apply to use of the Security Protocol with TM the Packet and VCA Services may be used on Global Virtual Channel with the Authentication Encryption or AuthenticatedEncryption Service and are protected by each of these services the VC_FSH Service may be used on Global Virtual Channel with the Authentication Encryption or AuthenticatedEncryption Service and may be protected by authentication but is not protected by encryption the VC_OCF VCF MC_FSH MC_OCF and MCF Services are not protected by the Authentication Encryption or AuthenticatedEncryption Services but may be used on the same Master Channel. NOTE The format of the TM Transfer Frame is defined in reference . The format of MAC is not computed over ASM OCF or ECF. The following restrictions apply to use of the Security Protocol with TC the MAPP MAPA VCP and VCA Services may be used on Global Virtual Channel with the Authentication Encryption or AuthenticatedEncryption Service and are protected by each of these services are not protected by the Authentication Encryption or AuthenticatedEncryption Services but may be used on the same Master Channel each SA shall be associated with one VC and one VC only. NOTE The format of the TC Transfer Frame is defined in reference . The format of MAC is not computed over CLTU Start Sequence or ECF. The following restrictions apply to use of the Security Protocol with AOS the Packet Bitstream and VCA Services may be used on Global Virtual Channel with the Authentication Encryption or AuthenticatedEncryption Services and are protected by each of these services the VC_OCF Service VCF MCF and Insert Services are not protected by the Authentication Encryption or AuthenticatedEncryption Services but may be used on the same Master Channel. NOTE The format of the AOS Transfer Frame is defined in reference . The format of MAC is not computed over ASM OCF or ECF. The following restrictions apply to use of the Security Protocol with USLP the MAP Packet MAP Octet Stream and MAP Access Services may be used on GMAP with the Authentication Encryption or AuthenticatedEncryption Services VCF MCF and Insert Services are not protected by the Authentication Encryption or AuthenticatedEncryption Services but may be used on the same Master Channel each SA shall be associated with one VC and one VC only if COPs are used OCF or ECF. Encryption is done on Transfer Frame Data Filed only. USLP ApplySecurity Payload may be protected using the service functions of the Security Protocol. In order to conserve bandwidth on the space link certain parameters associated with the Security Protocol are handled by management rather than by inline communications protocol. The managed parameters are generally those which tend to be static for long periods of time and whose change signifies major reconfiguration of the service provider associated with particular mission. These managed parameters are intended to be included in any serviceprovider system that manages SAs. set of procedures to manage Security Associations and Keys is specified in reference . These parameters are defined in an abstract sense and are not intended to imply any particular implementation of management system. The majority of managed parameters are the parameters of the SA data base managed by both the sending and receiving ends which must match one another in order to operate correctly. and used on the physical channel shall be treated as also applicable to the Security Security Association Data Base Parameters Held Static Security Association Data Base Parameters Held Static Security Association Data Base Parameters That Vary Dynamically NOTE This table has been built using the authentication and encryption algorithms the AuthenticationEncryption Algorithms and for Key Length refer to those in Moreover as the protocol defined in this book is quite independent from the applied algorithms users would still be able if needed to apply this protocol with other algorithms defined via bilateral agreement among agencies. This is shown by the AgencySpecific value. An implementer of the Security Protocol shall verify conformance with this Recommended Standard by completing Protocol Implementation Conformance Statement based on CCSDSdefined PICS proforma for the protocol. NOTE compliant PICS proforma is provided in annex of this document. To evaluate conformance of particular implementation it is necessary to have statement of which capabilities and options have been implemented for given protocol specification. Such statement is called Protocol Implementation Conformance Statement . This annex provides the PICS proforma for the Space Data Link Security Protocol in compliance with the relevant requirements and in accordance with the relevant guidance given in ISOIEC 96467. If it is claimed to conform to this Recommended Standard the actual PICS proforma to be filled in by supplier shall be technically equivalent to the text of the PICS proforma in this annex and shall preserve the numberingnaming and ordering of the PICS proforma items. PICS that conforms to this Recommended Standard shall be conforming PICS proforma completed in accordance with the instructions for completion given in A2. Users of this Recommended Standard may freely reproduce this PICS proforma so that it can be used for its intended purpose and may further publish the completed PICS. In order to reduce the size of tables in the PICS proforma notations have been introduced and Support. The definition of each of these follows. Optional support is permitted for conformance to the standard. If implemented it must conform to the specifications and restrictions contained in the standard. These restrictions may affect the optionality of other items. The item is optional but support of at least one of the options labeled with the same number is mandatory. The definitions for the qualification statements used in this annex are written under the tables in which they appear. The item is conditional . The definitions for the conditional statements used in this annex are The item is not applicable. The Support column shall be completed by the supplier or implementer to indicate the level of implementation of each feature. The proforma has been designed such that the only entries The item is not applicable. Each line within the PICS proforma that requires implementation detail to be entered is numbered at the lefthand edge of the line. This numbering is included as means of uniquely identifying all possible implementation details within the PICS proforma. The need for such unique referencing has been identified by the testing bodies. The means of referencing individual responses should be to specify the following sequence solidus character the reference number of the row in which the response appears if and only if more than one response occurs in the row identified by the reference number then each possible entry is implicitly labeled etc. from left to right and this letter is appended to the sequence. An example of the use of this notation would be A41 which refers to the SDLS implementations support for the TM Space Data Link Protocol. The implementer shall complete all entries in the column marked Support. In certain clauses of the PICS proforma further guidance for completion may be necessary. Such guidance shall supplement the guidance given in this clause and shall have scope restricted to the clause in which it appears. In addition other specifically identified information shall be provided by the implementer as requested. No changes shall be made to the proforma except the completion as required. Recognizing that the level of detail required may in some instances exceed the space available for responses number of responses specifically allow for the addition of appendices to the PICS. The SDLS Protocol is the only base standard referenced in this PICS proforma. In the tables below numbers in the Reference column refer to NOTE The System Conformance Statement is identified in ISOIEC 96467 . It contains declaration of the layers of the Reference Model Are all mandatory features implemented NOTE If No answer is given to this question then the implementation does not conform to the SDLS standard. Nonsupported mandatory capabilities are to be identified in the PICS with an explanation of why the implementation is nonconforming. Support for at least one of is Support for at least one of is Communications security attempts to ensure the confidentiality integrity andor authenticity of transmitted data as required depending on the threat the mission security policy and the desire of the mission planners. It is possible for single data unit to require all three of these security attributes to ensure that the transmitted data is not disclosed not altered and Security concerns specific to the Security Protocol design are addressed in more detail in It may be necessary to apply security services at multiple layers within the protocol stack to account for distributed processing and crosssupport to account for different classes of data or end users or to account for protection of data during unprotected portions of the complete endtoend transmission . The specification of security services at other layers is outside the scope of this document. where they can be implemented. Reference contains more information regarding the The Security Protocol provides no protection against denialofservice attacks against the communications channel such as radiofrequency jamming. The Security Protocol provides no protection against traffic flow analysis. When encryption is used careful choice of algorithm and mode will provide protection to the Transfer Frame Data Field but an attacker can use the Spacecraft ID Virtual Channel ID TC MAP ID OCF The Security Protocol provides no cryptographic key management protocol. Specifying the successful implementation of cryptographic key management or operational key change criteria The Security Protocol provides no protection to TC or USLP COP control commands nor to The Security Protocol foresees the existence of clear mode for certain VCs. If clear mode is implemented the conditions under which and by which it is activated should be carefully analyzed as those might introduce major security vulnerabilities. If encryption is implemented without authentication the Security Protocol provides no protection against data substitution attacks. In addition it may be possible for an attacker to reverseengineer the encryption key and compromise data confidentiality if portions of the original plaintext are predictable. Specific potential threats and attack scenarios are addressed in more detail in reference . Without authentication unauthorized commands or software might be uploaded to spacecraft or data received from source masquerading as the spacecraft. Without data integrity corrupted commands or software might be uploaded to spacecraft potentially resulting in the loss of the mission harm to people and property or loss of life . Without data integrity corrupted telemetry might be retrieved from spacecraft which could result in an incorrect course of action being taken. If confidentiality is not implemented data flowing to or from spacecraft might be visible to unauthorized entities resulting in disclosure of sensitive or private information. This Recommended Standard defines no new information registries. The recommendations of this document do not require any action from SANA. implementing the provisions of this Recommended Standard. CCSDS RECOMMENDED STANDARD FOR SPACE DATA LINK SECURITY Information TechnologyOpen Systems InterconnectionBasic Reference Model The Basic Model. 2nd ed. International Standard ISOIEC 749811994. Geneva Information Processing SystemsOpen Systems InterconnectionBasic Reference Washington . CCSDS June 2018. Data System Standards CCSDS 130.0G3. Washington . Information TechnologyOpen Systems InterconnectionConformance Testing Space Data System Standards CCSDS 211.0B6. Washington . The baseline implementation to be used for interoperability testing and operation is authenticated encryption using the Advanced Encryption Standard algorithm in the GaloisCounter Mode as defined in reference . Additionally the key is 256 bits in total length the input initialization vector is 96 bits in total length where all 96 bits are the output MAC is 128 bits in total length. GCM normally uses simple incrementing counter as its initialization vector. separate antireplay Sequence Number is unnecessary therefore the Sequence GCM does not require padding therefore the length of the Pad Length field shown in The baseline implementation uses Security Trailer of 16 octets in length. The format of the The baseline implementation uses an authentication mask in which all of the mask bits The baseline implementation to be used for interoperability testing and operation is authentication using the AES algorithm used in the Cipherbased Message Authentication Code mode as defined in reference . Additionally the key is 256 bits in total length the antireplay sequence number is 32 bits in total length where all 32 bits are the output MAC is 128 bits in total length. NOTE The CMAC mode of operation performs no encryption and does not require an initialization vector nor padding therefore the length of the Initialization Vector The baseline implementation uses Security Trailer of 16 octets in length. The format of the The baseline implementation uses an authentication mask in which all of the mask bits The baseline implementation to be used for interoperability testing and operation is authenticated encryption using the AES algorithm used in the GCM as defined in the key is 256 bits in total length the input initialization vector is 96 bits in total length where all 96 bits are the output MAC is 128 bits in total length. GCM normally uses simple incrementing counter as its initialization vector. separate antireplay Sequence Number is unnecessary therefore the Sequence GCM does not require padding therefore the length of the Pad Length field shown in The baseline implementation uses Security Trailer of 16 octets in length. The format of the The baseline implementation uses an authentication mask in which all of the mask bits The baseline implementation to be used for interoperability testing and operation is authenticated encryption using the AES algorithm used in the GCM as defined in the key is 256 bits in total length the input initialization vector is 96 bits in total length where all 96 bits are the output MAC is 128 bits in total length. CCSDS RECOMMENDED STANDARD FOR SPACE DATA LINK SECURITY GCM normally uses simple incrementing counter as its initialization vector. separate antireplay Sequence Number is unnecessary therefore the Sequence GCM does not require padding therefore the length of the Pad Length field shown in The baseline implementation uses Security Trailer of 16 octets in length. The format of the The baseline implementation uses an authentication mask in which all of the mask bits

Boston Columbus Indianapolis New York San Francisco Upper Saddle River Amsterdam Cape Town Dubai London Madrid Milan Paris Montreal Toronto Delhi Mexico City Sao Paulo Sydney Hong Kong Seoul Singapore Tapei Tokyo Many of the designations by manufacturers and sellers to distinguish their products are claimed as trademarks. Where those designations appear in this book and the publisher was aware of trademark claim the designations have been printed in initial caps or all caps. prohibited reproduction storage in retrieval system or transmission in any form or by any means electronic mechanical photocopying recording or likewise. To obtain permission to use material from this work please submit written request to Pearson Education Inc. Permissions Department 501 Boylston Street Suite 900 Boston Massachusetts 02116. Computer networks Andrew . Tanenbaum David . Wetherall. 5th ed. 1. Computer networks. Wetherall . II. Title. To Suzanne Barbara Daniel Aron Marvin Matilde This book is now in its ﬁfth edition. Each edition has corresponded to dif ferent phase in the way computer networks were used. When the ﬁrst edition ap peared in 1980 networks were an academic curiosity. When the second edition appeared in 1988 networks were used by universities and large businesses. When the third edition appeared in 1996 computer networks especially the Internet had become daily reality for millions of people. By the fourth edition in 2003 wire less networks and mobile computers had become commonplace for accessing the Web and the Internet. Now in the ﬁfth edition networks are about content dis tribution and mobile Among the many changes in this book the most important one is the addition of Prof. David . Wetherall as coauthor. David brings rich background in net years ago. He has worked with the Internet and wireless networks ever since and is professor at the University of Washington where he has been teaching and doing research on computer networks and related topics for the past decade. Of course the book also has many changes to keep up with the everchanging world of computer networks. Among these are revised and new material on Realtime media Delaytolerant networks networks 802.11 and RFID and sensor networks are discussed as examples of computer networks. Material on the original Ethernetwith its vampire taps has been removed along with the material on ATM. modulation and 3G net works . New technologies are discussed including Fiber to the Home and powerline networking. brief description of the modern codes that are important in practice . The examples of protocols now use Packet over SONET and ADSL. Sadly the material on protocol veriﬁcation has been removed as it is little used. accordingly including gigabit Ethernet 802.11 802.16 Bluetooth and RFID. quality of service and internetworking. The sec tions on BGP OSPF and CIDR have been expanded as has the treatment of multicast routing. Anycast routing is now included. moved. New material describes delaytolerant networking and congestion control gestion control. The material removed described connectionoriented network lay ers something rarely seen any more. ial on DNS and email is similar to that in the fourth edition in the past few years there have been many developments in the use of the Web streaming media and CDNs and peertopeer networks. graphy for conﬁdentiality and authenticity. Material on the techniques used in 802.11 security and Kerberos V5 added. these are to papers and books written in 2000 or later and the rest are citations to Computer books are full of acronyms. This one is no exception. By the time you are ﬁnished reading this one the following should ring bell ADSL AES TSAP UDP UMTS URL VLAN VSAT WAN WDM and XML. But dont worry. Each will appear in boldface type and be carefully deﬁned before it is used. As fun test see how many you can identify before reading the book write To help instructors use this book as text for courses ranging in length from They provide material on network technologies that is useful but can be omitted from short course without loss of continuity. Of course students should be The following protected instructors resource materials are available on the publishers Web site at For username and password please contact your local Pearson representative. Resources for students are available through the openaccess Companion Web Web resources links to tutorials organizations FAQs and more Many people helped us during the course of the ﬁfth edition. We would espe cially like to thank Emmanuel Agu Yoris Au Nikhil Bhargava Michael Buettner John Day Kevin Fall Ronald Fulle Ben Greenstein Daniel Halperin Bob Kinicki Tadayoshi Kohno Sarvish Kulkarni Hank Levy Ratul Mahajan Craig Partridge Michael Piatek Joshua Smith Neil Spring David Teneyuca Tammy VanDe grift and Bo Yuan for providing ideas and feedback. Melody Kadenko and Julie Svendsen provided administrative support to David. Shivakant Mishra and Paul Nagin prepared the PowerPoint slides. Stephen Turner artfully revised the Web resources and the simulators that accompany the text. Our copyeditor Rachel Head is an odd hybrid she has the eye of an eagle and the memory of an elephant. After reading all her corrections both of us won dered how we ever made it past third grade. this 19 times now and still has endless patience and love. Barbara and Marvin now know the difference between good textbooks and bad ones and are always an inspiration to produce good ones. Daniel and Matilde are welcome additions to our family. Aron is unlikely to read this book soon but he likes the nice pictures aged to keep smile on my face. Thank you . Each of the past three centuries was dominated by single new technology. The 18th century was the era of the great mechanical systems accompanying the Industrial Revolution. The 19th century was the age of the steam engine. During the 20th century the key technology was information gathering processing and distribution. Among other developments we saw the installation of worldwide telephone networks the invention of radio and television the birth and unpre cedented growth of the computer industry the launching of communication satel lites and of course the Internet. As result of rapid technological progress these areas are rapidly converging in the 21st century and the differences between collecting transporting storing and processing information are quickly disappearing. Organizations with hun dreds of offices spread over wide geographical area routinely expect to be able button. As our ability to gather process and distribute information grows the de mand for ever more sophisticated information processing grows even faster. Although the computer industry is still young compared to other industries computers have made spectacular pro gress in short time. During the first two decades of their existence computer systems were highly centralized usually within single large room. Not infre quently this room had glass walls through which visitors could gawk at the great electronic wonder inside. mediumsized company or university might have had one or two computers while very large institutions had at most few dozen. The idea that within forty years vastly more powerful computers smaller than postage stamps would be mass produced by the billions was pure science fiction. The merging of computers and communications has had profound influence on the way computer systems are organized. The oncedominant concept of the computer center as room with large computer to which users bring their work for processing is now totally obsolete . The old model of single com puter serving all of the organizations computational needs has been replaced by one in which large number of separate but interconnected computers do the job. These systems are called computer networks. The design and organization of these networks are the subjects of this book. Throughout the book we will use the term computer network to mean col lection of autonomous computers interconnected by single technology. Two computers are said to be interconnected if they are able to exchange information. The connection need not be via copper wire fiber optics microwaves infrared and communication satellites can also be used. Networks come in many sizes shapes and forms as we will see later. They are usually connected together to make larger networks with the Internet being the most wellknown example of network of networks. There is considerable confusion in the literature between computer network and distributed system. The key distinction is that in distributed system collection of independent computers appears to its users as single coherent sys tem. Usually it has single model or paradigm that it presents to the users. Of ten layer of software on top of the operating system called middleware is responsible for implementing this model. wellknown example of distributed system is the World Wide Web. It runs on top of the Internet and presents In computer network this coherence model and software are absent. Users are exposed to the actual machines without any attempt by the system to make the machines look and act in coherent way. If the machines have different hard ware and different operating systems that is fully visible to the users. If user wants to run program on remote machine he has to log onto that machine and In effect distributed system is software system built on top of network. The software gives it high degree of cohesiveness and transparency. Thus the distinction between network and distributed system lies with the software rather than with the hardware. Nevertheless there is considerable overlap between the two subjects. For ex ample both distributed systems and computer networks need to move files around. The difference lies in who invokes the movement the system or the user. He should be read as he or she throughout this book. Although this book primarily focuses on networks many of the topics are also im portant in distributed systems. For more information about distributed systems see Tanenbaum and Van Steen . some time to pointing out why people are interested in computer networks and what they can be used for. After all if nobody were interested in computer net works few of them would be built. We will start with traditional uses at com panies then move on to home networking and recent developments regarding Most companies have substantial number of computers. For example company may have computer for each worker and use them to design products write brochures and do the payroll. Initially some of these computers may have worked in isolation from the others but at some point management may have decided to connect them to be able to distribute information throughout the com goal is to make all programs equipment and especially data available to anyone An obvious and widespread example is having group of office workers share common printer. None of the individuals really needs private printer and highvolume networked printer is often cheaper faster and easier to maintain than large collection of individual printers. However probably even more important than sharing physical resources such as printers and tape backup systems is sharing information. Companies small and large are vitally dependent on computerized information. Most companies have customer records product information inventories financial statements tax information and much more online. If all of its computers suddenly went down bank could not last more than five minutes. modern manufacturing plant with computercontrolled assembly line would not last even 5 seconds. Even small travel agency or threeperson law firm is now highly dependent on computer net works for allowing employees to access relevant information and documents For smaller companies all the computers are likely to be in single office or perhaps single building but for larger ones the computers and employees may be scattered over dozens of offices and plants in many countries. Nevertheless sales person in New York might sometimes need access to product inventory database in Singapore. Networks called VPNs may be used to join the individual networks at different sites into one extended net work. In other words the mere fact that user happens to be 15000 km away from his data should not prevent him from using the data as though they were local. This goal may be summarized by saying that it is an attempt to end the In the simplest of terms one can imagine companys information system as consisting of one or more databases with company information and some number of employees who need to access them remotely. In this model the data are stor ed on powerful computers called servers. Often these are centrally housed and maintained by system administrator. In contrast the employees have simpler machines called clients on their desks with which they access remote data for example to include in spreadsheets they are constructing. The client and we have shown the network as simple oval without any detail. We will use this form when we mean network in the most abstract sense. When more detail is required it will be provided. This whole arrangement is called the clientserver model. It is widely used and forms the basis of much network usage. The most popular realization is that model is applicable when the client and server are both in the same building but also when they are far apart. For example is employed with the remote Web server being the server and the users personal computer being the client. Under most conditions one server can handle large number of clients simultaneously. If we look at the clientserver model in detail we see that two processes are involved one on the client machine and one on the server machine. Communication takes the form of the client process sending message over the network to the server process. The client process then waits for reply message. When the server process gets the request it performs the requested work or looks up the requested data and sends back reply. These messages are second goal of setting up computer network has to do with people rather than information or even computers. computer network can provide powerful communication medium among employees. Virtually every company that has two or more computers now has email which employees gener ally use for great deal of daily communication. In fact common gripe around the water cooler is how much email everyone has to deal with much of it quite meaningless because bosses have discovered that they can send the same message to all their subordinates at the push of button. Telephone calls between employees may be carried by the computer network instead of by the phone company. This technology is called IP telephony or Voice over IP when Internet technology is used. The microphone and speaker at each end may belong to VoIPenabled phone or the employees com puter. Companies find this wonderful way to save on their telephone bills. Other richer forms of communication are made possible by computer net and hear each other as they hold meeting. This technique is powerful tool for eliminating the cost and time previously devoted to travel. Desktop sharing lets remote workers see and interact with graphical computer screen. This makes it easy for two or more people who work far apart to read and write shared black document the others can see the change immediately instead of waiting several days for letter. Such speedup makes cooperation among farflung groups of people easy where it previously had been impossible. More ambitious forms of remote coordination such as telemedicine are only now starting to be used but may become much more important. It is some times said that communication and transportation are having race and which ever wins will make the other obsolete. third goal for many companies is doing business electronically especially with customers and suppliers. This new model is called ecommerce and it has grown rapidly in recent years. Airlines bookstores and other retailers have discovered that many customers like the convenience of shop ping from home. Consequently many companies provide catalogs of their goods and services online and take orders online. Manufacturers of automobiles air craft and computers among others buy subsystems from variety of suppliers and then assemble the parts. Using computer networks manufacturers can place orders electronically as needed. This reduces the need for large inventories and enhances efficiency. In 1977 Ken Olsen was president of the Digital Equipment Corporation then the number two computer vendor in the world . When asked why Dig ital was not going after the personal computer market in big way he said There is no reason for any individual to have computer in his home. History showed otherwise and Digital no longer exists. People initially bought computers for word processing and games. Recently the biggest reason to buy home com puter was probably for Internet access. Now many consumer electronic devices such as settop boxes game consoles and clock radios come with embedded computers and computer networks especially wireless networks and home net works are broadly used for entertainment including listening to looking at and creating music photos and videos. Internet access provides home users with connectivity to remote computers. As with companies home users can access information communicate with other people and buy products and services with ecommerce. The main benefit now comes from connecting outside of the home. Bob Metcalfe the inventor of Ether net hypothesized that the value of network is proportional to the square of the number of users because this is roughly the number of different connections that may be made . This hypothesis is known as Metcalfes law. It helps to explain how the tremendous popularity of the Internet comes from its Access to remote information comes in many forms. It can be surfing the World Wide Web for information or just for fun. Information available includes the arts business cooking government health history hobbies recreation sci ence sports travel and many others. Fun comes in too many ways to mention plus some ways that are better left unmentioned. Many newspapers have gone online and can be personalized. For example it is sometimes possible to tell newspaper that you want everything about corrupt politicians big fires scandals involving celebrities and epidemics but no foot ball thank you. Sometimes it is possible to have the selected articles downloaded to your computer while you sleep. As this trend continues it will cause massive unemployment among 12yearold paperboys but newspapers like it because dis tribution has always been the weakest link in the whole production chain. Of money in this new world something not entirely obvious since Internet users expect everything to be free. The next step beyond newspapers is the online digital library. Many professional organizations such as the ACM . In this form individu als who form loose group can communicate with others in the group as shown people there is no fixed division into clients and servers. Many peertopeer systems such BitTorrent do not have any central database of content. Instead each user maintains his own database locally and provides list of other nearby people who are members of the system. new other members to inspect for more content and more names. This lookup process can be repeated indefinitely to build up large local database of what is out there. It is an activity that would get tedious for people but computers excel at it. Peertopeer communication is often used to share music and videos. It really hit the big time around 2000 with music sharing service called Napster that was of recorded history . Legal applica tions for peertopeer communication also exist. These include fans sharing pub lic domain music families sharing photos and movies and users downloading public software packages. In fact one of the most popular Internet applications of all email is inherently peertopeer. This form of communication is likely to grow considerably in the future. All of the above applications involve interactions between person and re mote database full of information. The second broad category of network use is persontoperson communication basically the 21st centurys answer to the 19th centurys telephone. Email is already used on daily basis by millions of people all over the world and its use is growing rapidly. It already routinely contains audio and video as well as text and pictures. Smell may take while. Any teenager worth his or her salt is addicted to instant messaging. This facility derived from the UNIX talk program in use since around 1970 allows two people to type messages at each other in real time. There are multiperson mes saging services too such as the Twitter service that lets people send short text messages called tweets to their circle of friends or other willing audiences. The Internet can be used by applications to carry audio and video . Besides being cheap way to call to distant friends these applications can provide rich experiences such as telelearning meaning attending 8 . classes without the inconvenience of having to get out of bed first. In the long run the use of networks to enhance humantohuman communication may prove more important than any of the others. It may become hugely important to people who are geographically challenged giving them the same access to services as people living in the middle of big city. Between persontoperson communications and accessing information are social network applications. Here the flow of information is driven by the rela tionships that people declare between each other. One of the most popular social Other social networking applications can make introductions via friends of friends send news messages to friends such as Twitter above and much more. Even more loosely groups of people can work together to create content. wiki for example is collaborative Web site that the members of community edit. The most famous wiki is the Wikipedia an encyclopedia anyone can edit but there are thousands of other wikis. Our third category is electronic commerce in the broadest sense of the term. Home shopping is already popular and enables users to inspect the online catalogs of thousands of companies. Some of these catalogs are interactive showing pro ducts from different viewpoints and in configurations that can be personalized. it online technical support may be consulted. Another area in which ecommerce is widely used is access to financial insti tutions. Many people already pay their bills manage their bank accounts and handle their investments electronically. This trend will surely continue as net works become more secure. One area that virtually nobody foresaw is electronic flea markets . Online auctions of secondhand goods have become massive industry. Unlike traditional ecommerce which follows the clientserver model online auctions are peertopeer in the sense that consumers can act as both buyers and sellers. Some of these forms of ecommerce have acquired cute little tags based on the fact that to and 2 are pronounced the same. The most popular ones are Our fourth category is entertainment. This has made huge strides in the home in recent years with the distribution of music radio and television programs and movies over the Internet beginning to rival that of traditional mechanisms. Users can find buy and download MP3 songs and DVDquality movies and add them to their personal collection. TV shows now reach many homes via IPTV systems that are based on IP technology instead of cable TV or radio transmissions. Media streaming applications let users tune into Internet radio sta tions or watch recent episodes of their favorite TV shows. Naturally all of this content can be moved around your house between different devices displays and speakers usually with wireless network. Soon it may be possible to search for any movie or television program ever made in any country and have it displayed on your screen instantly. New films may become interactive where the user is occasionally prompted for the story direction with alternative scenarios provided for all cases. Live television may also become interactive with the audience participating in quiz shows choosing among contestants and so Another form of entertainment is game playing. Already we have multiperson realtime simulation games like hideandseek in virtual dungeon and flight simulators with the players on one team trying to shoot down the players on the opposing team. Virtual worlds provide persistent setting in which thousands of users can experience shared reality with threedimensional graphics. Our last category is ubiquitous computing in which computing is embedded into everyday life as in the vision of Mark Weiser . Many homes are al ready wired with security systems that include door and window sensors and there are many more sensors that can be folded in to smart home monitor such usage over the network. This would save money as there would be no need to send out meter readers. And your smoke detectors could call the fire department instead of making big noise . As the cost of sensing and communication drops more and more measurement and re porting will be done with networks. Increasingly consumer electronic devices are networked. For example some highend cameras already have wireless network capability and use it to send photos to nearby display for viewing. Professional sports photographers can also send their photos to their editors in realtime first wirelessly to an access point then over the Internet. Devices such as televisions that plug into the wall can use powerline networks to send information throughout the house over the wires that carry electricity. It may not be very surprising to have these objects on the network but objects that we do not think of as computers may sense and com municate information too. For example your shower may record water usage monitoring application when you are done to help save on your water bill. technology called RFID will push this idea even further in the future. RFID tags are passive chips the size of stamps and they can already be affixed to books passports pets credit cards and other items in the home and out. This lets RFID readers locate and communicate with the items over distance of up to several meters depending on the kind of RFID. Originally RFID was commercialized to replace barcodes. It has not succeeded yet because barcodes are free and RFID tags cost few cents. Of course RFID tags offer much more and their price is rapidly declining. They may turn the real world into the Internet of things . Mobile computers such as laptop and handheld computers are one of the fastestgrowing segments of the computer industry. Their sales have already overtaken those of desktop computers. Why would anyone want one People on the go often want to use their mobile devices to read and send email tweet watch movies download music play games or simply to surf the Web for information. They want to do all of the things they do at home and in the office. Naturally they want to do them from anywhere on land sea or in the air. Connectivity to the Internet enables many of these mobile uses. Since having wired connection is impossible in cars boats and airplanes there is lot of interest in wireless networks. Cellular networks operated by the telephone com panies are one familiar kind of wireless network that blankets us with coverage for mobile phones. Wireless hotspots based on the 802.11 standard are another kind of wireless network for mobile computers. They have sprung up everywhere that people go resulting in patchwork of coverage at cafes hotels airports schools trains and planes. Anyone with laptop computer and wireless modem can just turn on their computer on and be connected to the Internet through the hotspot as though the computer were plugged into wired network. Wireless networks are of great value to fleets of trucks taxis delivery vehi cles and repairpersons for keeping in contact with their home base. For example in many cities taxi drivers are independent businessmen rather than being em ployees of taxi company. In some of these cities the taxis have display the driver can see. When customer calls up central dispatcher types in the pickup and destination points. This information is displayed on the drivers displays and beep sounds. The first driver to hit button on the display gets the call. Wireless networks are also important to the military. If you have to be able to fight war anywhere on Earth at short notice counting on using the local net working infrastructure is probably not good idea. It is better to bring your own. Although wireless networking and mobile computing are often related they wireless and mobile wireless networks. Even notebook computers are sometimes wired. For example if traveler plugs notebook computer into the wired net work jack in hotel room he has mobility without wireless network. Conversely some wireless computers are not mobile. In the home and in offices or hotels that lack suitable cabling it can be more convenient to connect desktop computers or media players wirelessly than to install wires. Installing wireless network may require little more than buying small box with some elec tronics in it unpacking it and plugging it in. This solution may be far cheaper than having workmen put in cable ducts to wire the building. ing around stores with handheld computers recording inventory. At many busy airports car rental return clerks work in the parking lot with wireless mobile com puters. They scan the barcodes or RFID chips of returning cars and their mobile device which has builtin printer calls the main computer gets the rental infor mation and prints out the bill on the spot. Perhaps the key driver of mobile wireless applications is the mobile phone. Text messaging or texting is tremendously popular. It lets mobile phone user type short message that is then delivered by the cellular network to another mobile subscriber. Few people would have predicted ten years ago that having teenagers tediously typing short text messages on mobile phones would be an immense money maker for telephone companies. But texting is very profitable since it costs the carrier but tiny fraction of one cent to relay text message service for which they charge far more. arrived and it will accelerate the growth of mobile applications. Smart phones such as the popular iPhone combine aspects of mobile phones and mobile com puters. The cellular networks to which they connect can provide fast data services for using the Internet as well as handling phone calls. Many ad vanced phones connect to wireless hotspots too and automatically switch between networks to choose the best option for the user. Other consumer electronics devices can also use cellular and hotspot networks to stay connected to remote computers. Electronic book readers can download newly purchased book or the next edition of magazine or todays newspaper with fresh images. with GPS receivers some services are intentionally GPSenabled phone and car probably have better idea of where you are than you do. So too are searches for nearby bookstore or Chinese restaurant or local and videos with the place at which they were made. This annotation is known as An area in which mobile phones are now starting to be used is mcommerce . Short text messages from the mobile are used to authorize payments for food in vending machines movie tickets and other small items instead of cash and credit cards. The charge then appears on the mobile phone bill. When equipped with NFC technology the mobile can act as an RFID smartcard and interact with nearby reader for payment. The driving forces behind this phenomenon are the mobile piece of the ecommerce pie. From the stores point of view this scheme may save them most of the credit card companys fee which can be several percent. Of course this plan may backfire since customers in store might use the RFID or barcode readers on their mobile devices to check out competitors prices before chased nearby and at what price. One huge thing that mcommerce has going for it is that mobile phone users are accustomed to paying for everything . If an Internet Web site charged fee to allow its customers to pay by credit card there would be an immense howling noise from the users. If however mobile phone operator its customers to pay for items in store by waving the phone at the cash register and then tacked on fee for this conveni ence it would probably be accepted as normal. Time will tell. No doubt the uses of mobile and wireless computers will grow rapidly in the future as the size of computers shrinks probably in ways no one can now foresee. Let us take quick look at some possibilities. Sensor networks are made up of nodes that gather and wirelessly relay information they sense about the state of the physical world. The nodes may be part of familiar items such as cars or phones or they may be small separate devices. For example your car might gather data system and upload this information to database . Those data can help find potholes plan trips around congested roads and tell you if you are gas guzzler compared to other drivers on the same stretch of road. Sensor networks are revolutionizing science by providing wealth of data on behavior that could not previously be observed. One example is tracking the migration of individual zebras by placing small sensor on each animal . Researchers have packed wireless computer into cube 1 mm on edge . With mobile computers this small even small birds rodents and insects can be tracked. Even mundane uses such as in parking meters can be significant because they make use of data that were not previously available. Wireless parking meters can accept credit or debit card payments with instant verification over the wireless would let drivers download recent parking map to their car so they can find an available spot more easily. Of course when meter expires it might also check parking enforcement. It has been estimated that city governments in the . alone could collect an additional 10 billion this way . Wearable computers are another promising application. Smart watches with radios have been part of our mental space since their appearance in the Dick Tracy comic strip in 1946 now you can buy them. Other such devices may be implanted such as pacemakers and insulin pumps. Some of these can be con the average PC and can be hacked easily . Computer networks like the printing press 500 years ago allow ordinary citizens to distribute and view content in ways that were not previously possible. But along with the good comes the bad as this newfound freedom brings with it few of them thorough study would require full book at least. Social networks message boards content sharing sites and host of other ap plications allow people to share their views with likeminded individuals. As long as the subjects are restricted to technical topics or hobbies like gardening not too The trouble comes with topics that people actually care about like politics religion or sex. Views that are publicly posted may be deeply offensive to some people. Worse yet they may not be politically correct. Furthermore opinions need not be limited to text highresolution color photographs and video clips are easily shared over computer networks. Some people take liveandletlive view but others feel that posting certain material is simply unacceptable and that such content must be censored. Different countries have different and conflicting laws in this area. Thus the debate rages. In the past people have sued network operators claiming that they are re are. The inevitable response is that network is like telephone company or the post office and cannot be expected to police what its users say. It should now come only as slight surprise to learn that some network opera tors block content for their own reasons. Some users of peertopeer applications had their network service cut off because the network operators did not find it pro fitable to carry the large amounts of traffic sent by those applications. Those same operators would probably like to treat different companies differently. If you are big company and pay well then you get good service but if you are smalltime player you get poor service. Opponents of this practice argue that peertopeer and other content should be treated in the same way because they are all just bits to the network. This argument for communications that are not dif ferentiated by their content or source or who is providing the content is known as network neutrality . It is probably safe to say that this debate will go Many other parties are involved in the tussle over content. For instance pi rated music and movies fueled the massive growth of peertopeer networks taken legal action. There are now automated systems that search peertopeer networks and fire off warnings to network operators and users who are suspected Even your printer might be mistaken for culprit . Computer networks make it very easy to communicate. They also make it easy for the people who run the network to snoop on the traffic. This sets up con read and write email at work. Many employers have claimed the right to read and possibly censor employee messages including messages sent from home com puter outside working hours. Not all employees agree with this especially the lat Another conflict is centered around government versus citizens rights. The FBI has installed systems at many Internet service providers to snoop on all in coming and outgoing email for nuggets of interest. One early system was origi nally called Carnivore but bad publicity caused it to be renamed to the more innocentsounding DCS1000 . The goal of such systems is to spy on millions of people in the hope of perhaps finding information about illegal activities. Unfortunately for the spies the Fourth Amendment to the . Constitution prohibits government searches without search warrant but the government often ignores it. Of course the government does not have monopoly on threatening peoples privacy. The private sector does its bit too by profiling users. For example small files called cookies that Web browsers store on users computers allow companies to track users activities in cyberspace and may also allow credit card numbers social security numbers and other confidential information to leak all over the Internet . Companies that provide Webbased services may maintain large amounts of personal information about their users that allows them to study user activities directly. For example Google can read your email and show you advertisements based on your interests if you use its email service 2003. As part of the process of providing service to your mobile device the net work operators learn where you are at different times of day. This allows them to track your movements. They may know which nightclub you frequent and which medical center you visit. Computer networks also offer the potential to increase privacy by sending anonymous messages. In some situations this capability may be desirable. Beyond preventing companies from learning your habits it provides for example way for students soldiers employees and citizens to blow the whistle on illegal behavior on the part of professors officers superiors and politicians without fear of reprisals. On the other hand in the United States and most other democracies the law specifically permits an accused person the right to confront and challenge his accuser in court so anonymous accusations cannot be used as evidence. The Internet makes it possible to find information quickly but great deal of it is ill considered misleading or downright wrong. That medical advice you plucked from the Internet about the pain in your chest may have come from Nobel Prize winner or from highschool dropout. Other information is frequently unwanted. Electronic junk mail has become part of life because spammers have collected millions of email address es and wouldbe marketers can cheaply send computergenerated messages to them. The resulting flood of spam rivals the flow messages from real people. Fortunately filtering software is able to read and discard the spam generated by other computers with lesser or greater degrees of success. messages containing active content can contain viruses that take over your computer. They might be used to steal your bank account passwords or to have your computer send spam as part of botnet or pool of compromised machines. Phishing messages masquerade as originating from trustworthy party for example your bank to try to trick you into revealing sensitive information for example credit card numbers. Identity theft is becoming serious problem as thieves collect enough information about victim to obtain credit cards and other documents in the victims name. It can be difficult to prevent computers from impersonating people on the In ternet. This problem has led to the development of CAPTCHAs in which com puter asks person to solve short recognition task for example typing in the letters shown in distorted image to show that they are human . This process is variation on the famous Turing test in which person asks ques tions over network to judge whether the entity responding is human. puter security seriously. If all messages were encrypted and authenticated it would be harder to commit mischief. Such technology is well established and we will study it in detail in Chap. 8. The problem is that hardware and software ven dors know that putting in security features costs money and their customers are are caused by buggy software which occurs because vendors keep adding more and more features to their programs which inevitably means more code and thus more bugs. tax on new features might help but that might be tough sell in some quarters. refund for defective software might be nice except it would bankrupt the entire software industry in the first year. laws. Electronic gambling provides an example. Computers have been simulating things for decades so why not simulate slot machines roulette wheels blackjack dealers and more gambling equipment Well because it is illegal in lot of places. The trouble is gambling is legal in lot of other places and casino owners there have grasped the potential for Internet gambling. What happens if the gambler the casino and the server are all in different coun tries with conflicting laws Good question. It is now time to turn our attention from the applications and social aspects of spinach. There is no generally accepted taxonomy into which all computer net works fit but two dimensions stand out as important transmission technology and scale. We will now examine each of these in turn. Broadly speaking there are two types of transmission technology that are in widespread use broadcast links and pointtopoint links. Pointtopoint links connect individual pairs of machines. To go from the source to the destination on network made up of pointtopoint links short mes sages called packets in certain contexts may have to first visit one or more inter mediate machines. Often multiple routes of different lengths are possible so finding good ones is important in pointtopoint networks. transmission with exactly one sender and exactly one receiver is sometimes called In contrast on broadcast network the communication channel is shared by all the machines on the network packets sent by any machine are received by all the others. An address field within each packet specifies the intended recipient. Upon receiving packet machine checks the address field. If the packet is in tended for the receiving machine that machine processes the packet if the packet is intended for some other machine it is just ignored. wireless network is common example of broadcast link with communi cation shared over coverage region that depends on the wireless channel and the transmitting machine. As an analogy consider someone standing in meeting room and shouting Watson come here. want you. Although the packet may actually be received by many people only Watson will respond the others Broadcast systems usually also allow the possibility of addressing packet to all destinations by using special code in the address field. When packet with this code is transmitted it is received and processed by every machine on the net work. This mode of operation is called broadcasting. Some broadcast systems also support transmission to subset of the machines which known as multicast An alternative criterion for classifying networks is by scale. Distance is im portant as classification metric because different technologies are used at dif size. At the top are the personal area networks networks that are meant for one person. Beyond these come longerrange networks. These can be divided into the connection of two or more networks is called an internetwork. The worldwide Internet is certainly the bestknown example of an internetwork. Soon we will have even larger internetworks with the Interplanetary Internet that connects networks across space . In this book we will be concerned with networks at all these scales. In the PANs let devices communicate over the range of person. common example is wireless network that connects computer with its peripherals. Almost every computer has an attached monitor keyboard mouse and printer. Without using wireless this connection must be done with cables. So many new users have hard time finding the right cables and plugging them into the right little holes that most computer vendors offer the option of sending technician to the users home to do it. To help these users some companies got together to design shortrange wireless network called Bluetooth to connect these components without wires. The idea is that if your devices have Bluetooth then you need no cables. You just put them down turn them on and they work together. For many people this ease of operation is big plus. In the simplest form Bluetooth networks use the masterslave paradigm of keyboard etc. as slaves. The master tells the slaves what addresses to use when they can broadcast how long they can transmit what frequencies they can use Bluetooth can be used in other settings too. It is often used to connect headset to mobile phone without cords and it can allow your digital music player to connect to your car merely being brought within range. completely different kind of PAN is formed when an embedded medical device such as pacemaker insulin pump or hearing aid talks to useroperated remote control. We will dis cuss Bluetooth in more detail in Chap. 4. PANs can also be built with other technologies that communicate over short ranges such as RFID on smartcards and library books. We will study RFID in The next step up is the LAN . LAN is privately owned network that operates within and nearby single building like home of fice or factory. LANs are widely used to connect personal computers and consu mer electronics to let them share resources and exchange informa tion. When LANs are used by companies they are called enterprise networks. Wireless LANs are very popular these days especially in homes older office buildings cafeterias and other places where it is too much trouble to install cables. In these systems every computer has radio modem and an antenna that it uses to communicate with other computers. In most cases each computer talks wireless router or base station relays packets between the wireless computers and also between them and the Internet. Being the AP is like being the popular kid as school because everyone wants to talk to you. However if other computers are close enough they can communicate directly with one an other in peertopeer configuration. There is standard for wireless LANs called IEEE 802.11 popularly known To wired network to hundreds of Mbps. We will discuss 802.11 in Chap. 4. Wired LANs use range of different transmission technologies. Most of them use copper wires but some use optical fiber. LANs are restricted in size which means that the worstcase transmission time is bounded and known in ad vance. Knowing these bounds helps with the task of designing network protocols. Typically wired LANs run at speeds of 100 Mbps to 1 Gbps have low delay and make very few errors. Newer LANs can op erate at up to 10 Gbps. Compared to wireless networks wired LANs exceed them in all dimensions of performance. It is just easier to send signals over wire or through fiber than through the air. The topology of many wired LANs is built from pointtopoint links. IEEE 802.3 popularly called Ethernet is by far the most common type of wired puter speaks the Ethernet protocol and connects to box called switch with pointtopoint link. Hence the name. switch has multiple ports each of which can connect to one computer. The job of the switch is to relay packets between computers that are attached to it using the address in each packet to determine which computer to send it to. To build larger LANs switches can be plugged into each other using their ports. What happens if you plug them together in loop Will the network still work Luckily the designers thought of this case. It is the job of the protocol to sort out what paths packets should travel to safely reach the intended computer. We will see how this works in Chap. 4. It is also possible to divide one large physical LAN into two smaller logical LANs. You might wonder why this would be useful. Sometimes the layout of the network equipment does not match the organizations structure. For example the engineering and finance departments of company might have computers on the same physical LAN because they are in the same wing of the building but it might be easier to manage the system if engineering and finance logically each had its own network Virtual LAN or VLAN. In this design each port is tagged with color say green for engineering and red for finance. The switch then forwards packets so that computers attached to the green ports are separated from the com puters attached to the red ports. Broadcast packets sent on red port for example will not be received on green port just as though there were two different LANs. We will cover VLANs at the end of Chap. 4. There are other wired LAN topologies too. In fact switched Ethernet is single linear cable. At most one machine could successfully transmit at time and distributed arbitration mechanism was used to resolve conflicts. It used simple algorithm computers could transmit whenever the cable was idle. If two or more packets collided each computer just waited random time and tried later. will learn about it in Chap. 4. Both wireless and wired broadcast networks can be divided into static and dynamic designs depending on how the channel is allocated. typical static al gorithm allowing each machine to broadcast only when its time slot comes up. ing its allocated slot so most systems attempt to allocate the channel dynamically . ty for example the base station in cellular networks which determines who goes next. It might do this by accepting multiple packets and prioritizing them accord there is no central entity each machine must decide for itself whether to transmit. You might think that this approach would lead to chaos but it does not. Later we will study many algorithms designed to bring order out of the potential chaos. It is worth spending little more time discussing LANs in the home. In the future it is likely that every appliance in the home will be capable of communi cating with every other appliance and all of them will be accessible over the In ternet. This development is likely to be one of those visionary concepts that nobody asked for but once they arrived nobody can imagine how they lived without them. Many devices are already capable of being networked. These include com puters entertainment devices such as TVs and DVDs phones and other consumer electronics such as cameras appliances like clock radios and infrastructure like utility meters and thermostats. This trend will only continue. For instance the average home probably has dozen clocks all of which could adjust to daylight savings time automatically if the clocks were on the Internet. Remote monitoring of the home is likely winner as many grown children would be willing to spend some money to help their aging parents live safely in their While we could think of the home network as just another LAN it is more likely to have different properties than other networks. First the networked de vices have to be very easy to install. Wireless routers are the most returned con sumer electronic item. People buy one because they want wireless network at home find that it does not work out of the box and then return it rather than listen to elevator music while on hold on the technical helpline. Second the network and devices have to be foolproof in operation. Air con ditioners used to have one knob with four settings OFF LOW MEDIUM and puter users are accustomed to putting up with products that do not work the car television and refrigeratorbuying public is far less tolerant. They expect pro ducts to work 100 without the need to hire geek. Third low price is essential for success. People will not pay 50 premium for an Internet thermostat because few people regard monitoring their home tem perature from work that important. For 5 extra though it might sell. Fourth it must be possible to start out with one or two devices and expand the reach of the network gradually. This means no format wars. Telling consumers to buy peripherals with IEEE 1394 interfaces and few years later retracting that and saying USB 2.0 is the interfaceofthemonth and then switch ing that to 802.11goops no make that 802.11nI mean 802.16 is going to make consumers very skittish. The network interface will have to remain stable for decades like the television broadcasting standards. Fifth security and reliability will be very important. Losing few files to an email virus is one thing having burglar disarm your security system from his mobile computer and then plunder your house is something quite different. An interesting question is whether home networks will be wired or wireless. Convenience and cost favors wireless networking because there are no wires to fit or worse retrofit. Security favors wired networking because the radio waves that wireless networks use are quite good at going through walls. Not everyone is overjoyed at the thought of having the neighbors piggybacking on their Internet connection and reading their email. In Chap. 8 we will study how encryption can be used to provide security but it is easier said than done with inexperienced third option that may be appealing is to reuse the networks that are already throughout the house. Powerline networks let devices that plug into outlets broadcast information throughout the house. You have to plug in the TV anyway and this way it can get Internet connectivity at the same time. The difficulty is how to carry both power and data signals at the same time. Part of the answer is that they use different frequency bands. In short home LANs offer many opportunities and challenges. Most of the latter relate to the need for the networks to be easy to manage dependable and secure especially in the hands of nontechnical users as well as low cost. MAN covers city. The bestknown ex amples of MANs are the cable television networks available in many cities. These systems grew from earlier community antenna systems used in areas with poor overtheair television reception. In those early systems large antenna was placed on top of nearby hill and signal was then piped to the subscribers At first these were locally designed ad hoc systems. Then companies began jumping into the business getting contracts from local governments to wire up en tire cities. The next step was television programming and even entire channels designed for cable only. Often these channels were highly specialized such as all news all sports all cooking all gardening and so on. But from their inception until the late 1990s they were intended for television reception only. When the Internet began attracting mass audience the cable TV network operators began to realize that with some changes to the system they could pro vide twoway Internet service in unused parts of the spectrum. At that point the cable TV system began to morph from simply way to distribute television to metropolitan area network. To first approximation MAN might look some nals and Internet being fed into the centralized cable headend for subsequent dis tribution to peoples homes. We will come back to this subject in detail in Chap. Cable television is not the only MAN though. Recent developments in high speed wireless Internet access have resulted in another MAN which has been standardized as IEEE 802.16 and is popularly known as WiMAX. We will look at it in Chap. 4. WAN spans large geographical area often country or continent. We will begin our discussion with wired WANs using the example of company with branch offices in different cities. and Brisbane. Each of these offices contains computers intended for running user programs. We will follow traditional usage and call these ma chines hosts. The rest of the network that connects these hosts is then called the communication subnet or just subnet for short. The job of the subnet is to carry messages from host to host just as the telephone system carries words from speaker to listener. In most WANs the subnet consists of two distinct components transmission lines and switching elements. Transmission lines move bits between machines. They can be made of copper wire optical fiber or even radio links. Most com panies do not have transmission lines lying about so instead they lease the lines from telecommunications company. Switching elements or just switches are specialized computers that connect two or more transmission lines. When data arrive on an incoming line the switching element must choose an outgoing line on which to forward them. These switching computers have been called by various names in the past the name router is now most commonly used. Unfortunately some people pronounce it rooter while others have it rhyme with doubter. Determining the correct pronunciation will be left as an exercise for the reader. short comment about the term subnet is in order here. Originally its only meaning was the collection of routers and communication lines that moved packets from the source host to the destination host. Readers should be aware that it has acquired second more recent meaning in conjunction with network ad dressing. We will discuss that meaning in Chap. 5 and stick with the original meaning until then. The WAN as we have described it looks similar to large wired LAN but there are some important differences that go beyond long wires. Usually in WAN the hosts and subnet are owned and operated by different people. In our example the employees might be responsible for their own computers while the companys IT department is in charge of the rest of the network. We will see clearer boundaries in the coming examples in which the network provider or tele Separation of the pure communication aspects of the network from the application aspects greatly simplifies the overall network design. second difference is that the routers will usually connect different kinds of networking technology. The networks inside the offices may be switched Ether net for example while the longdistance transmission lines may be SONET links . Some device needs to join them. The astute reader will notice that this goes beyond our definition of network. This means that many WANs will in fact be internetworks or composite networks that are made up of more than one network. We will have more to say about internet dual computers as was the case for connecting to LANs or it could be entire LANs. This is how larger networks are built from smaller ones. As far as the sub net is concerned it does the same job. We are now in position to look at two other varieties of WANs. First rather than lease dedicated transmission lines company might connect its offices to the Internet This allows connections to be made between the offices as virtual links that use the underlying capacity of the Internet. This arrangement shown in cated arrangement VPN has the usual advantage of virtualization which is that it provides flexible reuse of resource . Consider how easy it is to add fourth office to see this. VPN also has the usual disadvantage of virtualization which is lack of control over the underlying resources. With dedicated line the capacity is clear. With VPN your mileage may vary with your Internet service. The second variation is that the subnet may be run by different company. The subnet operator is known as network service provider and the offices are nect to other customers too as long as they can pay and it can provide service. Since it would be disappointing network service if the customers could only send packets to each other the subnet operator will also connect to other networks that are part of the Internet. Such subnet operator is called an ISP and the subnet is an ISP network. Its customers who connect to the ISP receive Internet service. each connecting pair of routers. If two routers that do not share transmission line wish to communicate they must do this indirectly via other routers. There may be many paths in the network that connect these two routers. How the net work makes the decision as to which path to use is called the routing algorithm. Many such algorithms exist. How each router makes the decision as to where to send packet next is called the forwarding algorithm. Many of them exist too. We will study some of both types in detail in Chap. 5. Other kinds of WANs make heavy use of wireless technologies. In satellite systems each computer on the ground has an antenna through which it can send data to and receive data from to satellite in orbit. All computers can hear the output from the satellite and in some cases they can also hear the upward transmissions of their fellow computers to the satellite as well. Satellite networks are inherently broadcast and are most useful when the broadcast property is im The cellular telephone network is another example of WAN that uses wire less technology. This system has already gone through three generations and fourth one is on the horizon. The first generation was analog and for voice only. The second generation was digital and for voice only. The third generation is dig ital and is for both voice and data. Each cellular base station covers distance much larger than wireless LAN with range measured in kilometers rather than tens of meters. The base stations are connected to each other by backbone net work that is usually wired. The data rates of cellular networks are often on the order of 1 Mbps much smaller than wireless LAN that can range up to on the order of 100 Mbps. We will have lot to say about these networks in Chap. 2. Many networks exist in the world often with different hardware and software. People connected to one network often want to communicate with people attached to different one. The fulfillment of this desire requires that different and fre quently incompatible networks be connected. collection of interconnected net works is called an internetwork or internet. These terms will be used in gen eric sense in contrast to the worldwide Internet which we will always capitalize. The Internet uses ISP networks to connect en terprise networks home networks and many other networks. We will look at the Internet in great detail later in this book. Subnets networks and internetworks are often confused. The term subnet makes the most sense in the context of wide area network where it refers to the collection of routers and communication lines owned by the network operator. As an analogy the telephone system consists of telephone switching offices connect ed to one another by highspeed lines and to houses and businesses by lowspeed lines. These lines and equipment owned and managed by the telephone com pany form the subnet of the telephone system. The telephones themselves are not part of the subnet. network is formed by the combination of subnet and its hosts. However the word network is often used in loose sense as well. subnet might be de network might also be described as network as in the case of the WAN in from other arrangements we will stick with our original definition of collection of computers interconnected by single technology. Let us say more about what constitutes an internetwork. We know that an in ternet is formed when distinct networks are interconnected. In our view connect ing LAN and WAN or connecting two LANs is the usual way to form an inter network but there is little agreement in the industry over terminology in this area. There are two rules of thumb that are useful. First if different organizations have paid to construct different parts of the network and each maintains its part we have an internetwork rather than single network. Second if the underlying tech nology is different in different parts we probably have an internetwork. To go deeper we need to talk about how two different networks can be con nected. The general name for machine that makes connection between two or more networks and provides the necessary translation both in terms of hardware and software is gateway. Gateways are distinguished by the layer at which they operate in the protocol hierarchy. We will have much more to say about lay higher layers are more tied to applications such as the Web and lower layers are more tied to transmission links such as Ethernet. Since the benefit of forming an internet is to connect computers across net works we do not want to use too lowlevel gateway or we will be unable to make connections between different kinds of networks. We do not want to use too highlevel gateway either or the connection will only work for particular ap plications. The level in the middle that is just right is often called the network layer and router is gateway that switches packets at the network layer. We can now spot an internet by finding network that has routers. The first computer networks were designed with the hardware as the main concern and the software as an afterthought. This strategy no longer works. Net software structuring technique in some detail. The approach described here forms the keystone of the entire book and will occur repeatedly later on. To reduce their design complexity most networks are organized as stack of layers or levels each one built upon the one below it. The number of layers the fer from network to network. The purpose of each layer is to offer certain ser vices to the higher layers while shielding those layers from the details of how the offered services are actually implemented. In sense each layer is kind of vir tual machine offering certain services to the layer above it. This concept is actually familiar one and is used throughout computer sci ence where it is variously known as information hiding abstract data types data encapsulation and objectoriented programming. The fundamental idea is that particular piece of software provides service to its users but keeps the details of its internal state and algorithms hidden from them. When layer on one machine carries on conversation with layer on anoth er machine the rules and conventions used in this conversation are collectively known as the layer protocol. Basically protocol is an agreement between the communicating parties on how communication is to proceed. As an analogy when woman is introduced to man she may choose to stick out her hand. He in turn may decide to either shake it or kiss it depending for example on wheth er she is an American lawyer at business meeting or European princess at formal ball. Violating the protocol will make communication more difficult if not completely impossible. corresponding layers on different machines are called peers. The peers may be software processes hardware devices or even human beings. In other words it is the peers that communicate by using the protocol to talk to each other. In reality no data are directly transferred from layer on one machine to layer on another machine. Instead each layer passes data and control infor mation to the layer immediately below it until the lowest layer is reached. Below layer 1 is the physical medium through which actual communication occurs. In cation by solid lines. Between each pair of adjacent layers is an interface. The interface defines which primitive operations and services the lower layer makes available to the upper one. When network designers decide how many layers to include in net work and what each one should do one of the most important considerations is defining clean interfaces between the layers. Doing so in turn requires that each layer perform specific collection of wellunderstood functions. In addition to minimizing the amount of information that must be passed between layers clear cut interfaces also make it simpler to replace one layer with completely different protocol or implementation because all that is required of the new protocol or implementation is that it offer exactly the same set of services to its upstairs neighbor as the old one did. It is common that different hosts use different implementations of the same protocol . In fact the protocol itself can change in some layer without the layers above and below it even noticing. set of layers and protocols is called network architecture. The specif ication of an architecture must contain enough information to allow an imple menter to write the program or build the hardware for each layer so that it will correctly obey the appropriate protocol. Neither the details of the implementation nor the specification of the interfaces is part of the architecture because these are hidden away inside the machines and not visible from the outside. It is not even necessary that the interfaces on all machines in network be the same provided that each machine can correctly use all the protocols. list of the protocols used by certain system one protocol per layer is called protocol stack. Network architectures protocol stacks and the protocols themselves are the principal sub jects of this book. An analogy may help explain the idea of multilayer communication. Imagine two philosophers one of whom speaks Urdu and English and one of whom speaks Chinese and French. Since they have no com mon language they each engage translator each of wishes to convey his affection for oryctolagus cuniculus to his peer. To do so he passes message across the 23 interface to his translator saying language known to both of them Dutch so the message is converted to Ik vind konijnen leuk. The choice of the language is the layer 2 protocol and is up to the layer 2 peer processes. The translator then gives the message to secretary for transmission for ex ample by email . When the message arrives at the other secretary it is passed to the local translator who translates it into French and passes it across the 23 interface to the second philosopher. Note that each proto col is completely independent of the other ones as long as the interfaces are not changed. The translators can switch from Dutch to say Finnish at will provided that they both agree and neither changes his interface with either layer 1 or layer 3. Similarly the secretaries can switch from email to telephone without disturb ing the other layers. Each process may add some information intended only for its peer. This information is not passed up to the layer above. Now consider more technical example how to provide communication to an application process running in layer 5 and given to layer 4 for transmission. to allow layer 4 on the destination machine to deliver the message. Other ex amples of control information used in some layers are sequence numbers sizes and times. In many networks no limit is placed on the size of messages transmitted in the layer 4 protocol but there is nearly always limit imposed by the layer 3 pro tocol. Consequently layer 3 must break up the incoming messages into smaller split into two parts 1 and 2 that will be transmitted separately. Layer 3 decides which of the outgoing lines to use and passes the packets to the resulting unit to layer 1 for physical transmission. At the receiving machine virtual and actual communication and the difference between protocols and inter faces. The peer processes in layer 4 for example conceptually think of their communication as being horizontal using the layer 4 protocol. Each one is likely to have procedures called something like SendToOtherSide and GetFrom OtherSide even though these procedures actually communicate with lower layers across the 34 interface and not with the other side. The peer process abstraction is crucial to all network design. Using it the unmanageable task of designing the complete network can be broken into several Although Sec. 1.3 is called Network Software it is worth pointing out that the lower layers of protocol hierarchy are frequently implemented in hardware or firmware. Nevertheless complex protocol algorithms are involved even if they are embedded in hardware. in layer after layer. Below we will briefly mention the more important ones. even though it is made up of collection of components that are themselves unreliable. Think about the bits of packet traveling through the network. There is chance that some of these bits will be received damaged due to fluke electrical noise random wireless signals hardware flaws software bugs and so on. How is it possible that we find and fix these errors One mechanism for finding errors in received information uses codes for er ror detection. Information that is incorrectly received can then be retransmitted until it is received correctly. More powerful codes allow for error correction where the correct message is recovered from the possibly incorrect bits that were originally received. Both of these mechanisms work by adding redundant infor mation. They are used at low layers to protect packets sent over individual links there are multiple paths between source and destination and in large network there may be some links or routers that are broken. Suppose that the network is down in Germany. Packets sent from London to Rome via Germany will not get through but we could instead send packets from London to Rome via Paris. The network should automatically make this decision. This topic is called routing. works grow larger and new designs emerge that need to be connected to the exist ing network. We have recently seen the key structuring mechanism used to sup port change by dividing the overall problem and hiding implementation details protocol layering. There are many other strategies as well. Since there are many computers on the network every layer needs mechan ism for identifying the senders and receivers that are involved in particular mes sage. This mechanism is called addressing or naming in the low and high lay ers respectively. An aspect of growth is that different network technologies often have dif ferent limitations. For example not all communication channels preserve the other example is differences in the maximum size of message that the networks can transmit. This leads to mechanisms for disassembling transmitting and then reassembling messages. This overall topic is called internetworking. shortage of telephone numbers and it is easy to get lost. Not many people have Designs that continue to work well when the network gets large are said to be hosts from their underlying resources such as the capacity of transmission lines. To do this well they need mechanisms that divide their resources so that one host does not interfere with another too much. Many designs share network bandwidth dynamically according to the short term needs of hosts rather than by giving each host fixed fraction of the band width that it may or may not use. This design is called statistical multiplexing meaning sharing based on the statistics of demand. It can be applied at low layers for single link or at high layers for network or even applications that use the from swamping slow receiver with data. Feedback from the receiver to the sender is often used. This subject is called flow control. Sometimes the problem is that the network is oversubscribed because too many computers want to send too much traffic and the network cannot deliver it all. This overloading of the network is called congestion. One strategy is for each computer to reduce its de mand when it experiences congestion. It too can be used in all layers. It is interesting to observe that the network has more resources to offer than simply bandwidth. For uses such as carrying live video the timeliness of delivery matters great deal. Most networks must provide service to applications that want this realtime delivery at the same time that they provide service to applications that want high throughput. Quality of service is the name given to mechanisms that reconcile these competing demands. different kinds of threats. One of the threats we have mentioned previously is that of eavesdropping on communications. Mechanisms that provide confidentiality defend against this threat and they are used in multiple layers. Mechanisms for authentication prevent someone from impersonating someone else. They might be used to tell fake banking Web sites from the real one or to let the cellular net work check that call is really coming from your phone so that you will pay the bill. Other mechanisms for integrity prevent surreptitious changes to messages such as altering debit my account 10 to debit my account 1000. All of these designs are based on cryptography which we shall study in Chap. 8. Layers can offer two different types of service to the layers above them con types and examine the differences between them. Connectionoriented service is modeled after the telephone system. To talk to someone you pick up the phone dial the number talk and then hang up. Simi larly to use connectionoriented network service the service user first estab lishes connection uses the connection and then releases the connection. The essential aspect of connection is that it acts like tube the sender pushes objects in at one end and the receiver takes them out at the other end. In most cases the order is preserved so that the bits arrive in the order they were sent. In some cases when connection is established the sender receiver and sub net conduct negotiation about the parameters to be used such as maximum makes proposal and the other side can accept it reject it or make counter proposal. circuit is another name for connection with associated resources cuit was path over copper wire that carried phone conversation. In contrast to connectionoriented service connectionless service is modeled after the postal system. Each message carries the full destination address and each one is routed through the intermediate nodes inside the system indepen dent of all the subsequent messages. There are different names for messages in different contexts packet is message at the network layer. When the inter mediate nodes receive message in full before sending it on to the next node this is called storeandforward switching. The alternative in which the onward transmission of message at node starts before it is completely received by the node is called cutthrough switching. Normally when two messages are sent to the same destination the first one sent will be the first one to arrive. However it is possible that the first one sent can be delayed so that the second one arrives Each kind of service can further be characterized by its reliability. Some ser vices are reliable in the sense that they never lose data. Usually reliable service is implemented by having the receiver acknowledge the receipt of each message so the sender is sure that it arrived. The acknowledgement process introduces overhead and delays which are often worth it but are sometimes undesirable. typical situation in which reliable connectionoriented service is appropri ate is file transfer. The owner of the file wants to be sure that all the bits arrive correctly and in the same order they were sent. Very few file transfer customers would prefer service that occasionally scrambles or loses few bits even if it is Reliable connectionoriented service has two minor variations message se quences and byte streams. In the former variant the message boundaries are pre served. When two 1024byte messages are sent they arrive as two distinct 1024 byte messages never as one 2048byte message. In the latter the connection is simply stream of bytes with no message boundaries. When 2048 bytes arrive at the receiver there is no way to tell if they were sent as one 2048byte message over network to phototypesetter as separate messages it might be important to preserve the message boundaries. On the other hand to download DVD movie byte stream from the server to the users computer is all that is needed. Mes sage boundaries within the movie are not relevant. For some applications the transit delays introduced by acknowledgements are unacceptable. One such application is digitized voice traffic for voice over IP. It is less disruptive for telephone users to hear bit of noise on the line from time to time than to experience delay waiting for acknowledgements. Similarly when transmitting video conference having few pixels wrong is no problem but having the image jerk along as the flow stops and starts to correct errors is irritat Not all applications require connections. For example spammers send elec tronic junkmail to many recipients. The spammer probably does not want to go to the trouble of setting up and later tearing down connection to recipient just to send them one item. Nor is 100 percent reliable delivery essential especially if it costs more. All that is needed is way to send single message that has high probability of arrival but no guarantee. Unreliable connectionless service is often called datagram service in analogy with telegram service which also does not return an acknowledgement to the sender. Despite it being unreliable it is the dominant form in most networks for reasons that will In other situations the convenience of not having to establish connection to send one message is desired but reliability is essential. The acknowledged datagram service can be provided for these applications. It is like sending reg istered letter and requesting return receipt. When the receipt comes back the sender is absolutely sure that the letter was delivered to the intended party and not lost along the way. Text messaging on mobile phones is an example. Still another service is the requestreply service. In this service the sender transmits single datagram containing request the reply contains the answer. Requestreply is commonly used to implement communication in the clientserver mobile phone client might send query to map server to retrieve the map data The concept of using unreliable communication may be confusing at first. After all why would anyone actually prefer unreliable communication to reliable First of all reliable communication may not be available in given layer. For example Ethernet does not provide reliable communication. Packets can occasionally be damaged in transit. It is up to higher protocol levels to recover from this problem. In particu lar many reliable services are built on top of an unreliable datagram service. Sec ond the delays inherent in providing reliable service may be unacceptable espe cially in realtime applications such as multimedia. For these reasons both reli able and unreliable communication coexist. service is formally specified by set of primitives available to user processes to access the service. These primitives tell the service to perform located in the operating system as it often is the primitives are normally system calls. These calls cause trap to kernel mode which then turns control of the ma chine over to the operating system to send the necessary packets. The set of primitives available depends on the nature of the service being pro vided. The primitives for connectionoriented service are different from those of connectionless service. As minimal example of the service primitives that They will be familiar to fans of the Berkeley socket interface as the primitives are These primitives might be used for requestreply interaction in clientser ver environment. To illustrate how We sketch simple protocol that implements the service using acknowledged datagrams. First the server executes LISTEN to indicate that it is prepared to accept in coming connections. common way to implement LISTEN is to make it block ing system call. After executing the primitive the server process is blocked until request for connection appears. Next the client process executes CONNECT to establish connection with the server. The CONNECT call needs to specify who to connect to so it might have parameter giving the servers address. The operating system then typically sends process is suspended until there is response. When the packet arrives at the server the operating system sees that the pack et is requesting connection. It checks to see if there is listener and if so it unblocks the listener. The server process can then establish the connection with the ACCEPT call. This sends response back to the client process to accept the Request for data connection. The arrival of this response then releases the client. At this point the client and server are both running and they have connection established. The obvious analogy between this protocol and real life is customer calling companys customer service manager. At the start of the day the service manager sits next to his telephone in case it rings. Later client places call. When the manager picks up the phone the connection is established. The next step is for the server to execute RECEIVE to prepare to accept the first request. Normally the server does this immediately upon being released from the LISTEN before the acknowledgement can get back to the client. The RECEIVE call blocks the server. Then the client executes SEND to transmit its request followed by the ex ecution of RECEIVE to get the reply. The arrival of the request packet at the server machine unblocks the server so it can handle the request. After it has done the work the server uses SEND to return the answer to the client . The arrival of this packet unblocks the client which can now inspect the answer. If the client has additional requests it can make them now. When the client is done it executes DISCONNECT to terminate the connection . Usually an initial DISCONNECT is blocking call suspending the client and sending packet to the server saying that the connection is no longer needed. nowledging the client and releasing the connection . When the servers packet gets back to the client machine the client process is released and the connection is broken. In nutshell this is how connectionoriented communication works. Of course life is not so simple. Many things can go wrong here. The timing can be wrong packets can get lost work with acknowledged datagrams so that we can ignore lost packets. Given that six packets are required to complete this protocol one might wonder why connectionless protocol is not used instead. The answer is that in perfect world it could be in which case only two packets would be needed one for the request and one for the reply. However in the face of large messages in either direction transmission errors and lost packets the situation changes. If the reply consisted of hundreds of packets some of which could be lost during transmission how would the client know if some pieces were missing How would the client know whether the last packet actually received was really the last packet sent Suppose the client wanted second file. How could it tell packet 1 from the second file from lost packet 1 from the first file that suddenly found its way to the client In short in the real world simple re we will study variety of protocols in detail that overcome these and other prob lems. For the moment suffice it to say that having reliable ordered byte stream between processes is sometimes very convenient. Services and protocols are distinct concepts. This distinction is so important that we emphasize it again here. service is set of primitives that layer provides to the layer above it. The service defines what operations the layer is prepared to perform on behalf of its users but it says nothing at all about how these operations are implemented. service relates to an interface between two layers with the lower layer being the service provider and the upper layer being the service user. protocol in contrast is set of rules governing the format and meaning of the packets or messages that are exchanged by the peer entities within layer. Entities use protocols to implement their service definitions. They are free to change their protocols at will provided they do not change the service visible to their users. In this way the service and the protocol are completely decoupled. This is key concept that any network designer should understand well. To repeat this crucial point services relate to the interfaces between layers as peer entities on different machines. It is very important not to confuse the two An analogy with programming languages is worth making. service is like an abstract data type or an object in an objectoriented language. It defines opera tions that can be performed on an object but does not specify how these operations are implemented. In contrast protocol relates to the implementation of the ser vice and as such is not visible to the user of the service. Many older protocols did not distinguish the service from the protocol. In ef fect typical layer might have had service primitive SEND PACKET with the user providing pointer to fully assembled packet. This arrangement meant that all changes to the protocol were immediately visible to the users. Most network de signers now regard such design as serious blunder. Now that we have discussed layered networks in the abstract it is time to look at some examples. We will discuss two important network architectures the OSI reference model and the TCPIP reference model. Although the protocols associ ated with the OSI model are not used any more the model itself is actually quite general and still valid and the features discussed at each layer are still very im portant. The TCPIP model has the opposite properties the model itself is not of much use but the protocols are widely used. For this reason we will look at both of them in detail. Also sometimes you can learn more from failures than from model is based on proposal developed by the International Standards Organiza tion as first step toward international standardization of the protocols used in the various layers . It was revised in 1995 . The model is called the ISO OSI Ref erence Model because it deals with connecting open systemsthat is systems that are open for communication with other systems. We will just call it the OSI model for short. The OSI model has seven layers. The principles that were applied to arrive at the seven layers can be briefly summarized as follows 1. layer should be created where different abstraction is needed. 2. Each layer should perform welldefined function. 3. The function of each layer should be chosen with an eye toward Physical layer hostrouter protocol 4. The layer boundaries should be chosen to minimize the information flow across the interfaces. 5. The number of layers should be large enough that distinct functions need not be thrown together in the same layer out of necessity and small enough that the architecture does not become unwieldy. Below we will discuss each layer of the model in turn starting at the bottom layer. Note that the OSI model itself is not network architecture because it does not specify the exact services and protocols to be used in each layer. It just tells what each layer should do. However ISO has also produced standards for all the layers although these are not part of the reference model itself. Each one has been published as separate international standard. The model is widely used although the associated protocols have been long forgotten. The physical layer is concerned with transmitting raw bits over communi sends 1 bit it is received by the other side as 1 bit not as 0 bit. Typical ques tions here are what electrical signals should be used to represent 1 and 0 how many nanoseconds bit lasts whether transmission may proceed simultaneously in both directions how the initial connection is established how it is torn down when both sides are finished how many pins the network connector has and what and timing interfaces as well as the physical transmission medium which lies The main task of the data link layer is to transform raw transmission facil ity into line that appears free of undetected transmission errors. It does so by masking the real errors so the network layer does not see them. It accomplishes this task by having the sender break up the input data into data frames and transmit the frames sequentially. If the service is reliable the receiver confirms correct receipt of each frame by send ing back an acknowledgement frame. as well is how to keep fast transmitter from drowning slow receiver in data. Some traffic regulation mechanism may be needed to let the transmitter know when the receiver can accept more data. control access to the shared channel. special sublayer of the data link layer the medium access control sublayer deals with this problem. The Network Layer determining how packets are routed from source to destination. Routes can be based on static tables that are wired into the network and rarely changed or can also be determined at the start of each conversation for example terminal ic being determined anew for each packet to reflect the current network load. If too many packets are present in the subnet at the same time they will get in one anothers way forming bottlenecks. Handling congestion is also responsi bility of the network layer in conjunction with higher layers that adapt the load they place on the network. More generally the quality of service provided token management and synchronization . Unlike the lower layers which are mostly concerned with moving bits around the presentation layer is concerned with the syntax and semantics of the infor mation transmitted. In order to make it possible for computers with different in ternal data representations to communicate the data structures to be exchanged can be defined in an abstract way along with standard encoding to be used on the wire. The presentation layer manages these abstract data structures and al lows higherlevel data structures to be defined and The application layer contains variety of protocols that are commonly needed by users. One widely used application protocol is HTTP which is the basis for the World Wide Web. cation protocols are used for file transfer electronic mail and network news. Let us now turn from the OSI reference model to the reference model used in the grandparent of all wide area computer networks the ARPANET and its suc cessor the worldwide Internet. Although we will give brief history of the ARPANET later it is useful to mention few key aspects of it now. The ARPANET was research network sponsored by the DoD . It eventually connected hundreds of universities and government instal lations using leased telephone lines. When satellite and radio networks were added later the existing protocols had trouble interworking with them so new reference architecture was needed. Thus from nearly the beginning the ability to connect multiple networks in seamless way was one of the major design goals. This architecture later became known as the TCPIP Reference Model after its two primary protocols. It was first described by Cerf and Kahn and later refined and defined as standard in the Internet community . The design philosophy behind the model is discussed by Clark . Given the DoDs worry that some of its precious hosts routers and internet work gateways might get blown to pieces at moments notice by an attack from the Soviet Union another major goal was that the network be able to survive loss of subnet hardware without existing conversations being broken off. In other words the DoD wanted connections to remain intact as long as the source and destination machines were functioning even if some of the machines or transmis sion lines in between were suddenly put out of operation. Furthermore since ap plications with divergent requirements were envisioned ranging from transferring files to realtime speech transmission flexible architecture was needed. All these requirements led to the choice of packetswitching network based on connectionless layer that runs across different networks. The lowest layer in the model the link layer describes what links such as serial lines and classic Eth ernet must do to meet the needs of this connectionless internet layer. It is not really layer at all in the normal sense of the term but rather an interface be tween hosts and transmission links. Early material on the TCPIP model has little The internet layer is the linchpin that holds the whole architecture together. job is to permit hosts to inject packets into any network and have them travel in dependently to the destination . They may even arrive in completely different order than they were sent in which case it is the job of higher layers to rearrange them if inorder delivery is desired. Note that internet is used here in generic sense even though this layer is present in The analogy here is with the mail system. person can drop se quence of international letters into mailbox in one country and with little luck most of them will be delivered to the correct address in the destination country. The letters will probably travel through one or more international mail gateways along the way but this is transparent to the users. Furthermore that each country has its own stamps preferred envelope sizes and delivery rules is hidden from the users. The internet layer defines an official packet format and protocol called IP plus companion protocol called ICMP that helps it function. The job of the internet layer is to deliver IP packets where they are supposed to go. Packet routing is clearly The layer above the internet layer in the TCPIP model is now usually called the transport layer. It is designed to allow peer entities on the source and desti nation hosts to carry on conversation just as in the OSI transport layer. Two endtoend transport protocols have been defined here. The first one TCP is reliable connectionoriented protocol that allows byte stream originating on one machine to be delivered without error on any other machine in the internet. It segments the incoming byte stream into discrete messages and passes each one on to the internet layer. At the destination the receiving TCP process reassembles the received messages into the output stream. TCP also handles flow control to make sure fast sender cannot swamp slow receiver with more messages than it can handle. The second protocol in this layer UDP is an unreliable connectionless protocol for applications that do not want TCPs sequencing or flow control and wish to provide their own. It is also widely used for oneshot clientservertype requestreply queries and applications in which prompt delivery is more important than accurate delivery such as transmitting the model was developed IP has been implemented on many other networks. The TCPIP model does not have session or presentation layers. No need for them was perceived. Instead applications simply include any session and pres entation functions that they require. Experience with the OSI model has proven this view correct these layers are of little use to most applications. On top of the transport layer is the application layer. It contains all the high erlevel protocols. The early ones included virtual terminal file trans fer and electronic mail . Many other protocols have been added to include the Domain Name System for mapping host names onto their net and RTP the protocol for delivering realtime media such as voice or movies. As mentioned earlier the strength of the OSI reference model is the model it self which has proven to be ex ceptionally useful for discussing computer networks. In contrast the strength of the TCPIP reference model is the protocols which have been widely used for many years. Since computer scientists like to have their cake and eat it too we This model has five layers running from the physical layer up through the link network and transport layers to the application layer. The physical layer specifies how to transmit bits across different kinds of media as electrical signals. The link layer is concerned with how to send finitelength messages between directly connected computers with specified levels of reliabil ity. Ethernet and 802.11 are examples of link layer protocols. The network layer deals with how to combine multiple links into networks and networks of networks into internetworks so that we can send packets between distant computers. This includes the task of finding the path along which to send the packets. IP is the main example protocol we will study for this layer. The transport layer strengthens the delivery guarantees of the Network layer usually with increased reliability and provide delivery abstractions such as reliable byte stream that match the needs of different applications. TCP is an important example of transport layer protocol. Many but not all networked applications have user interfaces such as Web browser. Our concern however is with the portion of the program that uses the network. This is the HTTP protocol in the case of the Web browser. There are also important support programs in the application layer such as the DNS that are used by many applications. of the OSI model for understanding network architectures but concentrate pri marily on protocols that are important in practice from TCPIP and related proto cols to newer ones such as 802.11 SONET and Bluetooth. The OSI and TCPIP reference models have much in common. Both are based on the concept of stack of independent protocols. Also the functionality of the layers is roughly similar. For example in both models the layers up through and including the transport layer are there to provide an endtoend net workindependent transport service to processes wishing to communicate. These layers form the transport provider. Again in both models the layers above tran sport are applicationoriented users of the transport service. Despite these fundamental similarities the two models also have many dif erence models. It is important to note that we are comparing the reference models here not the corresponding protocol stacks. The protocols themselves will be dis cussed later. For an entire book comparing and contrasting TCPIP and OSI see Probably the biggest contribution of the OSI model is that it makes the distinction between these three concepts explicit. Each layer performs some services for the layer above it. The service definition tells what the layer does not how entities above it access it or how the layer works. It defines the layers semantics. layers interface tells the processes above it how to access it. It specifies what the parameters are and what results to expect. It too says nothing about how the layer works inside. use any protocols it wants to as long as it gets the job done . It can also change them at will without affecting software in These ideas fit very nicely with modern ideas about objectoriented pro gramming. An object like layer has set of methods that proc esses outside the object can invoke. The semantics of these methods define the set of services that the object offers. The methods parameters and results form the objects interface. The code internal to the object is its protocol and is not visible or of any concern outside the object. The TCPIP model did not originally clearly distinguish between services in terfaces and protocols although people have tried to retrofit it after the fact to make it more OSIlike. For example the only real services offered by the internet layer are SEND IP PACKET and RECEIVE IP PACKET. As consequence the proto cols in the OSI model are better hidden than in the TCPIP model and can be replaced relatively easily as the technology changes. Being able to make such changes transparently is one of the main purposes of having layered protocols in the first place. The OSI reference model was devised before the corresponding protocols were invented. This ordering meant that the model was not biased toward one particular set of protocols fact that made it quite general. The downside of this ordering was that the designers did not have much experience with the subject and did not have good idea of which functionality to put in which layer. For example the data link layer originally dealt only with pointtopoint net works. When broadcast networks came around new sublayer had to be hacked into the model. Furthermore when people started to build real networks using the OSI model and existing protocols it was discovered that these networks did not match the required service specifications so convergence sublayers had to be grafted onto the model to provide place for papering over the differences. would have one network run by the government and using the OSI protocols so no thought was given to internetworking. To make long story short things did not turn out that way. With TCPIP the reverse was true the protocols came first and the model was really just description of the existing protocols. There was no problem with the protocols fitting the model. They fit perfectly. The only trouble was that the model did not fit any other protocol stacks. Consequently it was not especially useful for describing other nonTCPIP networks. Turning from philosophical matters to more specific ones an obvious dif ference between the two models is the number of layers the OSI model has seven layers and the TCPIP model has four. Both have network transport and application layers but the other layers are different. Another difference is in the area of connectionless versus connectionoriented communication. The OSI model supports both connectionless and connection oriented communication in the network layer but only connectionoriented com munication in the transport layer where it counts . The TCPIP model supports only one mode in the network layer but both in the transport layer giving the users choice. This choice is especially important for simple requestresponse protocols. Neither the OSI model and its protocols nor the TCPIP model and its proto cols are perfect. Quite bit of criticism can be and has been directed at both of We will begin with OSI and examine TCPIP afterward. At the time the second edition of this book was published it appeared to many experts in the field that the OSI model and its protocols were going to take over the world and push everything else out of their way. This did not hap pen. Why look back at some of the reasons may be useful. They can be sum 4. Bad politics. First let us look at reason one bad timing. The time at which standard is established is absolutely critical to its success. David Clark of . has theory of standards that he calls the apocalypse of the two elephants which is illustrated the subject is first discovered there is burst of research activity in the form of discussions papers and meetings. After while this activity subsides corpora tions discover the subject and the billiondollar wave of investment hits. It is essential that the standards be written in the trough in between the two elephants. If they are written too early the subject may still be poorly understood the result is bad stan dard. If they are written too late so many companies may have already made ma jor investments in different ways of doing things that the standards are effectively ignored. If the interval between the two elephants is very short the people developing the standards may get crushed. It now appears that the standard OSI protocols got crushed. The competing TCPIP protocols were already in widespread use by research universities by the time the OSI protocols appeared. While the billiondollar wave of investment had not yet hit the academic market was large enough that many vendors had begun cautiously offering TCPIP products. When OSI came around they did not want to support second protocol stack until they were forced to so there were no ini tial offerings. With every company waiting for every other company to go first no company went first and OSI never happened. The second reason that OSI never caught on is that both the model and the protocols are flawed. The choice of seven layers was more political than techni cal and two of the layers are nearly empty whereas two other ones are overfull. The OSI model along with its associated service definitions and protocols is extraordinarily complex. When piled up the printed standards occupy signifi cant fraction of meter of paper. They are also difficult to implement and ineffi cient in operation. In this context riddle posed by Paul Mockapetris and cited What do you get when you cross mobster with an international standard Someone who makes you an offer you cant understand. In addition to being incomprehensible another problem with OSI is that some functions such as addressing flow control and error control reappear again and again in each layer. Saltzer et al. for example have pointed out that to be effective error control must be done in the highest layer so that repeating it over and over in each of the lower layers is often unnecessary and inefficient. Given the enormous complexity of the model and the protocols it will come as no surprise that the initial implementations were huge unwieldy and slow. Everyone who tried them got burned. It did not take long for people to associate OSI with poor quality. Although the products improved in the course of time the image stuck. In contrast one of the first implementations of TCPIP was part of Berkeley UNIX and was quite good . People began using it quickly which led to large user community which led to improvements which led to an even larger community. Here the spiral was upward instead of downward. On account of the initial implementation many people especially in academia thought of TCPIP as part of UNIX and UNIX in the 1980s in academia was not unlike parenthood and apple pie. OSI on the other hand was widely thought to be the creature of the European telecommunication ministries the European Community and later the . Gov ernment. This belief was only partly true but the very idea of bunch of govern ment bureaucrats trying to shove technically inferior standard down the throats of the poor researchers and programmers down in the trenches actually develop ing computer networks did not aid OSIs cause. Some people viewed this de velopment in the same light as IBM announcing in the 1960s that PLI was the language of the future or the DoD correcting this later by announcing that it was does not clearly distinguish the concepts of services interfaces and protocols. Good software engineering practice requires differentiating between the specif ication and the implementation something that OSI does very carefully but TCPIP does not. Consequently the TCPIP model is not much of guide for de signing new networks using new technologies. Second the TCPIP model is not at all general and is poorly suited to describ ing any protocol stack other than TCPIP. Trying to use the TCPIP model to describe Bluetooth for example is completely impossible. Third the link layer is not really layer at all in the normal sense of the term as used in the context of layered protocols. It is an interface . The distinction between an interface and layer is crucial and one should not be sloppy about it. Fourth the TCPIP model does not distinguish between the physical and data link layers. These are completely different. The physical layer has to do with the transmission characteristics of copper wire fiber optics and wireless communica tion. The data link layers job is to delimit the start and end of frames and get them from one side to the other with the desired degree of reliability. proper model should include both as separate layers. The TCPIP model does not do this. well implemented many of the other protocols were ad hoc generally produced by couple of graduate students hacking away until they got tired. The protocol implementations were then distributed free which resulted in their becoming widely used deeply entrenched and thus hard to replace. Some of them are bit of an embarrassment now. The virtual terminal protocol TELNET for example was designed for tencharacterpersecond mechanical Teletype terminal. It knows nothing of graphical user interfaces and mice. Nevertheless it is still in use some 30 years later. The subject of computer networking covers many different kinds of networks large and small well known and less well known. They have different goals amples to get an idea of the variety one finds in the area of computer networking. We will start with the Internet probably the best known network and look at its history evolution and technology. Then we will consider the mobile phone network. Technically it is quite different from the Internet contrasting nicely with it. Next we will introduce IEEE 802.11 the dominant standard for wireless tend the reach of the network to include the physical world and everyday objects. The Internet is not really network at all but vast collection of different networks that use certain common protocols and provide certain common ser vices. It is an unusual system in that it was not planned by anyone and is not con trolled by anyone. To better understand it let us start from the beginning and see how it has developed and why. For wonderful history of the Internet John Naughtons book is highly recommended. It is one of those rare books that Of course countless technical books have been written about the Internet and its protocols as well. For more information see for example Maufer . The story begins in the late 1950s. At the height of the Cold War the . DoD wanted commandandcontrol network that could survive nuclear war. At that time all military communications used the public telephone network which was considered vulnerable. The reason for this belief can be gleaned from which was connected to thousands of telephones. These switching offices were in turn connected to higherlevel switching offices to form national hierarchy with only small amount of redundancy. The vulnerability of the system was that the destruction of few key toll offices could fragment it into tributed switching system. Around 1960 the DoD awarded contract to the RAND Corporation to find solution. One of its employees Paul Baran came up with the highly distributed ing offices were now much longer than analog signals could travel without distor tion Baran proposed using digital packetswitching technology. Baran wrote sev the Pentagon liked the concept and asked ATT then the . national tele phone monopoly to build prototype. ATT dismissed Barans ideas out of hand. The biggest and richest corporation in the world was not about to allow some young whippersnapper tell it how to build telephone system. They said Barans network could not be built and the idea was killed. Several years went by and still the DoD did not have better commandand control system. To understand what happened next we have to go back all the way to October 1957 when the Soviet Union beat the . into space with the launch of the first artificial satellite Sputnik. When President Eisenhower tried to find out who was asleep at the switch he was appalled to find the Army Navy and Air Force squabbling over the Pentagons research budget. His immediate response was to create single defense research organization ARPA the Advanced Research Projects Agency. ARPA had no scientists or laboratories in fact it had nothing more than an office and small budget. It did its work by issuing grants and contracts to universities and com panies whose ideas looked promising to it. In 1967 the attention of Larry Roberts program manager at ARPA who was working. He contacted various experts to decide what to do. One of them Wes ley Clark suggested building packetswitched subnet connecting each host to After some initial skepticism Roberts bought the idea and presented some what vague paper about it at the ACM SIGOPS Symposium on Operating System Principles held in Gatlinburg Tennessee in late 1967 . Much to Roberts surprise another paper at the conference described similar system that had not only been designed but actually fully implemented under the direction of Donald Davies at the National Physical Laboratory in England. The NPL system was not national system but it demonstrated that packet switching could be made to work. Fur thermore it cited Barans now discarded earlier work. Roberts came away from Gatlinburg determined to build what later became known as the ARPANET. The subnet would consist of minicomputers called IMPs connected by 56kbps transmission lines. For high reliability each IMP would be connected to at least two other IMPs. The subnet was to be datagram subnet so if some lines and IMPs were destroyed messages could be automatically rerouted along alternative paths. Each node of the network was to consist of an IMP and host in the same room connected by short wire. host could send messages of up to 8063 bits to its IMP which would then break these up into packets of at most 1008 bits and forward them independently toward the destination. Each packet was received in its entirety before being forwarded so the subnet was the first electronic store andforward packetswitching network. ARPA then put out tender for building the subnet. Twelve companies bid for it. After evaluating all the proposals ARPA selected BBN consulting firm based in Cambridge Massachusetts and in December 1968 awarded it contract to build the subnet and write the subnet software. BBN chose to use specially modified Honeywell DDP316 minicomputers with 12K 16bit words of core memory as the IMPs. The IMPs did not have disks since moving parts were con sidered unreliable. The IMPs were interconnected by 56kbps lines leased from telephone companies. Although 56 kbps is now the choice of teenagers who can not afford DSL or cable it was then the best money could buy. The software was split into two parts subnet and host. The subnet software consisted of the IMP end of the hostIMP connection the IMPIMP protocol and source IMP to destination IMP protocol designed to improve reliability. The Outside the subnet software was also needed namely the host end of the hostIMP connection the hosthost protocol and the application software. It soon became clear that BBN was of the opinion that when it had accepted message on hostIMP wire and placed it on the hostIMP wire at the destination its job was Roberts had problem though the hosts needed software too. To deal with it he convened meeting of network researchers mostly graduate students at Snowbird Utah in the summer of 1969. The graduate students expected some network expert to explain the grand design of the network and its software to them and then assign each of them the job of writing part of it. They were astounded what to do on their own. Nevertheless somehow an experimental network went online in December 1969 with four nodes at UCLA UCSB SRI and the University of Utah. These four were chosen because all had large number of ARPA contracts and all had different and completely incompatible host computers . The first hosttohost message had been sent two months earlier from the UCLA node by team led by Len Kleinrock to the SRI node. The network grew quickly as more IMPs were delivered and ARPANET grew in the first 3 years. March 1971. April 1972. September 1972. In addition to helping the fledgling ARPANET grow ARPA also funded re search on the use of satellite networks and mobile packet radio networks. In one now famous demonstration truck driving around in California used the packet radio network to send messages to SRI which were then forwarded over the ARPANET to the East Coast where they were shipped to University College in London over the satellite network. This allowed researcher in the truck to use computer in London while driving around in California. This experiment also demonstrated that the existing ARPANET protocols were not suitable for running over different networks. This observation led to more research on protocols culminating with the invention of the TCPIP model and protocols . TCPIP was specifically designed to handle communication over internetworks something becoming increasingly important as more and more networks were hooked up to the ARPANET. To encourage adoption of these new protocols ARPA awarded several con tracts to implement TCPIP on different computer platforms including IBM DEC and HP systems as well as for Berkeley UNIX. Researchers at the Univer sity of California at Berkeley rewrote TCPIP with new programming interface called sockets for the upcoming 4.2BSD release of Berkeley UNIX. They also wrote many application utility and management programs to show how con venient it was to use the network with sockets. The timing was perfect. Many universities had just acquired second or third VAX computer and LAN to connect them but they had no networking software. When 4.2BSD came along with TCPIP sockets and many network utilities the complete package was adopted immediately. Furthermore with TCPIP it was easy for the LANs to connect to the ARPANET and many did. During the 1980s additional networks especially LANs were connected to the ARPANET. As the scale increased finding hosts became increasingly expen sive so DNS was created to organize machines into do mains and map host names onto IP addresses. Since then DNS has become generalized distributed database system for storing variety of information relat ed to naming. We will study it in detail in Chap. 7. By the late 1970s NSF saw the enor mous impact the ARPANET was having on university research allowing scien tists across the country to share data and collaborate on research projects. How ever to get on the ARPANET university had to have research contract with the DoD. Many did not have contract. NSFs initial response was to fund the Computer Science Network in 1981. It connected computer science de partments and industrial research labs to the ARPANET via dialup and leased lines. In the late 1980s the NSF went further and decided to design successor to the ARPANET that would be open to all university research groups. To have something concrete to start with NSF decided to build backbone network to connect its six supercomputer centers in San Diego Boulder Cham paign Pittsburgh Ithaca and Princeton. Each supercomputer was given little brother consisting of an LSI11 microcomputer called fuzzball. The fuzzballs were connected with 56kbps leased lines and formed the subnet the same hard ware technology the ARPANET used. The software technology was different however the fuzzballs spoke TCPIP right from the start making it the first NSF also funded some regional networks that connected to the backbone to allow users at thousands of universities research labs libraries and museums to access any of the supercomputers and to communicate with one another. The complete network including backbone and the regional networks was called NSFNET. It connected to the ARPANET through link between an IMP and fuzzball in the CarnegieMellon machine room. The first NSFNET NSFNET was an instantaneous success and was overloaded from the word go. NSF immediately began planning its successor and awarded contract to the Michiganbased MERIT consortium to run it. Fiber optic channels at 448 kbps backbone. IBM PCRTs were used as routers. This too was soon overwhelmed and by 1990 the second backbone was upgraded to 1.5 Mbps. As growth continued NSF realized that the government could not continue financing networking forever. Furthermore commercial organizations wanted to join but were forbidden by NSFs charter from using networks NSF paid for. Consequently NSF encouraged MERIT MCI and IBM to form nonprofit cor poration ANS as the first step along the road to commercialization. In 1990 ANS took over NSFNET and upgraded the 1.5Mbps links to 45 Mbps to form ANSNET. This network operated for 5 years and was then sold to America Online. But by then various companies were offer ing commercial IP service and it was clear the government should now get out of the networking business. To ease the transition and make sure every regional network could communi cate with every other regional network NSF awarded contracts to four different network operators to establish NAP . These operators were PacBell Ameritech MFS and Sprint . Every network operator that wanted to provide back bone service to the NSF regional networks had to connect to all the NAPs. This arrangement meant that packet originating on any regional network had choice of backbone carriers to get from its NAP to the destinations NAP. Con sequently the backbone carriers were forced to compete for the regional net works business on the basis of service and price which was the idea of course. As result the concept of single default backbone was replaced by commer cially driven competitive infrastructure. Many people like to criticize the Federal Government for not being innovative but in the area of networking it was DoD and NSF that created the infrastructure that formed the basis for the Internet and then handed it over to industry to operate. During the 1990s many other countries and regions also built national re search networks often patterned on the ARPANET and NSFNET. These in cluded EuropaNET and EBONE in Europe which started out with 2Mbps lines and then upgraded to 34Mbps lines. Eventually the network infrastructure in Europe was handed over to industry as well. The Internet has changed great deal since those early days. It exploded in size with the emergence of the World Wide Web in the early 1990s. Recent data from the Internet Systems Consortium puts the number of visible In ternet hosts at over 600 million. This guess is only lowball estimate but it far exceeds the few million hosts that were around when the first conference on the WWW was held at CERN in 1994. The way we use the Internet has also changed radically. Initially applications such as emailforacademics newsgroups remote login and file transfer dom inated. Later it switched to emailforeveryman then the Web and peertopeer content distribution such as the nowshuttered Napster. Now realtime media dis tribution social networks and microblogging are taking off. These switches brought richer kinds of media to the Internet and hence much more traffic. In fact the dominant traffic on the Internet seems to change with some regularity as for example new and better ways to work with music or The architecture of the Internet has also changed great deal as it has grown looks like today. The picture is complicated by continuous upheavals in the businesses of telephone companies cable companies and ISPs that often make it hard to tell who is doing what. One driver of these upheavals is telecom munications convergence in which one network is used for previously different uses. For example in triple play one company sells you telephony TV and Internet service over the same network connection on the assumption that this will save you money. Consequently the description given here will be of necessity somewhat simpler than reality. And what is true today may not be true tomorrow. Internet the computer is connected to an Internet Service Provider or simply ISP from who the user purchases Internet access or connectivity. This lets the computer exchange packets with all of the other accessible hosts on the Internet. The user might send packets to surf the Web or for any of thousand other uses it does not matter. There are many kinds of Internet access and they are usually distinguished by how much bandwidth they provide and how much they cost but the most important attribute is connectivity. common way to connect to an ISP is to use the phone line to your house in which case your phone company is your ISP. DSL short for Digital Subscriber Line reuses the telephone line that connects to your house for digital data transmission. The computer is connected to device called DSL modem that converts between digital packets and analog signals that can pass unhindered over the telephone line. At the other end device called DSLAM converts between signals and packets. is higherbandwidth way to use the local telephone line than to send bits over traditional telephone call instead of voice conversation. That is called dialup and done with different kind of modem at both ends. The word modem is short for modulator demodulator and refers to any device that converts between digi tal bits and analog signals. Another method is to send signals over the cable TV system. Like DSL this is way to reuse existing infrastructure in this case otherwise unused cable TV channels. The device at the home end is called cable modem and the device at the cable headend is called the CMTS . DSL and cable provide Internet access at rates from small fraction of megabitsec to multiple megabitsec depending on the system. These rates are much greater than dialup rates which are limited to 56 kbps because of the nar row bandwidth used for voice calls. Internet access at much greater than dialup speeds is called broadband. The name refers to the broader bandwidth that is used for faster networks rather than any particular speed. The access methods mentioned so far are limited by the bandwidth of the last mile or last leg of transmission. By running optical fiber to residences fast er Internet access can be provided at rates on the order of 10 to 100 Mbps. This design is called FTTH . For businesses in commercial areas it may make sense to lease highspeed transmission line from the offices Wireless is used for Internet access too. An example we will explore shortly is Mbps or higher to mobile phones and fixed subscribers in the coverage area. We can now move packets between the home and the ISP. We call the loca tion at which customer packets enter the ISP network for service the ISPs POP . We will next explain how packets are moved between the POPs of different ISPs. From this point on the system is fully digital and packet ISP networks may be regional national or international in scope. We have already seen that their architecture is made up of longdistance transmission lines that interconnect routers at POPs in the different cities that the ISPs serve. This equipment is called the backbone of the ISP. If packet is destined for host served directly by the ISP that packet is routed over the backbone and delivered to the host. Otherwise it must be handed over to another ISP. ISPs connect their networks to exchange traffic at IXPs . The connected ISPs are said to peer with each other. There are many ISP networks overlap geographically. Basically an IXP is room full of routers at least one per ISP. LAN in the room connects all the routers so packets can be forwarded from any ISP backbone to any other ISP backbone. IXPs can be large and independently owned facilities. One of the largest is the Amsterdam In ternet Exchange to which hundreds of ISPs connect and through which they exchange hundreds of gigabitssec of traffic. The peering that happens at IXPs depends on the business relationships be tween ISPs. There are many possible relationships. For example small ISP might pay larger ISP for Internet connectivity to reach distant hosts much as customer purchases service from an Internet provider. In this case the small ISP is said to pay for transit. Alternatively two large ISPs might decide to exchange traffic so that each ISP can deliver some traffic to the other ISP without having to pay for transit. One of the many paradoxes of the Internet is that ISPs who pub licly compete with one another for customers often privately cooperate to do peer ing . The path packet takes through the Internet depends on the peering choices of the ISPs. If the ISP delivering packet peers with the destination ISP it might deliver the packet directly to its peer. Otherwise it might route the packet to the nearest place at which it connects to paid transit provider so that provider can the path packet takes will not be the shortest path through the Internet. At the top of the food chain are small handful of companies like ATT and Sprint that operate large international backbone networks with thousands of rout ers connected by highbandwidth fiber optic links. These ISPs do not pay for transit. They are usually called tier 1 ISPs and are said to form the backbone of the Internet since everyone else must connect to them to be able to reach the en Companies that provide lots of content such as Google and Yahoo locate their computers in data centers that are well connected to the rest of the Internet. These data centers are designed for computers not humans and may be filled data centers let customers put equipment such as servers at ISP POPs so that short fast connections can be made between the servers and the ISP backbones. The Internet hosting industry has become increasingly virtualized so that it is now common to rent virtual machine that is run on server farm instead of installing physical computer. These data centers are so large that electricity is major cost so data centers are some times built in areas where electricity is cheap. This ends our quick tour of the Internet. We will have great deal to say about the individual components and their design algorithms and protocols in means to be on the Internet is changing. It used to be that machine was on the Internet if it ran the TCPIP protocol stack had an IP address and could send IP packets to all the other machines on the Internet. However ISPs often reuse IP addresses depending on which computers are in use at the moment and home networks often share one IP address between multiple computers. This practice undermines the second condition. Security measures such as firewalls can also partly block computers from receiving packets undermining the third condition. Despite these difficulties it makes sense to regard such machines as being on the Internet while they are connected to their ISPs. Also worth mentioning in passing is that some companies have interconnected all their existing internal networks often using the same technology as the Inter net. These intranets are typically accessible only on company premises or from company notebooks but otherwise work the same way as the Internet. People love to talk on the phone even more than they like to surf the Internet and this has made the mobile phone network the most successful network in the world. It has more than four billion subscribers worldwide. To put this number in perspective it is roughly 60 of the worlds population and more than the number of Internet hosts and fixed telephone lines combined . The architecture of the mobile phone network has changed greatly over the past 40 years along with its tremendous growth. Firstgeneration mobile phone systems transmitted voice calls as continuously varying signals rather than sequences of bits. AMPS which was deployed in the United States in 1982 was widely used first generation system. Secondgeneration mobile phone systems switched to trans mitting voice calls in digital form to increase capacity improve security and offer text messaging. GSM which was deployed starting in 1991 and has become the most widely used mobile phone system in the world is 2G system. The third generation or 3G systems were initially deployed in 2001 and offer both digital voice and broadband digital data services. They also come with lot of jargon and many different standards to choose from. 3G is loosely defined by providing rates of at least 2 Mbps for stationary or walking users and 384 kbps in moving vehicle. UMTS also called WCDMA is the main Mbps on the downlink and almost 6 Mbps on the uplink. Future releases will use multiple antennas and radios to provide even greater speeds for users. The scarce resource in 3G systems as in 2G and 1G systems before them is radio spectrum. Governments license the right to use parts of the spectrum to the mobile phone network operators often using spectrum auction in which network operators submit bids. Having piece of licensed spectrum makes it easier to de sign and operate systems since no one else is allowed transmit on that spectrum but it often costs serious amount of money. In the UK in 2000 for example five 3G licenses were auctioned for total of about 40 billion. It is the scarcity of spectrum that led to the cellular network design shown in interference between users the coverage area is divided into cells. Within cell users are assigned channels that do not interfere with each other and do not cause too much interference for adjacent cells. This allows for good reuse of the spec trum or frequency reuse in the neighboring cells which increases the capacity of the network. In 1G systems which carried each voice call on specific fre quency band the frequencies were carefully chosen so that they did not conflict with neighboring cells. In this way given frequency might only be reused once in several cells. Modern 3G systems allow each cell to use all frequencies but in way that results in tolerable level of interference to the neighboring cells. There are variations on the cellular design including the use of directional or sec tored antennas on cell towers to further reduce interference but the basic idea is The architecture of the mobile phone network is very different than that of the for the radio communication protocol that is used over the air between the mobile device and the cellular base station. Advances in the air in terface over the past decades have greatly increased wireless data rates. The UMTS air interface is based on Code Division Multiple Access tech nique that we will study in Chap. 2. The cellular base station together with its controller forms the radio access network. This part is the wireless side of the mobile phone network. The con troller node or RNC controls how the spectrum is used. The base station implements the air interface. It is called Node tem porary label that stuck. The rest of the mobile phone network carries the traffic for the radio access network. It is called the core network. The UMTS core network evolved from the core network used for the 2G GSM system that came before it. However something surprising is happening in the UMTS core network. Since the beginning of networking war has been going on between the peo ple who support packet networks and the people who support circuit networks . The main proponents of packets come from the Internet community. In connectionless design every packet is routed independently of every other packet. As consequence if some routers go down during session no harm will be done as long as the system can Radio access network the destination even if it is different from that which previous packets used. The circuit camp comes from the world of telephone companies. In the tele phone system caller must dial the called partys number and wait for connec tion before talking or sending data. This connection setup establishes route through the telephone system that is maintained until the call is terminated. All words or packets follow the same route. If line or switch on the path goes down the call is aborted making it less fault tolerant than connectionless design. The advantage of circuits is that they can support quality of service more easi ly. By setting up connection in advance the subnet can reserve resources such as link bandwidth switch buffer space and CPU. If an attempt is made to set up call and insufficient resources are available the call is rejected and the caller gets kind of busy signal. In this way once connection has been set up the connection will get good service. With connectionless network if too many packets arrive at the same router at the same moment the router will choke and probably lose packets. The sender will eventually notice this and resend them but the quality of service will be jerky and unsuitable for audio or video unless the network is lightly loaded. Needless to say providing adequate audio quality is something telephone companies care about very much hence their preference for connections. equipment in the core network. This shows the mobile phone network in transi tion with mobile phone companies able to implement one or sometimes both of the alternatives. Older mobile phone networks used circuitswitched core in the style of the traditional phone network to carry voice calls. This legacy is seen in the UMTS network with the MSC GMSC and MGW elements that set up connections over circuitswitched core network such as the PSTN . Data services have become much more important part of the mobile phone network than they used to be starting with text messaging and early packet data services such as GPRS in the GSM system. These older data services ran at tens of kbps but users wanted more. Newer mo bile phone networks carry packet data at rates of multiple Mbps. For comparison voice call is carried at rate of 64 kbps typically 34x less with compression. To carry all this data the UMTS core network nodes connect directly to packetswitched network. The SGSN and the GGSN deliver data packets to and from mobiles and interface to external packet networks such as the Internet. This transition is set to continue in the mobile phone networks that are now being planned and deployed. Internet protocols are even used on mobiles to set up connections for voice calls over packet data network in the manner of voice overIP. IP and packets are used all the way from the radio access through to the core network. Of course the way that IP networks are designed is also changing audio and jerky video would not impress paying customers. We will return to this subject in Chap. 5. Another difference between mobile phone networks and the traditional Inter net is mobility. When user moves out of the range of one cellular base station and into the range of another one the flow of data must be rerouted from the old to the new cell base station. This technique is known as handover or handoff Either the mobile device or the base station may request handover when the quality of the signal drops. In some cell networks usually those based on CDMA technology it is possible to connect to the new base station before disconnecting from the old base station. This improves the connection quality for the mobile be cause there is no break in service the mobile is actually connected to two base stations for short while. This way of doing handover is called soft handover to distinguish it from hard handover in which the mobile disconnects from the old base station before connecting to the new one. coming call. Each mobile phone network has HSS profile information that is used for authentication and authorization. In this way each mobile can be found by contacting the HSS. security much more seriously than Internet companies for long time because of the need to bill for service and avoid fraud. Unfortunately that is not saying much. Nevertheless in the evolution from 1G through 3G technologies mobile phone companies have been able to roll out some basic security mechan isms for mobiles. Starting with the 2G GSM system the mobile phone was divided into handset and removable chip containing the subscribers identity and account information. The chip is informally called SIM card short for Subscriber Identity Module. SIM cards can be switched to different handsets to activate them and they provide basis for security. When GSM customers travel to other countries on vacation or business they often bring their handsets but buy new SIM card for few dollars upon arrival in order to make local calls with no roaming To reduce fraud information on SIM cards is also used by the mobile phone network to authenticate subscribers and check that they are allowed to use the net work. With UMTS the mobile also uses the information on the SIM card to check that it is talking to legitimate network. Another aspect of security is privacy. Wireless signals are broadcast to all nearby receivers so to make it difficult to eavesdrop on conversations crypto graphic keys on the SIM card are used to encrypt transmissions. This approach provides much better privacy than in 1G systems which were easily tapped but is not panacea due to weaknesses in the encryption schemes. Mobile phone networks are destined to play central role in future networks. They are now more about mobile broadband applications than voice calls and this has major implications for the air interfaces core network architecture and secu rity of future networks. 4G technologies that are faster and better are on the draw ing board under the name of LTE even as 3G design and deployment continues. Other wireless technologies also offer broadband In ternet access to fixed and mobile clients notably 802.16 networks under the com mon name of WiMAX. It is entirely possible that LTE and WiMAX are on col lision course with each other and it is hard to predict what will happen to them. Almost as soon as laptop computers appeared many people had dream of walking into an office and magically having their laptop computer be connected to the Internet. Consequently various groups began working on ways to accomplish this goal. The most practical approach is to equip both the office and the laptop computers with shortrange radio transmitters and receivers to allow them to talk. Work in this field rapidly led to wireless LANs being marketed by variety of companies. The trouble was that no two of them were compatible. The prolifera tion of standards meant that computer equipped with brand radio would not work in room equipped with brand base station. In the mid 1990s the indus try decided that wireless LAN standard might be good idea so the IEEE com mittee that had standardized wired LANs was given the task of drawing up wire less LAN standard. The first decision was the easiest what to call it. All the other LAN standards had numbers like 802.1 802.2 and 802.3 up to 802.10 so the wireless LAN stan dard was dubbed 802.11. common slang name for it is WiFi but it is an impor tant standard and deserves respect so we will call it by its proper name 802.11. The rest was harder. The first problem was to find suitable frequency band that was available preferably worldwide. The approach taken was the opposite of that used in mobile phone networks. Instead of expensive licensed spectrum 802.11 systems operate in unlicensed bands such as the ISM bands defined by ITUR . All devices are allowed to use this spectrum provided that they limit their transmit power to let different devices coexist. Of course this means that 802.11 radios may find themselves competing with cordless phones garage door openers and microwave ovens. 802.11 networks are made up of clients such as laptops and mobile phones and infrastructure called APs that is installed in buildings. Access points are sometimes called base stations. The access points connect to the wired network and all communication between clients goes through an access point. It is also possible for clients that are in radio range to talk directly such as two com puters in an office without an access point. This arrangement is called an ad hoc network. It is used much less often than the access point mode. Both modes are 802.11 transmission is complicated by wireless conditions that vary with even small changes in the environment. At the frequencies used for 802.11 radio sig nals can be reflected off solid objects so that multiple echoes of transmission may reach receiver along different paths. The echoes can cancel or reinforce each other causing the received signal to fluctuate greatly. This phenomenon is The key idea for overcoming variable wireless conditions is path diversity or the sending of information along multiple independent paths. In this way the To wired network information is likely to be received even if one of the paths happens to be poor due to fade. These independent paths are typically built into the digital modula tion scheme at the physical layer. Options include using different frequencies cross the allowed band following different spatial paths between different pairs of antennas or repeating bits over different periods of time. Nonfaded signal standard defined wireless LAN that ran at either 1 Mbps or 2 Mbps by hopping between frequencies or spreading the signal across the allowed spectrum. Almost immediately people complained that it was too slow so work began on faster standards. The spread spectrum design was extended and became the 802.11b standard running at rates up to 11 Mbps. The 802.11a and 802.11g standards switched to different modulation scheme called OFDM . It divides wide band of spectrum into many narrow slices over which different bits are sent in parallel. This improved scheme which we will study in Chap. 2 boosted the 802.11ag bit rates up to 54 Mbps. That is significant increase but people still wanted more It uses wider frequency bands and up to four antennas per computer to achieve rates up to 450 Mbps. Since wireless is inherently broadcast medium 802.11 radios also have to deal with the problem that multiple transmissions that are sent at the same time will collide which may interfere with reception. To handle this problem 802.11 uses CSMA scheme that draws on ideas from classic wired Ethernet which ironically drew from an early wireless network developed in Hawaii and called ALOHA. Computers wait for short random interval before transmitting and defer their transmissions if they hear that some one else is already transmitting. This scheme makes it less likely that two com puters will send at the same time. It does not work as well as in the case of wired transmitting to computer but the radio range of As transmitter is too short to reach computer . If wants to transmit to it can listen before starting but the fact that it does not hear anything does not mean that its transmission will succeed. The inability of to hear before starting causes some collisions to oc cur. After any collision the sender then waits another longer random delay and enough in practice. Another problem is that of mobility. If mobile client is moved away from the access point it is using and into the range of different access point some way of handing it off is needed. The solution is that an 802.11 network can consist of multiple cells each with its own access point and distribution system that con nects the cells. The distribution system is often switched Ethernet but it can use any technology. As the clients move they may find another access point with better signal than the one they are currently using and change their association. From the outside the entire system looks like single wired LAN. That said mobility in 802.11 has been of limited value so far compared to mobility in the mobile phone network. Typically 802.11 is used by nomadic cli Mobility is not really needed for nomadic usage. Even when 802.11 mobility is used it extends over single 802.11 network which might cover at most large building. Future schemes will need to provide mobility across different networks and across different technologies . broadcast it is easy for nearby computers to receive packets of information that were not intended for them. To prevent this the 802.11 standard included an en cryption scheme known as WEP . The idea was to make wireless security like that of wired security. It is good idea but unfor tunately the scheme was flawed and soon broken . It has since been replaced with newer schemes that have different cryptographic details in the 802.11i standard also called WiFi Protected Access initially called WPA but now replaced by WPA2. 802.11 has caused revolution in wireless networking that is set to continue. Beyond buildings it is starting to be installed in trains planes boats and automo biles so that people can surf the Internet wherever they go. Mobile phones and all manner of consumer electronics from game consoles to digital cameras can com municate with it. We will come back to it in detail in Chap. 4. The networks we have studied so far are made up of computing devices that are easy to recognize from computers to mobile phones. With Radio Frequency IDentification everyday objects can also be part of computer network. An RFID tag looks like postage stampsized sticker that can be affixed to an object so that it can be tracked. The object might be cow passport book or shipping pallet. The tag consists of small microchip with unique identifier and an antenna that receives radio transmissions. RFID readers installed at tracking points find tags when they come into range and interrogate identities managing the supply chain timing races and replacing barcodes. There are many kinds of RFID each with different properties but perhaps the most fascinating aspect of RFID technology is that most RFID tags have neither an electric plug nor battery. Instead all of the energy needed to operate them is supplied in the form of radio waves by RFID readers. This technology is called passive RFID to distinguish it from the active RFID in which there is power source on the tag. One common form of RFID is UHF RFID . It is used on shipping pallets and some drivers licenses. Readers send signals in the 902928 MHz band in the United States. Tags communicate at distances of several meters by changing the way they reflect the reader signals the reader is able to pick up these reflections. This way of operating is called backscatter. Another popular kind of RFID is HF RFID . It operates at 13.56 MHz and is likely to be in your passport credit cards books and noncontact payment systems. HF RFID has short range typically meter or less because the physical mechanism is based on induction rather than back scatter. There are also other forms of RFID using other frequencies such as LF RFID which was developed before HF RFID and used for animal tracking. It is the kind of RFID likely to be in your cat. RFID readers must somehow solve the problem of dealing with multiple tags within reading range. This means that tag cannot simply respond when it hears reader or the signals from multiple tags may collide. The solution is similar to the approach taken in 802.11 tags wait for short random interval before re sponding with their identification which allows the reader to narrow down indivi dual tags and interrogate them further. Security is another problem. The ability of RFID readers to easily track an ob ject and hence the person who uses it can be an invasion of privacy. Unfor tunately it is difficult to secure RFID tags because they lack the computation and communication power to run strong cryptographic algorithms. Instead weak measures like passwords are used. If an identity card can be remotely read by an official at border what is to stop the same card from being tracked by other people without your knowledge Not much. RFID tags started as identification chips but are rapidly turning into full later queried so that information about what has happened to the tagged object can be stored with it. Rieback et al. demonstrated that this means that all passport might be used to spread an RFID virus. step up in capability from RFID is the sensor network. Sensor networks are deployed to monitor aspects of the physical world. So far they have mostly been used for scientific experimentation such as monitoring bird habitats vol canic activity and zebra migration but business applications including healthcare monitoring equipment for vibration and tracking of frozen refrigerated or other wise perishable goods cannot be too far behind. Sensor nodes are small computers often the size of key fob that have tem perature vibration and other sensors. Many nodes are placed in the environment that is to be monitored. Typically they have batteries though they may scavenge energy from vibrations or the sun. As with RFID having enough energy is key challenge and the nodes must communicate carefully to be able to deliver their sensor information to an external collection point. common strategy is for the This design is called multihop network. RFID and sensor networks are likely to become much more capable and per vasive in the future. Researchers have already combined the best of both technolo gies by prototyping programmable RFID tags with light movement and other sensors . Many network vendors and suppliers exist each with its own ideas of how things should be done. Without coordination there would be complete chaos and users would get nothing done. The only way out is to agree on some network standards. Not only do good standards allow different computers to communicate but they also increase the market for products adhering to the standards. larger market leads to mass production economies of scale in manufacturing better im plementations and other benefits that decrease price and further increase ac world of international standardization. But let us first discuss what belongs in standard. reasonable person might assume that standard tells you how pro tocol should work so that you can do good job of implementing it. That person Standards define what is needed for interoperability no more no less. That lets the larger market emerge and also lets companies compete on the basis of how good their products are. For example the 802.11 standard defines many transmission rates but does not say when sender should use which rate which is key factor in good performance. That is up to whoever makes the product. Often getting to interoperability this way is difficult since there are many imple mentation choices and standards usually define many options. For 802.11 there trade group called the WiFi Alliance was started to work on interoperability with in the 802.11 standard. Similarly protocol standard defines the protocol over the wire but not the service interface inside the box except to help explain the protocol. Real service interfaces are often proprietary. For example the way TCP interfaces to IP within computer does not matter for talking to remote host. It only matters that the re mote host speaks TCPIP. In fact TCP and IP are commonly implemented toget her without any distinct interface. That said good service interfaces like good APIs are valuable for getting protocols used and the best ones can become very popular. Standards fall into two categories de facto and de jure. De facto standards are those that have just happened without any formal plan. HTTP the protocol on which the Web runs started life as de facto stan dard. It was part of early WWW browsers developed by Tim BernersLee at CERN and its use took off with the growth of the Web. Bluetooth is another ex ample. It was originally developed by Ericsson but now everyone is using it. De jure standards in contrast are adopted through the rules of some formal standardization body. International standardization authori ties are generally divided into two classes those established by treaty among national governments and those comprising voluntary nontreaty organizations. In the area of computer network standards there are several organizations of each type notably ITU ISO IETF and IEEE all of which we will discuss below. In practice the relationships between standards companies and stan dardization bodies are complicated. De facto standards often evolve into de jure standards especially if they are successful. This happened in the case of HTTP which was quickly picked up by IETF. Standards bodies often ratify each others standards in what looks like patting one another on the back to increase the market for technology. These days many ad hoc business alliances that are formed around particular technologies also play significant role in developing Partnership Project is collaboration between telecommunications associations that drives the UMTS 3G mobile phone standards. country to country. At one extreme is the United States which has over 2000 sep arate privately owned telephone companies. few more were added with the breakup of ATT in 1984 and the Telecommunications Act of 1996 that overhauled regulation to foster competition. At the other extreme are countries in which the national government has complete monopoly on all communication including the mail telegraph tele phone and often radio and television. Much of the world falls into this category. others it is simply branch of the government usually known as the PTT . Worldwide the trend is toward liberal ization and competition and away from government monopoly. Most European countries have now privatized their PTTs but elsewhere the process is still only slowly gaining steam. With all these different suppliers of services there is clearly need to provide compatibility on worldwide scale to ensure that people in one country can call their counterparts in another one. Actually this need has existed for long time. In 1865 representatives from many European governments met to form the predecessor to todays ITU . Its job was to standardize international telecommunications which in those days meant telegraphy. Even then it was clear that if half the countries used Morse code and the other half used some other code there was going to be prob lem. When the telephone was put into international service ITU took over the job of standardizing telephony as well. became an agency of the United Nations. ITU has about 200 governmental members including almost every member of the United Nations. Since the United States does not have PTT somebody else had to represent it in ITU. This task fell to the State Department probably on the grounds that ITU had to do with foreign countries the State Departments spe cialty. ITU also has more than 700 sector and associate members. They include telephone companies telecom equipment manu facturers computer vendors chip manufacturers and other interested com panies . ITU has three main sectors. We will focus primarily on ITUT the Telecom munications Standardization Sector which is concerned with telephone and data communication systems. Before 1993 this sector was called CCITT which is an acronym for its French name Comite Consultatif International Telegraphique et ITUR the Radiocommunications Sector is concerned with coordinating the use by competing interest groups of radio frequencies worldwide. The other sector is ITUD the Development Sector. It promotes the development of information and communication technologies to narrow the digital divide between countries with effective access to the information technologies and coun tries with limited access. ITUTs task is to make technical recommendations about telephone tele graph and data communication interfaces. These often become internationally recognized standards though technically the recommendations are only sugges tions that governments can adopt or ignore as they wish . In practice country that wishes to adopt telephone standard different from that used by the rest of the world is free to do so but at the price of cutting itself off from everyone else. This might work for North Korea but elsewhere it would be Study Groups often as large as 400 people that cover topics ranging from tele phone billing to multimedia services to security. SG 15 for example standardizes the DSL technologies popularly used to connect to the Internet. In order to make it possible to get anything at all done the Study Groups are divided into Working Parties which are in turn divided into Expert Teams which are in turn divided into ad hoc groups. Once bureaucracy always bureaucracy. Despite all this ITUT actually does get things done. Since its inception it has produced more than 3000 recommendations many of which are widely used in practice. For example Recommendation .264 is widely used for video compression and .509 public key certificates are used for secure Web browsing and digitally signed email. As the field of telecommunications completes the transition started in the 1980s from being entirely national to being entirely global standards will become increasingly important and more and more organizations will want to become involved in setting them. For more information about ITU see Irmer . International standards are produced and published by ISO voluntary nontreaty organization founded in 1946. Its members are the national standards organizations of the 157 member countries. These members include ANSI BSI AFNOR DIN and 153 others. bolts to telephone pole coatings with all frequencies above this cutoff frequency attenuated. The width of the frequency range transmitted without being strongly attenuated is called the bandwidth. In practice the cutoff is not really sharp so often the quoted bandwidth is from 0 to the frequency at which the received power has fallen by half. The bandwidth is physical property of the transmission medium that de pends on for example the construction thickness and length of wire or fiber. Filters are often used to further limit the bandwidth of signal. 802.11 wireless channels are allowed to use up to roughly 20 MHz for example so 802.11 radios filter the signal bandwidth to this size. As another example traditional television channels occupy 6 MHz each on wire or over the air. This filtering lets more signals share given region of spectrum which improves the overall ef ficiency of the system. It means that the frequency range for some signals will not start at zero but this does not matter. The bandwidth is still the width of the band of frequencies that are passed and the information that can be carried de pends only on this width and not on the starting and ending frequencies. Signals that run from 0 up to maximum frequency are called baseband signals. Signals that are shifted to occupy higher range of frequencies as is the case for all wire less transmissions are called passband signals. were so low that only the lowest frequencies were transmitted that the amount of information that sig nal such as an electromagnetic wave can carry depends on the received power and networking people like fiber optics so much. Many GHz of bandwidth are avail able to tap for data transmission in the microwave band and even more in fiber because it is further to the right in our logarithmic scale. As an example consider Eq. to find the start and end frequencies from the start and end wavelengths we find the frequency range to be about 30000 GHz. With reasonable signal tonoise ratio of 10 dB this is 300 Tbps. Most transmissions use relatively narrow frequency band . They concentrate their signals in this narrow band to use the spectrum efficiently and obtain reasonable data rates by transmitting with enough power. However in some cases wider band is used with three variations. In frequency hopping spread spectrum the transmitter hops from frequency to frequency hundreds of times per second. It is popular for military communication because it makes transmissions hard to detect and next to impossible to jam. It also offers good resistance to multipath fading and narrowband interference because the receiver will not be stuck on an impaired frequency for long enough to shut down commu nication. This robustness makes it useful for crowded parts of the spectrum such as the ISM bands we will describe shortly. This technique is used commercially As curious footnote the technique was coinvented by the Austrianborn sex goddess Hedy Lamarr the first woman to appear nude in motion picture on the piano. For their invention she and her friend the mu sical composer George Antheil received . patent 2292387. However they were unable to convince the . Navy that their invention had any practical use and never received any royalties. Only years after the patent expired did it be second form of spread spectrum direct sequence spread spectrum uses code sequence to spread the data signal over wider frequency band. It is widely used commercially as spectrally efficient way to let multiple signals share the same frequency band. These signals can be given different codes method called CDMA that we will return to later in this chap forms the basis of 3G mobile phone networks and is also used in GPS . Even without different codes direct sequence spread spec trum like frequency hopping spread spectrum can tolerate narrowband inter ference and multipath fading because only fraction of the desired signal is lost. It is used in this role in older 802.11b wireless LANs. For fascinating and de tailed history of spread spectrum communication see Scholtz . third method of communication with wider band is UWB communication. UWB sends series of rapid pulses varying their positions to communicate information. The rapid transitions lead to signal that is spread thinly over very wide frequency band. UWB is defined as signals that have bandwidth of at least 500 MHz or at least 20 of the center frequency of width UWB has the potential to communicate at high rates. Because it is spread across wide band of frequencies it can tolerate substantial amount of relative ly strong interference from other narrowband signals. Just as importantly since UWB has very little energy at any given frequency when used for shortrange transmission it does not cause harmful interference to those other narrowband radio signals. It is said to underlay the other signals. This peaceful coexistence has led to its application in wireless PANs that run at up to 1 Gbps although com mercial success has been mixed. It can also be used for imaging through solid ob We will now discuss how the various parts of the electromagnetic spectrum of narrow frequency band unless otherwise stated. Radio frequency waves are easy to generate can travel long distances and can penetrate buildings easily so they are widely used for communication both indoors and outdoors. Radio waves also are omnidirectional meaning that they travel in all directions from the source so the transmitter and receiver do not have to be carefully aligned physically. Sometimes omnidirectional radio is good but sometimes it is bad. In the 1970s General Motors decided to equip all its new Cadillacs with computercon trolled antilock brakes. When the driver stepped on the brake pedal the computer pulsed the brakes on and off instead of locking them on hard. One fine day an Ohio Highway Patrolman began using his new mobile radio to call headquarters and suddenly the Cadillac next to him began behaving like bucking bronco. When the officer pulled the car over the driver claimed that he had done nothing and that the car had gone crazy. Eventually pattern began to emerge Cadillacs would sometimes go berserk but only on major highways in Ohio and then only when the Highway Patrol was watching. For long long time General Motors could not understand why Cadil lacs worked fine in all the other states and also on minor roads in Ohio. Only after much searching did they discover that the Cadillacs wiring made fine an tenna for the frequency used by the Ohio Highway Patrols new radio system. The properties of radio waves are frequency dependent. At low frequencies radio waves pass through obstacles well but the power falls off sharply with dis tance from the sourceat least as fast as 1r 2 in airas the signal energy is spread more thinly over larger surface. This attenuation is called path loss. At high frequencies radio waves tend to travel in straight lines and bounce off obsta cles. Path loss still reduces power though the received signal can depend strongly on reflections as well. Highfrequency radio waves are also absorbed by rain and other obstacles to larger extent than are lowfrequency ones. At all frequencies radio waves are subject to interference from motors and other electrical equip It is interesting to compare the attenuation of radio waves to that of signals in guided media. With fiber coax and twisted pair the signal drops by the same fraction per unit distance for example 20 dB per 100m for twisted pair. With dB per doubling in free space. This behavior means that radio waves can travel long distances and interference between users is problem. For this reason all governments tightly regulate the use of radio transmitters with few notable ex In the VLF LF and MF bands radio waves follow the ground as illustrated frequencies less at the higher ones. AM radio broadcasting uses the MF band which is why the ground waves from Boston AM radio stations cannot be heard easily in New York. Radio waves in these bands pass through buildings easily which is why portable radios work indoors. The main problem with using these bands for data communication is their low bandwidth . ture of the earth. In the HF band they bounce off the ionosphere. In the HF and VHF bands the ground waves tend to be absorbed by the earth. However the waves that reach the ionosphere layer of charged particles cir cling the earth at height of 100 to 500 km are refracted by it and sent back to can bounce several times. Amateur radio operators use these bands to talk long distance. The military also communicate in the HF and VHF bands. Above 100 MHz the waves travel in nearly straight lines and can therefore be narrowly focused. Concentrating all the energy into small beam by means of parabolic antenna gives much higher signal tonoise ratio but the transmitting and receiving antennas must be accurately aligned with each other. In addition this directionality allows multiple trans mitters lined up in row to communicate with multiple receivers in row without interference provided some minimum spacing rules are observed. Before fiber optics for decades these microwaves formed the heart of the longdistance tele phone transmission system. In fact MCI one of ATTs first competitors after it was deregulated built its entire system with microwave communications passing between towers tens of kilometers apart. Even the companys name reflected this . MCI has since gone over to fiber and through long series of corporate mergers and bankruptcies in the telecommunications shuffle has become part of Verizon. Microwaves travel in straight line so if the towers are too far apart the earth will get in the way . Thus re peaters are needed periodically. The higher the towers are the farther apart they can be. The distance between repeaters goes up very roughly with the square root of the tower height. For 100meterhigh towers repeaters can be 80 km apart. Unlike radio waves at lower frequencies microwaves do not pass through buildings well. In addition even though the beam may be well focused at the transmitter there is still some divergence in space. Some waves may be refracted off lowlying atmospheric layers and may take slightly longer to arrive than the direct waves. The delayed waves may arrive out of phase with the direct wave and thus cancel the signal. This effect is called multipath fading and is often serious problem. It is weather and frequency dependent. Some operators keep 10 of their channels idle as spares to switch on when multipath fading tem porarily wipes out some frequency band. The demand for more and more spectrum drives operators to yet higher fre quencies. Bands up to 10 GHz are now in routine use but at about 4 GHz new problem sets in absorption by water. These waves are only few centimeters long and are absorbed by rain. This effect would be fine if one were planning to build huge outdoor microwave oven for roasting passing birds but for communi cation it is severe problem. As with multipath fading the only solution is to shut off links that are being rained on and route around them. telephone communication mobile phones television distribution and other pur poses that severe shortage of spectrum has developed. It has several key advan tages over fiber. The main one is that no right of way is needed to lay down cables. By buying small plot of ground every 50 km and putting microwave tower on it one can bypass the telephone system entirely. This is how MCI man aged to get started as new longdistance telephone company so quickly. Putting up two simple towers and putting antennas on each one may be cheaper than burying 50 km of fiber through congested urban area or up over mountain and it may also be cheaper than leasing the telephone com panys fiber especially if the telephone company has not yet even fully paid for the copper it ripped out when it put in the fiber. To prevent total chaos there are national and international agreements about who gets to use which frequencies. Since everyone wants higher data rate everyone wants more spectrum. National governments allocate spectrum for AM and FM radio television and mobile phones as well as for telephone companies police maritime navigation military government and many other competing so devices that work in multiple countries can be manufactured. However coun tries are not bound by ITURs recommendations and the FCC . Even when piece of spectrum has been allocated to some use such as which frequencies. Three algorithms were widely used in the past. The oldest al gorithm often called the beauty contest requires each carrier to explain why its proposal serves the public interest best. Government officials then decide which of the nice stories they enjoy most. Having some government official award prop erty worth billions of dollars to his favorite company often leads to bribery corr uption nepotism and worse. Furthermore even scrupulously honest govern ment official who thought that foreign company could do better job than any of the national companies would have lot of explaining to do. This observation led to algorithm 2 holding lottery among the interested companies. The problem with that idea is that companies with no interest in using the spectrum can enter the lottery. If say fast food restaurant or shoe store chain wins it can resell the spectrum to carrier at huge profit and with no risk. Bestowing huge windfalls on alert but otherwise random companies has been severely criticized by many which led to algorithm 3 auction off the bandwidth to the highest bidder. When the British government auctioned off the frequencies needed for thirdgeneration mobile systems in 2000 it expected to get about 4 billion. It actually received about 40 billion because the carriers got into feed ing frenzy scared to death of missing the mobile boat. This event switched on nearby governments greedy bits and inspired them to hold their own auctions. It worked but it also left some of the carriers with so much debt that they are close to bankruptcy. Even in the best cases it will take many years to recoup the completely different approach to allocating frequencies is to not allocate them at all. Instead let everyone transmit at will but regulate the power used so that stations have such short range that they do not interfere with each other. Accordingly most governments have set aside some frequency bands called the ISM bands for unlicensed usage. Garage door openers cordless phones radiocontrolled toys wireless mice and numerous other wireless household devices use the ISM bands. To minimize interference ISM bands limit their transmit power and use other techniques to spread their signals over range of frequencies. Devices may also need to take care to avoid interference with radar installations. United States for example the bands that networking devices use in practice able in most countries and widely used for 802.11bg and Bluetooth though it is subject to interference from microwave ovens and radar installations. The 5GHz Infrastructure bands. The 5GHz bands are relatively undeveloped but since they have the most bandwidth and are used by 802.11a they are quickly gaining The unlicensed bands have been roaring success over the past decade. The ability to use the spectrum freely has unleashed huge amount of innovation in wireless LANs and PANs evidenced by the widespread deployment of technolo gies such as 802.11 and Bluetooth. To continue this innovation more spectrum is needed. One exciting development in the . is the FCC decision in 2009 to allow unlicensed use of white spaces around 700 MHz. White spaces are fre quency bands that have been allocated but are not being used locally. The tran sition from analog to alldigital television broadcasts in the . in 2010 freed up white spaces around 700 MHz. The only difficulty is that to use the white spaces unlicensed devices must be able to detect any nearby licensed trans mitters including wireless microphones that have first rights to use the frequency Another flurry of activity is happening around the 60GHz band. The FCC opened 57 GHz to 64 GHz for unlicensed operation in 2001. This range is an enormous portion of spectrum more than all the other ISM bands combined so it can support the kind of highspeed networks that would be needed to stream highdefinition TV through the air across your living room. At 60 GHz radio waves are absorbed by oxygen. This means that signals do not propagate far making them well suited to shortrange networks. The high frequencies posed an initial challenge for equipment makers but products are now Unguided infrared waves are widely used for shortrange communication. The remote controls used for televisions VCRs and stereos all use infrared com munication. They are relatively directional cheap and easy to build but have major drawback they do not pass through solid objects. In general as we go from longwave radio toward visible light the waves behave more and more like light and less and less like radio. On the other hand the fact that infrared waves do not pass through solid walls well is also plus. It means that an infrared system in one room of building will not interfere with similar system in adjacent rooms or buildings you cannot control your neighbors television with your remote control. Furthermore securi ty of infrared systems against eavesdropping is better than that of radio systems precisely for this reason. Therefore no government license is needed to operate an infrared system in contrast to radio systems which must be licensed outside the ISM bands. Infrared communication has limited use on the desktop for ex ample to connect notebook computers and printers with the IrDA standard but it is not major player in the communication game. Unguided optical signaling or freespace optics has been in use for centuries. Paul Revere used binary optical signaling from the Old North Church just prior to his famous ride. more modern application is to connect the LANs in two build Optical signaling using lasers is inherently unidirectional so each end needs its own laser and its own photodetec tor. This scheme offers very high bandwidth at very low cost and is relatively secure because it is difficult to tap narrow laser beam. It is also relatively easy to install and unlike microwave transmission does not require an FCC license. The lasers strength very narrow beam is also its weakness here. Aiming laser beam 1 mm wide at target the size of pin head 500 meters away requires the marksmanship of latterday Annie Oakley. Usually lenses are put into the system to defocus the beam slightly. To add to the difficulty wind and tempera ture changes can distort the beam and laser beams also cannot penetrate rain or thick fog although they normally work well on sunny days. However many of One of the authors once attended conference at modern hotel in Europe at which the conference organizers thoughtfully provided room full of terminals to allow the attendees to read their email during boring presentations. Since the local PTT was unwilling to install large number of telephone lines for just 3 days the organizers put laser on the roof and aimed it at their universitys computer science building few kilometers away. They tested it the night before the conference and it worked perfectly. At 9 . on bright sunny day the link failed completely and stayed down all day. The pattern repeated itself the next two days. It was not until after the conference that the organizers discovered the problem heat from the sun during the daytime caused convection currents to rise the beam and made it dance around the detector much like shimmering road on hot day. The lesson here is that to work well in difficult conditions as well as good conditions unguided optical links need to be engineered with sufficient off the building tems. bidirectional system with two lasers is pictured here. Unguided optical communication may seem like an exotic networking tech nology today but it might soon become much more prevalent. We are surrounded by cameras and displays . Data communication can be layered on top of these displays by en coding information in the pattern at which LEDs turn on and off that is below the threshold of human perception. Communicating with visible light in this way is inherently safe and creates lowspeed network in the immediate vicinity of the display. This could enable all sorts of fanciful ubiquitous computing scenarios. The flashing lights on emergency vehicles might alert nearby traffic lights and vehicles to help clear path. Informational signs might broadcast maps. Even fes tive lights might broadcast songs that are synchronized with their display. In the 1950s and early 1960s people tried to set up communication systems by bouncing signals off metallized weather balloons. Unfortunately the received signals were too weak to be of any practical use. Then the . Navy noticed kind of permanent weather balloon in the skythe moonand built an opera tional system for shiptoshore communication by bouncing signals off it. Further progress in the celestial communication field had to wait until the first communication satellite was launched. The key difference between an artificial satellite and real one is that the artificial one can amplify the signals before sending them back turning strange curiosity into powerful communication Communication satellites have some interesting properties that make them attractive for many applications. In its simplest form communication satellite can be thought of as big microwave repeater in the sky. It contains several transponders each of which listens to some portion of the spectrum amplifies the incoming signal and then rebroadcasts it at another frequency to avoid inter ference with the incoming signal. This mode of operation is known as bent pipe. Digital processing can be added to separately manipulate or redirect data streams in the overall band or digital information can even be received by the sat ellite and rebroadcast. Regenerating signals in this way improves performance compared to bent pipe because the satellite does not amplify noise in the upward signal. The downward beams can be broad covering substantial fraction of the earths surface or narrow covering an area only hundreds of kilometers in diame According to Keplers law the orbital period of satellite varies as the radius of the orbit to the 32 power. The higher the satellite the longer the period. Near the surface of the earth the period is about 90 minutes. Consequently loworbit satellites pass out of view fairly quickly so many of them are needed to provide continuous coverage and ground antennas must track them. At an altitude of about 35800 km the period is 24 hours. At an altitude of 384000 km the period is about one month as anyone who has observed the moon regularly can testify. highly charged particles trapped by the earths magnetic field. Any satellite flying within them would be destroyed fairly quickly by the particles. These factors lead to three regions in which satellites can be placed safely. These regions and some Lower Van Allen belt altitude above the earth roundtrip delay time and number of satellites needed for global coverage. In 1945 the science fiction writer Arthur . Clarke calculated that satellite at an altitude of 35800 km in circular equatorial orbit would appear to remain motionless in the sky so it would not need to be tracked . He went on to describe complete communication system that used these geo stationary satellites including the orbits solar panels radio frequencies and launch procedures. Unfortunately he concluded that satellites were impractical due to the impossibility of putting powerhungry fragile vacuum tube amplifiers into orbit so he never pursued this idea further although he wrote some science fiction stories about it. The invention of the transistor changed all that and the first artificial commu nication satellite Telstar was launched in July 1962. Since then communication satellites have become multibillion dollar business and the only aspect of outer space that has become highly profitable. These highflying satellites are often called GEO satellites. With current technology it is unwise to have geostationary satellites spaced much closer than 2 degrees in the 360degree equatorial plane to avoid inter ference. With spacing of 2 degrees there can only be 3602 180 of these sat ellites in the sky at once. However each transponder can use multiple frequen cies and polarizations to increase the available bandwidth. process is highly political with countries barely out of the stone age demanding their orbit slots . Other countries however maintain that national property rights do not extend up to the moon and that no country has legal right to the orbit slots above its territory. To add to the fight commercial telecommunication is not the only application. Tele vision broadcasters governments and the military also want piece of the orbit Modern satellites can be quite large weighing over 5000 kg and consuming several kilowatts of electric power produced by the solar panels. The effects of solar lunar and planetary gravity tend to move them away from their assigned orbit slots and orientations an effect countered by onboard rocket motors. This finetuning activity is called station keeping. However when the fuel for the motors has been exhausted the satellite drifts and tumbles helplessly so it has to be turned off. Eventually the orbit decays and the satellite reenters the atmosphere and burns up . because the downlink transmissions interfere with existing microwave users. Consequently ITU has allocated certain frequency bands to satellite users. The commercial satellite traffic. Two frequency ranges are assigned in it the lower one for downlink traffic and the upper one for uplink traffic . To allow traffic to go both ways at the same time two channels are required. These channels are already overcrowded because they are also used by the common carriers for terrestrial microwave links. The and bands were added by international agreement in 2000. However they are narrow and also The nexthighest band available to commercial telecommunication carriers is the Ku band. This band is not congested and at its higher fre quencies satellites can be spaced as close as 1 degree. However another problem exists rain. Water absorbs these short microwaves well. Fortunately heavy storms are usually localized so using several widely separated ground stations in stead of just one circumvents the problem but at the price of extra antennas extra cables and extra electronics to enable rapid switching between stations. Band width has also been allocated in the Ka band for commercial satellite traffic but the equipment needed to use it is expensive. In addition to these com mercial bands many government and military bands also exist. modern satellite has around 40 transponders most often with 36MHz bandwidth. Usually each transponder operates as bent pipe but recent satellites have some onboard processing capacity allowing more sophisticated operation. In the earliest satellites the division of the transponders into channels was static the bandwidth was simply split up into fixed frequency bands. Nowadays each transponder beam is divided into time slots with various users taking turns. We will study these two techniques . These tiny terminals have 1meter or smaller an tennas and can put out about 1 watt of power. The uplink is generally good for up to 1 Mbps but the downlink is often up to several megabitssec. Direct broadcast satellite television uses this technol ogy for oneway transmission. In many VSAT systems the microstations do not have enough power to com municate directly with one another . Instead special ground station the hub with large highgain antenna is needed to relay traffic sender or the receiver has large antenna and powerful amplifier. The tradeoff is longer delay in return for having cheaper enduser stations. VSATs have great potential in rural areas. It is not widely appreciated but over half the worlds population lives more than hours walk from the nearest telephone. Stringing telephone wires to thousands of small villages is far beyond the budgets of most Third World governments but installing 1meter VSAT dishes powered by solar cells is often feasible. VSATs provide the technology that will wire the world. Communication satellites have several properties that are radically different from terrestrial pointtopoint links. To begin with even though signals to and from satellite travel at the speed of light the long roundtrip distance introduces substantial delay for GEO satellites. Depending on the distance between the user and the ground station and the elevation of the msec. typical value is 270 msec . For comparison purposes terrestrial microwave links have propagation delay of roughly 3 μseckm and coaxial cable or fiber optic links have delay of approximately 5 μseckm. The latter are slower than the former because electro magnetic signals travel faster in air than in solid materials. Another important property of satellites is that they are inherently broadcast media. It does not cost more to send message to thousands of stations within transponders footprint than it does to send to one. For some applications this property is very useful. For example one could imagine satellite broadcasting wide area. Even when broadcasting can be simulated with pointtopoint lines satellite broadcasting may be much cheaper. On the other hand from privacy point of view satellites are complete disaster everybody can hear everything. Encryption is essential when security is required. Satellites also have the property that the cost of transmitting message is in dependent of the distance traversed. call across the ocean costs no more to ser vice than call across the street. Satellites also have excellent error rates and can be deployed almost instantly major consideration for disaster response and mili tary communication. At much lower altitudes between the two Van Allen belts we find the MEO satellites. As viewed from the earth these drift slowly in longitude taking something like 6 hours to circle the earth. Accordingly they must be tracked as they move through the sky. Because they are lower than the GEOs they have smaller footprint on the ground and require less powerful transmitters to reach them. Currently they are used for navigation systems rather than telecommunications so we will not examine them further here. The constel lation of roughly 30 GPS satellites orbiting at about 20200 km are examples of MEO satellites. Moving down in altitude we come to the LEO satellites. Due to their rapid motion large numbers of them are needed for complete sys tem. On the other hand because the satellites are so close to the earth the ground stations do not need much power and the roundtrip delay is only few millisec ine two examples of satellite constellations for voice service Iridium and Glo For the first 30 years of the satellite era loworbit satellites were rarely used because they zip into and out of view so quickly. In 1990 Motorola broke new loworbit satellites for the Iridium project . The plan was later revised to use only 66 satellites so the project should have been renamed Dysprosium but that probably sounded too much like disease. The idea was that as soon as one satellite went out of view another would replace it. This proposal set off feeding frenzy among other communication companies. All of sudden everyone wanted to launch chain of loworbit satellites. After seven years of cobbling together partners and financing communication service began in November 1998. Unfortunately the commercial demand for large heavy satellite telephones was negligible because the mobile phone network had grown in spectacular way since 1990. As consequence Iridium was not profitable and was forced into bankruptcy in August 1999 in one of the most spec tacular corporate fiascos in history. The satellites and other assets were later purchased by an investor for 25 million at kind of extraterres trial garage sale. Other satellite business ventures promptly followed suit. The Iridium service restarted in March 2001 and has been growing ever since. It provides voice data paging fax and navigation service everywhere on land air and sea via handheld devices that communicate directly with the Iridium sat ellites. Customers include the maritime aviation and oil exploration industries as well as people traveling in parts of the world lacking telecom infrastructure . The Iridium satellites are positioned at an altitude of 750 km in circular polar cells and capacity of 3840 channels some of which are used for paging and navigation while others are used for data and voice. With six satellite necklaces the entire earth is covered as suggested by er at the North Pole contacting satellite directly overhead. Each satellite has four neighbors with which it can communicate two in the same necklace and two in adjacent necklaces . The satellites relay the call across this An alternative design to Iridium is Globalstar. It is based on 48 LEO satel lites but uses different switching scheme than that of Iridium. Whereas Iridium relays calls from satellite to satellite which requires sophisticated switching equipment in the satellites Globalstar uses traditional bentpipe design. The Satellite switches up by the large ground station at Santas Workshop. The call is then routed via terrestrial network to the ground station nearest the callee and delivered by bentpipe connection as shown. The advantage of this scheme is that it puts much of the complexity on the ground where it is easier to manage. Also the use of large ground station antennas that can put out powerful signal and receive weak one means that lowerpowered telephones can be used. After all the tele phone puts out only few milliwatts of power so the signal that gets back to the ground station is fairly weak even after having been amplified by the satellite. Satellites continue to be launched at rate of around 20 per year including everlarger satellites that now weigh over 5000 kilograms. But there are also very small satellites for the more budgetconscious organization. To make space re search more accessible academics from Cal Poly and Stanford got together in 1999 to define standard for miniature satellites and an associated launcher that would greatly lower launch costs . CubeSats are satellites in units of 10 cm 10 cm 10 cm cubes each weighing no more than 1 kilogram that can be launched for as little as 40000 each. The launcher flies as sec ondary payload on commercial space missions. It is basically tube that takes up cubesats have launched so far with many more in the works. Most of them com municate with ground stations on the UHF and VHF bands. comparison between satellite communication and terrestrial communication is instructive. As recently as 25 years ago case could be made that the future of communication lay with communication satellites. After all the telephone system had changed little in the previous 100 years and showed no signs of changing in the next 100 years. This glacial movement was caused in no small part by the regulatory environment in which the telephone companies were expected to pro vide good voice service at reasonable prices and in return got guaranteed profit on their investment. For people with data to transmit 1200bps modems were available. That was pretty much all there was. The introduction of competition in 1984 in the United States and somewhat later in Europe changed all that radically. Telephone companies began replacing their longhaul networks with fiber and introduced highbandwidth services like ADSL . They also stopped their longtime practice of charging artificially high prices to longdistance users to subsidize local service. All of sudden terrestrial fiber connections looked like the winner. Nevertheless communication satellites have some major niche markets that fiber does not address. First when rapid deployment is critical satellites win easily. quick response is useful for military communica tion systems in times of war and disaster response in times of peace. Following the massive December 2004 Sumatra earthquake and subsequent tsunami for ex ample communications satellites were able to restore communications to first re sponders within 24 hours. This rapid response was possible because there is de veloped satellite service provider market in which large players such as Intelsat with over 50 satellites can rent out capacity pretty much anywhere it is needed. For customers served by existing satellite networks VSAT can be set up easily and quickly to provide megabitsec link to elsewhere in the world. second niche is for communication in places where the terrestrial infra Many people nowadays want to communicate population density but do not do an adequate job in other places . Conversely Iridium provides voice service everywhere on Earth even at the South Pole. Terrestrial infrastructure can also be expensive to install depending on the terrain and necessary rights of way. Indonesia for example has Launching one satellite was cheaper than stringing thousands of undersea cables among the 13677 islands in the archipelago. third niche is when broadcasting is essential. message sent by satellite can be received by thousands of ground stations at once. Satellites are used to dis tribute much network TV programming to local stations for this reason. There is now large market for satellite broadcasts of digital TV and radio directly to end users with satellite receivers in their homes and cars. All sorts of other content can be broadcast too. For example an organization transmitting stream of stock bond or commodity prices to thousands of dealers might find satellite system to be much cheaper than simulating broadcasting on the ground. In short it looks like the mainstream communication of the future will be ter restrial fiber optics combined with cellular radio but for some specialized uses satellites are better. However there is one caveat that applies to all of this economics. Although fiber offers more bandwidth it is conceivable that terres trial and satellite communication could compete aggressively on price. If ad vances in technology radically cut the cost of deploying satellite or loworbit satellites catch on in big way it is not certain that fiber will win all markets. Now that we have studied the properties of wired and wireless channels we turn our attention to the problem of sending digital information. Wires and wire less channels carry analog signals such as continuously varying voltage light intensity or sound intensity. To send digital information we must devise analog signals to represent bits. The process of converting between bits and signals that represent them is called digital modulation. We will start with schemes that directly convert bits into signal. These schemes result in baseband transmission in which the signal occupies frequen cies from zero up to maximum that depends on the signaling rate. It is common for wires. Then we will consider schemes that regulate the amplitude phase or frequency of carrier signal to convey bits. These schemes result in passband transmission in which the signal occupies band of frequencies around the fre quency of the carrier signal. It is common for wireless and optical channels for which the signals must reside in given frequency band. Channels are often shared by multiple signals. After all it is much more con venient to use single wire to carry several signals than to install wire for every signal. This kind of sharing is called multiplexing. It can be accomplished in several different ways. We will present methods for time frequency and code di vision multiplexing. all widely used for wires fiber terrestrial wireless and satellite channels. In the The most straightforward form of digital modulation is to use positive volt age to represent 1 and negative voltage to represent 0. For an optical fiber the presence of light might represent 1 and the absence of light might represent 0. This scheme is called NRZ . The odd name is for his torical reasons and simply means that the signal follows the data. An example is Once sent the NRZ signal propagates down the wire. At the other end the receiver converts it into bits by sampling the signal at regular intervals of time. This signal will not look exactly like the signal that was sent. It will be attenuated and distorted by the channel and noise at the receiver. To decode the bits the re ceiver maps the signal samples to the closest symbols. For NRZ positive volt age will be taken to indicate that 1 was sent and negative voltage will be taken to indicate that 0 was sent. NRZ is good starting point for our studies because it is simple but it is sel dom used by itself in practice. More complex schemes can convert bits to signals that better meet engineering considerations. These schemes are called line codes. Below we describe line codes that help with bandwidth efficiency clock recov With NRZ the signal may cycle between the positive and negative levels up to every 2 bits . This means that we need bandwidth of at least B2 Hz when the bit rate is bitssec. This relation comes from the Nyquist rate . It is fundamental limit so we cannot run NRZ faster without using more bandwidth. Bandwidth is often limited resource even for wired channels Higherfrequency signals are increasingly attenuated making them less useful and higherfrequency signals also require faster electronics. One strategy for using limited bandwidth more efficiently is to use more than two signaling levels. By using four voltages for instance we can send 2 bits at once as single symbol. This design will work as long as the signal at the re ceiver is sufficiently strong to distinguish the four levels. The rate at which the signal changes is then half the bit rate so the needed bandwidth has been reduced. We call the rate at which the signal changes the symbol rate to distinguish it from the bit rate. The bit rate is the symbol rate multiplied by the number of bits per symbol. An older name for the symbol rate particularly in the context of de vices called telephone modems that convey digital data over telephone lines is the baud rate. In the literature the terms bit rate and baud rate are often used incorrectly. Note that the number of signal levels does not need to be power of two. Often it is not with some of the levels used for protecting against errors and sim plifying the design of the receiver. For all schemes that encode bits into symbols the receiver must know when one symbol ends and the next symbol begins to correctly decode the bits. With NRZ in which the symbols are simply voltage levels long run of 0s or 1s leaves the signal unchanged. After while it is hard to tell the bits apart as 15 zeros look much like 16 zeros unless you have very accurate clock. Accurate clocks would help with this problem but they are an expensive solu tion for commodity equipment. Remember we are timing bits on links that run at many megabitssec so the clock would have to drift less than fraction of microsecond over the longest permitted run. This might be reasonable for slow links or short messages but it is not general solution. One strategy is to send separate clock signal to the receiver. Another clock line is no big deal for computer buses or short cables in which there are many lines in parallel but it is wasteful for most network links since if we had another line to send signal we could use it to send data. clever trick here is to mix the clock signal with the data signal by XORing them together so that no extra line is sition in every bit time so it runs at twice the bit rate. When it is XORed with the 0 level it makes lowtohigh transition that is simply the clock. This transition is logical 0. When it is XORed with the 1 level it is inverted and makes highto low transition. This transition is logical 1. This scheme is called Manchester encoding and was used for classic Ethernet. The downside of Manchester encoding is that it requires twice as much band width as NRZ because of the clock and we have learned that bandwidth often matters. different strategy is based on the idea that we should code the data to ensure that there are enough transitions in the signal. Consider that NRZ will quent transitions it will be easy for the receiver to stay synchronized with the in coming stream of symbols. As step in the right direction we can simplify the situation by coding 1 as transition and 0 as no transition or vice versa. This coding is called NRZI An example is shown in computer peripherals uses NRZI. With it long runs of 1s do not cause problem. Of course long runs of 0s still cause problem that we must fix. If we were the telephone company we might simply require that the sender not transmit too many 0s. Older digital telephone lines in the . called T1 lines did in fact re quire that no more than 15 consecutive 0s be sent for them to work correctly. To really fix the problem we can break up runs of 0s by mapping small groups of bits to be transmitted so that groups with successive 0s are mapped to slightly longer patterns that do not have too many consecutive 0s. wellknown code to do this is called 4B5B. Every 4 bits is mapped into a5bit pattern with fixed translation table. The five bit patterns are chosen so that there will never be run of more than three consecutive 0s. The mapping is 100 overhead of Manchester encoding. Since there are 16 input combinations and 32 output combinations some of the output combinations are not used. Put ting aside the combinations with too many successive 0s there are still some codes left. As bonus we can use these nondata codes to represent physical layer control signals. For example in some uses 11111 represents an idle line and 11000 represents the start of frame. An alternative approach is to make the data look random known as scram bling. In this case it is very likely that there will be frequent transitions. scrambler works by XORing the data with pseudorandom sequence before it is transmitted. This mixing will make the data as random as the pseudorandom se quence . The receiver then XORs the incoming bits with the same pseudorandom sequence to recover the real data. For this to be practical the pseudorandom sequence must be easy to create. It is commonly given as the seed to simple random number generator. Scrambling is attractive because it adds no bandwidth or time overhead. In fact it often helps to condition the signal so that it does not have its energy in dominant frequency components that might radiate electromagnetic interference. Scrambling helps because random signals tend to be white or have energy spread across the frequency components. However scrambling does not guarantee that there will be no long runs. It is possible to get unlucky occasionally. If the data are the same as the pseudorandom sequence they will XOR to all 0s. This outcome does not generally occur with long pseudorandom sequence that is difficult to predict. However with short or predictable sequence it might be possible for malicious users to send bit patterns of the standards for sending IP packets over SONET links in the telephone system had this defect . It was possible for users to send cer Signals that have as much positive voltage as negative voltage even over short periods of time are called balanced signals. They average to zero which means that they have no DC electrical component. The lack of DC component is an advantage because some channels such as coaxial cable or lines with transform ers strongly attenuate DC component due to their physical properties. Also one method of connecting the receiver to the channel called capacitive coupling passes only the AC portion of signal. In either case if we send signal whose average is not zero we waste energy as the DC component will be filtered out. Balancing helps to provide transitions for clock recovery since there is mix of positive and negative voltages. It also provides simple way to calibrate re ceivers because the average of the signal can be measured and used as decision threshold to decode symbols. With unbalanced signals the average may be drift away from the true decision level due to density of 1s for example which would cause more symbols to be decoded with errors. straightforward way to construct balanced code is to use two voltage lev els to represent logical 1 with 0 representing logical zero. To send 1 the transmitter alternates between the 1 and 1 levels so that they always average out. This scheme is called bipolar encoding. In tele terminology in which 1 is called mark and 0 is called space. An ex Bipolar encoding adds voltage level to achieve balance. Alternatively we can use mapping like 4B5B to achieve balance . An example of this kind of balanced code is the 8B10B line code. It maps 8 bits of input to 10 bits of output so it is 80 efficient just like the 4B5B line code. The 8 bits are split into group of 5 bits which is mapped to 6 bits and group of 3 bits which is mapped to 4 bits. The 6bit and 4bit symbols are then concatenated. In each group some input patterns can be mapped to balanced output patterns that have the same number of 0s and 1s. For example 001 is mapped to 1001 which is balanced. But there are not enough combinations for all output patterns to be balanced. For these cases each input pattern is mapped to two output patterns. One will have an extra 1 and the alternate will have an extra 0. For example 000 is mapped to both 1011 and its complement 0100. As input bits are mapped to output bits the encoder remembers the disparity from the previous symbol. The disparity is the total number of 0s or 1s by which the signal is out of balance. The encoder then selects either an output pattern or its alternate to reduce the disparity. With 8B10B the disparity will be at most 2 bits. Thus the signal will never be far from balanced. There will also never be more than five consecutive 1s or 0s to help with clock recovery. Often we want to use range of frequencies that does not start at zero to send information across channel. For wireless channels it is not practical to send very low frequency signals because the size of the antenna needs to be fraction of the signal wavelength which becomes large. In any case regulatory con straints and the need to avoid interference usually dictate the choice of frequen cies. Even for wires placing signal in given frequency band is useful to let different kinds of signals coexist on the channel. This kind of transmission is call ed passband transmission because an arbitrary band of frequencies is used to pass terms of bandwidth or the width of the frequency band. The absolute frequency values do not matter for capacity. This means that we can take baseband signal that occupies 0 to Hz and shift it up to occupy passband of to Hz with out changing the amount of information that it can carry even though the signal will look different. To process signal at the receiver we can shift it back down to baseband where it is more convenient to detect symbols. Digital modulation is accomplished with passband transmission by regulating or modulating carrier signal that sits in the passband. We can modulate the am plitude frequency or phase of the carrier signal. Each of these methods has cor responding name. In ASK two different amplitudes are used to represent 0 and 1. An example with nonzero and zero level is ols. Similarly with FSK two or more different tones form of PSK the carrier wave is systematically shifted 0 or 180 degrees at each symbol period. Because there are two phases it is called BPSK . Binary here refers to the two symbols better scheme that uses the channel bandwidth more efficiently is to use four shifts . 45 135 225 or 315 degrees to transmit 2 bits of information per sym shift keying. Phase shift keying. We can combine these schemes and use more levels to transmit more bits per symbol. Only one of frequency and phase can be modulated at time because they are related with frequency being the rate of change of phase over time. Usually amplitude and phase are modulated in combination. Three examples are 45 135 225 and 315 degrees. The phase of dot is indicated by the angle line from it to the origin makes with the positive xaxis. The amplitude of dot is the see modulation scheme with denser constellation. Sixteen combinations of amplitudes and phase are used so the modulation scheme can be used to transmit 4 bits per symbol. It is called QAM16 where QAM stands for Quadrature Am different combinations so 6 bits can be transmitted per symbol. It is called QAM64. Even higherorder QAMs are used too. As you might suspect from these constellations it is easier to build electronics to produce symbols as com bination of values on each axis than as combination of amplitude and phase values. That is why the patterns look like squares rather than concentric circles. The constellations we have seen so far do not show how bits are assigned to symbols. When making the assignment an important consideration is that small burst of noise at the receiver not lead to many bit errors. This might happen if we assigned consecutive bit values to adjacent symbols. With QAM16 for example if one symbol stood for 0111 and the neighboring symbol stood for 1000 if the re ceiver mistakenly picks the adjacent symbol it will cause all of the bits to be wrong. better solution is to map bits to symbols so that adjacent symbols differ QAM16 constellation that has been Gray coded. Now if the receiver decodes the symbol in error it will make only single bit error in the expected case that the decoded symbol is close to the transmitted symbol. The modulation schemes we have seen let us send one signal to convey bits along wired or wireless link. However economies of scale play an important role in how we use networks. It costs essentially the same amount of money to in stall and maintain highbandwidth transmission line as lowbandwidth line be tween two different offices . Consequently multiplexing schemes have been developed to share lines among many signals. FDM takes advantage of passband trans mission to share channel. It divides the spectrum into frequency bands with each user having exclusive possession of some band in which to send their signal. AM radio broadcasting illustrates FDM. The allocated spectrum is about 1 MHz roughly 500 to 1500 kHz. Different frequencies are allocated to different logical channels each operating in portion of the spectrum with the interchannel separation great enough to prevent interference. phone channels multiplexed using FDM. Filters limit the usable bandwidth to about 3100 Hz per voicegrade channel. When many channels are multiplexed to gether 4000 Hz is allocated per channel. The excess is called guard band. It keeps the channels well separated. First the voice channels are raised in frequen cy each by different amount. Then they can be combined because no two chan nels now occupy the same portion of the spectrum. Notice that even though there are gaps between the channels thanks to the guard bands there is some overlap between adjacent channels. The overlap is there because real filters do not have ideal sharp edges. This means that strong spike at the edge of one channel will be felt in the adjacent one as nonthermal noise. This scheme has been used to multiplex calls in the telephone system for many years but multiplexing in time is now preferred instead. However FDM continues to be used in telephone networks as well as cellular terrestrial wireless and satellite networks at higher level of granularity. When sending digital data it is possible to divide the spectrum efficiently without using guard bands. In OFDM the channel bandwidth is divided into many subcarriers that indepen dently send data . The subcarriers are packed tightly together in the frequency domain. Thus signals from each subcarrier extend into adjacent The original bandwidths. The bandwidths raised in frequency. The multiplexed channel. designed so that it is zero at the center of the adjacent subcarriers. The subcarriers can therefore be sampled at their center frequencies without interference from their neighbors. To make this work guard time is needed to repeat portion of the symbol signals in time so that they have the desired frequency response. However this overhead is much less than is needed for many guard bands. The idea of OFDM has been around for long time but it is only in the last decade that it has been widely adopted following the realization that it is possible to implement OFDM efficiently in terms of Fourier transform of digital data over all subcarriers . OFDM is used in 802.11 cable networks and power line networking and is planned for fourthgeneration cellular systems. Usually one highrate stream of digital infor mation is split into many lowrate streams that are transmitted on the subcarriers in parallel. This division is valuable because degradations of the channel are easi er to cope with at the subcarrier level some subcarriers may be very degraded and excluded in favor of subcarriers that are received well. An alternative to FDM is TDM . Here the users take turns each one periodically getting the entire bandwidth for little burst of time. An example of three streams being multi fixed time slot and output to the aggregate stream. This stream runs at the sum rate of the individual streams. For this to work the streams must be synchronized in time. Small intervals of guard time analogous to frequency guard band may TDM is used widely as part of the telephone and cellular networks. To avoid one point of confusion let us be clear that it is quite different from the alternative STDM . The prefix statistical is added to indicate that the individual streams contribute to the multiplexed stream not on fixed schedule but according to the statistics of their demand. STDM is packet switching by another name. There is third kind of multiplexing that works in completely different way than FDM and TDM. CDM is form of spread spectrum communication in which narrowband signal is spread out over wider frequency band. This can make it more tolerant of interference as well as allowing multiple signals from different users to share the same frequency band. Because code division multiplexing is mostly used for the latter purpose it is com monly called CDMA . CDMA allows each station to transmit over the entire frequency spectrum all the time. Multiple simultaneous transmissions are separated using coding theory. Before getting into the algorithm let us consider an analogy an airport lounge with many pairs of people conversing. TDM is comparable to pairs of people in the room taking turns speaking. FDM is comparable to the pairs of people speak ing at different pitches some highpitched and some lowpitched such that each pair can hold its own conversation at the same time as but independently of the others. CDMA is comparable to each pair of people talking at once but in dif ferent language. The Frenchspeaking couple just hones in on the French reject ing everything that is not French as noise. Thus the key to CDMA is to be able to extract the desired signal while rejecting everything else as random noise. somewhat simplified description of CDMA follows. In CDMA each bit time is subdivided into short intervals called chips. Typically there are 64 or 128 chips per bit but in the example given here we will use 8 chipsbit for simplicity. Each station is assigned unique mbit code called chip sequence. For pedagogical purposes it is convenient to use bipolar nota tion to write these codes as sequences of 1 and 1. We will show chip se quences in parentheses. To transmit 1 bit station sends its chip sequence. To transmit 0 bit it sends the negation of its chip sequence. No other patterns are permitted. Thus for 8 if station is assigned the chip sequence it can send 1 bit by transmiting the chip sequence and 0 by transmitting . It is really signals with these voltage levels that are sent but it is sufficient for us to think in terms of the sequences. Increasing the amount of information to be sent from bitssec to mb chipssec for each station means that the bandwidth needed for CDMA is greater by factor of than the bandwidth needed for station not using CDMA . If we have 1MHz band available for 100 stations with FDM each one would have 10 kHz and could MHz so the chip rate is 100 chips per bit to spread the stations bit rate of 10 kbps across the channel. stations and the signals that they represent. Each station has its own unique chip sequence. Let us use the symbol to indicate the mchip vector for station and for its negation. All chip sequences are pairwise orthogonal by which we mean that the normalized inner product of any two distinct chip sequences and is 0. It is known how to generate such orthogonal chip se quences using method known as Walsh codes. In mathematical terms ortho gonality of the chip sequences can be expressed as follows In plain English as many pairs are the same as are different. This orthogonality property will prove crucial later. Note that if 0 then is also 0. The normalized inner product of any chip sequence with itself is 1 This follows because each of the terms in the inner product is 1 so the sum is . Also note that 1. represent Six examples of transmissions. Recovery of station Cs signal. During each bit time station can transmit 1 it can transmit 0 or it can be silent and transmit nothing. We assume for now that all stations are synchronized in time so all chip sequences begin at the same instant. When two or more sta tions transmit simultaneously their bipolar sequences add linearly. For example if in one chip period three stations output 1 and one station outputs 1 2 will be received. One can think of this as signals that add as voltages superimposed on the channel three stations output 1 and one station outputs 1 so that 2 is tions transmitting 1 bit at the same time. In the first example transmits 1 bit bits so we get the sum of their bipolar chip sequences namely To recover the bit stream of an individual station the receiver must know that stations chip sequence in advance. It does the recovery by computing the nor malized inner product of the received chip sequence and the chip sequence of the station whose bit stream it is trying to recover. If the received chip sequence is and the receiver is trying to listen to station whose chip sequence is it just computes the normalized inner product . To see why this works just imagine that two stations and both transmit 1 bit at the same time that transmits 0 bit as is the case in the third example. The receiver sees the sum and computes The first two terms vanish because all pairs of chip sequences have been carefully chosen to be orthogonal as shown in Eq. Now it should be clear why this property must be imposed on the chip sequences. To make the decoding process more concrete we show six examples in station from each of the six signals 1 through 6. It calculates the bit by sum then taking 18 of the result . The examples include cases where is silent sends 1 bit and sends 0 bit individually and in combination with other transmissions. As shown the correct bit is decoded each time. It is just like speaking French. In principle given enough computing capacity the receiver can listen to all the senders at once by running the decoding algorithm for each of them in paral lel. In real life suffice it to say that this is easier said than done and it is useful to know which senders might be transmitting. In the ideal noiseless CDMA system we have studied here the number of sta tions that send concurrently can be made arbitrarily large by using longer chip se quences. For 2n stations Walsh codes can provide 2n orthogonal chip sequences of length 2n. However one significant limitation is that we have assumed that all the chips are synchronized in time at the receiver. This synchronization is not even approximately true in some applications such as cellular networks . It leads to different de nous CDMA differs from synchronous CDMA. As well as cellular networks CDMA is used by satellites and cable networks. We have glossed over many complicating factors in this brief introduction. En gineers who want to gain deep understanding of CDMA should read Viterbi ground in communication engineering however. When two computers owned by the same company or organization and locat ed close to each other need to communicate it is often easiest just to run cable between them. LANs work this way. However when the distances are large or there are many computers or the cables have to pass through public road or other public right of way the costs of running private cables are usually prohibitive. Furthermore in just about every country in the world stringing private transmis sion lines across public property is also illegal. Consequently the network designers must rely on the existing telecommunication facilities. These facilities especially the PSTN were usually designed many years ago with completely different goal in mind transmitting the human voice in moreorless recognizable form. Their To see the size of the problem consider that cheap commodity cable running be tween two computers can transfer data at 1 Gbps or more. In contrast typical Mbps. The difference between the two is the difference between cruising in an airplane and taking leisurely stroll. Nonetheless the telephone system is tightly intertwined with computer networks so it is worth devoting some time to study it in detail. The limiting factor for networking purposes turns out to be the last mile over which customers connect not the trunks and switches inside the telephone network. This situation is changing with the gradual rollout of fiber and digital technology at the edge of the network but it will take time and money. During the long wait computer systems designers used to working with systems that give at least three orders of magnitude better performance have devoted much time and effort to fig ure out how to use the telephone network efficiently. it works. For additional information about the innards of the telephone system see Soon after Alexander Graham Bell patented the telephone in 1876 there was an enormous demand for his new invention. The initial market was for the sale of telephones which came in pairs. It was up to the customer to string single wire between them. If telephone owner wanted to talk to other telephone owners separate wires had to be strung to all houses. Within year the cities were covered with wires passing over houses and trees in wild jumble. It became immediately obvious that the model was not going to work. To his credit Bell saw this problem early on and formed the Bell Telephone Company which opened its first switching office in 1878. The company ran wire to each customers house or office. To make call the customer would crank the phone to make ringing sound in the telephone company office to attract the attention of an operator who would then manually connect the caller to the callee by using short jumper cable to connect the caller Twolevel hierarchy. Pretty soon Bell System switching offices were springing up everywhere and people wanted to make longdistance calls between cities so the Bell System began to connect the switching offices. The original problem soon returned to connect every switching office to every other switching office by means of wire between them quickly became unmanageable so secondlevel switching offices were invented. After while multiple secondlevel offices were needed as illus By 1890 the three major parts of the telephone system were in place the switching offices the wires between the customers and the switching offices and the longdistance connections between the switching offices. For short technical history of the telephone system see Hawley . While there have been improvements in all three areas since then the basic Bell System model has remained essentially intact for over 100 years. The fol lowing description is highly simplified but gives the essential flavor nevertheless. Each telephone has two copper wires coming out of it that go directly to the tele phone companys nearest end office . The dis tance is typically 1 to 10 km being shorter in cities than in rural areas. In the United States alone there are about 22000 end offices. The twowire connections between each subscribers telephone and the end office are known in the trade as the local loop. If the worlds local loops were stretched out end to end they would extend to the moon and back 1000 times. At one time 80 of ATTs capital value was the copper in the local loops. ATT was then in effect the worlds largest copper mine. Fortunately this fact was not well known in the investment community. Had it been known some cor porate raider might have bought ATT ended all telephone service in the United States ripped out all the wire and sold it to copper refiner for quick payback. If subscriber attached to given end office calls another subscriber attached to the same end office the switching mechanism within the office sets up direct electrical connection between the two local loops. This connection remains intact for the duration of the call. If the called telephone is attached to another end office different procedure has to be used. Each end office has number of outgoing lines to one or more nearby switching centers called toll offices . These lines are called toll connecting trunks. The num ber of different kinds of switching centers and their topology varies from country to country depending on the countrys telephone density. If both the callers and callees end offices happen to have toll connecting trunk to the same toll office the connection may be established within the toll office. telephone network consisting only of telephones end offices and toll If the caller and callee do not have toll office in common path will have to be established between two toll offices. The toll offices communicate with each other via highbandwidth intertoll trunks . Prior to the 1984 breakup of ATT the . telephone system used hierarchical rout ing to find path going to higher levels of the hierarchy until there was switch ing office in common. This was then replaced with more flexible nonhierarchical variety of transmission media are used for telecommunication. modern office buildings where the wiring is commonly Category 5 local loops to homes mostly consist of Category 3 twisted pairs with fiber just starting to appear. Between switching offices coaxial cables microwaves and especially fiber optics are widely used. In the past transmission throughout the telephone system was analog with the actual voice signal being transmitted as an electrical voltage from source to destination. With the advent of fiber optics digital electronics and computers all the trunks and switches are now digital leaving the local loop as the last piece of analog technology in the system. Digital transmission is preferred because it is not necessary to accurately reproduce an analog waveform after it has passed from 1 is enough. This property makes digital transmission more reliable than analog. It is also cheaper and easier to maintain. 1. Local loops . 2. Trunks . 3. Switching offices . After short digression on the politics of telephones we will come back to each of these three components in some detail. The local loops provide everyone ac cess to the whole system so they are critical. Unfortunately they are also the lect multiple calls together and send them out over the same fiber. This calls for mentally different ways of doing switching we will look at both. For decades prior to 1984 the Bell System provided both local and longdis tance service throughout most of the United States. In the 1970s the . Federal Government came to believe that this was an illegal monopoly and sued to break it up. The government won and on January 1 1984 ATT was broken up into ATT Long Lines 23 BOCs and few other pieces. The 23 BOCs were grouped into seven regional BOCs to make them economically viable. The entire nature of telecommunication in the United States was changed overnight by court order . The exact specifications of the divestiture were described in the socalled competition better service and lower longdistance rates for consumers and busi nesses. However prices for local service rose as the cross subsidies from long distance calling were eliminated and local service had to become self supporting. Many other countries have now introduced competition along similar lines. Of direct relevance to our studies is that the new competitive framework caused key technical feature to be added to the architecture of the telephone net work. To make it clear who could do what the United States was divided up into about as big as the area covered by one area code. Within each LATA there was one LEC with monopoly on traditional telephone THE PUBLIC SWITCHED TELEPHONE NETWORK service within its area. The most important LECs were the BOCs although some LATAs contained one or more of the 1500 independent telephone companies op erating as LECs. The new feature was that all interLATA traffic was handled by different kind of company an IXC . Originally ATT Long Lines was the only serious IXC but now there are wellestablished competitors such as Verizon and Sprint in the IXC business. One of the concerns at the breakup was to ensure that all the IXCs would be treated equally in terms of line quality tariffs and the number of digits their customers would have to dial to use ample LATAs each with several end offices. LATAs 2 and 3 also have small hierarchy with tandem offices . LEC switching offices. Each hexagon belongs to the IXC whose number is in it. Any IXC that wishes to handle calls originating in LATA can build switching office called POP there. The LEC is required to connect each IXC to every end office either directly as in LATAs 1 and 3 or indirectly as in LATA 2. Furthermore the terms of the connection both techni cal and financial must be identical for all IXCs. This requirement enables sub scriber in say LATA 1 to choose which IXC to use for calling subscribers in As part of the MFJ the IXCs were forbidden to offer local telephone service and the LECs were forbidden to offer interLATA telephone service although both were free to enter any other business such as operating fried chicken restau rants. In 1984 that was fairly unambiguous statement. Unfortunately technolo gy has funny way of making the law obsolete. Neither cable television nor mo bile phones were covered by the agreement. As cable television went from one way to two way and mobile phones exploded in popularity both LECs and IXCs began buying up or merging with cable and mobile operators. By 1995 Congress saw that trying to maintain distinction between the vari cessibility for competition but allow cable TV companies local telephone com panies longdistance carriers and mobile operators to enter one anothers busi nesses. The idea was that any company could then offer its customers single integrated package containing cable TV telephone and information services and that different companies would compete on service and price. The bill was en acted into law in February 1996 as major overhaul of telecommunications regu lation. As result some BOCs became IXCs and some other companies such as cable television operators began offering local telephone service in competition One interesting property of the 1996 law is the requirement that LECs imple ment local number portability. This means that customer can change local telephone companies without having to get new telephone number. Portability for mobile phone numbers followed suit in 2003. These provisions removed huge hurdle for many people making them much more inclined to switch LECs. As result the . telecommunications landscape became much more competitive and other countries have followed suit. Often other countries wait to see how this kind of experiment works out in the . If it works well they do the same thing if it works badly they try some It is now time to start our detailed study of how the telephone system works. Let us begin with the part that most people are familiar with the twowire local loop coming from telephone company end office into houses. The local loop is also frequently referred to as the last mile although the length can be up to several miles. It has carried analog information for over 100 years and is likely to continue doing so for some years to come due to the high cost of converting to Much effort has been devoted to squeezing data networking out of the copper local loops that are already deployed. Telephone modems send digital data be tween computers over the narrow channel the telephone network provides for voice call. They were once widely used but have been largely displaced by broadband technologies such as ADSL that. reuse the local loop to send digital data from customer to the end office where they are siphoned off to the Internet. Both modems and ADSL must deal with the limitations of old local loops rel atively narrow bandwidth attenuation and distortion of signals and susceptibility to electrical noise such as crosstalk. In some places the local loop has been modernized by installing optical fiber to the home. Fiber is the way of the future. These installations support computer networks from the ground up with the local loop having ample bandwidth for data services. The limiting factor is what people will pay not the To send bits over the local loop or any other physical channel for that matter they must be converted to analog signals that can be transmitted over the channel. is converted back to bits. device that converts between stream of digital bits and an analog signal that represents the bits is called modem which is short for modulator demodu lator. Modems come in many varieties telephone modems DSL modems cable modems wireless modems etc. The modem may be built into the computer or be separate box . Logically the modem is inserted between Telephone modems are used to send bits between two computers over voicegrade telephone line in place of the conversation that usually fills the line. The main difficulty in doing so is that voicegrade telephone line is limited to than four orders of magnitude less than the bandwidth that is used for Ethernet or 802.11 . Unsurprisingly the data rates of telephone modems are also four orders of magnitude less than that of Ethernet and 802.11. Let us run the numbers to see why this is the case. The Nyquist theorem tells us that even with perfect 3000Hz line there is no point in sending symbols at rate faster than 6000 baud. In practice most modems send at rate of 2400 symbolssec or 2400 baud and focus on get ting multiple bits per symbol while allowing traffic in both directions at the same time . The humble 2400bps modem uses 0 volts for logical 0 and 1 volt for logi cal 1 with 1 bit per symbol. One step up it can use four different symbols as in the four phases of QPSK so with 2 bitssymbol it can get data rate of 4800 bps. long progression of higher rates has been achieved as technology has im proved. Higher rates require larger set of symbols or constellation. With many symbols even small amount of noise in the detected amplitude or phase can re sult in an error. To reduce the chance of errors standards for the higherspeed modems use some of the symbols for error correction. The schemes are known as TCM . The .32 modem standard uses 32 constellation points to transmit 4 data bits and 1 check bit per symbol at 2400 baud to achieve 9600 bps with error cor rection. The next step above 9600 bps is 14400 bps. It is called .32 bis and transmits 6 data bits and 1 check bit per symbol at 2400 baud. Then comes .34 which achieves 28800 bps by transmitting 12 data bitssymbol at 2400 baud. The bis which uses 14 data bitssymbol at 2400 baud to achieve 33600 bps. Why stop here The reason that standard modems stop at 33600 is that the Shannon limit for the telephone system is about 35 kbps based on the average length of local loops and the quality of these lines. Going faster than this would violate the laws of physics . However there is one way we can change the situation. At the telephone company end office the data are converted to digital form for transmission within the telephone network . The 35kbps limit is for the situation in which there are two local loops one at each end. Each of these adds noise to the signal. If we could get rid of one of these local loops we would increase the SNR and the maximum rate would be doubled. This approach is how 56kbps modems are made to work. One end typically an ISP gets highquality digital feed from the nearest end office. Thus when one end of the connection is highquality signal as it is with most ISPs now the maximum data rate can be as high as 70 kbps. Between two home users with modems and analog lines the maximum is still 33.6 kbps. The reason that 56kbps modems are in use has to do with the Nyquist theorem. telephone channel is carried inside the tele phone system as digital samples. Each telephone channel is 4000 Hz wide when THE PUBLIC SWITCHED TELEPHONE NETWORK the guard bands are included. The number of samples per second needed to reconstruct it is thus 8000. The number of bits per sample in the . is 8 one of which may be used for control purposes allowing 56000 bitssec of user data. In Europe all 8 bits are available to users so 64000bitsec modems could have been used but to get international agreement on standard 56000 was chosen. The end result is the .90 and .92 modem standards. They provide for 56kbps downstream channel and 33.6kbps and 48kbps upstream channel respectively. The asymmetry is because there is usually more data transported from the ISP to the user than the other way. It also means that more of the limited bandwidth can be allocated to the downstream channel to increase the chances of it actually working at 56 kbps. Digital Subscriber Lines for job well done. Meanwhile the cable TV industry was offering speeds up to part of their business the telephone companies began to realize they need ed more competitive product. Their answer was to offer new digital services over the local loop. Initially there were many overlapping highspeed offerings all under the gen eral name of xDSL for various . Services with more bandwidth than standard telephone service are sometimes called broadband al though the term really is more of marketing concept than specific technical concept. Later we will discuss what has become the most popular of these ser vices ADSL . We will also use the term DSL or xDSL as shorthand for all flavors. The reason that modems are so slow is that telephones were invented for car rying the human voice and the entire system has been carefully optimized for this purpose. Data have always been stepchildren. At the point where each local loop terminates in the end office the wire runs through filter that attenuates all fre quencies below 300 Hz and above 3400 Hz. The cutoff is not sharp300 Hz and though the distance between the 3 dB points is 3100 Hz. Data on the wire are thus also restricted to this narrow band. The trick that makes xDSL work is that when customer subscribes to it the incoming line is connected to different kind of switch one that does not have this filter thus making the entire capacity of the local loop available. The limiting factor then becomes the physics of the local loop which supports roughly 1 MHz not the artificial 3100 Hz bandwidth created by the filter. Unfortunately the capacity of the local loop falls rather quickly with distance from the end office as the signal is increasingly degraded along the wire. It also depends on the thickness and general quality of the twisted pair. plot of the sumes that all the other factors are optimal . When it picks speed to offer it is simultaneously picking radius from its end offices beyond which the service cannot be offered. This means that when distant customers try to sign up for the service they may be told Thanks lot for your interest but you live 100 meters too far from the nearest end office to get this ser vice. Could you please move The lower the chosen speed is the larger the radius and the more customers are covered. But the lower the speed the less attractive the service is and the fewer the people who will be willing to pay for it. This is where business meets technology. The xDSL services have all been designed with certain goals in mind. First the services must work over the existing Category 3 twisted pair local loops. Sec ond they must not affect customers existing telephones and fax machines. Third they must be much faster than 56 kbps. Fourth they should be always on with just monthly charge and no perminute charge. To meet the technical goals the available 1.1 MHz spectrum on the local loop is divided into 256 independent channels of 4312.5 Hz each. This arrangement is used to send data over these channels though it is often called DMT in the context of ADSL. Channel 0 is used for POTS . Channels 15 are not used to keep the voice and data sig nals from interfering with each other. Of the remaining 250 channels one is used for upstream control and one is used for downstream control. The rest are avail able for user data. In principle each of the remaining channels can be used for fullduplex data stream but harmonics crosstalk and other effects keep practical systems well 256 4kHz Channels below the theoretical limit. It is up to the provider to determine how many chan nels are used for upstream and how many for downstream. 5050 mix of upstream and downstream is technically possible but most providers allocate something like 8090 of the bandwidth to the downstream channel since most users download more data than they upload. This choice gives rise to the in ADSL. common split is 32 channels for upstream and the rest downstream. It is also possible to have few of the highest upstream channels be bidirectional for increased bandwidth although making this optimization requires adding special circuit to cancel echoes. allows speeds of as much as 8 Mbps downstream and 1 Mbps upstream. It was provements to allow speeds of as much as 12 Mbps downstream and 1 Mbps up Mbps by doubling the bandwidth to use 2.2 MHz over the twisted pair. However the numbers quoted here are bestcase speeds for good lines close to the exchange. Few lines support these rates and few pro Mbps upstream and 8 Mbps downstream and 2 Mbps upstream . symbolssec. The line quality in each channel is constantly monitored and the data rate is adjusted by using larger or smaller constellation like those in symbol sent on channel with high SNR and down to 2 1 or no bits per sym bol sent on channel with low SNR depending on the standard. phone company technician must install NID on the customers premises. This small plastic box marks the end of the telephone com panys property and the start of the customers property. Close to the NID is splitter an analog filter that separates the 04000Hz band used by POTS from the data. The POTS signal is routed to the existing telephone or fax machine. The data signal is routed to an ADSL modem which uses digital signal processing to implement OFDM. Since most ADSL modems are external the computer must be connected to them at high speed. Usually this is done using Ethernet USB cable or 802.11. At the other end of the wire on the end office side corresponding splitter is installed. Here the voice portion of the signal is filtered out and sent to the nor mal voice switch. The signal above 26 kHz is routed to new kind of device call ed DSLAM which contains the same kind of digital signal processor as the ADSL modem. Once the bits have been recovered from the signal packets are formed and sent off to the ISP. This complete separation between the voice system and ADSL makes it rel atively easy for telephone company to deploy ADSL. All that is needed is buy ing DSLAM and splitter and attaching the ADSL subscribers to the splitter. Other highbandwidth services require much greater changes to the existing switching equipment. on the customers premises. Installing these can only be done by telephone company technician necessitating an expensive truck roll . Therefore an alternative splitterless design but without the customers splitter. The existing telephone line is used as is. The only difference is that microfilter has to be inserted into each telephone jack THE PUBLIC SWITCHED TELEPHONE NETWORK between the telephone or ADSL modem and the wire. The microfilter for the telephone is lowpass filter eliminating frequencies above 3400 Hz the microfil kHz. However this system is not as reliable as having splitter so .lite can be used only up to 1.5 Mbps . For more Deployed copper local loops limit the performance of ADSL and telephone modems. To let them provide faster and better network services telephone com panies are upgrading local loops at every opportunity by installing optical fiber all the way to houses and offices. The result is called FttH . While FttH technology has been available for some time deployments only began to take off in 2005 with growth in the demand for highspeed Internet from cus tomers used to DSL and cable who wanted to download movies. Around 4 of Several variations of the form FttX exist. They are used to note that the fiber deployment may reach close to the house. In this case copper pro vides fast enough speeds over the last short distance. The choice of how far to lay the fiber is an economic one balancing cost with expected revenue. In any case the point is that optical fiber has crossed the traditional barrier of the last mile. We will focus on FttH in our discussion. Like the copper wires before it the fiber local loop is passive. This means no powered equipment is required to amplify or otherwise process signals. The fiber simply carries signals between the home and the end office. This in turn reduces cost and improves reliability. Usually the fibers from the houses are joined together so that only single fiber reaches the end office per group of up to 100 houses. In the downstream di rection optical splitters divide the signal from the end office so that it reaches all the houses. Encryption is needed for security if only one house should be able to decode the signal. In the upstream direction optical combiners merge the signals from the houses into single signal that is received at the end office. This architecture is called PON and it is shown for downstream transmission and another wavelength for upstream transmission. Even with the splitting the tremendous bandwidth and low attenuation of km. The actual data rates and other details depend on the type of PON. Two kinds are common. GPONs come from the world of telecom munications so they are defined by an ITU standard. EPONs splittercombiner are more in tune with the world of networking so they are defined by an IEEE standard. Both run at around gigabit and can carry traffic for different services including Internet video and voice. For example GPONs provide 2.4 Gbps downstream and 1.2 or 2.4 Gbps upstream. Some protocol is needed to share the capacity of the single fiber at the end office between the different houses. The downstream direction is easy. The end office can send messages to each different house in whatever order it likes. In the upstream direction however messages from different houses cannot be sent at the same time or different signals would collide. The houses also cannot hear each others transmissions so they cannot listen before transmitting. The solution is that equipment at the houses requests and is granted time slots to use by equip ment in the end office. For this to work there is ranging process to adjust the transmission times from the houses so that all the signals received at the end office are synchronized. The design is similar to cable modems which we cover Trunks in the telephone network are not only much faster than the local loops they are different in two other respects. The core of the telephone network carries digital information not analog information that is bits not voice. This necessi haul trunks. The trunks carry thousands even millions of calls simultaneously. This sharing is important for achieving economies of scale since it costs essen tially the same amount of money to install and maintain highbandwidth trunk as lowbandwidth trunk between two switching offices. It is accomplished with Below we will briefly examine how voice signals are digitized so that they can be transported by the telephone network. After that we will see how TDM is used to carry bits on trunks including the TDM system used for fiber optics THE PUBLIC SWITCHED TELEPHONE NETWORK . Then we will turn to FDM as it is applied to fiber optics which is call Early in the development of the telephone network the core handled voice calls as analog information. FDM techniques were used for many years to multi plex 4000Hz voice channels into larger and larger units. For example 12 calls in the 60 kHzto108 kHz band is known as group and five groups are known as supergroup and so on. These FDM methods are still used over some copper wires and microwave channels. However FDM requires analog circuitry and is not amenable to being done by computer. In contrast TDM can be handled entirely by digital elec tronics so it has become far more widespread in recent years. Since TDM can only be used for digital data and the local loops produce analog signals conver sion is needed from analog to digital in the end office where all the individual local loops come together to be combined onto outgoing trunks. The analog signals are digitized in the end office by device called codec . The codec makes 8000 samples per second because the Nyquist theorem says that this is sufficient to capture all the information from the 4kHz telephone channel bandwidth. At lower sam pling rate information would be lost at higher one no extra information would be gained. Each sample of the amplitude of the signal is quantized to an 8bit This technique is called PCM . It forms the heart of the modern telephone system. As consequence virtually all time intervals μsec or 64 kbps. At the other end of the call an analog signal is recreated from the quantized samples by playing them out over time. It will not be ex actly the same as the original analog signal even though we sampled at the Nyquist rate because the samples were quantized. To reduce the error due to quantization the quantization levels are unevenly spaced. logarithmic scale is used that gives relatively more bits to smaller signal amplitudes and relatively fewer bits to large signal amplitudes. In this way the error is proportional to the signal amplitude. specified in standard ITU .711. An equivalent way to think about this process is to imagine that the dynamic range of the signal is compressed before it is quantized and then expanded when the analog signal is recreated. For this reason it is called companding. It is also possible to compress the samples after they are digitized so that they require much less than 64 kbps. However we will leave this topic for when we explore audio applications such as voice over IP. TDM based on PCM is used to carry multiple voice calls over trunks by send ing sample from each call every 125 μsec. When digital transmission began emerging as feasible technology ITU was unable to reach agreement on an international standard for PCM. Consequently variety of incompatible schemes are now in use in different countries around the world. The method used in North America and Japan is the T1 carrier depicted in ed T1 but following widespread industry tradition we will not make that subtle distinction here. The T1 carrier consists of 24 voice channels multiplexed toget her. Each of the 24 channels in turn gets to insert 8 bits into the output stream. frame consists of 24 8 192 bits plus one extra bit for control purposes yielding 193 bits every 125 μsec. This gives gross data rate of 1.544 Mbps of which 8 kbps is for signaling. The 193rd bit is used for frame synchronization and signaling. In one variation the 193rd bit is used across group of 24 frames call ed an extended superframe. Six of the bits in the 4th 8th 12th 16th 20th and 24th positions take on the alternating pattern 001011 . Normally the receiver keeps checking for this pattern to make sure that it has not lost synchronization. Six more bits are used to send an error check code to help the receiver confirm that it is synchronized. If it does get out of sync the receiver can scan for the pat THE PUBLIC SWITCHED TELEPHONE NETWORK bits are used for control information for operating and maintaining the network information inband meaning in the same channel as the data by using some of the data bits. This design is one form of channelassociated signaling because each channel has its own private signaling subchannel. In one arrangement the least significant bit out of an 8bit sample on each channel is used in every sixth frame. It has the colorful name of robbedbit signaling. The idea is that few stolen bits will not matter for voice calls. No one will hear the difference. For data however it is another story. Delivering the wrong bits is unhelpful provide clear channels in which all of the bits may be used to send data. Clear channels are what businesses who lease T1 line want when they send data across the telephone network in place of voice samples. Signaling for any voice calls is then handled outofband meaning in separate channel from the data. Often the signaling is done with commonchannel signaling in which there is shared signaling channel. One of the 24 channels may be used for this purpose. Outside North America and Japan the 2.048Mbps E1 carrier is used instead of T1. This carrier has 32 8bit data samples packed into the basic 125μsec frame. Thirty of the channels are used for information and up to two are used for signaling. Each group of four frames provides 64 signaling bits half of which are used for signaling and half of which are used for frame synchronization or are reserved for each country to use Time division multiplexing allows multiple T1 carriers to be multiplexed into four T1 channels being multiplexed into one T2 channel. The multiplexing at T2 and above is done bit for bit rather than byte for byte with the 24 voice channels that make up T1 frame. Four T1 streams at 1.544 Mbps should generate 6.176 Mbps but T2 is actually 6.312 Mbps. The extra bits are used for framing and re covery in case the carrier slips. T1 and T3 are widely used by customers whereas T2 and T4 are only used within the telephone system itself so they are not well At the next level seven T2 streams are combined bitwise to form T3 stream. Then six T3 streams are joined to form T4 stream. At each step small amount of overhead is added for framing and recovery in case the synchronization be tween sender and receiver is lost. Just as there is little agreement on the basic carrier between the United States and the rest of the world there is equally little agreement on how it is to be multi plexed into higherbandwidth carriers. The . scheme of stepping up by 4 7 and 6 did not strike everyone else as the way to go so the ITU standard calls for multiplexing four streams into one stream at each level. Also the framing and recovery data are different in the . and ITU standards. The ITU hierarchy for 32 128 512 2048 and 8192 channels runs at speeds of 2.048 8.848 34.304 In the early days of fiber optics every telephone company had its own proprietary optical TDM system. After ATT was broken up in 1984 local tele phone companies had to connect to multiple longdistance carriers all with dif ferent optical TDM systems so the need for standardization became obvious. In 1985 Bellcore the RBOCs research arm began working on standard called SONET . Later ITU joined the effort which resulted in SONET standard and set of parallel ITU recommendations in 1989. The ITU recommendations are called SDH but differ from SONET only in minor ways. Virtually all the longdistance telephone traffic in the United States and much of it elsewhere now uses trunks running SONET in the physical layer. For additional information about SONET see Bellamy Goralski and Shepard . The SONET design had four major goals. First and foremost SONET had to make it possible for different carriers to interwork. Achieving this goal required defining common signaling standard with respect to wavelength timing fram Second some means was needed to unify the . European and Japanese digital systems all of which were based on 64kbps PCM channels but combined them in different ways. Third SONET had to provide way to multiplex multiple digital channels. At the time SONET was devised the highestspeed digital carrier actually used widely in the United States was T3 at 44.736 Mbps. T4 was defined but not used THE PUBLIC SWITCHED TELEPHONE NETWORK much and nothing was even defined above T4 speed. Part of SONETs mission was to continue the hierarchy to gigabitssec and beyond. standard way to mul tiplex slower channels into one SONET channel was also needed. Fourth SONET had to provide support for operations administration and maintenance which are needed to manage the network. Previous systems did not do this very well. An early decision was to make SONET traditional TDM system with the entire bandwidth of the fiber devoted to one channel containing time slots for the various subchannels. As such SONET is synchronous system. Each sender and receiver is tied to common clock. The master clock that controls the system has an accuracy of about 1 part in 109. Bits on SONET line are sent out at extreme ly precise intervals controlled by the master clock. The basic SONET frame is block of 810 bytes put out every 125 μsec. Since SONET is synchronous frames are emitted whether or not there are any useful data to send. Having 8000 framessec exactly matches the sampling rate of the PCM channels used in all digital telephony systems. times per second for gross data rate of 51.84 Mbps. This layout is the basic SONET channel called STS1 . All SONET trunks are multiples of STS1. The first three columns of each frame are reserved for system management overhead is generated and checked at the start and end of each line. SONET transmitter sends backtoback 810byte frames without gaps be tween them even when there are no data . From the receivers point of view all it sees is continuous bit stream so how does it know where each frame begins The answer is that the first 2 bytes of each frame contain fixed pattern that the receiver searches for. If it finds this pattern in the same place in large number of consecutive frames it assumes that it is in sync with the sender. In theory user could insert this pattern into the payload in regular way but in practice it cannot be done due to the multiplexing of multiple users into the same frame and other reasons. Mbps of user data. This user data could be voice samples T1 and other carriers swallowed whole or packets. SONET is simply convenient container for tran sporting bits. The SPE which carries the user data does not always begin in row 1 column 4. The SPE can begin anywhere within the frame. pointer to the first byte is contained in the first row of the line the endtoend path sublayer protocol. The ability to allow the SPE to begin anywhere within the SONET frame and tem. For example if payload arrives at the source while dummy SONET frame is being constructed it can be inserted into the current frame instead of being held until the start of the next one. STS1 to STS768 have been defined ranging from roughly T3 line to 40 Gbps. Even higher rates will surely be defined over time with OC3072 at 160 Gbps being the next in line if and when it becomes technologically feasible. The opti cal carrier corresponding to STSn is called OCn but is bit for bit the same except for certain bit reordering needed for synchronization. The SDH names are dif ferent and they start at OC3 because ITUbased systems do not have rate near multiples of four. The gross data rate includes all the overhead. The SPE data head and counts only the 87 payload columns. As an aside when carrier such as OC3 is not multiplexed but carries the data from only single source the letter is appended to the designation so OC3 indicates 155.52Mbps carrier consisting of three separate OC1 carriers but OC3c indicates data stream from single source at 155.52 The three OC1 streams within an OC3c stream are interleaved by columnfirst column 1 from stream 1 then column 1 from stream 2 then column 1 from stream 3 followed by column 2 from stream 1 and so onleading to form of frequency division multiplexing is used as well as TDM to harness the tremendous bandwidth of fiber optic channels. It is called WDM . The basic principle of WDM on fibers is dep with its energy present at different wavelength. The four beams are combined onto single shared fiber for transmission to distant destination. At the far end the beam is split up over as many fibers as there were on the input side. Each out put fiber contains short specially constructed core that filters out all but one wavelength. The resulting signals can be routed to their destination or recombin ed in different ways for additional multiplexed transport. There is really nothing new here. This way of operating is just frequency di vision multiplexing at very high frequencies with the term WDM owing to the description of fiber optic channels by their wavelength or color rather than fre quency. As long as each channel has its own frequency range and all the ranges are disjoint they can be multiplexed together on the longhaul fiber. The only difference with electrical FDM is that an optical system using diffraction grating is completely passive and thus highly reliable. The reason WDM is popular is that the energy on single channel is typically only few gigahertz wide because that is the current limit of how fast we can con vert between electrical and optical signals. By running many channels in parallel on different wavelengths the aggregate bandwidth is increased linearly with the number of channels. Since the bandwidth of single fiber band is about 25000 1 bitHz . WDM technology has been progressing at rate that puts computer technolo gy to shame. WDM was invented around 1990. The first commercial systems had eight channels of 2.5 Gbps per channel. By 1998 systems with 40 channels Longhaul shared fiber of 2.5 Gbps were on the market. By 2006 there were products with 192 channels of 10 Gbps and 64 channels of 40 Gbps capable of moving up to 2.56 Tbps. This bandwidth is enough to transmit 80 fulllength DVD movies per second. The channels are also packed tightly on the fiber with 200 100 or as little as 50 GHz of separation. Technology demonstrations by companies after bragging rights have shown 10 times this capacity in the lab but going from the lab to the field usually takes at least few years. When the number of channels is very large and the wavelengths are spaced close together the system is referred to as DWDM One of the drivers of WDM technology is the development of alloptical com ponents. Previously every 100 km it was necessary to split up all the channels and convert each one to an electrical signal for amplification separately before reconverting them to optical signals and combining them. Nowadays alloptical amplifiers can regenerate the entire signal once every 1000 km without the need However it is also possible to build WDM systems that are switched in the opti cal domain. In such device the output filters are tunable using FabryPerot or MachZehnder interferometers. These devices allow the selected frequencies to be changed dynamically by control computer. This ability provides large amount of flexibility to provision many different wavelength paths through the telephone network from fixed set of fibers. For more information about optical From the point of view of the average telephone engineer the phone system is divided into two principal parts outside plant and inside plant . We have just looked at the outside plant. Now it is time to examine the inside plant. Two different switching techniques are used by the network nowadays circuit switching and packet switching. The traditional telephone system is based on cir cuit switching but packet switching is beginning to make inroads with the rise of voice over IP technology. We will go into circuit switching in some detail and contrast it with packet switching. Both kinds of switching are important enough that we will come back to them when we get to the network layer. Conceptually when you or your computer places telephone call the switch ing equipment within the telephone system seeks out physical path all the way from your telephone to the receivers telephone. This technique is called circuit represents carrier switching office . In this example each office has three incoming lines and three outgoing lines. When call passes through switching office physical connection is established be tween the line on which the call came in and one of the output lines as shown by the dotted lines. In the early days of the telephone the connection was made by the operator plugging jumper cable into the input and output sockets. In fact surprising lit tle story is associated with the invention of automatic circuit switching equipment. It was invented by 19thcentury Missouri undertaker named Almon . Strowger. Shortly after the telephone was invented when someone died one of the survivors would call the town operator and say Please connect me to an undertaker. Un fortunately for Mr. Strowger there were two undertakers in his town and the other ones wife was the town telephone operator. He quickly saw that either he was going to have to invent automatic telephone switching equipment or he was going to go out of business. He chose the first option. For nearly 100 years the circuitswitching equipment used worldwide was known as Strowger gear. of the physical path between the two telephones may in fact be microwave or fiber links onto which thousands of calls are multiplexed. Nevertheless the basic idea is valid once call has been set up dedicated path between both ends exists and will continue to exist until the call is finished. An important property of circuit switching is the need to set up an endtoend path before any data can be sent. The elapsed time between the end of dialing and the start of ringing can easily be 10 sec more on longdistance or international calls. During this time interval the telephone system is hunting for path as request signal must propagate all the way to the destination and be acknowledged. For many computer applications long setup times are undesirable. As consequence of the reserved path between the calling parties once the setup has been completed the only delay for data is the propagation time for the electromagnetic signal about 5 msec per 1000 km. Also as consequence of the established path there is no danger of congestionthat is once the call has been put through you never get busy signals. Of course you might get one before the connection has been established due to lack of switching or trunk capacity. Packet Switching 42 and described in Chap. 1. With this technology packets are sent as soon as they are available. There is no need to set up dedicated path in advance unlike Call request signal with circuit switching. It is up to routers to use storeandforward transmission to send each packet on its way to the destination on its own. This procedure is unlike circuit switching in which the result of the connection setup is the reserva tion of bandwidth all the way from the sender to the receiver. All data on the cir cuit follows this path. Among other properties having all the data follow the same path means that it cannot arrive out of order. With packet switching there is no fixed path so different packets can follow different paths depending on net work conditions at the time they are sent and they may arrive out of order. Packetswitching networks place tight upper limit on the size of packets. This ensures that no user can monopolize any transmission line for very long so that packetswitched networks can handle interactive traf fic. It also reduces delay since the first packet of long message can be for warded before the second one has fully arrived. However the storeandforward delay of accumulating packet in the routers memory before it is sent on to the next router exceeds that of circuit switching. With circuit switching the bits just flow through the wire continuously. Packet and circuit switching also differ in other ways. Because no bandwidth is reserved with packet switching packets may have to wait to be forwarded. This introduces queuing delay and congestion if many packets are sent at the same time. On the other hand there is no danger of getting busy signal and being unable to use the network. Thus congestion occurs at different times with circuit switching and packet switching . If circuit has been reserved for particular user and there is no traffic its bandwidth is wasted. It cannot be used for other traffic. Packet switching does not waste bandwidth and thus is more efficient from system perspective. Under standing this tradeoff is crucial for comprehending the difference between circuit switching and packet switching. The tradeoff is between guaranteed service and wasting resources versus not guaranteeing service and not wasting resources. Packet switching is more fault tolerant than circuit switching. In fact that is why it was invented. If switch goes down all of the circuits using it are termi nated and no more traffic can be sent on any of them. With packet switching packets can be routed around dead switches. rithm. With circuit switching charging has historically been based on distance and time. For mobile phones distance usually does not play role except for in free minutes costs more than one with 1000 free minutes and sometimes nights or volume of traffic is. For home users ISPs usually charge flat monthly rate be cause it is less work for them and their customers can understand this model but backbone carriers charge regional networks based on the volume of their traffic. works have used circuit switching to provide highquality telephone calls and computer networks have used packet switching for simplicity and efficiency. However there are notable exceptions. Some older computer networks have been circuit switched under the covers and some newer telephone networks use packet switching with voice over IP technology. This looks just like stan dard telephone call on the outside to users but inside the network packets of voice data are switched. This approach has let upstarts market cheap international calls via calling cards though perhaps with lower call quality than the incumbents. The traditional telephone system even if it someday gets multigigabit endto end fiber will still not be able to satisfy growing group of users people on the go. People now expect to make phone calls and to use their phones to check Storeandforward transmission email and surf the Web from airplanes cars swimming pools and while jogging in the park. Consequently there is tremendous amount of interest in wireless The mobile phone system is used for wide area voice and data communica tion. Mobile phones have gone through three distinct generations widely called 1G 2G and 3G. The generations are 2. Digital voice. 3. Digital voice and data . Although most of our discussion will be about the technology of these sys tems it is interesting to note how political and tiny marketing decisions can have huge impact. The first mobile system was devised in the . by ATT and single system and mobile phone purchased in California also worked in New York. In contrast when mobile phones came to Europe every country de vised its own system which resulted in fiasco. Europe learned from its mistake and when digital came around the govern mentrun PTTs got together and standardized on single system so any European mobile phone will work anywhere in Europe. By then the . had de cided that government should not be in the standardization business so it left digi tal to the marketplace. This decision resulted in different equipment manufact urers producing different kinds of mobile phones. As consequence in the . two majorand completely incompatibledigital mobile phone systems were deployed as well as other minor systems. Despite an initial lead by the . mobile phone ownership and usage in Europe is now far greater than in the . Having single system that works any where in Europe and with any provider is part of the reason but there is more. second area where the . and Europe differed is in the humble matter of phone numbers. In the . mobile phones are mixed in with regular telephones. Thus there is no way for caller to see if say 2345678 is fixed tele phone or mobile phone . To keep people from getting nervous about placing calls the telephone companies decided to make the mobile phone owner pay for incoming calls. As consequence many people hesitated buying mobile phone for fear of running up big bill by just re ceiving calls. In Europe mobile phone numbers have special area code so they are instantly recognizable. Consequently the usual rule of caller pays also applies to mobile phones in Europe . prepaid mobile phones in Europe . These can be pur chased in many stores with no more formality than buying digital camera. You euros and can be recharged when the balance drops to zero. As consequence practically every teenager and many small children in Europe have mobile phones so their parents can locate them without the danger of the child running up huge bill. If the mobile phone is used only occasionally its use is essentially free since there is no monthly charge or charge for incoming calls. Enough about the politics and marketing aspects of mobile phones. Now let us look at the technology starting with the earliest system. Mobile radiotele phones were used sporadically for maritime and military communication during the early decades of the 20th century. In 1946 the first system for carbased tele phones was set up in St. Louis. This system used single large transmitter on top of tall building and had single channel used for both sending and receiving. To talk the user had to push button that enabled the transmitter and disabled the receiver. Such systems known as pushtotalk systems were installed in several cities beginning in the late 1950s. CB radio taxis and police cars often use this In the 1960s IMTS was installed. It too used highpowered transmitter on top of hill but it had two frequencies one for sending and one for receiving so the pushtotalk button was no longer needed. Since all communication from the mobile telephones went inbound on different channel than the outbound signals the mobile users could not hear each other . IMTS supported 23 channels spread out from 150 MHz to 450 MHz. Due to the small number of channels users often had to wait long time before getting dial tone. Also due to the large power of the hilltop transmitters adjacent sys tems had to be several hundred kilometers apart to avoid interference. All in all All that changed with AMPS invented by Bell Labs and first installed in the United States in 1982. It was also used in England where it was called TACS and in Japan where it was called MCSL1. AMPS was formally retired in 2008 but we will look at it to understand the con text for the 2G and 3G systems that improved on it. In all mobile phone systems geographic region is divided up into cells which is why the devices are sometimes called cell phones. In AMPS the cells are typically 10 to 20 km across in digital systems the cells are smaller. Each cell uses some set of frequencies not used by any of its neighbors. The key idea that gives cellular systems far more capacity than previous systems is the use of relatively small cells and the reuse of transmission frequencies in nearby cells. Whereas an IMTS system 100 km across can have only one call on each frequency an AMPS system might have 100 10km cells in the same area and be able to have 10 to 15 calls on each frequency in widely separated cells. Thus the cellular design increases the system capacity by at least an order of magnitude more as the cells get smaller. Furthermore smaller cells mean that less power is needed which leads to smaller and cheaper transmitters and the cells are all the same size. They are grouped in units of seven cells. Each letter indicates group of frequencies. Notice that for each frequency set there is buffer about two cells wide where that frequency is not reused providing for good separation and low interference. with the Roman Catholic Church since the latter owns substantial number of exalted potential antenna sites worldwide all conveniently under single man In an area where the number of users has grown to the point that the system is overloaded the power can be reduced and the overloaded cells split into smaller users smaller cells can be used. companies sometimes create temporary microcells using portable towers with satellite links at sporting events rock concerts and other places where large num bers of mobile users congregate for few hours. At the center of each cell is base station to which all the telephones in the cell transmit. The base station consists of computer and transmitterreceiver connected to an antenna. In small system all the base stations are connected to single device called an MSC or MTSO . In larger one several MSCs may be needed all of which are connected to secondlevel MSC and so on. The MSCs are essen tially end offices as in the telephone system and are in fact connected to at least one telephone system end office. The MSCs communicate with the base stations each other and the PSTN using packetswitching network. At any instant each mobile telephone is logically in one specific cell and un der the control of that cells base station. When mobile telephone physically leaves cell its base station notices the telephones signal fading away and asks all the surrounding base stations how much power they are getting from it. When ting the strongest signal under most conditions that is the cell where the tele phone is now located. The telephone is then informed of its new boss and if call is in progress it is asked to switch to new channel . This process called handoff takes about 300 msec. Channel assignment is done by the MSC the nerve center of the sys tem. The base stations are really just dumb radio relays. AMPS uses FDM to separate the channels. The system uses 832 fullduplex channels each consisting of pair of simplex channels. This arrangement is known as FDD . The 832 simplex channels from 824 to 849 MHz are used for mobile to base station transmission and 832 simplex channels from 869 to 894 MHz are used for base station to mobile transmission. Each of these simplex channels is 30 kHz wide. The 832 channels are divided into four categories. Control channels are used to manage the system. Paging channels alert mobile users to calls for them. Access channels are used for call channels are reserved in each cell for control the actual number of voice channels available per cell is much smaller than 832 typically about 45. Each mobile telephone in AMPS has 32bit serial number and 10digit telephone number in its programmable readonly memory. The telephone number is represented as 3digit area code in 10 bits and 7digit subscriber number in 24 bits. When phone is switched on it scans preprogrammed list of 21 control channels to find the most powerful signal. The phone then broadcasts its 32bit serial number and 34bit telephone number. Like all the control information in AMPS this packet is sent in digital form multiple times and with an errorcor recting code even though the voice channels themselves are analog. When the base station hears the announcement it tells the MSC which records the existence of its new customer and also informs the customers home reregisters about once every 15 minutes. To make call mobile user switches on the phone enters the number to be called on the keypad and hits the SEND button. The phone then transmits the number to be called and its own identity on the access channel. If collision oc curs there it tries again later. When the base station gets the request it informs the MSC. If the caller is customer of the MSCs company the MSC looks for an idle channel for the call. If one is found the channel number is sent back on the control channel. The mobile phone then auto matically switches to the selected voice channel and waits until the called party picks up the phone. Incoming calls work differently. To start with all idle phones continuously listen to the paging channel to detect messages directed at them. When call is placed to mobile phone packet is sent to the callees home MSC to find out where it is. packet is then sent to the base station in its current cell which sends broadcast on the paging channel of the form Unit 14 are you there The called phone responds with Yes on the access channel. The base then says something like Unit 14 call for you on channel 3. At this point the called phone switches to channel 3 and starts making ringing sounds . The first generation of mobile phones was analog the second generation is digital. Switching to digital has several advantages. It provides capacity gains by allowing voice signals to be digitized and compressed. It improves security by al lowing voice and control signals to be encrypted. This in turn deters fraud and eavesdropping whether from intentional scanning or echoes of other calls due to Just as there was no worldwide standardization during the first generation there was also no worldwide standardization during the second either. Several different systems were developed and three have been widely deployed. that coexists with AMPS and uses TDM to place multiple calls on the same fre quency channel. It is described in International Standard IS54 and its successor IS136. GSM has emerged as the dominant system and while it was slow to catch on in the . it is now used vir tually everywhere in the world. Like DAMPS GSM is based on mix of FDM and TDM. CDMA described in International Standard IS95 is completely different kind of system and is based on neither FDM mor TDM. While CDMA has not become the dominant 2G system its technology has become the basis for 3G systems. Also the name PCS is sometimes used in the marketing literature to indicate secondgeneration sys tem. Originally it meant mobile phone using the 1900 MHz band but that dis tinction is rarely made now. We will now describe GSM since it is the dominant 2G system. In the next GSM started life in the 1980s as an effort to produce single European 2G standard. The task was assigned to telecommunications group called and were quick success. It soon became clear that GSM was going to be more than European success with uptake stretching to countries as far away as Aus tralia so GSM was renamed to have more worldwide appeal. GSM and the other mobile phone systems we will study retain from 1G sys tems design based on cells frequency reuse across cells and mobility with handoffs as subscribers move. It is the details that differ. Here we will briefly discuss some of the main properties of GSM. However the printed GSM stan gineering aspects of the system especially the design of receivers to handle mul tipath signal propagation and synchronizing transmitters and receivers. None of this will be even mentioned here. ture though the components have different names. The mobile itself is now di vided into the handset and removable chip with subscriber and account infor mation called SIM card short for Subscriber Identity Module. It is the SIM card that activates the handset and contains secrets that let the mobile and the net work identify each other and encrypt conversations. SIM card can be removed and plugged into different handset to turn that handset into your mobile as far as the network is concerned. The mobile talks to cell base stations over an air interface that we will de scribe in moment. The cell base stations are each connected to BSC that controls the radio resources of cells and handles handoff. The BSC in turn is connected to an MSC that routes calls and con nects to the PSTN . To be able to route calls the MSC needs to know where mobiles can currently be found. It maintains database of nearby mobiles that are associated with the We will now describe the air interface in some detail. GSM runs on range of frequencies worldwide including 900 1800 and 1900 MHz. More spectrum is allocated than for AMPS in order to support much larger number of users. GSM is frequency division duplex cellular system like AMPS. That is each mobile transmits on one frequency and receives on another higher frequency . However unlike with AMPS with GSM single frequency pair is split by timedivision multiplexing into time slots. In this way it is shared by multiple mobiles. To handle multiple mobiles GSM channels are much wider than the AMPS GSM system operating in the 900MHz region has 124 pairs of simplex chan nels. Each simplex channel is 200 kHz wide and supports eight separate con nections on it using time division multiplexing. Each currently active station is assigned one time slot on one channel pair. Theoretically 992 channels can be supported in each cell but many of them are not available to avoid frequency long to the same connection four of them in each direction. Transmitting and re ceiving does not happen in the same time slot because the GSM radios cannot transmit and receive at the same time and it takes time to switch from one to the other. If the mobile device assigned to 890.4935.4 MHz and time slot 2 wanted to transmit to the base station it would use the lower four shaded slots putting some data in each slot until all the data had slot TDM system. Each TDM slot has specific structure and groups of TDM slots form mul data frame that occupies the channel for 577 μsec . Each data frame starts and ends with three 0 bits for frame del ineation purposes. It also contains two 57bit Information fields each one having control bit that indicates whether the following Information field is for voice or data. Between the Information fields is 26bit Sync field that is used by the receiver to synchronize to the senders frame boundaries. data frame is transmitted in 547 μsec but transmitter is only allowed to send one data frame every 4.615 msec since it is sharing the channel with seven other stations. The gross rate of each channel is 270833 bps divided among eight users. However as with AMPS the overhead eats up large fraction of the band width ultimately leaving 24.7 kbps worth of payload per user before error cor rection. After error correction 13 kbps is left for speech. While this is substan tially less than 64 kbps PCM for uncompressed voice signals in the fixed tele phone network compression on the mobile device can reach these levels with lit tle loss of quality. multiframe slot 12 is used for control and slot 25 is reserved for future use so only 24 are available for user traffic. multiframe is also used. Some of these slots are used to hold several control channels used to manage the system. The broadcast control channel is continuous stream of output from the base station containing the base stations to see when they have moved into new cell. and call setup. In particular each BSC maintains database of mobile stations currently under its jurisdiction the VLR. Information needed to maintain the VLR is sent on the dedicated control channel. logical subchannels. The first of these subchannels is the paging channel which the base station uses to announce incoming calls. Each mobile station monitors it continuously to watch for calls it should answer. The second is the random ac cess channel which allows users to request slot on the dedicated control chan nel. If two requests collide they are garbled and have to be retried later. Using the dedicated control channel slot the station can set up call. The assigned slot is announced on the third subchannel the access grant channel. MSC manages it completely without help from the mobile devices. With time slots in GSM the mobile is neither sending nor receiving most of the time. The idle slots are an opportunity for the mobile to measure signal quality to other nearby base stations. It does so and sends this information to the BSC. The BSC can use it to determine when mobile is leaving one cell and entering another so it can perform the handoff. This design is called MAHO and there is not enough data capacity to keep users happy. 3G mobile telephony is all about pro viding enough wireless bandwidth to keep these future users happy. ITU tried to get bit more specific about this vision starting back around IMT2000 network was supposed to provide to its users are 1. Highquality voice transmission. 2. Messaging . 3. Multimedia . Additional services might be video conferencing telepresence group game play ing and mcommerce . Furthermore all these services are supposed to be available worldwide instantly and with quality of service guarantees. ITU envisioned single worldwide technology for IMT2000 so manufact urers could build single device that could be sold and used anywhere in the world . Having single technology would also make life much simpler for network opera tors and would encourage more people to use the services. Format wars such as the Betamax versus VHS battle with videorecorders are not good for business. As it turned out this was bit optimistic. The number 2000 stood for three things the year it was supposed to go into service the frequency it was supposed to operate at and the bandwidth the service should have . It did not make it on any of the three counts. Nothing was imple GHz so devices could roam seamlessly from country to country. China reserved Mbps is not currently feasible for users who are too mobile . More realistic is 2 Mbps for stationary indoor users 384 kbps for people walk ing and 144 kbps for connections in cars. Despite these initial setbacks much has been accomplished since then. Sev eral IMT proposals were made and after some winnowing it came down to two main ones. The first one WCDMA was proposed by Ericsson and was pushed by the European Union which called it UMTS . Europe wanted system that interworked with GSM whereas the . wanted system that was compatible with one already widely deployed in the . Each side also supported its local company and matured to the point that it became the technical basis for To make CDMA work in the mobile phone setting requires more than the we described synchronous CDMA in which the chip sequences are exactly orthogonal. This design works when all users are synchronized on the start time of their chip sequences as in the case of the base station transmitting to mobiles. The base station can transmit the chip sequences starting at the same time so that the signals will be orthogonal and able to be separated. However it is difficult to synchronize the transmissions of independent mobile phones. Without care their transmissions would arrive at the base station at different times with no guarantee of orthogonality. To let mobiles send to the base station without synchronization we want code sequences that are orthogonal to each other at all possible offsets not simply when they are aligned at the start. While it is not possible to find sequences that are exactly orthogonal for this general case long pseudorandom sequences come close enough. They have the property that with high probability they have low crosscorrelation with each other at all offsets. This means that when one sequence is multiplied by another sequence and summed up to compute the inner product the result will be small it would be zero if they were orthogonal. This lets receiver filter unwanted transmissions out of the received signal. Also the autocorrelation of pseudorandom sequences is also small with high probability except at zero off set. This means that when one sequence is multiplied by delayed copy of itself and summed the result will be small except when the delay is zero. This lets receiver lock onto the beginning of the wanted transmission in the received signal. The use of pseudorandom sequences lets the base station receive CDMA mes sages from unsynchronized mobiles. However an implicit assumption in our dis cussion of CDMA is that the power levels of all mobiles are the same at the re ceiver. If they are not small crosscorrelation with powerful signal might overwhelm large autocorrelation with weak signal. Thus the transmit power on mobiles must be controlled to minimize interference between competing sig nals. It is this interference that limits the capacity of CDMA systems. The power levels received at base station depend on how far away the trans mitters are as well as how much power they transmit. There may be many mobile stations at varying distances from the base station. good heuristic to equalize the received power is for each mobile station to transmit to the base station at the inverse of the power level it receives from the base station. In other words mobile station receiving weak signal from the base station will use more power than one getting strong signal. For more accuracy the base station also gives each mobile feedback to increase decrease or hold steady its transmit power. The feedback is frequent because good power control is im portant to minimize interference. Another improvement over the basic CDMA scheme we described earlier is to allow different users to send data at different rates. This trick is accomplished naturally in CDMA by fixing the rate at which chips are transmitted and assigning users chip sequences of different lengths. For example in WCDMA the chip rate is 3.84 Mchipssec and the spreading codes vary from 4 to 256 chips. With 256 chip code around 12 kbps is left after error correction and this capacity is suffi cient for voice call. With 4chip code the user data rate is close to 1 Mbps. Intermediatelength codes give intermediate rates to get to multiple Mbps the mobile must use more than one 5MHz channel at once. Now let us describe the advantages of CDMA given that we have dealt with can improve capacity by taking advantage of small periods when some trans mitters are silent. In polite voice calls one party is silent while the other talks. On average the line is busy only 40 of the time. However the pauses may be small and are difficult to predict. With TDM or FDM systems it is not possible to reas sign time slots or frequency channels quickly enough to benefit from these small silences. However in CDMA by simply not transmitting one user lowers the in terference for other users and it is likely that some fraction of users will not be transmitting in busy cell at any given time. Thus CDMA takes advantage of ex pected silences to allow larger number of simultaneous calls. Second with CDMA each cell uses the same frequencies. Unlike GSM and AMPS FDM is not needed to separate the transmissions of different users. This eliminates complicated frequency planning tasks and improves capacity. It also makes it easy for base station to use multiple directional antennas or sectored antennas instead of an omnidirectional antenna. Directional antennas concen trate signal in the intended direction and reduce the signal and hence inter ference in other directions. This in turn increases capacity. Three sector designs are common. The base station must track the mobile as it moves from sector to sector. This tracking is easy with CDMA because all frequencies are used in all Third CDMA facilitates soft handoff in which the mobile is acquired by the new base station before the previous one signs off. In this way there is no loss of frequencies are used in each cell. The alternative is hard handoff in which the old base station drops the call before the new one acquires it. If the new one is unable to acquire it the call is disconnected abruptly. Users tend to notice this but it is inevitable occasionally with the current design. Hard handoff is the norm with FDM designs to avoid the cost of having the mobile transmit or receive on two frequencies simultaneously. Much has been written about 3G most of it praising it as the greatest thing since sliced bread. Meanwhile many operators have taken cautious steps in the direction of 3G by going to what is sometimes called 2.5G although 2.1G might be more accurate. One such system is EDGE which is just GSM with more bits per symbol. The trouble is more bits per symbol also means more errors per symbol so EDGE has nine different schemes for modulation and error correction differing in terms of how much of the bandwidth is devoted to fixing the errors introduced by the higher speed. EDGE is one step along an evolutionary path that is defined from GSM to Similarly there is an evolutionary path defined for operators to upgrade from IS95 to CDMA2000 networks. Even though 3G networks are not fully deployed yet some researchers regard 3G as done deal. These people are already working on 4G systems under the name of LTE . Some of the proposed features of 4G in clude high bandwidth ubiquity seamless integration with other wired and wireless IP networks including 802.11 access points adap tive resource and spectrum management and high quality of service for multi media. For more information see Astely et al. and Larmo et al. Meanwhile wireless networks with 4G levels of performance are already of mobile WiMAX see Ahmadi . To say the industry is in state of flux is huge understatement. Check back in few years to see what has happened. We have now studied both the fixed and wireless telephone systems in fair amount of detail. Both will clearly play major role in future networks. But there is another major player that has emerged over the past decade for Internet access cable television networks. Many people nowadays get their telephone and vision as network in more detail and contrast it with the telephone systems we and Jones DuttaRoy and Fellows and Jones . Cable television was conceived in the late 1940s as way to provide better reception to people living in rural or mountainous areas. The system initially con sisted of big antenna on top of hill to pluck the television signal out of the air an amplifier called the headend to strengthen it and coaxial cable to deliver it In the early years cable television was called Community Antenna Televis ion. It was very much momandpop operation anyone handy with electronics could set up service for his town and the users would chip in to pay the costs. As the number of subscribers grew additional cables were spliced onto the origi nal cable and amplifiers were added as needed. Transmission was one way from the headend to the users. By 1970 thousands of independent systems existed. In 1974 Time Inc. started new channel Home Box Office with new content distributed only on cable. Other cableonly channels followed focusing on news sports cooking and many other topics. This development gave rise to two changes in the industry. First large corporations began buying up existing cable systems and laying new cable to acquire new subscribers. Second there was now need to connect multiple systems often in distant cities in order to dis tribute the new cable channels. The cable companies began to lay cable between the cities to connect them all into single system. This pattern was analogous to what happened in the telephone industry 80 years earlier with the connection of previously isolated end offices to make longdistance calling possible. Over the course of the years the cable system grew and the cables between the various cities were replaced by highbandwidth fiber similar to what happened in the telephone system. system with fiber for the longhaul runs and coaxial cable to the houses is called an HFC system. The electro optical converters that interface between the optical and electrical parts of the sys tem are called fiber nodes. Because the bandwidth of fiber is so much greater than that of coax fiber node can feed multiple coaxial cables. Part of modern Over the past decade many cable operators decided to get into the Internet access business and often the telephony business as well. Technical differences between the cable plant and telephone plant had an effect on what had to be done to achieve these goals. For one thing all the oneway amplifiers in the system had to be replaced by twoway amplifiers to support upstream as well as down stream transmissions. While this was happening early Internet over cable sys tems used the cable television network for downstream transmissions and dial up connection via the telephone network for upstream transmissions. It was clever workaround but not much of network compared to what it could be. the neighborhoods single cable is shared by many houses whereas in the tele phone system every house has its own private local loop. When used for televis ion broadcasting this sharing is natural fit. All the programs are broadcast on the cable and it does not matter whether there are 10 viewers or 10000 viewers. When the same cable is used for Internet access however it matters lot if there are 10 users or 10000. If one user decides to download very large file that bandwidth is potentially being taken away from other users. The more users there are the more competition there is for bandwidth. The telephone system does not have this particular property downloading large file over an ADSL line does not reduce your neighbors bandwidth. On the other hand the bandwidth of coax is much higher than that of twisted pairs so you can get lucky if your neighbors do not use the Internet much. The way the cable industry has tackled this problem is to split up long cables and connect each one directly to fiber node. The bandwidth from the headend to each fiber node is effectively infinite so as long as there are not too many sub scribers on each cable segment the amount of traffic is manageable. Typical cables nowadays have 5002000 houses but as more and more people subscribe to Internet over cable the load may become too great requiring more splitting and more fiber nodes. Throwing off all the TV channels and using the cable infrastructure strictly for Internet access would probably generate fair number of irate customers so cable companies are hesitant to do this. Furthermore most cities heavily regulate what is on the cable so the cable operators would not be allowed to do this even if they really wanted to. As consequence they needed to find way to have tele vision and Internet peacefully coexist on the same cable. The solution is to build on frequency division multiplexing. Cable television channels in North America occupy the 54550 MHz region . These channels are 6MHz wide including guard bands and can carry one traditional analog television channel or several digital television channels. In Europe the low end is usually 65 MHz and the channels are 68 MHz wide for the higher resolution required by PAL and SECAM but otherwise cables can also operate well above 550 MHz often at up to 750 MHz or more. The solution chosen was to introduce upstream channels in the 542 MHz band and use the frequencies at the high end for the down Note that since the television signals are all downstream it is possible to use upstream amplifiers that work only in the 542 MHz region and downstream an asymmetry in the upstream and downstream bandwidths because more spec trum is available above television than below it. On the other hand most users want more downstream traffic so cable operators are not unhappy with this fact of life. As we saw earlier telephone companies usually offer an asymmetric DSL service even though they have no technical reason for doing so. In addition to upgrading the amplifiers the operator has to upgrade the headend too from dumb amplifier to an intelligent digital computer system with highbandwidth fiber interface to an ISP. Often the name gets upgraded as well from headend to CMTS . In the following text we will refrain from doing name upgrade and stick with the tra Internet access requires cable modem device that has two interfaces on it one to the computer and one to the cable network. In the early years of cable In ternet each operator had proprietary cable modem which was installed by cable company technician. However it soon became apparent that an open stan dard would create competitive cable modem market and drive down prices thus encouraging use of the service. Furthermore having the customers buy cable modems in stores and install them themselves would eliminate the dreaded truck rolls. Consequently the larger cable operators teamed up with company called CableLabs to produce cable modem standard and to test products for compli ance. This standard called DOCSIS MHz downstream channel and modulate it with QAM64 or if the cable quality is exceptionally good QAM256. With 6MHz channel and QAM64 we get about 36 Mbps. When the overhead is subtracted the net payload is about 27 Mbps. With QAM 256 the net payload is about 39 Mbps. The European values are 13 larger. For upstream there is more RF noise because the system was not originally designed for data and noise from multiple subscribers is funneled to the headend so more conservative scheme is used. This ranges from QPSK to QAM128 where some of the symbols are used for error protection with Trellis Coded Mod ulation. With fewer bits per symbol on the upstream the asymmetry between TDM is then used to share bandwidth on the upstream across multiple sub scribers. Otherwise their transmissions would collide at the headend. Time is di vided into minislots and different subscribers send in different minislots. To make this work the modem determines its distance from the headend by sending it special packet and seeing how long it takes to get the response. This process is called ranging. It is important for the modem to know its distance to get the timing right. Each upstream packet must fit in one or more consecutive minislots at the headend when it is received. The headend announces the start of new round of minislots periodically but the starting gun is not heard at all modems si multaneously due to the propagation time down the cable. By knowing how far it is from the headend each modem can compute how long ago the first minislot really started. Minislot length is network dependent. typical payload is 8 bytes. During initialization the headend assigns each modem to minislot to use for requesting upstream bandwidth. When computer wants to send packet it transfers the packet to the modem which then requests the necessary number of minislots for it. If the request is accepted the headend puts an acknowledgement on the downstream channel telling the modem which minislots have been reserved for its packet. The packet is then sent starting in the minislot allocated to it. Ad As rule multiple modems will be assigned the same minislot which leads to contention. Two different possibilities exist for dealing with it. The first is that CDMA is used to share the minislot between subscribers. This solves the con tention problem because all subscribers with CDMA code sequence can send at the same time albeit at reduced rate. The second option is that CDMA is not used in which case there may be no acknowledgement to the request because of collision. In this case the modem just waits random time and tries again. After each successive failure the randomization time is doubled. so there is no contention and no need for minislots which is actually just statistical time division multi plexing. For another the amount of traffic downstream is usually much larger than upstream so fixed packet size of 204 bytes is used. Part of that is Reed Solomon errorcorrecting code and some other overhead leaving user payload of 184 bytes. These numbers were chosen for compatibility with digital television using MPEG2 so the TV and downstream data channels are formatted the same Which is better ADSL or cable That is like asking which operating system is better. Or which language is better. Or which religion. Which answer you get depends on whom you ask. Let us compare ADSL and cable on few points. Both use fiber in the backbone but they differ on the edge. Cable uses coax ADSL uses twisted pair. The theoretical carrying capacity of coax is hundreds of times more than twisted pair. However the full capacity of the cable is not avail able for data users because much of the cables bandwidth is wasted on useless stuff such as television programs. In practice it is hard to generalize about effective capacity. ADSL providers give specific statements about the bandwidth and generally achieve about 80 of it consistently. Cable providers may artificially cap the bandwidth to each user to help them make performance predictions but they cannot really give guarantees because the effective capacity depends on how many people are currently active on the users cable segment. Sometimes it may be better than ADSL and sometimes it may be worse. What can be annoying though is the unpredictability. Having great service one minute does not guarantee great service the next minute since the biggest bandwidth hog in town may have just turned on his computer. As an ADSL system acquires more users their increasing numbers have little effect on existing users since each user has dedicated connection. With cable as more subscribers sign up for Internet service performance for existing users will drop. The only cure is for the cable operator to split busy cables and connect each one to fiber node directly. Doing so costs time and money so there are business pressures to avoid it. As an aside we have already studied another system with shared channel like cable the mobile telephone system. Here too group of userswe could call them cellmatesshare fixed amount of bandwidth. For voice traffic which is fairly smooth the bandwidth is rigidly divided in fixed chunks among the active users using FDM and TDM. But for data traffic this rigid division is very ineffi cient because data users are frequently idle in which case their reserved band width is wasted. As with cable more dynamic means is used to allocate the shared bandwidth. phone but not all users are close enough to their end offices to get ADSL. On the other hand not everyone has cable but if you do have cable and the company pro vides Internet access you can get it. Distance to the fiber node or headend is not ution medium few businesses have it. Being pointtopoint medium ADSL is inherently more secure than cable. Any cable user can easily read all the packets going down the cable. For this rea son any decent cable provider will encrypt all traffic in both directions. Never theless having your neighbor get your encrypted messages is still less secure than having him not get anything at all. The telephone system is generally more reliable than cable. For example it has backup power and continues to work normally even during power outage. With cable if the power to any amplifier along the chain fails all downstream users are cut off instantly. even required to do so by law. Such is not always the case with cable operators. The conclusion is that ADSL and cable are much more alike than they are dif ferent. They offer comparable service and as competition between them heats up probably comparable prices. The physical layer is the basis of all networks. Nature imposes two funda mental limits on all channels and these determine their bandwidth. These limits are the Nyquist limit which deals with noiseless channels and the Shannon limit which deals with noisy channels. Transmission media can be guided or unguided. The principal guided media are twisted pair coaxial cable and fiber optics. Unguided media include terres trial radio microwaves infrared lasers through the air and satellites. Digital modulation methods send bits over guided and unguided media as ana log signals. Line codes operate at baseband and signals can be placed in passband by modulating the amplitude frequency and phase of carrier. Chan nels can be shared between users with time frequency and code division multi key element in most wide area networks is the telephone system. Its main components are the local loops trunks and switches. ADSL offers speeds up to lel. This far exceeds the rates of telephone modems. PONs bring fiber to the home for even greater access rates than ADSL. Trunks carry digital information. They are multiplexed with WDM to provi sion many high capacity links over individual fibers as well as with TDM to switching are important. For mobile applications the fixed telephone system is not suitable. Mobile phones are currently in widespread use for voice and increasingly for data. They have gone through three generations. The first generation 1G was analog and dominated by AMPS. 2G was digital with GSM presently the most widely de ployed mobile phone system in the world. 3G is digital and based on broadband CDMA with WCDMA and also CDMA2000 now being deployed. An alternative system for network access is the cable television system. It has gradually evolved from coaxial cable to hybrid fiber coax and from television to television and Internet. Potentially it offers very high bandwidth but the band width in practice depends heavily on the other users because it is shared. 1. Compute the Fourier coefficients for the function 2. noiseless 4kHz channel is sampled every 1 msec. What is the maximum data rate How does the maximum data rate change if the channel is noisy with signaltonoise 3. Television channels are 6 MHz wide. How many bitssec can be sent if fourlevel dig ital signals are used Assume noiseless channel. 4. If binary signal is sent over 3kHz channel whose signaltonoise ratio is 20 dB what is the maximum achievable data rate 5. What signaltonoise ratio is needed to put T1 carrier on 50kHz line 6. What are the advantages of fiber optics over copper as transmission medium Is there any downside of using fiber optics over copper 8. It is desired to send sequence of computer screen images over an optical fiber. The screen is 2560 1600 pixels each pixel being 24 bits. There are 60 screen images per second. How much bandwidth is needed and how many microns of wavelength are needed for this band at 1.30 microns 9. Is the Nyquist theorem true for highquality singlemode optical fiber or only for 10. Radio antennas often work best when the diameter of the antenna is equal to the wave length of the radio wave. Reasonable antennas range from 1 cm to 5 meters in diame ter. What frequency range does this cover 11. laser beam 1 mm wide is aimed at detector 1 mm wide 100 away on the roof of before it misses the detector 12. The 66 loworbit satellites in the Iridium project are divided into six necklaces around the earth. At the altitude they are using the period is 90 minutes. What is the average interval for handoffs for stationary transmitter 13. Calculate the endtoend transit time for packet for both GEO MEO and LEO satellites. 14. What is the latency of call originating at the North Pole to reach the South Pole if the call is routed via Iridium satellites Assume that the switching time at the satellites is 10 microseconds and earths radius is 6371 km. 15. What is the minimum bandwidth needed to achieve data rate of bitssec if the sig nal is transmitted using NRZ MLT3 and Manchester encoding Explain your 16. Prove that in 4B5B encoding signal transition will occur at least every four bit 17. How many end office codes were there pre1984 when each end office was named by its threedigit area code and the first three digits of the local number Area codes started with digit in the range 29 had 0 or 1 as the second digit and ended with any digit. The first two digits of local number were always in the range 29. The third digit could be any digit. 18. simple telephone system consists of two end offices and single toll office to which each end office is connected by 1MHz fullduplex trunk. The average telephone is used to make four calls per 8hour workday. The mean call duration is 6 min. Ten percent of the calls are long distance . What is the maximum number of telephones an end office can support Explain why telephone company may decide to support lesser number of telephones than this maximum number at the end office. 19. regional telephone company has 10 million subscribers. Each of their telephones is connected to central office by copper twisted pair. The average length of these twisted pairs is 10 km. How much is the copper in the local loops worth Assume is 9.0 gramscm3 and that copper sells for 6 per kilogram. 20. Is an oil pipeline simplex system halfduplex system fullduplex system or none of the above What about river or walkietalkiestyle communication 21. The cost of fast microprocessor has dropped to the point where it is now possible to put one in each modem. How does that affect the handling of telephone line errors these parameters achieve at 1200 symbolssecond 23. What is the maximum bit rate achievable in .32 standard modem if the baud rate is 1200 and no error correction is used 24. How many frequencies does fullduplex QAM64 modem use 25. Ten signals each requiring 4000 Hz are multiplexed onto single channel using FDM. What is the minimum bandwidth required for the multiplexed channel As sume that the guard bands are 400 Hz wide. 26. Why has the PCM sampling time been set at 125 μsec 27. What is the percent overhead on T1 carrier That is what percent of the 1.544 Mbps are not delivered to the end user How does it relate to the percent overhead in OC1 or OC768 lines 28. Compare the maximum data rate of noiseless 4kHz channel using Analog encoding with 2 bits per sample. The T1 PCM system. 29. If T1 carrier system slips and loses track of where it is it tries to resynchronize using the first bit in each frame. How many frames will have to be inspected on average to resynchronize with probability of 0.001 of being wrong 30. What is the difference if any between the demodulator part of modem and the coder part of codec 31. SONET clocks have drift rate of about 1 part in 109. How long does it take for the drift to equal the width of 1 bit Do you see any practical implications of this calcula tion If so what 32. How long will it take to transmit 1GB file from one VSAT to another using hub and circuit switching is used with 1.2 sec circuit setup time. 33. Calculate the transmit time in the previous problem if packet switching is used instead. number can be derived from the SONET OC3 parameters. What will be the gross SPE and user data rates of an OC3072 line taries . VT is partial payload that can be inserted into an STS1 frame and combined with other partial payloads to fill the data frame. VT1.5 uses 3 columns VT2 uses 4 columns VT3 uses 6 columns and VT6 uses 12 columns of an STS1 DS2 service 36. What is the available user bandwidth in an OC12c connection 37. Three packetswitching networks each contain nodes. The first network has star topology with central switch the second is ring and the third is fully interconnected with wire from every node to every other node. What are the best average and worstcase transmission paths in hops 38. Compare the delay in sending an xbit message over khop path in circuitswitched network and in packetswitched network. The circuit setup time is sec the propagation delay is sec per hop the packet size is bits and the data rate is bps. Under what conditions does the packet network have lower delay Also ex plain the conditions under which packetswitched network is preferable to circuit switched network. 39. Suppose that bits of user data are to be transmitted over khop path in packet with . The bit rate of the lines is bps and the propagation delay is negligi ble. What value of minimizes the total delay 40. In typical mobile phone system with hexagonal cells it is forbidden to reuse fre quency band in an adjacent cell. If 840 frequencies are available how many can be used in given cell shapes of individual cells are typically irregular. Give possible reason why this might be. How do these irregular shapes affect frequency assignment to each cell 42. Make rough estimate of the number of PCS microcells 100 in diameter it would take to cover San Francisco . 43. Sometimes when mobile user crosses the boundary from one cell to another the cur rent call is abruptly terminated even though all transmitters and receivers are func tioning perfectly. Why 44. Suppose that and are simultaneously transmitting 0 bits using CDMA sys 45. Consider different way of looking at the orthogonality property of CDMA chip se quences. Each bit in pair of sequences can match or not match. Express the ortho gonality property in terms of matches and mismatches. did each one send are added. Provide the chip sequences of these stations. 48. At the low end the telephone system is star shaped with all the local loops in neigh borhood converging on an end office. In contrast cable television consists of single long cable snaking its way past all the houses in the same neighborhood. Suppose that future TV cable were 10Gbps fiber instead of copper. Could it be used to simulate the telephone model of everybody having their own private line to the end office If so how many onetelephone houses could be hooked up to single fiber 49. cable company decides to provide Internet access over cable in neighborhood con lowing 100 Mbps downstream bandwidth per cable. To attract customers the com pany decides to guarantee at least 2 Mbps downstream bandwidth to each house at any time. Describe what the cable company needs to do to provide this guarantee. how many Mbps does cable system allocate to upstream and how many to down 51. How fast can cable user receive data if the network is otherwise idle Assume that 54Mbps Wireless. 52. Multiplexing STS1 multiple data streams called tributaries plays an important role in SONET. 31 multiplexer multiplexes three input STS1 tributaries onto one out put STS3 stream. This multiplexing is done byte for byte. That is the first three out put bytes are the first bytes of tributaries 1 2 and 3 respectively. the next three out put bytes are the second bytes of tributaries 1 2 and 3 respectively and so on. Write program that simulates this 31 multiplexer. Your program should consist of five processes. The main process creates four processes one each for the three STS1 tri butaries and one for the multiplexer. Each tributary process reads in an STS1 frame from an input file as sequence of 810 bytes. They send their frames to the multiplexer process. The multiplexer process receives these bytes and outputs an STS3 frame by writing it to standard output. Use pipes for communi cation among processes. 53. Write program to implement CDMA. Assume that the length of chip sequence is eight and the number of stations transmitting is four. Your program consists of three sets of processes four transmitter processes one joiner process and four receiver processes . The main program which also acts as the joiner process first reads four chip sequences from the standard input and sequence of 4 bits and forks off four pairs of transmitter and receiver processes. Each pair of transmitterre ceiver processes is assigned one chip sequence and each transmitter process is assigned 1 bit . Next each transmitter process computes the signal to be transmitted and sends it to the joiner process. After receiving signals from all four transmitter processes the joiner process combines the signals and sends the combined signal to the four receiver processes. Each receiver process then computes the bit it has re ceived and prints it to standard output. Use pipes for communication between proc model the data link layer. This study deals with algorithms for achieving re liable efficient communication of whole units of information called frames between two adjacent machines. By adjacent we mean that the two machines are connected by communication channel that acts conceptually like wire . The essential property of channel that makes it wirelike is that the bits are delivered in exactly the same order in which they are sent. At first you might think this problem is so trivial that there is nothing to studymachine just puts the bits on the wire and machine just takes them Unfortunately communication channels make errors occasionally. thermore they have only finite data rate and there is nonzero propagation delay between the time bit is sent and the time it is received. These limitations have important implications for the efficiency of the data transfer. The protocols used for communications must take all these factors into consideration. These will start our study of its protocols by looking at the nature of errors and how they can be detected and corrected. Then we will study series of increasingly com The data link layer uses the services of the physical layer to send and receive bits over communication channels. It has number of functions including 1. Providing welldefined service interface to the network layer. 2. Dealing with transmission errors. 3. Regulating the flow of data so that slow receivers are not swamped by fast senders. To accomplish these goals the data link layer takes the packets it gets from the network layer and encapsulates them into frames for transmission. Each frame Receiving machine many of the principles we will study here such as error control and flow control are found in transport and other protocols as well. That is because reliability is an overall goal and it is achieved when all the layers work together. In fact in many networks these functions are found mostly in the upper layers with the data link layer doing the minimal job that is good enough. However no matter where they are found the principles are pretty much the same. They often show up in their simplest and purest forms in the data link layer making this good place to examine them in detail. The function of the data link layer is to provide services to the network layer. The principal service is transferring data from the network layer on the source ma chine to the network layer on the destination machine. On the source machine is an entity call it process in the network layer that hands some bits to the data link layer for transmission to the destination. The job of the data link layer is to transmit the bits to the destination machine so they can be handed over to the net esses communicating using data link protocol. For this reason we will impli The data link layer can be designed to offer various services. The actual ser vices that are offered vary from protocol to protocol. Three reasonable possibili 3. Acknowledged connectionoriented service. Unacknowledged connectionless service consists of having the source ma chine send independent frames to the destination machine without having the destination machine acknowledge them. Ethernet is good example of data link layer that provides this class of service. No logical connection is established be forehand or released afterward. If frame is lost due to noise on the line no attempt is made to detect the loss or recover from it in the data link layer. This class of service is appropriate when the error rate is very low so recovery is left to higher layers. It is also appropriate for realtime traffic such as voice in which late data are worse than bad data. The next step up in terms of reliability is acknowledged connectionless ser vice. When this service is offered there are still no logical connections used but each frame sent is individually acknowledged. In this way the sender knows whether frame has arrived correctly or been lost. If it has not arrived within specified time interval it can be sent again. This service is useful over unreliable channels such as wireless systems. 802.11 is good example of this class It is perhaps worth emphasizing that providing acknowledgements in the data link layer is just an optimization never requirement. The network layer can al ways send packet and wait for it to be acknowledged by its peer on the remote machine. If the acknowledgement is not forthcoming before the timer expires the sender can just send the entire message again. The trouble with this strategy is that it can be inefficient. Links usually have strict maximum frame length imposed by the hardware and known propagation delays. The network layer does not know these parameters. It might send large packet that is broken up into say 10 frames of which 2 are lost on average. It would then take very long time for the packet to get through. Instead if individual frames are acknowledged and retransmitted then errors can be corrected more directly and more quickly. On reliable channels such as fiber the overhead of heavyweight data link protocol may be unnecessary but on wireless channels it is well Getting back to our services the most sophisticated service the data link layer can provide to the network layer is connectionoriented service. With this service the source and destination machines establish connection before any data are transferred. Each frame sent over the connection is numbered and the data link layer guarantees that each frame sent is indeed received. Furthermore it guaran tees that each frame is received exactly once and that all frames are received in the right order. Connectionoriented service thus provides the network layer proc esses with the equivalent of reliable bit stream. It is appropriate over long unre liable links such as satellite channel or longdistance telephone circuit. If acknowledged connectionless service were used it is conceivable that lost ac knowledgements could cause frame to be sent and received several times wast When connectionoriented service is used transfers go through three distinct phases. In the first phase the connection is established by having both sides ini tialize variables and counters needed to keep track of which frames have been re ceived and which ones have not. In the second phase one or more frames are ac up the variables buffers and other resources used to maintain the connection. To provide service to the network layer the data link layer must use the ser vice provided to it by the physical layer. What the physical layer does is accept raw bit stream and attempt to deliver it to the destination. If the channel is noisy as it is for most wireless and some wired links the physical layer will add some redundancy to its signals to reduce the bit error rate to tolerable level. However the bit stream received by the data link layer is not guaranteed to be error free. Some bits may have different values and the number of bits received may be less than equal to or more than the number of bits transmitted. It is up to the data link layer to detect and if necessary correct errors. The usual approach is for the data link layer to break up the bit stream into discrete frames compute short token called checksum for each frame and in clude the checksum in the frame when it is transmitted. just before each accidental flag byte in the data. Thus framing flag byte can be distinguished from one in the data by the absence or presence of an escape byte before it. The data link layer on the receiving end re moves the escape bytes before giving the data to the network layer. This techni que is called byte stuffing. Of course the next question is what happens if an escape byte occurs in the middle of the data The answer is that it too is stuffed with an escape byte. At the receiver the first escape byte is removed leaving the data byte that follows it . Some examples are shown the same as the original byte sequence. We can still search for frame boundary by looking for two flag bytes in row without bothering to undo escapes. one used in PPP which is used to carry packets over quences before and after byte stuffing. The third method of delimiting the bit stream gets around disadvantage of byte stuffing which is that it is tied to the use of 8bit bytes. Framing can be also be done at the bit level so frames can contain an arbitrary number of bits made up of units of any size. It was developed for the once very popular HDLC protocol. Each frame begins and ends with special bit pattern 01111110 or 0x7E in hexadecimal. This pattern is flag byte. When ever the senders data link layer encounters five consecutive 1s in the data it automatically stuffs 0 bit into the outgoing bit stream. This bit stuffing is anal ogous to byte stuffing in which an escape byte is stuffed into the outgoing charac ter stream before flag byte in the data. It also ensures minimum density of transitions that help the physical layer maintain synchronization. USB uses bit stuffing for this reason. When the receiver sees five consecutive incoming 1 bits followed by 0 bit it automatically destuffs the 0 bit. Just as byte stuffing is completely transparent to the network layer in both computers so is bit stuffing. If the user data contain the flag pattern 01111110 this flag is transmitted as 011111010 but With bit stuffing the boundary between two frames can be unambiguously recognized by the flag pattern. Thus if the receiver loses track of where it is all it has to do is scan the input for flag sequences since they can only occur at frame boundaries and never within the data. the line. The data as they are stored in the receivers memory after destuf With both bit and byte stuffing side effect is that the length of frame now bytes in the data 100 bytes might be carried in frame of roughly 100 bytes. If however the data consists solely of flag bytes each flag byte will be escaped and the frame will become roughly 200 bytes long. With bit stuffing the increase would be roughly 12.5 as 1 bit is added to every byte. The last method of framing is to use shortcut from the physical layer. We saw in Chap. 2 that the encoding of bits as signals often includes redundancy to help the receiver. This redundancy means that some signals will not occur in reg ular data. For example in the 4B5B line code 4 data bits are mapped to 5 signal bits to ensure sufficient bit transitions. This means that 16 out of the 32 signal possibilities are not used. We can use some reserved signals to indicate the start and end of frames. In effect we are using coding violations to delimit frames. The beauty of this scheme is that because they are reserved signals it is easy to find the start and end of frames and there is no need to stuff the data. Many data link protocols use combination of these methods for safety. common pattern used for Ethernet and 802.11 is to have frame begin with welldefined pattern called preamble. This pattern might be quite long to allow the receiver to prepare for an incoming packet. The to locate the end of the frame. Having solved the problem of marking the start and end of each frame we come to the next problem how to make sure all frames are eventually delivered to the network layer at the destination and in the proper order. Assume for the moment that the receiver can tell whether frame that it receives contains correct or faulty information . For unacknowledged connectionless service it might be fine if the sender just kept outputting frames without regard to whether they were arriving properly. But for reliable connectionoriented service it would not be fine at all. The usual way to ensure reliable delivery is to provide the sender with some feedback about what is happening at the other end of the line. Typically the pro tocol calls for the receiver to send back special control frames bearing positive or negative acknowledgements about the incoming frames. If the sender receives positive acknowledgement about frame it knows the frame has arrived safely. On the other hand negative acknowledgement means that something has gone wrong and the frame must be transmitted again. An additional complication comes from the possibility that hardware troubles may cause frame to vanish completely . In this case the receiver will not react at all since it has no reason to react. Similarly if the ac knowledgement frame is lost the sender will not know how to proceed. It should be clear that protocol in which the sender transmits frame and then waits for an acknowledgement positive or negative will hang forever if frame is ever lost due to for example malfunctioning hardware or faulty communication channel. This possibility is dealt with by introducing timers into the data link layer. When the sender transmits frame it generally also starts timer. The timer is set to expire after an interval long enough for the frame to reach the destination be processed there and have the acknowledgement propagate back to the sender. Normally the frame will be correctly received and the acknowledgement will get back before the timer runs out in which case the timer will be canceled. However if either the frame or the acknowledgement is lost the timer will go off alerting the sender to potential problem. The obvious solution is to just transmit the frame again. However when frames may be transmitted multiple times there is danger that the receiver will accept the same frame two or more times and pass it to the network layer more than once. To prevent this from hap pening it is generally necessary to assign sequence numbers to outgoing frames so that the receiver can distinguish retransmissions from originals. that each frame is ultimately passed to the network layer at the destination exactly once no more and no less is an important part of the duties of the data link layer sophisticated examples to see how this management is done. layers as well is what to do with sender that systematically wants to transmit frames faster than the receiver can accept them. This situation can occur when the sender is running on fast powerful computer and the receiver is running on slow lowend machine. common situation is when smart phone requests blasts the data at the poor helpless phone until it is completely swamped. Even if the transmission is error free the receiver may be unable to handle the frames as fast as they arrive and will lose some. Clearly something has to be done to prevent this situation. Two approaches are commonly used. In the first one feedbackbased flow control the receiver sends back information to the sender giving it permission to send more data or at least telling the sender how the receiver is doing. In the second one ratebased flow control the protocol has builtin mechanism that limits the rate at which senders may transmit data without using feedback from the receiver. because ratebased schemes are only seen as part of the transport layer . Feedbackbased schemes are seen at both the link layer and higher layers. The latter is more common these days in which case the link layer hardware is de signed to run fast enough that it does not cause loss. For example hardware im plementations of the link layer as NICs are some times said to run at wire speed meaning that they can handle frames as fast as they can arrive on the link. Any overruns are then not link problem so they are handled by higher layers. Various feedbackbased flow control schemes are known but most of them use the same basic principle. The protocol contains welldefined rules about when sender may transmit the next frame. These rules often prohibit frames from being sent until the receiver has granted permission either implicitly or ex plicitly. For example when connection is set up the receiver might say You may send me frames now but after they have been sent do not send any more until have told you to continue. We will examine the details shortly. We saw in Chap. 2 that communication channels have range of charac teristics. Some channels like optical fiber in telecommunications networks have tiny error rates so that transmission errors are rare occurrence. But other chan nels especially wireless links and aging local loops have error rates that are ord ers of magnitude larger. For these links transmission errors are the norm. They cannot be avoided at reasonable expense or cost in terms of performance. The conclusion is that transmission errors are here to stay. We have to learn how to Network designers have developed two basic strategies for dealing with er rors. Both add redundant information to the data that is sent. One strategy is to include enough redundant information to enable the receiver to deduce what the transmitted data must have been. The other is to include only enough redundancy to allow the receiver to deduce that an error has occurred and have it request retransmission. The former strategy uses errorcorrecting codes and the latter uses errordetecting codes. The use of errorcorrecting codes is often referred to as FEC . Each of these techniques occupies different ecological niche. On channels that are highly reliable such as fiber it is cheaper to use an errordetecting code and just retransmit the occasional block found to be faulty. However on channels such as wireless links that make many errors it is better to add redundancy to block was. FEC is used on noisy channels because retransmissions are just as likely to be in error as the first transmission. key consideration for these codes is the type of errors that are likely to oc cur. Neither errorcorrecting codes nor errordetecting codes can handle all pos sible errors since the redundant bits that offer protection are as likely to be re ceived in error as the data bits . It would be nice if the channel treated redundant bits differently than data bits but it does not. They are all just bits to the channel. This means that to avoid undetected er rors the code must be strong enough to handle the expected errors. One model is that errors are caused by extreme values of thermal noise that overwhelm the signal briefly and occasionally giving rise to isolated singlebit er rors. Another model is that errors tend to come in bursts rather than singly. This model follows from the physical processes that generate themsuch as deep fade on wireless channel or transient electrical interference on wired channel Both models matter in practice and they have different tradeoffs. Having the errors come in bursts has both advantages and disadvantages over isolated single bit errors. On the advantage side computer data are always sent in blocks of bits. Suppose that the block size was 1000 bits and the error rate was 0.001 per bit. If errors were independent most blocks would contain an error. If the errors came in bursts of 100 however only one block in 100 would be affected on average. The disadvantage of burst errors is that when they do occur they are much harder to correct than isolated errors. known perhaps because the physical layer received an analog signal that was far from the expected value for 0 or 1 and declared the bit to be lost. This situation is called an erasure channel. It is easier to correct errors in erasure channels than in channels that flip bits because even if the value of the bit has been lost at least we know which bit is in error. However we often do not have the benefit of eras We will examine both errorcorrecting codes and errordetecting codes next. Please keep two points in mind though. First we cover these codes in the link layer because this is the first place that we have run up against the problem of reli ably transmitting groups of bits. However the codes are widely used because reliability is an overall concern. Errorcorrecting codes are also seen in the physi cal layer particularly for noisy channels and in higher layers particularly for realtime media and content distribution. Errordetecting codes are commonly used in link network and transport layers. The second point to bear in mind is that error codes are applied mathematics. Unless you are particularly adept at Galois fields or the properties of sparse matrices you should get codes with good properties from reliable source rather than making up your own. In fact this is what many protocol standards do with the same codes coming up again and again. In the material below we will study simple code in detail and then briefly describe advanced codes. In this way we can understand the tradeoffs from the simple code and talk about the codes that are used in practice via the advanced codes. We will examine four different errorcorrecting codes 4. LowDensity Parity Check codes. All of these codes add redundancy to the information that is sent. frame con sists of data bits and redundant bits. In block code the check bits are computed solely as function of the data bits with which they are associated as though the bits were looked up in large table to find their corresponding check bits. In systematic code the data bits are sent directly along with the check bits rather than being encoded themselves be fore they are sent. In linear code the check bits are computed as linear function of the data bits. Exclusive OR or modulo 2 addition is popu lar choice. This means that encoding can be done with operations such as matrix are linear systematic block codes unless otherwise noted. Let the total length of block be . We will describe this as an code. An nbit unit containing data and check bits is referred to as an bit codeword. The code rate or simply rate is the fraction of the codeword that carries information that is not redundant or mn. The rates used in practice vary widely. They might be 12 for noisy channel in which case half of the received information is redundant or close to 1 for highquality channel with only small number of check bits added to large message. To understand how errors can be handled it is necessary to first look closely at what an error really is. Given any two codewords that may be transmitted or receivedsay 10001001 and 10110001it is possible to determine how many corresponding bits differ. In this case 3 bits differ. To determine how many bits differ just XOR the two codewords and count the number of 1 bits in the result. The number of bit positions in which two codewords differ is called the Ham ming distance . Its significance is that if two codewords are Hamming distance apart it will require singlebit errors to convert one into Given the algorithm for computing the check bits it is possible to construct complete list of the legal codewords and from this list to find the two codewords with the smallest Hamming distance. This distance is the Hamming distance of the complete code. In most data transmission applications all 2m possible data messages are legal but due to the way the check bits are computed not all of the 2n possible codewords are used. In fact when there are check bits only the small fraction of 2m 2n or 12r of the possible messages will be legal codewords. It is the sparseness with which the message is embedded in the space of codewords that al lows the receiver to detect and correct errors. The errordetecting and errorcorrecting properties of block code depend on its Hamming distance. To reliably detect errors you need distance 1 code because with such code there is no way that singlebit errors can change valid codeword into another valid codeword. When the receiver sees an illegal codeword it can tell that transmission error has occurred. Similarly to correct errors you need distance 2d 1 code because that way the legal codewords are so far apart that even with changes the original codeword is still closer than any other codeword. This means the original codeword can be uniquely determined based on the assumption that larger number of errors are less likely. As simple example of an errorcorrecting code consider code with only This code has distance of 5 which means that it can correct double errors or detect quadruple errors. If the codeword 0000000111 arrives and we expect only single or doublebit errors the receiver will know that the original must have triple error changes 0000000111 the error will not be corrected properly. Alternatively if we expect all of these errors we can detect them. None of the received codewords are legal codewords so an error must have occurred. It should be apparent that in this ex ample we cannot both correct double errors and detect quadruple errors because this would require us to interpret received codeword in two different ways. In our example the task of decoding by finding the legal codeword that is closest to the received codeword can be done by inspection. Unfortunately in the task can be timeconsuming search. Instead practical codes are designed so that they admit shortcuts to find what was likely the original codeword. Imagine that we want to design code with message bits and check bits that will allow all single errors to be corrected. Each of the 2m legal messages has illegal codewords at distance of 1 from it. These are formed by systematically inverting each of the bits in the nbit codeword formed from it. Thus each of the 2m legal messages requires 1 bit patterns dedicated to it. Since the total number of bit patterns is 2n we must have 2m 2n. Using this Given this puts lower limit on the number of check bits needed to correct sin This theoretical lower limit can in fact be achieved using method due to Hamming . In Hamming codes the bits of the codeword are numbered consecutively starting with bit 1 at the left end bit 2 to its immediate right and so on. The bits that are powers of 2 are check bits. The rest are filled up with the data bits. This pattern is shown for an forces the modulo 2 sum or parity of some collection of bits including itself to be even . bit may be included in several check bit computations. To see which check bits the data bit in position contributes to rewrite as sum of powers of 2. For example 11 1 2 8 and 29 1 4 8 16. bit is checked by just those check bits occurring in its expansion . In the example the check bits are computed for even parity sums for message that is the ASCII letter . This construction gives code with Hamming distance of 3 which means that it can correct single errors . The reason for the very careful numbering of message and check bits becomes apparent in the decoding process. When codeword arrives the receiver redoes the check bit computa tions including the values of the received check bits. We call these the check re sults. If the check bits are correct then for even parity sums each check result should be zero. In this case the codeword is accepted as valid. If the check results are not all zero however an error has been detected. The set of check results forms the error syndrome that is used to pinpoint and correct sults are 0 1 0 and 1 for 8 4 2 and 1 respectively. This gives syndrome of 0101 or 4 15. By the design of the scheme this means that the fifth bit is in error. Flipping the incorrect bit and dis carding the check bits gives the correct message of an ASCII . Hamming distances are valuable for understanding block codes and Ham ming codes are used in errorcorrecting memory. However most networks use stronger codes. The second code we will look at is convolutional code. This code is the only one we will cover that is not block code. In convolutional code an encoder processes sequence of input bits and generates sequence of output bits. There is no natural message size or encoding boundary as in block code. The output depends on the current and previous input bits. That is the encoder has memory. The number of previous bits on which the output depends is called the constraint length of the code. Convolutional codes are specified in terms of their rate and constraint length. Convolutional codes are widely used in deployed networks for example as part of the GSM mobile phone system in satellite communications and in 802.11. known as the NASA convolutional code of 12 and 7 since it was first used for the Voyager space missions starting in 1977. Since then it has been liberally reused for example as part of 802.11. righthand side that are XOR sums of the input and internal state. Since it deals with bits and performs linear operations this is binary linear convolutional code. Since 1 input bit produces 2 output bits the code rate is 12. It is not sys tematic since none of the output bits is simply the input bit. The internal state is kept in six memory registers. Each time another bit is in put the values in the registers are shifted to the right. For example if 111 is input and the initial state is all zeros the internal state written left to right will become 100000 110000 and 111000 after the first second and third bits have been input. The output bits will be 11 followed by 10 and then 01. It takes seven shifts to flush an input completely so that it does not affect the output. The constraint length of this code is thus 7. convolutional code is decoded by finding the sequence of input bits that is most likely to have produced the observed sequence of output bits . For small values of this is done with widely used algorithm de veloped by Viterbi . The algorithm walks the observed sequence keeping for each step and for each possible internal state the input sequence that would have produced the observed sequence with the fewest errors. The input se quence requiring the fewest errors at the end is the most likely message. Convolutional codes have been popular in practice because it is easy to factor the uncertainty of bit being 0 or 1 into the decoding. For example suppose 1V is the logical 0 level and 1V is the logical 1 level we might receive 0.9V and 0.1V for 2 bits. Instead of mapping these signals to 1 and 0 right away we would like to treat 0.9V as very likely 1 and 0.1V as maybe 0 and cor rect the sequence as whole. Extensions of the Viterbi algorithm can work with these uncertainties to provide stronger error correction. This approach of working with the uncertainty of bit is called softdecision decoding. Conversely decid ing whether each bit is 0 or 1 before subsequent error correction is called harddecision decoding. The third kind of errorcorrecting code we will describe is the ReedSolomon code. Like Hamming codes ReedSolomon codes are linear block codes and they are often systematic too. Unlike Hamming codes which operate on individ ual bits ReedSolomon codes operate on bit symbols. Naturally the mathemat ics are more involved so we will describe their operation by analogy. ReedSolomon codes are based on the fact that every degree polynomial is uniquely determined by 1 points. For example line having the form ax is determined by two points. Extra points on the same line are redundant which is helpful for error correction. Imagine that we have two data points that represent line and we send those two data points plus two check points chosen to lie on the same line. If one of the points is received in error we can still recover the data points by fitting line to the received points. Three of the points will lie on the line and one point the one in error will not. By finding the line we have cor rected the error. ReedSolomon codes are actually defined as polynomials that operate over finite fields but they work in similar manner. For bit symbols the codewords are 2m1 symbols long. popular choice is to make 8 so that symbols are bytes. codeword is then 255 bytes long. The code is widely used it adds 32 redundant symbols to 233 data symbols. Decoding with error correction is done with an algorithm developed by Berlekamp and Massey that can effi ciently perform the fitting task for moderatelength codes . ReedSolomon codes are widely used in practice because of their strong errorcorrection properties particularly for burst errors. They are used for DSL data over cable satellite communications and perhaps most ubiquitously on CDs DVDs and Bluray discs. Because they are based on bit symbols singlebit error and an mbit burst error are both treated simply as one symbol error. When 2t redundant symbols are added ReedSolomon code is able to correct up to errors in any of the transmitted symbols. This means for example that the code which has 32 redundant symbols can correct up to 16 symbol errors. Since the symbols may be consecutive and they are each 8 bits an error burst of up to 128 bits can be corrected. The situation is even better if the error model is one of erasures . In this case up to 2t errors can be corrected. ReedSolomon codes are often used in combination with other codes such as convolutional code. The thinking is as follows. Convolutional codes are effective at handling isolated bit errors but they will fail likely with burst of errors if there are too many errors in the received bit stream. By adding ReedSolomon code within the convolutional code the ReedSolomon decoding can mop up the error bursts task at which it is very good. The overall code then provides good protection against both single and burst errors. Parity Check code. LDPC codes are linear block codes that were invented by Robert Gallagher in his doctoral thesis . Like most theses they were promptly forgotten only to be reinvented in 1995 when advances in comput ing power had made them practical. In an LDPC code each output bit is formed from only fraction of the input bits. This leads to matrix representation of the code that has low density of 1s hence the name for the code. The received codewords are decoded with an approximation algorithm that iteratively improves on best fit of the received data to legal codeword. This corrects errors. LDPC codes are practical for large block sizes and have excellent errorcor rection abilities that outperform many other codes in practice. For this reason they are rapidly being included in new pro tocols. They are part of the standard for digital video broadcasting 10 Gbps more of them in future networks. Errorcorrecting codes are widely used on wireless links which are notori ously noisy and error prone when compared to optical fibers. Without errorcor recting codes it would be hard to get anything through. However over fiber or highquality copper the error rate is much lower so error detection and retrans mission is usually more efficient there for dealing with the occasional error. We will examine three different errordetecting codes. They are all linear 3. Cyclic Redundancy Checks . To see how they can be more efficient than errorcorrecting codes consider the first errordetecting code in which single parity bit is appended to the data. The parity bit is chosen so that the number of 1 bits in the codeword is even that 10 check bits are needed. Thus megabit of data would require 10000 check bits. To merely detect block with single 1bit error one parity bit per block will suffice. Once every 1000 blocks block will be found to be in error and an extra block will have to be transmitted to repair the error. The total overhead for the error detection and re transmission method is only 2001 bits per megabit of data versus 10000 bits for One difficulty with this scheme is that single parity bit can only reliably detect singlebit error in the block. If the block is badly garbled by long burst error the probability that the error will be detected is only 0.5 which is hardly ac ceptable. The odds can be improved considerably if each block to be sent is regarded as rectangular matrix bits wide and bits high. Now if we compute and send one parity bit for each row up to bit errors will be reliably detected as long as there is at most one error per row. However there is something else we can do that provides better protection against burst errors we can compute the parity bits over the data in different order than the order in which the data bits are transmitted. Doing so is called interleaving. In this case we will compute parity bit for each of the columns and send all the data bits as rows sending the rows from top to bottom and the bits in each row from left to right in the usual manner. At the last row we send Interleaving is general technique to convert code that detects burst error of length 7 occurs the bits that are in error are spread across dif ferent columns. At most 1 bit in each of the columns will be affected so the parity bits on those columns will detect the error. This method uses parity bits on blocks of kn data bits to detect single burst error of length or less. burst of length 1 will pass undetected however if the first bit is inverted the last bit is inverted and all the other bits are correct. If the block is badly garbled by long burst or by multiple shorter bursts the probability that any of the columns will have the correct parity by accident is 0.5 so the probability of bad block being accepted when it should not be is 2n. The second kind of errordetecting code the checksum is closely related to groups of parity bits. The word checksum is often used to mean group of check bits associated with message regardless of how are calculated. group of parity bits is one example of checksum. However there are other stronger checksums based on running sum of the data bits of the message. The checksum is usually placed at the end of the message as the complement of the sum func tion. This way errors may be detected by summing the entire received codeword both data bits and checksum. If the result comes out to be zero no error has been One example of checksum is the 16bit Internet checksum used on all Inter net packets as part of the IP protocol . This checksum is sum of the message bits divided into 16bit words. Because this method operates on words rather than on bits as in parity errors that leave the parity unchanged can still alter the sum and be detected. For example if the lowest order bit in two different words is flipped from 0 to 1 parity check across these bits would fail to detect an error. However two 1s will be added to the 16bit checksum to produce different result. The error can then be detected. The Internet checksum is computed in ones complement arithmetic instead of as the modulo 216 sum. In ones complement arithmetic negative number is the bitwise complement of its positive counterpart. Modern computers run twos complement arithmetic in which negative number is the ones complement plus one. On twos complement computer the ones complement sum is equivalent to taking the sum modulo 216 and adding any overflow of the high order bits back into the loworder bits. This algorithm gives more uniform coverage of the data by the checksum bits. Otherwise two highorder bits can be added overflow and be lost without changing the sum. There is another benefit too. Ones comple ment has two representations of zero all 0s and all 1s. This allows one value to indicate that there is no checksum without the need for another field. For decades it has always been assumed that frames to be checksummed con tain random bits. All analyses of checksum algorithms have been made under this assumption. Inspection of real data by Partridge et al. has shown this as sumption to be quite wrong. As consequence undetected errors are in some cases much more common than had been previously thought. The Internet checksum in particular is efficient and simple but provides weak protection in some cases precisely because it is simple sum. It does not detect the deletion or addition of zero data nor swapping parts of the message and it provides weak protection against message splices in which parts of two packets are put together. These errors may seem very unlikely to occur by random proc esses but they are just the sort of errors that can occur with buggy hardware. better choice is Fletchers checksum . It includes posi tional component adding the product of the data and its position to the running sum. This provides stronger detection of changes in the position of data. Although the two preceding schemes may sometimes be adequate at higher layers in practice third and stronger kind of errordetecting code is in wide spread use at the link layer the CRC also known as polynomial code. Polynomial codes are based upon treating bit strings as representations of polynomials with coefficients of 0 and 1 only. kbit frame is regarded as the coefficient list for polynomial with terms ranging from 1 to 0. Such polynomial is said to be of degree 1. The highorder bit is the coefficient of 1 the next bit is the coefficient of 2 and so on. For example 110001 has 6 bits and thus represents sixterm polynomial with Polynomial arithmetic is done modulo 2 according to the rules of algebraic field theory. It does not have carries for addition or borrows for subtraction. Both addition and subtraction are identical to exclusive OR. For example Long division is carried out in exactly the same way as it is in binary except that the subtraction is again done modulo 2. divisor is said to go into dividend if the dividend has as many bits as the divisor. When the polynomial code method is employed the sender and receiver must agree upon generator polynomial in advance. Both the high and low order bits of the generator must be 1. To compute the CRC for some frame with bits corresponding to the polynomial the frame must be longer than the generator polynomial. The idea is to append CRC to the end of the frame in such way that the polynomial represented by the checksummed frame is divisi ble by . When the receiver gets the checksummed frame it tries dividing it by . If there is remainder there has been transmission error. The algorithm for computing the CRC is as follows 1. Let be the degree of . Append zero bits to the loworder end of the frame so it now contains bits and corresponds to the polynomial rM. 2. Divide the bit string corresponding to into the bit string corres ponding to rM using modulo 2 division. 3. Subtract the remainder from the bit string corresponding to rM using modulo 2 subtraction. The re sult is the checksummed frame to be transmitted. Call its polynomial It should be clear that is divisible by . In any division problem if you diminish the dividend by the remainder what is left over is divisi ble by the divisor. For example in base 10 if you divide 210278 by 10941 the remainder is 2399. If you then subtract 2399 from 210278 what is left over is divisible by 10941. Now let us analyze the power of this method. What kinds of errors will be de tected Imagine that transmission error occurs so that instead of the bit string for arriving arrives. Each 1 bit in corresponds to bit that has been inverted. If there are 1 bits in singlebit errors have occurred. single burst error is characterized by an initial 1 mixture of 0s and 1s and Upon receiving the checksummed frame the receiver divides it by that is it computes . TG is 0 so the result of the computa tion is simply EG. Those errors that happen to correspond to polynomials containing as factor will slip by all other errors will be caught. If there has been singlebit error where determines which bit is in error. If contains two or more terms it will never divide into so all If there have been two isolated singlebit errors where . Alternatively this can be written as . If we assume that is not divisible by sufficient condition for all double errors to be detected is that does not divide 1 for any up to the maximum value of . Simple lowdegree polynomials that give pro tection to long frames are known. For example 15 14 1 will not divide 1 for any value of below 32768. If there are an odd number of bits in error contains an odd number of terms . Interestingly no polynomial with an odd number of terms has 1 as factor in the modulo 2 system. By making 1 factor of we can catch all errors with an odd number of inverted bits. burst errors of length . burst error of length can be represented by where determines how far from the righthand end of the re ceived frame the burst is located. If contains an 0 term it will not have as factor so if the degree of the parenthesized expression is less than the degree of the remainder can never be zero. If the burst length is 1 the remainder of the division by will be zero if and only if the burst is identical to . By definition of burst the first and last bits must be 1 so whether it matches depends on the 1 intermediate bits. If all combinations are regarded as equally likely the probability of such an incor rect frame being accepted as valid is ½r 1. It can also be shown that when an error burst longer than 1 bits occurs or when several shorter bursts occur the probability of bad frame getting through unnoticed is ½r assuming that all bit patterns are equally likely. Certain polynomials have become international standards. The one used in Among other desirable properties it has the property that it detects all bursts of length 32 or less and all bursts affecting an odd number of bits. It has been used widely since the 1980s. However this does not mean it is the best choice. Using an exhaustive computational search Castagnoli et al. and Koopman found the best CRCs. These CRCs have Hamming distance of 6 for typical message sizes while the IEEE standard CRC32 has Hamming distance of only Although the calculation required to compute the CRC may seem complicat ed it is easy to compute and verify CRCs in hardware with simple shift register circuits . In practice this hardware is nearly always used. Dozens of networking standards include various CRCs including virtually all LANs and pointtopoint links . The rest of the link layer process and the network layer process run on the main CPU as part of the operating system with the soft ware for the link layer process often taking the form of device driver. Howev er other implementations are also possible . Actually the preferred implementation changes from decade to decade with technology tradeoffs. In any event treating the three layers as separate processes makes the discussion conceptually cleaner and also serves to emphasize the independence of the layers. Another key assumption is that machine wants to send long stream of data to machine using reliable connectionoriented service. Later we will consid er the case where also wants to send data to simultaneously. is assumed to have an infinite supply of data ready to send and never has to wait for data to be produced. Instead when As data link layer asks for data the network layer is al ways able to comply immediately. We also assume that machines do not crash. That is these protocols deal with As far as the data link layer is concerned the packet passed across the inter face to it from the network layer is pure data whose every bit is to be delivered to the destinations network layer. The fact that the destinations network layer may When the data link layer accepts packet it encapsulates the packet in checksum . The frame is then transmitted to the data link layer on the other machine. We will assume that there exist suitable library procedures to physical layer to send frame and from physical layer to receive frame. These procedures compute and append or check the checksum so that we do not need to worry about it as part of the protocols Initially the receiver has nothing to do. It just sits around waiting for some that the data link layer is waiting for something to happen by the procedure call sequence or ack numbers typedef struct unsigned char data packet the network layer packet Wait for an event to happen return its type in event. void wait for event Fetch packet from the network layer for transmission on the channel. void from network layer Deliver information from an inbound frame to the network layer. void to network layer Go get an inbound frame from the physical layer and copy it to . void from physical layer Pass the frame to the physical layer for transmission. void to physical layer Start the clock running and enable the timeout event. void start timer Stop the clock and disable the timeout event. void stop timer Start an auxiliary timer and enable the ack timeout event. void start ack timer Stop the auxiliary timer and disable the ack timeout event. void stop ack timer Allow the network layer to cause network layer ready event. void enable network layer Forbid the network layer from causing network layer ready event. void disable network layer Macro inc is expanded inline increment circularly. tions are located in the file protocol. wait for event. This procedure only returns when something has hap pened . Upon return the variable event tells what hap pened. The set of possible events differs for the various protocols to be described and will be defined separately for each protocol. Note that in more realistic situation the data link layer will not sit in tight loop waiting for an event as we have suggested but will receive an interrupt which will cause it to stop whatever it was doing and go handle the incoming frame. Nevertheless for simplicity we will ignore all the details of parallel activity within the data link layer and assume that it is dedicated full time to handling just our one channel. When frame arrives at the receiver the checksum is recomputed. If the checksum in the frame is incorrect the data link layer is so informed . If the inbound frame arrived undamaged the data link layer is also informed so that it can acquire the frame for inspection using from physical layer. As soon as the receiving data link layer has acquired an undamaged frame it checks the control There is good reason why the network layer must never be given any part of rate. As long as the network layer knows nothing at all about the data link proto col or the frame format these things can be changed without requiring changes to the network layers software. This happens whenever new NIC is installed in computer. Providing rigid interface between the network and data link layers greatly simplifies the design task because communication protocols in different layers can evolve independently. to be discussed later. Five data structures are defined there boolean seq nr packet frame kind and frame. boolean is an enumerated type and can take on the values true and false. seq nr is small integer used to number the frames so that we can tell them apart. These sequence numbers run from 0 up to and in cluding MAX SEQ which is defined in each protocol needing it. packet is the unit of information exchanged between the network layer and the data link layer on the same machine or between network layer peers. In our model it always contains MAX PKT bytes but more realistically it would be of variable length. frame is composed of four fields kind seq ack and info the first three of which contain control information and the last of which may contain actual data to The kind field tells whether there are any data in the frame because some of the protocols distinguish frames containing only control information from those containing data as well. The seq and ack fields are used for sequence numbers and acknowledgements respectively their use will be described in more detail later. The info field of data frame contains single packet the info field of control frame is not used. more realistic implementation would use variable length info field omitting it altogether for control frames. Again it is important to understand the relationship between packet and frame. The network layer builds packet by taking message from the transport link layer for inclusion in the info field of an outgoing frame. When the frame ar rives at the destination the data link layer extracts the packet from the frame and passes the packet to the network layer. In this manner the network layer can act as though machines can exchange packets directly. tines whose details are implementation dependent and whose inner workings will not concern us further in the following discussions. The procedure wait for event sits in tight loop waiting for something to happen as mentioned earlier. The procedures to network layer and from network layer are used by the data link layer to pass packets to the network layer and accept packets from the network layer respectively. Note that from physical layer and to physical layer pass frames between the data link layer and the physical layer. In other words to net work layer and from network layer deal with the interface between layers 2 and 3 whereas from physical layer and to physical layer deal with the interface be tween layers 1 and 2. In most of the protocols we assume that the channel is unreliable and loses entire frames upon occasion. To be able to recover from such calamities the sending data link layer must start an internal timer or clock whenever it sends frame. If no reply has been received within certain predetermined time interval the clock times out and the data link layer receives an interrupt signal. In our protocols this is handled by allowing the procedure wait for event to return event timeout. The procedures start timer and stop timer turn the timer on and off respectively. Timeout events are possible only when the timer is run ning and before stop timer is called. It is explicitly permitted to call start timer while the timer is running such call simply resets the clock to cause the next timeout after full timer interval has elapsed . The procedures start ack timer and stop ack timer control an auxiliary timer used to generate acknowledgements under certain conditions. The procedures enable network layer and disable network layer are used in the more sophisticated protocols where we no longer assume that the network layer always has packets to send. When the data link layer enables the network layer the network layer is then permitted to interrupt when it has packet to be sent. We indicate this with event network layer ready. When the network layer is disabled it may not cause such events. By being careful about when it enables and disables its network layer the data link layer can prevent the network layer from swamping it with packets for which it has no buffer space. Frame sequence numbers are always in the range 0 to MAX SEQ where MAX SEQ is different for the different protocols. It is frequently necessary to advance sequence number by 1 circularly . The macro inc performs this incrementing. It has been defined as macro be cause it is used inline within the critical path. As we will see later the factor limiting network performance is often protocol processing so defining simple op erations like this as macros does not affect the readability of the code but does im prove performance. shortly. To save space and to provide convenient reference they have been extracted and listed together but conceptually they should be merged with the protocols themselves. In this merging is done by putting the definitions in preprocessor to include them in the protocol files. As an initial example we will consider protocol that is as simple as it can be because it does not worry about the possibility of anything going wrong. Data are transmitted in one direction only. Both the transmitting and receiving network layers are always ready. Processing time can be ignored. Infinite buffer space is available. And best of all the communication channel between the data link lay ers never damages or loses frames. This thoroughly unrealistic protocol which we will nickname Utopia is simply to show the basic structure on which we The protocol consists of two distinct procedures sender and receiver. The sender runs in the data link layer of the source machine and the receiver runs in the data link layer of the destination machine. No sequence numbers or acknowl edgements are used here so MAX SEQ is not needed. The only event type pos sible is frame arrival . The sender is in an infinite while loop just pumping data out onto the line as fast as it can. The body of the loop consists of three actions go fetch packet from the network layer construct an outbound frame using the variable and send the frame on its way. Only the info field of the frame is used by this protocol because the other fields have to do with error and flow control and there are no errors or flow control restrictions here. The receiver is equally simple. Initially it waits for something to happen the only possibility being the arrival of an undamaged frame. Eventually the frame arrives and the procedure wait for event returns with event set to frame arrival . The call to from physical layer removes the newly arrived frame from the hardware buffer and puts it in the variable where the re layer and the data link layer settles back to wait for the next frame effectively suspending itself until the frame arrives. Protocol 1 provides for data transmission in one direction only from sender to receiver. The communication channel is assumed to be error free and the receiver is assumed to be able to process all the input infinitely quickly. Consequently the sender just sits in loop pumping data out onto the line as The utopia protocol is unrealistic because it does not handle either flow con trol or error correction. Its processing is close to that of an unacknowledged con even an unacknowledged connectionless service would do some error detection. Now we will tackle the problem of preventing the sender from flooding the receiver with frames faster than the latter is able to process them. This situation can easily happen in practice so being able to prevent it is of great importance. The communication channel is still assumed to be error free however and the data traffic is still simplex. One solution is to build the receiver to be powerful enough to process con tinuous stream of backtoback frames . It must have sufficient buffering and processing abilities to run at the line rate and must be able to pass the frames that are received to the network layer quickly enough. However this is worstcase solution. It requires dedicated hardware and can be wasteful of resources if the utilization of the link is mostly low. Moreover it just shifts the problem of deal ing with sender that is too fast elsewhere in this case to the network layer. more general solution to this problem is to have the receiver provide feed back to the sender. After having passed packet to its network layer the receiver sends little dummy frame back to the sender which in effect gives the sender permission to transmit the next frame. After having sent frame the sender is re quired by the protocol to bide its time until the little dummy frame arrives. This delay is simple example of flow control protocol. Protocols in which the sender sends one frame and then waits for an acknowl ample of simplex stopandwait protocol. Although data traffic in this example is simplex going only from the sender to the receiver frames do travel in both directions. Consequently the communica tion channel between the two data link layers needs to be capable of bidirectional information transfer. However this protocol entails strict alternation of flow first the sender sends frame then the receiver sends frame then the sender sends another frame then the receiver sends another one and so on. half duplex physical channel would suffice here. As in protocol 1 the sender starts out by fetching packet from the network layer using it to construct frame and sending it on its way. But now unlike in protocol 1 the sender must wait until an acknowledgement frame arrives before looping back and fetching the next packet from the network layer. The sending data link layer need not even inspect the incoming frame as there is only one pos sibility. The incoming frame is always an acknowledgement. The only difference between receiver1 and receiver2 is that after delivering packet to the network layer receiver2 sends an acknowledgement frame back to the sender before entering the wait loop again. Because only the arrival of the any particular information in it. Now let us consider the normal situation of communication channel that makes errors. Frames may be either damaged or lost completely. However we assume that if frame is damaged in transit the receiver hardware will detect this Protocol 2 also provides for onedirectional flow of data from sender to receiver. The communication channel is once again assumed to be error free as in protocol 1. However this time the receiver has only finite buffer capacity and finite processing speed so the protocol must explicitly prevent the sender from flooding the receiver with data faster than it can be handled. send dummy frame to awaken sender when it computes the checksum. If the frame is damaged in such way that the checksum is nevertheless correctan unlikely occurrencethis protocol can fail . At first glance it might seem that variation of protocol 2 would work adding timer. The sender could send frame but the receiver would only send an ac knowledgement frame if the data were correctly received. If damaged frame ar rived at the receiver it would be discarded. After while the sender would time out and send the frame again. This process would be repeated until the frame This scheme has fatal flaw in it though. Think about the problem and try to discover what might go wrong before reading further. To see what might go wrong remember that the goal of the data link layer is to provide errorfree transparent communication between network layer proc esses. The network layer on machine gives series of packets to its data link layer which must ensure that an identical series of packets is delivered to the net work layer on machine by its data link layer. In particular the network layer on has no way of knowing that packet has been lost or duplicated so the data link layer must guarantee that no combination of transmission errors however unlike ly can cause duplicate packet to be delivered to network layer. Consider the following scenario 1. The network layer on gives packet 1 to its data link layer. The packet is correctly received at and passed to the network layer on . sends an acknowledgement frame back to . 2. The acknowledgement frame gets lost completely. It just never ar rives at all. Life would be great deal simpler if the channel man gled and lost only data frames and not control frames but sad to say the channel is not very discriminating. 3. The data link layer on eventually times out. Not having received an acknowledgement it assumes that its data frame was lost or damaged and sends the frame containing packet 1 again. 4. The duplicate frame also arrives intact at the data link layer on and is unwittingly passed to the network layer there. If is sending file to part of the file will be duplicated . In other words the protocol will fail. Clearly what is needed is some way for the receiver to be able to distinguish frame that it is seeing for the first time from retransmission. The obvious way frame it sends. Then the receiver can check the sequence number of each arriving frame to see if it is new frame or duplicate to be discarded. Since the protocol must be correct and the sequence number field in the head er is likely to be small to use the link efficiently the question arises what is the vide 1 bit few bits 1 byte or multiple bytes for sequence number depending on the protocol. The important point is that it must carry sequence numbers that are large enough for the protocol to work correctly or it is not much of protocol. The only ambiguity in this protocol is between frame and its direct suc cessor 1. If frame is lost or damaged the receiver will not acknowledge it so the sender will keep trying to send it. Once it has been correctly received the receiver will send an acknowledgement to the sender. It is here that the potential trouble crops up. Depending upon whether the acknowledgement frame gets back to the sender correctly or not the sender may try to send or 1. At the sender the event that triggers the transmission of frame 1 is the ar rival of an acknowledgement for frame . But this situation implies that 1 has been correctly received and furthermore that its acknowledgement has also been correctly received by the sender. Otherwise the sender would not have begun with let alone have been considering 1. As consequence the only ambiguity is between frame and its immediate predecessor or successor not be tween the predecessor and successor themselves. 1bit sequence number is therefore sufficient. At each instant of time the receiver expects particular sequence number next. When frame con taining the correct sequence number arrives it is accepted and passed to the net work layer then acknowledged. Then the expected sequence number is incre mented modulo 2 . Any arriving frame con taining the wrong sequence number is rejected as duplicate. However the last valid acknowledgement is repeated so that the sender can eventually discover that the frame has been received. the sender waits for positive acknowledgement before advancing to the next data item are often called ARQ or PAR . Like protocol 2 this one also trans mits data only in one direction. Protocol 3 differs from its predecessors in that both sender and receiver have variable whose value is remembered while the data link layer is in the wait state. The sender remembers the sequence number of the next frame to send in next frame to send the receiver remembers the sequence number of the next frame expected in frame expected. Each protocol has short initialization phase before entering the infinite loop. After transmitting frame the sender starts the timer running. If it was al ready running it will be reset to allow another full timer interval. The interval should be chosen to allow enough time for the frame to get to the receiver for the receiver to process it in the worst case and for the acknowledgement frame to propagate back to the sender. Only when that interval has elapsed is it safe to as sume that either the transmitted frame or its acknowledgement has been lost and to send duplicate. If the timeout interval is set too short the sender will transmit unnecessary frames. While these extra frames will not affect the correctness of the protocol they will hurt performance. After transmitting frame and starting the timer the sender waits for some thing exciting to happen. Only three possibilities exist an acknowledgement frame arrives undamaged damaged acknowledgement frame staggers in or the timer expires. If valid acknowledgement comes in the sender fetches the next packet from its network layer and puts it in the buffer overwriting the previous packet. It also advances the sequence number. If damaged frame arrives or the timer expires neither the buffer nor the sequence number is changed so that et or duplicate are then sent. When valid frame arrives at the receiver its sequence number is checked to see if it is duplicate. If not it is accepted passed to the network layer and an acknowledgement is generated. Duplicates and damaged frames are not passed to the network layer but they do cause the last correctly received frame to be acknowledged to signal the sender to advance to the next frame or retransmit In the previous protocols data frames were transmitted in one direction only. In most practical situations there is need to transmit data in both directions. One way of achieving fullduplex data transmission is to run two instances of one of the previous protocols each using separate link for simplex data traffic . Each link is then comprised of forward channel and reverse channel . In both cases the capaci ty of the reverse channel is almost entirely wasted. better idea is to use the same link for data in both directions. After all in protocols 2 and 3 it was already being used to transmit frames both ways and the reverse channel normally has the same capacity as the forward channel. In this model the data frames from to are intermixed with the acknowledgement frame the receiver can tell whether the frame is data or an acknowledgement. Although interleaving data and control frames on the same link is big im provement over having two separate physical links yet another improvement is possible. When data frame arrives instead of immediately sending separate control frame the receiver restrains itself and waits until the network layer passes it the next packet. The acknowledgement is attached to the outgoing data frame free ride on the next outgoing data frame. The technique of temporarily delaying outgoing acknowledgements so that they can be hooked onto the next outgoing data frame is known as piggybacking. The principal advantage of using piggybacking over having distinct acknowl edgement frames is better use of the available channel bandwidth. The ack field generally means lighter processing load at the receiver. In the next protocol to costs more than few bits. However piggybacking introduces complication not present with separate acknowledgements. How long should the data link layer wait for packet onto Protocol 3 allows unidirectional data flow over an unreliable channel. must be 1 for protocol 3 typedef enum frame arrival cksum err timeout event type send acknowledgement which to piggyback the acknowledgement If the data link layer waits longer than the senders timeout period the frame will be retransmitted defeating the whole purpose of having acknowledgements. If the data link layer were an oracle and could foretell the future it would know when the next network layer packet was going to come in and could decide either to wait for it or send separate ac knowledgement immediately depending on how long the projected wait was going to be. Of course the data link layer cannot foretell the future so it must resort to some ad hoc scheme such as waiting fixed number of milliseconds. If new packet arrives quickly the acknowledgement is piggybacked onto it. Otherwise if no new packet has arrived by the end of this time period the data link layer just sends separate acknowledgement frame. The next three protocols are bidirectional protocols that belong to class cal led sliding window protocols. The three differ among themselves in terms of ef ficiency complexity and buffer requirements as discussed later. In these as in all sliding window protocols each outbound frame contains sequence number ranging from 0 up to some maximum. The maximum is usually 2n 1 so the se quence number fits exactly in an nbit field. The stopandwait sliding window protocol uses 1 restricting the sequence numbers to 0 and 1 but more sophis The essence of all sliding window protocols is that at any instant of time the sender maintains set of sequence numbers corresponding to frames it is permit ted to send. These frames are said to fall within the sending window. Similarly the receiver also maintains receiving window corresponding to the set of frames it is permitted to accept. The senders window and the receivers window need not have the same lower and upper limits or even have the same size. In some protocols they are fixed in size but in others they can grow or shrink over the course of time as frames are sent and received. Although these protocols give the data link layer more freedom about the order in which it may send and receive frames we have definitely not dropped the requirement that the protocol must deliver packets to the destination network layer in the same order they were passed to the data link layer on the sending machine. Nor have we changed the requirement that the physical communication channel is wirelike that is it must deliver all frames in the order sent. The sequence numbers within the senders window represent frames that have been sent or can be sent but are as yet not acknowledged. Whenever new packet arrives from the network layer it is given the next highest sequence number and the upper edge of the window is advanced by one. When an acknowledgement comes in the lower edge is advanced by one. In this way the window continu Since frames currently within the senders window may ultimately be lost or damaged in transit the sender must keep all of these frames in its memory for possible retransmission. Thus if the maximum window size is the sender needs buffers to hold the unacknowledged frames. If the window ever grows to its tially. After the first frame has been sent. After the first frame has been received. After the first acknowledgement has been received. maximum size the sending data link layer must forcibly shut off the network layer until another buffer becomes free. The receiving data link layers window corresponds to the frames it may ac cept. Any frame falling within the window is put in the receivers buffer. When frame whose sequence number is equal to the lower edge of the window is re ceived it is passed to the network layer and the window is rotated by one. Any frame falling outside the window is discarded. In all of these cases subsequent acknowledgement is generated so that the sender may work out how to proceed. Note that window size of 1 means that the data link layer only accepts frames in order but for larger windows this is not so. The network layer in contrast is al ways fed data in the proper order regardless of the data link layers window size. no frames are outstanding so the lower and upper edges of the senders window are equal but as time goes on the situation progresses as shown. Unlike the send ers window the receivers window always remains at its initial size rotating as the next frame is accepted and delivered to the network layer. Before tackling the general case let us examine sliding window protocol with window size of 1. Such protocol uses stopandwait since the sender transmits frame and waits for its acknowledgement before sending the next one. some variables. Next frame to send tells which frame the sender is trying to send. Similarly frame expected tells which frame the receiver is expecting. In both cases 0 and 1 are the only possibilities. Protocol 4 is bidirectional. must be 1 for protocol 4 typedef enum frame arrival cksum err timeout event type Under normal circumstances one of the two data link layers goes first and transmits the first frame. In other words only one of the data link layer programs should contain the to physical layer and start timer procedure calls outside the main loop. The starting machine fetches the first packet from its network layer builds frame from it and sends it. When this frame arrives the receiv ing data link layer checks to see if it is duplicate just as in protocol 3. If the frame is the one expected it is passed to the network layer and the receivers win The acknowledgement field contains the number of the last frame received without error. If this number agrees with the sequence number of the frame the sender is trying to send the sender knows it is done with the frame stored in buff er and can fetch the next packet from its network layer. If the sequence number disagrees it must continue trying to send the same frame. Whenever frame is received frame is also sent back. Now let us examine protocol 4 to see how resilient it is to pathological scen arios. Assume that computer is trying to send its frame 0 to computer and that is trying to send its frame 0 to . Suppose that sends frame to but As timeout interval is little too short. Consequently may time out repeatedly sending series of identical frames all with seq 0 and ack 1. When the first valid frame arrives at computer it will be accepted and frame expected will be set to value of 1. All the subsequent frames received will be rejected because is now expecting frames with sequence number 1 not 0. Furthermore since all the duplicates will have ack 1 and is still waiting for an acknowledgement of 0 will not go and fetch new packet from its network After every rejected duplicate comes in will send frame containing seq 0 and ack 0. Eventually one of these will arrive correctly at causing to begin sending the next packet. No combination of lost frames or premature timeouts can cause the protocol to deliver duplicate packets to either network layer to skip packet or to deadlock. The protocol is correct. However to show how subtle protocol interactions can be we note that pe culiar situation arises if both sides simultaneously send an initial packet. This tion of the protocol is shown. In the peculiarity is illustrated. If waits for As first frame before sending one of its own the sequence is as shown in and every frame is accepted. However if and simultaneously initiate communication their first frames cross and the data link layers then get into situation . In each frame arrival brings new packet for the network layer there are no duplicates. In half of the frames contain duplicates even though there are no transmission errors. Simi lar situations can occur as result of premature timeouts even when one side clearly starts first. In fact if multiple premature timeouts occur frames may be sent three or more times wasting valuable bandwidth. sends case. The notation is . An asterisk indicates where network layer accepts packet. Until now we have made the tacit assumption that the transmission time re quired for frame to arrive at the receiver plus the transmission time for the ac knowledgement to come back is negligible. Sometimes this assumption is clearly false. In these situations the long roundtrip time can have important implications for the efficiency of the bandwidth utilization. As an example consider 50kbps satellite channel with 500msec roundtrip propagation delay. Let us imagine trying to use protocol 4 to send 1000bit frames via the satellite. At 0 the sender starts sending the first frame. At 20 msec the frame has been com pletely sent. Not until 270 msec has the frame fully arrived at the receiver and not until 520 msec has the acknowledgement arrived back at the sender under the best of circumstances . This means that the sender was blocked 500520 or 96 of the time. In other words only 4 of the available bandwidth was used. Clear ly the combination of long transit time high bandwidth and short frame length is disastrous in terms of efficiency. The problem described here can be viewed as consequence of the rule re quiring sender to wait for an acknowledgement before sending another frame. If we relax that restriction much better efficiency can be achieved. Basically the solution lies in allowing the sender to transmit up to frames before blocking in stead of just 1. With large enough choice of the sender will be able to con tinuously transmit frames since the acknowledgements will arrive for previous frames before the window becomes full preventing the sender from blocking. To find an appropriate value for we need to know how many frames can fit inside the channel as they propagate from sender to receiver. This capacity is de termined by the bandwidth in bitssec multiplied by the oneway transit time or the bandwidthdelay product of the link. We can divide this quantity by the number of bits in frame to express it as number of frames. Call this quantity BD. Then should be set to 2BD 1. Twice the bandwidthdelay is the number of frames that can be outstanding if the sender continuously sends frames when the roundtrip time to receive an acknowledgement is considered. The 1 is because an acknowledgement frame will not be sent until after complete frame For the example link with bandwidth of 50 kbps and oneway transit time of 250 msec the bandwidthdelay product is 12.5 kbit or 12.5 frames of 1000 bits each. 2BD 1 is then 26 frames. Assume the sender begins sending frame 0 as before and sends new frame every 20 msec. By the time it has finished sending 26 frames at 520 msec the acknowledgement for frame 0 will have just arri ved. Thereafter acknowledgements will arrive every 20 msec so the sender will always get permission to continue just when it needs it. From then onwards 25 or 26 unacknowledged frames will always be outstanding. Put in other terms the senders maximum window size is 26. For smaller window sizes the utilization of the link will be less than 100 since the sender will be blocked sometimes. We can write the utilization as the This value is an upper bound because it does not allow for any frame processing time and treats the acknowledgement frame as having zero length since it is usually short. The equation shows the need for having large window when ever the bandwidthdelay product is large. If the delay is high the sender will ra pidly exhaust its window even for moderate bandwidth as in the satellite ex ample. If the bandwidth is high even for moderate delay the sender will exhaust its window quickly unless it has large window . With stopandwait for which 1 if there is even one frames worth of propagation delay the efficiency will be less than 50. This technique of keeping multiple frames in flight is an example of pipelin ing. Pipelining frames over an unreliable communication channel raises some damaged or lost Large numbers of succeeding frames will arrive at the receiver before the sender even finds out that anything is wrong. When damaged frame arrives at the receiver it obviously should be discarded but what should the re ceiver do with all the correct frames following it Remember that the receiving data link layer is obligated to hand packets to the network layer in sequence. Two basic approaches are available for dealing with errors in the presence of by data link layer receivers window size is 1 and receivers window size is large. One option called gobackn is for the receiver simply to discard all subsequent frames sending no acknowledgements for the discarded frames. This strategy corresponds to receive window of size 1. In other words the data link layer refuses to accept any frame except the next one it must give to the network layer. If the senders window fills up before the timer runs out the pipeline will begin to empty. Eventually the sender will time out and retransmit all unacknowledged frames in order starting with the damaged or lost one. This approach can waste lot of bandwidth if the error rate is high. is large. Frames 0 and 1 are correctly received and acknowledged. Frame 2 however is damaged or lost. The sender unaware of this problem continues to send frames until the timer for frame 2 expires. Then it backs up to frame 2 and starts over with it sending 2 3 4 etc. all over again. The other general strategy for handling errors when frames are pipelined is called selective repeat. When it is used bad frame that is received is discarded but any good frames received after it are accepted and buffered. When the sender times out only the oldest unacknowledged frame is retransmitted. If that frame arrives correctly the receiver can deliver to the network layer in sequence all the frames it has buffered. Selective repeat corresponds to receiver window larger than 1. This approach can require large amounts of data link layer memory if the window is large. Selective repeat is often combined with having the receiver send negative acknowledgement when it detects an error for example when it receives checksum error or frame out of sequence. NAKs stimulate retransmission be fore the corresponding timer expires and thus improve performance. and frame 2 is lost. When frame 3 arrives at the receiver the data link layer there notices that it has missed frame so it sends back NAK for 2 but buffers 3. When frames 4 and 5 arrive they too are buffered by the data link layer instead of being passed to the network layer. Eventually the NAK 2 gets back to the sender which immediately resends frame 2. When that arrives the data link layer now has 2 3 4 and 5 and can pass all of them to the network layer in the correct order. It can also acknowledge all frames up to and including 5 as shown in the and send it of its own accord but that may be quite while later. These two alternative approaches are tradeoffs between efficient use of band width and data link layer buffer space. Depending on which resource is scarcer the receiving data link layer only accepts frames in order frames following an error are discarded. In this protocol for the first time we have dropped the as sumption that the network layer always has an infinite supply of packets to send. When the network layer has packet it wants to send it can cause net work layer ready event to happen. However to enforce the flow control limit on the sender window or the number of unacknowledged frames that may be out standing at any time the data link layer must be able to keep the network layer from bothering it with more work. The library procedures enable network layer and disable network layer do this job. The maximum number of frames that may be outstanding at any instant is not the same as the size of the sequence number space. For gobackn MAX SEQ distinct sequence numbers . We will see an even tighter restriction for the next protocol selective repeat. To see why this res triction is required consider the following scenario with MAX SEQ 7 1. The sender sends frames 0 through 7. 2. piggybacked acknowledgement for 7 comes back to the sender. 3. The sender sends another eight frames again with sequence numbers 4. Now another piggybacked acknowledgement for frame 7 comes in. Protocol 5 allows multiple outstanding frames. The sender may transmit up to MAX SEQ frames without waiting for an ack. In addition unlike in the previous protocols the network layer is not assumed to have new packet all the time. Instead the network layer causes network layer ready event when there is packet to send. typedef enum frame arrival cksum err timeout network layer ready event type include protocol. static boolean between Return true if circularly false otherwise. static void send data insert sequence number into frame .ack piggyback ack Accept save and transmit new frame. from network layer fetch new packet expand the senders window send data transmit the frame advance lower edge of receivers window Ack implies 1 2 etc. Check for this. while just ignore bad frames trouble retransmit all outstanding frames for send data resend frame The question is this did all eight frames belonging to the second batch arrive suc cessfully or did all eight get lost In both cases the receiver would be sending frame 7 as the acknowledgement. The sender has no way of telling. For this reason the maximum number of out standing frames must be restricted to MAX SEQ. Although protocol 5 does not buffer the frames arriving after an error it does not escape the problem of buffering altogether. Since sender may have to retransmit all the unacknowledged frames at future time it must hang on to all transmitted frames until it knows for sure that they have been accepted by the re ceiver. When an acknowledgement comes in for frame frames 1 2 and so on are also automatically acknowledged. This type of acknowledgement is called cumulative acknowledgement. This property is especially important when some of the previous acknowledgementbearing frames were lost or gar bled. Whenever any acknowledgement comes in the data link layer checks to see if any buffers can now be released. If buffers can be released previously blocked network layer can now be al lowed to cause more network layer ready events. For this protocol we assume that there is always reverse traffic on which to piggyback acknowledgements. Protocol 4 does not need this assumption since it sends back one frame every time it receives frame even if it has already sent that frame. In the next protocol we will solve the problem of oneway traffic in an Because protocol 5 has multiple outstanding frames it logically needs multi ple timers one per outstanding frame. Each frame times out independently of all the other ones. However all of these timers can easily be simulated in software using single hardware clock that causes interrupts periodically. The pending timeouts form linked list with each node of the list containing the number of clock ticks until the timer expires the frame being timed and pointer to the next Frame being timed outs. The situation after the first timeout has expired. As an illustration of how the timers could be implemented consider the ex the real time is 100000.000 three timeouts are pending at 100000.005 100000.013 and 100000.019. Every time the hardware clock ticks the real the tick counter becomes zero timeout is caused and the node is removed from be scanned when start timer or stop timer is called it does not require much work per tick. In protocol 5 both of these routines have been given parameter indicating which frame is to be timed. The gobackn protocol works well if errors are rare but if the line is poor it wastes lot of bandwidth on retransmitted frames. An alternative strategy the selective repeat protocol is to allow the receiver to accept and buffer the frames following damaged or lost one. In this protocol both sender and receiver maintain window of outstanding and acceptable sequence numbers respectively. The senders window size starts out at 0 and grows to some predefined maximum. The receivers window in con trast is always fixed in size and equal to the predetermined maximum. The re ceiver has buffer reserved for each sequence number within its fixed window. Associated with each buffer is bit telling whether the buffer is full or empty. Whenever frame arrives its sequence number is checked by the function between to see if it falls within the window. If so and if it has not already been re ceived it is accepted and stored. This action is taken without regard to whether or not the frame contains the next packet expected by the network layer. Of course it must be kept within the data link layer and not passed to the network layer until all the lowernumbered frames have already been delivered to the network layer Nonsequential receive introduces further constraints on frame sequence num bers compared to protocols in which frames are only accepted in order. We can illustrate the trouble most easily with an example. Suppose that we have 3bit sequence number so that the sender is permitted to transmit up to seven frames before being required to wait for an acknowledgement. Initially the senders and The sender now transmits frames 0 through 6. The receivers window allows it to accept any frame with sequence number between 0 and 6 inclusive. All seven frames arrive correctly so the receiver acknowledges them and advances its window to allow receipt of 7 0 It is at this point that disaster strikes in the form of lightning bolt hitting the telephone pole and wiping out all the acknowledgements. The protocol should operate correctly despite this disaster. The sender eventually times out and re transmits frame 0. When this frame arrives at the receiver check is made to see Protocol 6 accepts frames out of order but passes packets to the network layer in order. Associated with each outstanding frame is timer. When the timer expires only that frame is retransmitted not all the outstanding frames as in protocol 5. define NR BUFS 2 typedef enum frame arrival cksum err timeout network layer ready ack timeout event type initial value is only for the simulator static boolean between Same as between in protocol 5 but shorter and more obscure. static void send frame Construct and send data ack or nak frame. kind data ack or nak if .info buffer only meaningful for data frames .ack expand the window from network layer fetch new packet send frame transmit the frame if no nak send frame else start ack timer if Pass frames and advance window. to network layer to see if separate ack is needed if betweennext frame to send send frame frame expected out buf while advance lower edge of senders window if send frame damaged frame send frame we timed out ack timer expired send ack if enable network layer else disable network layer within the new window so it is accepted as new frame. The receiver also sends acknowledgement for frame 6 since 0 through 6 have been re The sender is happy to learn that all its transmitted frames did actually arrive correctly so it advances its window and immediately sends frames 7 0 1 2 3 4 and 5. Frame 7 will be accepted by the receiver and its packet will be passed di rectly to the network layer. Immediately thereafter the receiving data link layer checks to see if it has valid frame 0 already discovers that it does and passes the old buffered packet to the network layer as if it were new packet. Conse quently the network layer gets an incorrect packet and the protocol fails. The essence of the problem is that after the receiver advanced its window the new range of valid sequence numbers overlapped the old one. Consequently the following batch of frames might be either duplicates or new ones . The poor re ceiver has no way of distinguishing these two cases. The way out of this dilemma lies in making sure that after the receiver has ad vanced its window there is no overlap with the original window. To ensure that there is no overlap the maximum window size should be at most half the range of With 3 bits the sequence numbers range from 0 to 7. Only four unacknowledged frames should be outstanding at any instant. That way if the receiver has just ac cepted frames 0 through 3 and advanced its window to permit acceptance of frames 4 through 7 it can unambiguously tell if subsequent frames are retransmis sions or new ones . In general the window size for protocol 6 will be 2. An interesting question is how many buffers must the receiver have Under no conditions will it ever accept frames whose sequence numbers are below the lower edge of the window or frames whose sequence numbers are above the upper edge of the window. Consequently the number of buffers needed is equal to the window size not to the range of sequence numbers. In the preceding example of 3bit sequence number four buffers numbered 0 through 3 are needed. When frame arrives it is put in buffer mod 4. Notice that although and mod 4 are competing for the same buffer they are never within the window at the same time because that would imply window size of at least 5. For the same reason the number of timers needed is equal to the number of buffers not to the size of the sequence space. Effectively timer is associated Protocol 6 also relaxes the implicit assumption that the channel is heavily loaded. We made this assumption in protocol 5 when we relied on frames being sent in the reverse direction on which to piggyback acknowledgements. If the re verse traffic is light the acknowledgements may be held up for long period of have been sent and received but not acknowledged. Initial situation with window size of 4. After 4 frames have been sent and received but not direction and no traffic in the other direction the protocol will block when the sender window reaches its maximum. To relax this assumption an auxiliary timer is started by start ack timer after an insequence data frame arrives. If no reverse traffic has presented itself before this timer expires separate acknowledgement frame is sent. An interrupt due to the auxiliary timer is called an ack timeout event. With this arrangement traffic flow in only one direction is possible because the lack of reverse data frames onto which acknowledgements can be piggybacked is no longer an obstacle. Only one auxiliary timer exists and if start ack timer is called while the timer is running it has no effect. The timer is not reset or extended since its purpose is to provide some minimum rate of acknowledgements. It is essential that the timeout associated with the auxiliary timer be appreci ably shorter than the timeout used for timing out data frames. This condition is required to ensure that correctly received frame is acknowledged early enough that the frames retransmission timer does not expire and retransmit the frame. Protocol 6 uses more efficient strategy than protocol 5 for dealing with er rors. Whenever the receiver has reason to suspect that an error has occurred it sends negative acknowledgement frame back to the sender. Such frame is request for retransmission of the frame specified in the NAK. In two cases the receiver should be suspicious when damaged frame arrives or frame other than the expected one arrives . To avoid making multi ple requests for retransmission of the same lost frame the receiver should keep track of whether NAK has already been sent for given frame. The variable no nak in protocol 6 is true if no NAK has been sent yet for frame expected. If the NAK gets mangled or lost no real harm is done since the sender will eventu ally time out and retransmit the missing frame anyway. If the wrong frame ar rives after NAK has been sent and lost no nak will be true and the auxiliary timer will be started. When it expires an ACK will be sent to resynchronize the In some situations the time required for frame to propagate to the destina tion be processed there and have the acknowledgement come back is constant. In these situations the sender can adjust its timer to be tight just slightly larger than the normal time interval expected between sending frame and receiving its acknowledgement. NAKs are not useful in this case. However in other situations the time can be highly variable. For example if the reverse traffic is sporadic the time before acknowledgement will be shorter when there is reverse traffic and longer when there is not. The sender is faced with the choice of either setting the interval to small value or setting it to large value . Both choices waste bandwidth. In general if the standard deviation of the acknowledgement interval is large compared to the interval itself the timer is set loose to be conservative. NAKs can then appreciably speed up retransmission of lost or damaged frames. Closely related to the matter of timeouts and NAKs is the question of deter mining which frame caused timeout. In protocol 5 it is always ack expected because it is always the oldest. In protocol 6 there is no trivial way to determine who timed out. Suppose that frames 0 through 4 have been transmitted meaning that the list of outstanding frames is 01234 in order from oldest to youngest. Now imagine that 0 times out 5 is transmitted 1 times out 2 times out and 6 is transmitted. At this point the list of outstanding frames is 3405126 from oldest to youngest. If all inbound traffic is lost for while the seven outstanding frames will time out in that order. To keep the example from getting even more complicated than it already is we have not shown the timer administration. Instead we just assume that the variable oldest frame is set upon timeout to indicate which frame timed out. Within single building LANs are widely used for interconnection but most widearea network infrastructure is built up from pointtopoint lines. In Chap. 4 we will look at LANs. Here we will examine the data link protocols found on pointtopoint lines in the Internet in two common situations. The first situation is when packets are sent over SONET optical fiber links in widearea networks. These links are widely used for example to connect routers in the different loca tions of an ISPs network. The second situation is for ADSL links running on the local loop of the tele phone network at the edge of the Internet. These links connect millions of individ uals and businesses to the Internet. The Internet needs pointtopoint links for these uses as well as dialup mo dems leased lines and cable modems and so on. standard protocol called PPP is used to send packets over these links. PPP is de fined in RFC 1661 and further elaborated in RFC 1662 and other RFCs . SONET and ADSL links both apply PPP but in different ways. SONET which we covered in Sec. 2.6.4 is the physical layer protocol that is most commonly used over the widearea optical fiber links that make up the back bone of communications networks including the telephone system. It provides bitstream that runs at welldefined rate for example 2.4 Gbps for an OC48 link. This bitstream is organized as fixedsize byte payloads that recur every 125 μsec whether or not there is user data to send. To carry packets across these links some framing mechanism is needed to distinguish occasional packets from the continuous bitstream in which they are transported. PPP runs on IP routers to provide this mechanism as shown in PPP improves on an earlier simpler protocol called SLIP and is used to handle error detection link configuration support multiple protocols permit authentication and more. With wide set of options PPP provides three main features 1. framing method that unambiguously delineates the end of one frame and the start of the next one. The frame format also handles error detection. 2. link control protocol for bringing lines up testing them negotiat ing options and bringing them down again gracefully when they are no longer needed. This protocol is called LCP for each net work layer supported. The PPP frame format was chosen to closely resemble the frame format of HDLC widely used instance of an earlier family of protocols since there was no need to reinvent the wheel. The primary difference between PPP and HDLC is that PPP is byte oriented rather than bit oriented. In particular PPP uses byte stuffing and all frames are an integral number of bytes. HDLC uses bit stuffing and allows frames of say 30.25 There is second major difference in practice however. HDLC provides re liable transmission with sliding window acknowledgements and timeouts in the manner we have studied. PPP can also provide reliable transmission in noisy en vironments such as wireless networks the exact details are defined in RFC 1663. However this is rarely done in practice. Instead an unnumbered mode is near ly always used in the Internet to provide connectionless unacknowledged service. standard HDLC flag byte of 0x7E . The flag byte is stuffed if it occurs within the Payload field using the escape byte 0x7D. The following byte is the escaped byte XORed with 0x20 which flips the 5th bit. For example 0x7D 0x5E is the escape sequence for the flag byte 0x7E. This means the start and end of frames can be searched for simply by scanning for the byte 0x7E since it will not occur elsewhere. The destuffing rule when receiving frame is to look for 0x7D remove it and XOR the following byte with 0x20. Also only one flag byte is needed between frames. Multiple flag bytes can be used to fill the link when there are no frames to be sent. After the startofframe flag byte comes the Address field. This field is al ways set to the binary value 11111111 to indicate that all stations are to accept the The Address field is followed by the Control field the default value of which is 00000011. This value indicates an unnumbered frame. Since the Address and Control fields are always constant in the default con figuration LCP provides the necessary mechanism for the two parties to negotiate an option to omit them altogether and save 2 bytes per frame. The fourth PPP field is the Protocol field. Its job is to tell what kind of packet AppleTalk. Codes starting with 1 bit are used for PPP configuration protocols including LCP and different NCP for each network layer protocol supported. byte using LCP. The designers were perhaps overly cautious in thinking that someday there might be more than 256 protocols in use. The Payload field is variable length up to some negotiated maximum. If the bytes is used. Padding may follow the payload if it is needed. After the Payload field comes the Checksum field which is normally 2 bytes but 4byte checksum can be negotiated. The 4byte checksum is in fact the same 32bit CRC whose generator polynomial is given at the end of Sec. 3.2.2. The 2 byte checksum is also an industrystandard CRC. PPP is framing mechanism that can carry the packets of multiple protocols over many types of physical layers. To use PPP over SONET the choices to make are spelled out in RFC 2615 . 4byte checksum is used since this is the primary means of detecting transmission errors over the physical link and network layers. It is recommended that the Address Control and Protocol fields not be compressed since SONET links already run at relative There is also one unusual feature. The PPP payload is scrambled before it is inserted into the SONET payload. Scrambling XORs the that the SONET bitstream needs frequent bit transitions for synchronization. These transitions come naturally with the variation in voice signals but in data communication the user chooses the information that is sent and might send packet with long run of 0s. With scrambling the likelihood of user being able Before PPP frames can be carried over SONET lines the PPP link must be es The link starts in the DEAD state which means that there is no connection at the physical layer. When physical layer connection is established the link moves to ESTABLISH. At this point the PPP peers exchange series of LCP packets each carried in the Payload field of PPP frame to select the PPP op tions for the link from the possibilities mentioned above. The initiating peer pro poses options and the responding peer either accepts or rejects them in whole or part. The responder can also make alternative proposals. If LCP option negotiation is successful the link reaches the AUTHENTICATE Now the two parties can check each others identities if desired. authentication is successful the NETWORK state is entered and series of NCP the NCP protocols because each one is specific to some network layer protocol and allows configuration requests to be made that are specific to that protocol. For IP for example the assignment of IP addresses to both ends of the link is the most important possibility. Once OPEN is reached data transport can take place. It is in this state that IP packets are carried in PPP frames across the SONET line. When data transport is finished the link moves into the TERMINATE state and from there it moves back to the DEAD state when the physical layer connection is dropped. ADSL connects millions of home subscribers to the Internet at megabitsec rates over the same telephone local loop that is used for plain old telephone ser vice. In Sec. 2.5.3 we described how device called DSL modem is added on the home side. It sends bits over the local loop to device called DSLAM pronounced deeslam in the telephone companys local office. Now we will explore in more detail how packets are carried over ADSL The overall picture for the protocols and devices used with ADSL is shown in sen to show the most popular scenario. Inside the home computer such as PC sends IP packets to the DSL modem using link layer like Ethernet. The DSL modem then sends the IP packets over the local loop to the DSLAM using the protocols that we are about to study. At the DSLAM the IP packets are extracted and enter an ISP network so that they may reach any destination on the Internet. the ADSL physical layer. They are based on digital modulation scheme called orthogonal frequency division multiplexing as we saw in Sec 2.5.3. Near the top of the stack just below the IP network layer is PPP. This protocol is the same PPP that we have just studied for packet over and carry IP packets. In between ADSL and PPP are ATM and AAL5. These are new protocols that we have not seen before. ATM was designed in the early 1990s and launched with incredible hype. It promised network tech voice data cable television telegraph carrier pigeon tin cans connected by strings tom toms and everything else into an integrated system that could do were similar to those we described concerning the OSI protocols that is bad tim ing technology implementation and politics. Nevertheless ATM was much more successful than OSI. While it has not taken over the world it remains wide ly used in niches including broadband access lines such as DSL and WAN links inside telephone networks. ATM is link layer that is based on the transmission of fixedlength cells of information. The Asynchronous in its name means that the cells do not always need to be sent in the way that bits are continuously sent over synchronous lines as in SONET. Cells only need to be sent when there is information to carry. ATM is connectionoriented technology. Each cell carries virtual circuit paths of established connections. The cells are each 53 bytes long consisting of 48byte payload plus 5byte cal layer link among different users in fine slices. This ability is useful when for example sending both voice and data over one link without having long data packets that would cause large variations in the delay of the voice samples. The unusual choice for the cell length is an indication of just how political the design of ATM was. The 48byte size for the payload was compromise to resolve deadlock between Europe which wanted 32byte cells and the . which wanted 64byte cells. To send data over an ATM network it needs to be mapped into sequence of cells. This mapping is done with an ATM adaptation layer in process called seg mentation and reassembly. Several adaptation layers have been defined for dif ferent services ranging from periodic voice samples to packet data. The main one used for packet data is AAL5 . gives the length and has 4byte CRC for error detection. Naturally the CRC is the same one used for PPP and IEEE 802 LANs like Ethernet. Crowcroft have shown that it is strong enough to detect nontraditional er rors such as cell reordering. As well as payload the AAL5 frame has padding. This rounds out the overall length to be multiple of 48 bytes so that the frame can be evenly divided into cells. No addresses are needed on the frame as the vir tual circuit identifier carried in each cell will get it to the right destination. Now that we have described ATM we have only to describe how PPP makes use of ATM in the case of ADSL. It is done with yet another standard called PPPoA . This standard is not really protocol . Only the PPP protocol and payload fields are placed in the AAL5 payload as whether the payload is an IP packet or packet from another protocol such as LCP. The far end knows that the cells contain PPP information because an ATM virtual circuit is set up for this purpose. Within the AAL5 frame PPP framing is not needed as it would serve no pur pose ATM and AAL5 already provide the framing. More framing would be worthless. The PPP CRC is also not needed because AAL5 already includes the very same CRC. This error detection mechanism supplements the ADSL physical layer coding of ReedSolomon code for error correction and 1byte CRC for the detection of any remaining errors not otherwise caught. This scheme has much more sophisticated errorrecovery mechanism than when packets are sent over SONET line because ADSL is much noisier channel. The task of the data link layer is to convert the raw bit stream offered by the physical layer into stream of frames for use by the network layer. The link layer can present this stream with varying levels of reliability ranging from con nectionless unacknowledged service to reliable connectionoriented service. Various framing methods are used including byte count byte stuffing and bit stuffing. Data link protocols can provide error control to detect or correct dam aged frames and to retransmit lost frames. To prevent fast sender from overrun ning slow receiver the data link protocol can also provide flow control. The sli ding window mechanism is widely used to integrate error control and flow control in simple way. When the window size is 1 packet the protocol is stopandwait. Codes for error correction and detection add redundant information to mes sages by using variety of mathematical techniques. Convolutional codes and ReedSolomon codes are widely deployed for error correction with lowdensity parity check codes increasing in popularity. The codes for error detection that are used in practice include cyclic redundancy checks and checksums. All these codes can be applied at the link layer as well as at the physical layer and higher layers. We examined series of protocols that provide reliable link layer using ac knowledgements and retransmissions or ARQ under more realistic assumptions. Starting from an errorfree environment in which the receiver can handle any frame sent to it we introduced flow control followed by error control with sequence numbers and the stopandwait algorithm. Then we used the sliding window algorithm to allow bidirectional communication and introduce the concept of piggybacking. The last two protocols pipeline the trans mission of multiple frames to prevent the sender from blocking on link with long propagation delay. The receiver can either discard all frames other than the next one in sequence or buffer outoforder frames and send negative acknowl edgements for greater bandwidth efficiency. The former strategy is gobackn protocol and the latter strategy is selective repeat protocol. The Internet uses PPP as the main data link protocol over pointtopoint lines. It provides connectionless unacknowledged service using flag bytes to delimit frames and CRC for error detection. It is used to carry packets across range of links including SONET links in widearea networks and ADSL links for the 1. An upperlayer packet is split into 10 frames each of which has an 80 chance of ar riving undamaged. If no error control is done by the data link protocol how many times must the message be sent on average to get the entire thing through 2. The following character encoding is used in data link protocol Show the bit sequence transmitted for the fourcharacter frame ESC FLAG when each of the following framing methods is used Flag bytes with byte stuffing. Starting and ending flag bytes with bit stuffing. 3. The following data fragment occurs in the middle of data stream for which the byte stuffing algorithm described in the text is used ESC ESC FLAG FLAG . What is the output after stuffing 4. What is the maximum overhead in bytestuffing algorithm 5. One of your classmates Scrooge has pointed out that it is wasteful to end each frame with flag byte and then begin the next one with second flag byte. One flag byte could do the job as well and byte saved is byte earned. Do you agree 6. bit string 0111101111101111110 needs to be transmitted at the data link layer. What is the string actually transmitted after bit stuffing 7. Can you think of any circumstances under which an openloop protocol might be preferable to the feedbacktype protocols discussed throughout 8. To provide more reliability than single parity bit can give an errordetecting coding scheme uses one parity bit for checking all the oddnumbered bits and second parity bit for all the evennumbered bits. What is the Hamming distance of this code 9. Sixteenbit messages are transmitted using Hamming code. How many check bits are needed to ensure that the receiver can detect and correct singlebit errors Show the bit pattern transmitted for the message 1101001100110101. Assume that even par ity is used in the Hamming code. 10. 12bit Hamming code whose hexadecimal value is 0xE4F arrives at receiver. What was the original value in hexadecimal Assume that not more than 1 bit is in 11. One way of detecting errors is to transmit data as block of rows of bits per row and add parity bits to each row and each column. The bitin the lowerright corner is parity bit that checks its row and its column. Will this scheme detect all single errors Double errors Triple errors Show that this scheme cannot detect some fourbit er 12. Suppose that data are transmitted in blocks of sizes 1000 bits. What is the maximum error rate under which error detection and retransmission mechanism is better than using Hamming code Assume that bit errors are independent of one another and no bit error occurs during retransmission. 13. block of bits with rows and columns uses horizontal and vertical parity bits for error detection. Suppose that exactly 4 bits are inverted due to transmission errors. Derive an expression for the probability that the error will be undetected. sequence is 10101010 and the internal state is initially all zero 15. Suppose that message 1001 1100 1010 0011 is transmitted using Internet Checksum . What is the value of the checksum 16. What is the remainder obtained by dividing 7 5 1 by the generator polynomial 17. bit stream 10011101 is transmitted using the standard CRC method described in the text. The generator polynomial is 3 1. Show the actual bit string transmitted. Sup pose that the third bit from the left is inverted during transmission. Show that this error is detected at the receivers end. Give an example of bit errors in the bit string transmitted that will not be detected by the receiver. 18. 1024bit message is sent that contains 992 data bits and 32 CRC bits. CRC is com puted using the IEEE 802 standardized 32degree CRC polynomial. For each of the following explain whether the errors during message transmission will be detected by There was 35bit long burst error. sulted in the receiver accepting two copies of the same frame due to loss of acknowl edgement frame. Is it possible that receiver may accept multiple copies of the same frame when none of the frames are lost 20. channel has bit rate of 4 kbps and propagation delay of 20 msec. For what range of frame sizes does stopandwait give an efficiency of at least 50 21. In protocol 3 is it possible for the sender to start the timer when it is already running If so how might this occur If not why is it impossible 22. 3000kmlong T1 trunk is used to transmit 64byte frames using protocol 5. If the propagation speed is 6 μseckm how many bits should the sequence numbers be 23. Imagine sliding window protocol using so many bits for sequence numbers that wraparound never occurs. What relations must hold among the four window edges and the window size which is constant and the same for both the sender and the re 24. If the procedure between in protocol 5 checked for the condition instead of the condition would that have any effect on the protocols correctness or ef ficiency Explain your answer. 25. In protocol 6 when data frame arrives check is made to see if the sequence num ber differs from the one expected and no nak is true. If both conditions hold NAK is sent. Otherwise the auxiliary timer is started. Suppose that the else clause were omitted. Would this change affect the protocols correctness 26. Suppose that the threestatement while loop near the end of protocol 6 was removed from the code. Would this affect the correctness of the protocol or just the per formance Explain your answer. 27. The distance from earth to distant planet is approximately 9 1010 . What is the Mbps pointtopoint link Assume that the frame size is 32 KB and the speed of light 28. In the previous problem suppose sliding window protocol is used instead. For what send window size will the link utilization be 100 You may ignore the protocol processing times at the sender and the receiver. invoked if the incoming frame is NAK and another condition is met. Give scenario where the presence of this other condition is essential. 30. Consider the operation of protocol 6 over 1Mbps perfect line. The maximum frame size is 1000 bits. New packets are generated 1 second apart. The timeout interval is 10 msec. If the special acknowledgement timer were eliminated unnecessary timeouts would occur. How many times would the average message be 31. In protocol 6 MAX SEQ 2n 1. While this condition is obviously desirable to the protocol work correctly for MAX SEQ 4 for example 32. Frames of 1000 bits are sent over 1Mbps channel using geostationary satellite whose propagation time from the earth is 270 msec. Acknowledgements are always bers are used. What is the maximum achievable channel utilization for Stopandwait transmissions for protocol 6 on heavily loaded 50kbps satellite channel with data time from the earth to the satellite is 270 msec. ACK frames never occur. NAK frames are 40 bits. The error rate for data frames is 1 and the error rate for NAK frames is negligible. The sequence numbers are 8 bits. 34. Consider an errorfree 64kbps satellite channel used to send 512byte data frames in one direction with very short acknowledgements coming back the other way. What is the maximum throughput for window sizes of 1 7 15 and 127 The earthsatellite propagation time is 270 msec. 35. 100kmlong cable runs at the T1 data rate. The propagation speed in the cable is 23 the speed of light in vacuum. How many bits fit in the cable 36. Give at least one reason why PPP uses byte stuffing instead of bit stuffing to prevent accidental flag bytes within the payload from causing confusion. 37. What is the minimum overhead to send an IP packet using PPP Count only the over 38. 100byte IP packet is transmitted over local loop using ADSL protocol stack. How 39. The goal of this lab exercise is to implement an errordetection mechanism using the standard CRC algorithm described in the text. Write two programs generator and verifier. The generator program reads from standard input line of ASCII text con taining an nbit message consisting of string of 0s and 1s. The second line is the bit polynomial also in ASCII. It outputs to standard output line of ASCII text with 0s and 1s representing the message to be transmitted. Then it outputs the poly nomial just as it read it in. The verifier program reads in the output of the generator program alter that inverts 1 bit on the first line depending on its argument but copies the rest of the two lines correctly. generator 1 the user community is generating frames at higher rate than the channel can handle and nearly every frame will suffer collision. For reasonable throughput we would expect 0 1. In addition to the new frames the stations also generate retransmissions of frames that previously suffered collisions. Let us further assume that the old and new frames combined are well modeled by Poisson distribution with mean of frames per frame time. Clearly . At low load there will be few collisions hence few retransmissions so . At high load there will be many collisions so . Under all loads the throughput is just the offered load times the probability 0 of transmission succeedingthat is GP 0 where 0 is the probability that frame does not suffer collision. frame will not suffer collision if no other frames are sent within one THE MEDIUM ACCESS CONTROL SUBLAYER shaded frame arrive undamaged Let be the time required to send one frame. If any other user has generated frame between time 0 and 0 the end of that frame will collide with the beginning of the shaded one. In fact the shaded frames fate was already sealed even before the first bit was sent but since in pure ALOHA station does not listen to the channel before transmitting it has no way of knowing that another frame was already underway. Similarly any other frame started between 0 and 0 2t will bump into the end of the shaded frame. The probability that frames are generated during given frame time in which frames are expected is given by the Poisson distribution so the probability of zero frames is just . In an interval two frame times long the mean number of frames generated is 2G. The probability of no frames being initiated during the entire vulnerable period is thus given by 0 2G. Using The relation between the offered traffic and the throughput is shown in about 0.184. In other words the best we can hope for is channel utilization of 18. This result is not very encouraging but with everyone transmitting at will we could hardly have expected 100 success rate. Soon after ALOHA came onto the scene Roberts published method for doubling the capacity of an ALOHA system. His proposal was to divide time into discrete intervals called slots each interval corresponding to one frame. This Pure ALOHA Ge2G approach requires the users to agree on slot boundaries. One way to achieve syn chronization would be to have one special station emit pip at the start of each in terval like clock. In Roberts method which has come to be known as slotted ALOHAin contrast to Abramsons pure ALOHAa station is not permitted to send when ever the user types line. Instead it is required to wait for the beginning of the next slot. Thus the continuous time ALOHA is turned into discrete time one. collisions that are now possible. The probability of no other traffic during the same slot as our test frame is then which leads to of 1e or about 0.368 twice that of pure ALOHA. If the system is operating at 1 the probability of an empty slot is 0.368 . The best we can hope for using slotted ALOHA is 37 of the slots empty 37 successes and 26 collisions. Operating at higher values of reduces the number of empties but increases the number of collisions exponentially. To see how this rapid growth of collisions with comes about consider the transmission of test frame. The probability that it will avoid collision is which is the probabil ity that all the other stations are silent in that slot. The probability of collision is then just 1 . The probability of transmission requiring exactly attempts is The expected number of transmissions per line typed at terminal is then As result of the exponential dependence of upon small increases in the channel load can drastically reduce its performance. Slotted ALOHA is notable for reason that may not be initially obvious. It was devised in the 1970s used in few early experimental systems then almost forgotten. When Internet access over the cable was invented all of sudden there was problem of how to allocate shared channel among multiple competing users. Slotted ALOHA was pulled out of the garbage can to save the day. Later having multiple RFID tags talk to the same RFID reader presented another varia tion on the same problem. Slotted ALOHA with dash of other ideas mixed in again came to the rescue. It has often happened that protocols that are perfectly valid fall into disuse for political reasons or due to everchanging technology trends. Then years later some clever person realizes that longdiscarded protocol solves his current prob that are not currently in widespread use but might easily be used in future applica tions provided that enough network designers are aware of them. Of course we will also study many protocols that are in current use as well. With slotted ALOHA the best channel utilization that can be achieved is 1e. This low result is hardly surprising since with stations transmitting at will with out knowing what the other stations are doing there are bound to be many collis ions. In LANs however it is often possible for stations to detect what other sta tions are doing and thus adapt their behavior accordingly. These networks can protocols for improving performance. Protocols in which stations listen for carrier and act accordingly are called carrier sense protocols. number of them have been proposed and they were long ago analyzed in detail. For example see Kleinrock The first carrier sense protocol that we will study here is called 1persistent CSMA . That is bit of mouthful for the sim plest CSMA scheme. When station has data to send it first listens to the chan nel to see if anyone else is transmitting at that moment. If the channel is idle the stations sends its data. Otherwise if the channel is busy the station just waits until it becomes idle. Then the station transmits frame. If collision occurs the station waits random amount of time and starts all over again. The protocol is called 1persistent because the station transmits with probability of 1 when it finds the channel idle. You might expect that this scheme avoids collisions except for the rare case of simultaneous sends but it in fact it does not. If two stations become ready in the middle of third stations transmission both will wait politely until the trans mission ends and then both will begin transmitting exactly simultaneously re sulting in collision. If they were not so impatient there would be fewer collis More subtly the propagation delay has an important effect on collisions. There is chance that just after station begins sending another station will be come ready to send and sense the channel. If the first stations signal has not yet reached the second one the latter will sense an idle channel and will also begin sending resulting in collision. This chance depends on the number of frames that fit on the channel or the bandwidthdelay product of the channel. If only tiny fraction of frame fits on the channel which is the case in most LANs since the propagation delay is small the chance of collision happening is small. The larger the bandwidthdelay product the more important this effect becomes and the worse the performance of the protocol. Even so this protocol has better performance than pure ALOHA because both stations have the decency to desist from interfering with the third stations frame. Exactly the same holds for slotted ALOHA. second carrier sense protocol is nonpersistent CSMA. In this protocol conscious attempt is made to be less greedy than in the previous one. As before station senses the channel when it wants to send frame and if no one else is sending the station begins doing so itself. However if the channel is already in use the station does not continually sense it for the purpose of seizing it im mediately upon detecting the end of the previous transmission. Instead it waits random period of time and then repeats the algorithm. Consequently this algo rithm leads to better channel utilization but longer delays than 1persistent The last protocol is ppersistent CSMA. It applies to slotted channels and works as follows. When station becomes ready to send it senses the channel. If it is idle it transmits with probability . With probability 1 it defers until the next slot. If that slot is also idle it either transmits or defers again with probabilities and . This process is repeated until either the frame has been transmitted or another station has begun transmitting. In the latter case the unlucky station acts as if there had been collision . If the station initially senses that the channel is busy it waits until the next slot and applies the above algorithm. IEEE 802.11 uses refinement of ppersistent CSMA that we will discuss in Sec. 4.4. Persistent and nonpersistent CSMA protocols are definitely an improvement over ALOHA because they ensure that no station begins to transmit while the channel is busy. However if two stations sense the channel to be idle and begin transmitting simultaneously their signals will still collide. Another improvement is for the stations to quickly detect the collision and abruptly stop transmitting since they are irretrievably garbled anyway. This strategy saves time and bandwidth. This protocol known as CSMACD is the basis of the classic Ethernet LAN so it is worth devoting some time to looking at it in detail. It is important to realize that collision detection is an analog proc ess. The stations hardware must listen to the channel while it is transmitting. If the signal it reads back is different from the signal it is putting out it knows that collision is occurring. The implications are that received signal must not be tiny compared to the transmitted signal and that the modu lation must be chosen to allow collisions to be detected . CSMACD as well as many other LAN protocols uses the conceptual model Any other station having frame to send may now attempt to do so. If two or more stations decide to transmit simultaneously there will be collision. If sta tion detects collision it aborts its transmission waits random period of time and then tries again . Therefore our model for CSMACD will consist of alternating con tention and transmission periods with idle periods occurring when all stations are quiet . Now let us look at the details of the contention algorithm. Suppose that two stations both begin transmitting at exactly time 0. How long will it take them to realize that they have collided The answer is vital to determining the length of the contention period and hence what the delay and throughput will be. The minimum time to detect the collision is just the time it takes the signal to propagate from one station to the other. Based on this information you might think that station that has not heard collision for time equal to the full cable propagation time after starting its transmission can be sure it has seized the cable. By seized we mean that all other stations know it is transmitting and will not interfere. This conclusion is wrong. Consider the following worstcase scenario. Let the time for signal to pro pagate between the two farthest stations be τ. At 0 one station begins trans mitting. At 0 τ ε an instant before the signal arrives at the most distant sta tion that station also begins transmitting. Of course it detects the collision al most instantly and stops but the little noise burst caused by the collision does not get back to the original station until time 2τ ε. In other words in the worst case station cannot be sure that it has seized the channel until it has transmitted for 2τ without hearing collision. With this understanding we can think of CSMACD contention as slotted On 1km long coaxial cable τ 5 μsec. The difference for CSMACD compared to slotted ALOHA is that slots in which only one station transmits are followed by the rest of frame. This difference will greatly improve performance if the frame time is much longer than the propagation time. Although collisions do not occur with CSMACD once station has unambi guously captured the channel they can still occur during the contention period. These collisions adversely affect the system performance especially when the THE MEDIUM ACCESS CONTROL SUBLAYER bandwidthdelay product is large such as when the cable is long and the frames are short. Not only do collisions reduce bandwidth but they make the time to send frame variable which is not good fit for realtime traffic such as voice over IP. CSMACD is also not universally applicable. the channel without any collisions at all not even during the contention period. Most of these protocols are not currently used in major systems but in rapidly changing field having some protocols with excellent properties available for fu ture systems is often good thing. In the protocols to be described we assume that there are exactly stations each programmed with unique address from 0 to 1. It does not matter that some stations may be inactive part of the time. We also assume that propagation delay is negligible. The basic question remains which station gets the channel In our first collisionfree protocol the basic bitmap method each con tention period consists of exactly slots. If station 0 has frame to send it trans mits 1 bit during the slot 0. No other station is allowed to transmit during this slot. Regardless of what station 0 does station 1 gets the opportunity to transmit 1 bit during slot 1 but only if it has frame queued. In general station may announce that it has frame to send by inserting 1 bit into slot . After all slots have passed by each station has complete knowledge of which stations wish to transmit. At that point they begin transmitting frames in numerical order before starting to transmit. High numbered stations rarely have to wait for the next scan. Since lownumbered sta tions must wait on average 1.5N slots and highnumbered stations must wait on average 0.5N slots the mean for all stations is slots. The channel efficiency at low load is easy to compute. The overhead per frame is bits and the amount of data is bits for an efficiency of . At high load when all the stations have something to send all the time the bit contention period is prorated over frames yielding an overhead of only 1 bit per frame or an efficiency of . The mean delay for frame is equal to the sum of the time it queues inside its station plus an additional once it gets to the head of its internal queue. This interval is how long it takes to wait for all other stations to have their turn sending frame and another bitmap. The essence of the bitmap protocol is that it lets every station transmit frame in turn in predefined order. Another way to accomplish the same thing is to pass small message called token from one station to the next in the same predefined order. The token represents permission to send. If station has frame queued for transmission when it receives the token it can send that frame before it passes the token to the next station. If it has no queued frame it simply passes the token. In token ring protocol the topology of the network is used to define the order in which stations send. The stations are connected one to the next in single ring. Passing the token to the next station then simply consists of receiving the token in from one direction and transmitting it out in the other direction as seen in will circulate around the ring and reach whichever station is the destination. How ever to stop the frame circulating indefinitely some station needs THE MEDIUM ACCESS CONTROL SUBLAYER to remove it from the ring. This station may be either the one that originally sent the frame after it has gone through complete cycle or the station that was the intended recipient of the frame. Note that we do not need physical ring to implement token passing. The channel connecting the stations might instead be single long bus. Each station then uses the bus to send the token to the next station in the predefined sequence. Possession of the token allows station to use the bus to send one frame as be fore. This protocol is called token bus. The performance of token passing is similar to that of the bitmap protocol though the contention slots and frames of one cycle are now intermingled. After sending frame each station must wait for all stations to send the token to their neighbors and the other 1 stations to send frame if they have one. subtle difference is that since all positions in the cycle are equiva lent there is no bias for low or highnumbered stations. For token ring each sta tion is also sending the token only as far as its neighboring station before the pro tocol takes the next step. Each token does not need to propagate to all stations be fore the protocol advances to the next step. Token rings have cropped up as MAC protocols with some consistency. An early token ring protocol was popular in the 1980s as an alternative to classic Ethernet. In the 1990s much faster token ring called FDDI was beaten out by switched Ethernet. In the 2000s token ring called RPR was defined as IEEE 802.17 to standardize the mix of metro politan area rings in use by ISPs. We wonder what the 2010s will have to offer. problem with the basic bitmap protocol and by extension token passing is that the overhead is 1 bit per station so it does not scale well to networks with thousands of stations. We can do better than that by using binary station ad dresses with channel that combines transmissions. station wanting to use the channel now broadcasts its address as binary bit string starting with the high order bit. All addresses are assumed to be the same length. The bits in each ad dress position from different stations are BOOLEAN ORed together by the chan nel when they are sent at the same time. We will call this protocol binary count down. It was used in Datakit . It implicitly assumes that the trans mission delays are negligible so that all stations see asserted bits essentially in To avoid conflicts an arbitration rule must be applied as soon as station sees that highorder bit position that is 0 in its address has been overwritten with 1 it gives up. For example if stations 0010 0100 1001 and 1010 are all trying to get the channel in the first bit time the stations transmit 0 0 1 and 1 re spectively. These are ORed together to form 1. Stations 0010 and 0100 see the 1 and know that highernumbered station is competing for the channel so they give up for the current round. Stations 1001 and 1010 continue. gives up. The winner is station 1010 because it has the highest address. After winning the bidding it may now transmit frame after which another bidding ernumbered stations have higher priority than lowernumbered stations which may be either good or bad depending on the context. The channel efficiency of this method is . If however the frame format has been cleverly chosen so that the senders address is the first field in the frame even these log2 bits are not wasted and the efficiency is 100. Binary countdown is an example of simple elegant and efficient protocol that is waiting to be rediscovered. Hopefully it will find new home some day. We have now considered two basic strategies for channel acquisition in broadcast network contention as in CSMA and collisionfree protocols. Each strategy can be rated as to how well it does with respect to the two important per formance measures delay at low load and channel efficiency at high load. Under conditions of light load contention is preferable due to its low delay . As the load increases contention becomes increasingly less attractive because the overhead associated with channel arbitration becomes greater. Just the reverse is true for the collisionfree proto cols. At low load they have relatively high delay but as the load increases the channel efficiency improves . Obviously it would be nice if we could combine the best properties of the contention and collisionfree protocols arriving at new protocol that used con tention at low load to provide low delay but used collisionfree technique at high load to provide good channel efficiency. Such protocols which we will call limitedcontention protocols do in fact exist and will conclude our study of car rier sense networks. Up to now the only contention protocols we have studied have been symmet ric. That is each station attempts to acquire the channel with some probability with all stations using the same . Interestingly enough the overall system per formance can sometimes be improved by using protocol that assigns different probabilities to different stations. Before looking at the asymmetric protocols let us quickly review the per formance of the symmetric case. Suppose that stations are contending for chan Each has probability of transmitting during each slot. probability that some station successfully acquires the channel during given slot is the probability that any one station transmits with probability and all other 1 stations defer each with probability 1 . This value is kpk 1. To find the optimal value of we differentiate with respect to set the result to zero and solve for . Doing so we find that the best value of is 1k. Substitut Pr of success are good but as soon as the number of stations reaches even five the probability has dropped close to its asymptotic value of 1e. ing the channel can be increased only by decreasing the amount of competition. The limitedcontention protocols do precisely that. They first divide the stations into groups. Only the members of group 0 are permitted Number of ready stations to compete for slot 0. If one of them succeeds it acquires the channel and trans mits its frame. If the slot lies fallow or if there is collision the members of group 1 contend for slot 1 etc. By making an appropriate division of stations into groups the amount of contention for each slot can be reduced thus operating each The trick is how to assign stations to slots. Before looking at the general case let us consider some special cases. At one extreme each group has but one member. Such an assignment guarantees that there will never be collisions be cause at most one station is contending for any given slot. We have seen such protocols before . The next special case is to assign two stations per group. The probability that both will try to transmit during slot is 2 which for small is negligible. As more and more stations are assigned to the same slot the probability of collision grows but the length of the bitmap scan needed to give everyone chance shrinks. The limiting case is single group containing all stations . What we need is way to assign stations to slots dynamically with many stations per slot when the load is low and few station per slot when the load is high. One particularly simple way of performing the necessary assignment is to use the algorithm devised by the . Army for testing soldiers for syphilis during World War II . In short the Army took blood sample from soldiers. portion of each sample was poured into single test tube. This mixed sample was then tested for antibodies. If none were found all the soldiers in the group were declared healthy. If antibodies were present two new mixed samples THE MEDIUM ACCESS CONTROL SUBLAYER were prepared one from soldiers 1 through N2 and one from the rest. The proc ess was repeated recursively until the infected soldiers were determined. venient to think of the stations as the leaves of binary tree as illustrated in slot 0 all stations are permitted to try to acquire the channel. If one of them does so fine. If there is collision then during slot 1 only those stations falling under node 2 in the tree may compete. If one of them acquires the channel the slot fol lowing the frame is reserved for those stations under node 3. If on the other hand two or more stations under node 2 want to transmit there will be collision during slot 1 in which case it is node 4s turn during slot 2. In essence if collision occurs during slot 0 the entire tree is searched depth first to locate all ready stations. Each bit slot is associated with some particular node in the tree. If collision occurs the search continues recursively with the nodes left and right children. If bit slot is idle or if only one station transmits in it the searching of its node can stop because all ready stations have been located. When the load on the system is heavy it is hardly worth the effort to dedicate slot 0 to node 1 because that makes sense only in the unlikely event that precisely should be skipped as well for the same reason. Put in more general terms at what level in the tree should the search begin Clearly the heavier the load the farther down the tree the search should begin. We will assume that each station has good estimate of the number of ready stations for example from monitoring To proceed let us number the levels of the tree from the top with node 1 in has fraction 2i of the stations below it. If the ready stations are uniformly distributed the expected number of them below specific node at level is just 2iq. Intuitively we would expect the optimal level to begin searching the tree to be the one at which the mean number of contending stations per slot is 1 that is the level at which 2iq 1. Solving this equation we find that log2 . Numerous improvements to the basic algorithm have been discovered and are discussed in some detail by Bertsekas and Gallager . For example consid collision will occur so 2 will be tried and discovered idle. It is pointless to probe node 3 since it is guaranteed to have collision . The probe of 3 can be skipped and 6 tried next. When this probe also turns up nothing 7 can be skipped and node tried next. system of laptop computers that communicate by radio can be regarded as wireless LAN as we discussed in Sec. 1.5.3. Such LAN is an example of broadcast channel. It also has somewhat different properties than wired LAN these protocols. In Sec. 4.4 we will look at 802.11 in detail. common configuration for wireless LAN is an office building with access points strategically placed around the building. The APs are wired together using copper or fiber and provide connectivity to the stations that talk to them. If the transmission power of the APs and laptops is adjusted to have range of tens of meters nearby rooms become like single cell and the entire building becomes like the cellular telephony systems we studied in Chap. 2 except that each cell only has one channel. This channel is shared by all the stations in the cell includ ing the AP. It typically provides megabitsec bandwidths up to 600 Mbps. We have already remarked that wireless systems cannot normally detect col lision while it is occurring. The received signal at station may be tiny perhaps million times fainter than the signal that is being transmitted. Finding it is like looking for ripple on the ocean. Instead acknowledgements are used to dis cover collisions and other errors after the fact. There is an even more important difference between wireless LANs and wired LANs. station on wireless LAN may not be able to transmit frames to or re ceive frames from all other stations because of the limited radio range of the sta tions. In wired LANs when one station sends frame all other stations receive it. The absence of this property in wireless LANs causes variety of complica We will make the simplifying assumption that each radio transmitter has some fixed range represented by circular coverage region within which another sta tion can sense and receive the stations transmission. It is important to realize that THE MEDIUM ACCESS CONTROL SUBLAYER in practice coverage regions are not nearly so regular because the propagation of radio signals depends on the environment. Walls and other obstacles that attenu ate and reflect signals may cause the range to differ markedly in different direc tions. But simple circular model will do for our purposes. naive approach to using wireless LAN might be to try CSMA just listen for other transmissions and only transmit if no one else is doing so. The trouble is this protocol is not really good way to think about wireless because what mat ters for reception is interference at the receiver not at the sender. To see the na ed. For our purposes it does not matter which are APs and which are laptops. The radio range is such that and are within each others range and can poten tially interfere with one another. can also potentially interfere with both and but not with . mitting to . and are exposed terminals when transmitting to and . First consider what happens when and transmit to as depicted in hear because is out of range. Thus will falsely conclude that it can transmit to . If does start transmitting it will interfere at wiping out the frame from . We want MAC protocol that will prevent this kind of collision from happening because it wastes bandwidth. The problem of station not being able to detect potential competitor for the medium because the competitor is too far away is called the hidden terminal problem. Now let us look at different situation transmitting to at the same time will hear transmission and falsely conclude that it may not send to . In fact such transmission would cause bad reception only in the zone between and where neither of the intended receivers is located. We want MAC protocol that prevents this kind of deferral from happening because it wastes bandwidth. The problem is called the exposed terminal problem. The difficulty is that before starting transmission station really wants to know whether there is radio activity around the receiver. CSMA merely tells it whether there is activity near the transmitter by sensing the carrier. With wire all signals propagate to all stations so this distinction does not exist. However only one transmission can then take place at once anywhere in the system. In system based on shortrange radio waves multiple transmissions can occur simul taneously if they all have different destinations and these destinations are out of range of one another. We want this concurrency to happen as the cell gets larger and larger in the same way that people at party should not wait for everyone in the room to go silent before they talk multiple conversations can take place at LANs is MACA . The basic idea behind it is for the sender to stimulate the receiver into outputting short frame so stations nearby can detect this transmission and avoid transmitting for the duration of the upcoming data frame. This technique is used instead of carrier sense. This short frame contains the length of the data frame that will eventu ally follow. Then replies with CTS frame as shown in frame. Upon receipt of the CTS frame begins transmission. with CTS to . Now let us see how stations overhearing either of these frames react. Any station hearing the RTS is clearly close to and must remain silent long enough for the CTS to be transmitted back to without conflict. Any station hearing the CTS is clearly close to and must remain silent during the upcoming data trans mission whose length it can tell by examining the CTS frame. THE MEDIUM ACCESS CONTROL SUBLAYER hears the RTS from but not the CTS from . As long as it does not interfere with the CTS it is free to transmit while the data frame is being sent. In contrast is within range of but not . It does not hear the RTS but does hear the CTS. Hearing the CTS tips it off that it is close to station that is about to receive frame so it defers sending anything until that frame is expected to be finished. Station hears both control messages and like must be silent until the data frame is complete. Despite these precautions collisions can still occur. For example and could both send RTS frames to at the same time. These will collide and be lost. In the event of collision an unsuccessful transmitter waits random amount of time and tries abstract so it is time to see how these principles apply to real systems. Many of the designs for personal local and metropolitan area networks have been stan dardized under the name of IEEE 802. few have survived but many have not Charles Darwin came back as member of the IEEE Standards Association to weed out the unfit. The most important of the survivors are 802.3 and 802.11 . Bluetooth is widely deployed but has now been standardized outside of 802.15. With 802.16 it is too early to tell. Please consult the 6th edition of this book to find out. We will begin our study of real systems with Ethernet probably the most ubi quitous kind of computer network in the world. Two kinds of Ethernet exist clas sic Ethernet which solves the multiple access problem using the techniques we switches are used to connect different computers. It is important to note that while they are both referred to as Ethernet they are quite different. Classic Ether net is the original form and ran at rates from 3 to 10 Mbps. Switched Ethernet is what Ethernet has become and runs at 100 1000 and 10000 Mbps in forms call ed fast Ethernet gigabit Ethernet and 10 gigabit Ethernet. In practice only switched Ethernet is used nowadays. We will discuss these historical forms of Ethernet in chronological order showing how they developed. Since Ethernet and IEEE 802.3 are identical except for minor difference many people use the terms Ethernet and IEEE 802.3 interchangeably. We will do so too. For more information about Ethernet see Spurgeon . The story of Ethernet starts about the same time as that of ALOHA when student named Bob Metcalfe got his bachelors degree at . and then moved up the river to get his Ph. at Harvard. During his studies he was exposed to Abramsons work. He became so interested in it that after graduating from Har vard he decided to spend the summer in Hawaii working with Abramson before starting work at Xerox PARC . When he got to PARC he saw that the researchers there had designed and built what would later be called personal computers. But the machines were isolated. Using his knowl edge of Abramsons work he together with his colleague David Boggs designed and implemented the first local area network . It used single long thick coaxial cable and ran at 3 Mbps. They called the system Ethernet after the luminiferous ether through which electromagnetic radiation was once thought to propagate. The Xerox Ethernet was so successful that DEC Intel and Xerox drew up standard in 1978 for 10Mbps Ethernet called the DIX standard. With minor change the DIX standard became the IEEE 802.3 standard in 1983. Unfor tunately for Xerox it already had history of making seminal inventions and then failing to commercialize on them story told in Fumbling the Future . When Xerox showed little interest in doing anything with Ethernet other than helping standardize it Metcalfe formed his own company 3Com to sell Ethernet adapters for PCs. It sold many millions of them. Classic Ethernet snaked around the building as single long cable to which all variety popularly called thick Ethernet resembled yellow garden hose with markings every 2.5 meters to show where to attach computers. It was succeeded by thin Ethernet which bent more easily and made connections using industrystandard BNC connectors. Thin Ethernet was much cheaper and easier to install but it could run for only 185 meters per segment each of which could handle only 30 machines . unamplified length over which the signal will propagate. To allow larger net works multiple cables can be connected by repeaters. repeater is physical layer device that receives amplifies and retransmits signals in both directions. As far as the software is concerned series of cable segments THE MEDIUM ACCESS CONTROL SUBLAYER connected by repeaters is no different from single cable . Over each of these cables information was sent using the Manchester encod ing we studied in Sec. 2.5. An Ethernet could contain multiple cable segments and multiple repeaters but no two transceivers could be more than 2.5 km apart and no path between any two transceivers could traverse more than four repeaters. The reason for this restriction was so that the MAC protocol which we will look at next would work correctly. ble of 8 bytes each containing the bit pattern 10101010 . This last byte is called the Start of Frame delimiter for 802.3. The Manchester encoding of this pattern produces 10MHz square wave for 6.4 μsec to allow the receivers clock to synchronize with the senders. The last two 1 bits tell the receiver that the rest of the frame is Next come two addresses one for the destination and one for the source. They are each 6 bytes long. The first transmitted bit of the destination address is 0 for ordinary addresses and 1 for group addresses. Group addresses allow multiple stations to listen to single address. When frame is sent to group address all the stations in the group receive it. Sending to group of stations is called multi casting. The special address consisting of all 1 bits is reserved for broadcasting. frame containing all 1s in the destination field is accepted by all stations on the network. Multicasting is more selective but it involves group management to define which stations are in the group. Conversely broadcasting does not dif ferentiate between stations at all so it does not require any group management. An interesting feature of station source addresses is that they are globally unique assigned centrally by IEEE to ensure that no two stations anywhere in the world have the same address. The idea is that any station can uniquely address bytes of the address field are used for an OUI . Values for this field are assigned by IEEE and indicate manufacturer. Manufacturers are assigned blocks of 224 addresses. The manufacturer assigns the last 3 bytes of the address and programs the complete address into the NIC before Next comes the Type or Length field depending on whether the frame is Eth ernet or IEEE 802.3. Ethernet uses Type field to tell the receiver what to do with the frame. Multiple networklayer protocols may be in use at the same time on the same machine so when an Ethernet frame arrives the operating system has to know which one to hand the frame to. The Type field specifies which process to give the frame to. For example type code of 0x0800 means that the data con tains an IPv4 packet. IEEE 802.3 in its wisdom decided that this field would carry the length of the frame since the Ethernet length was determined by looking inside the dataa layering violation if ever there was one. Of course this meant there was no way trol protocol within the data. It uses 8 bytes to convey the 2 bytes of protocol type information. Unfortunately by the time 802.3 was published so much hardware and software for DIX Ethernet was already in use that few manufacturers and users were enthusiastic about repackaging the Type and Length fields. In 1997 IEEE threw in the towel and said that both ways were fine with it. Fortunately all the Type fields in use before 1997 had values greater than 1500 then well established as the maximum data size. Now the rule is that any number there less than or equal to 0x600 can be interpreted as Length and any number greater than 0x600 can be interpreted as Type. Now IEEE can maintain that everyone is using its standard and everybody else can keep on doing what they were already doing without feeling guilty about it. Next come the data up to 1500 bytes. This limit was chosen somewhat arbi trarily at the time the Ethernet standard was cast in stone mostly based on the fact THE MEDIUM ACCESS CONTROL SUBLAYER that transceiver needs enough RAM to hold an entire frame and RAM was expensive in 1978. larger upper limit would have meant more RAM and hence more expensive transceiver. In addition to there being maximum frame length there is also minimum frame length. While data field of 0 bytes is sometimes useful it causes prob lem. When transceiver detects collision it truncates the current frame which means that stray bits and pieces of frames appear on the cable all the time. To make it easier to distinguish valid frames from garbage Ethernet requires that valid frames must be at least 64 bytes long from destination address to checksum including both. If the data portion of frame is less than 46 bytes the Pad field is used to fill out the frame to the minimum size. Another reason for having minimum length frame is to prevent station from completing the transmission of short frame before the first bit has even reached the far end of the cable where it may collide with one end of the network sends off frame. Let us call the propagation time for this frame to reach the other end τ. Just before the frame gets to the other end the most distant station starts transmitting. When detects that it is receiving more power than it is putting out it knows that collision has occurred so it aborts its transmission and generates 48bit noise burst to warn all other stations. In other words it jams the ether to make sure the sender does not miss the collision. At about time 2τ the sender sees the noise burst and aborts its transmission too. It then waits random time before trying again. If station tries to transmit very short frame it is conceivable that colli sion will occur but the transmission will have completed before the noise burst gets back to the station at 2τ. The sender will then incorrectly conclude that the frame was successfully sent. To prevent this situation from occurring all frames must take more than 2τ to send so that the transmission is still taking place when the noise burst gets back to the sender. For 10Mbps LAN with maximum length of 2500 meters and four repeaters the roundtrip time has been determined to be nearly 50 μsec in the worst case. Therefore the shortest allowed frame must take at least this long to transmit. At 10 Mbps bit takes 100 nsec so safety this number was rounded up to 512 bits or 64 bytes. Sec. 3.2. In fact it is defined exactly by the generator polynomial we gave there which popped up for PPP ADSL and other links too. This CRC is an error detecting code that is used to determine if the bits of the frame have been received It just does error detection with the frame dropped if an error is Classic Ethernet uses the 1persistent CSMACD algorithm that we studied in Sec. 4.2. This descriptor just means that stations sense the medium when they have frame to send and send the frame as soon as the medium becomes idle. They monitor the channel for collisions as they send. If there is collision they abort the transmission with short jam signal and retransmit after random inter Let us now see how the random interval is determined when collision time is divided into discrete slots whose length is equal to the worstcase round by Ethernet the slot time has been set to 512 bit times or 51.2 μsec. After the first collision each station waits either 0 or 1 slot times at random before trying again. If two stations collide and each one picks the same random number they will collide again. After the second collision each one picks either 0 1 2 or 3 at random and waits that number of slot times. If third collision occurs the next time the number of slots to wait is chosen at random from the interval 0 to 23 1. In general after collisions random number between 0 and 2i 1 is chosen and that number of slots is skipped. However after 10 collisions have been reached the randomization interval is frozen at maximum of 1023 slots. After computer. Further recovery is up to higher layers. This algorithm called binary exponential backoff was chosen to dynami cally adapt to the number of stations trying to send. If the randomization interval for all collisions were 1023 the chance of two stations colliding for second time would be negligible but the average wait after collision would be hundreds of slot times introducing significant delay. On the other hand if each station always THE MEDIUM ACCESS CONTROL SUBLAYER delayed for either 0 or 1 slots then if 100 stations ever tried to send at once they would collide over and over until 99 of them picked 1 and the remaining station picked 0. This might take years. By having the randomization interval grow ex ponentially as more and more consecutive collisions occur the algorithm ensures low delay when only few stations collide but also ensures that the collisions are resolved in reasonable interval when many stations collide. Truncating the backoff at 1023 keeps the bound from growing too large. If there is no collision the sender assumes that the frame was probably suc cessfully delivered. That is neither CSMACD nor Ethernet provides acknowl edgements. This choice is appropriate for wired and optical fiber channels that have low error rates. Any errors that do occur must then be detected by the CRC and recovered by higher layers. For wireless channels that have more errors we will see that acknowledgements are used. Now let us briefly examine the performance of classic Ethernet under condi tions of heavy and constant load that is with stations always ready to transmit. rigorous analysis of the binary exponential backoff algorithm is complicated. Instead we will follow Metcalfe and Boggs and assume constant retransmission probability in each slot. If each station transmits during conten tion slot with probability the probability that some station acquires the chan is maximized when 1k with 1e as . The probability that the contention interval has exactly slots in it is Aj 1 so the mean number of Since each slot has duration 2τ the mean contention interval is 2τA. Assuming optimal the mean number of contention slots is never more than so is at most 2τe 5.4τ. If the mean frame takes sec to transmit when many stations have frames to Here we see where the maximum cable distance between any two stations enters interval which is why the Ethernet standard specifies maximum cable length. It is instructive to formulate Eq. in terms of the frame length the net work bandwidth the cable length and the speed of signal propagation for the optimal case of contention slots per frame. With FB Eq. When the second term in the denominator is large network efficiency will be low. More specifically increasing network bandwidth or distance reduces efficiency for given frame size. Unfortunately much research on net work hardware is aimed precisely at increasing this product. People want high bandwidth over long distances yet classic Ether net implemented in this manner is not the best system for these applications. We tions for 2τ 51.2 μsec and data rate of 10 Mbps using Eq. With 64 byte slot time it is not surprising that 64byte frames are not efficient. On the other hand with 1024byte frames and an asymptotic value of 64byte slots per contention interval the contention period is 174 bytes long and the efficiency is 85. This result is much better than the 37 efficiency of slotted ALOHA. It is probably worth mentioning that there has been large amount of theoreti cal performance analysis of Ethernet . Most of the results should be taken with grain of salt for two reasons. First virtually all of the theoretical work assumes Poisson traffic. As researchers have begun looking at real data it now appears that network traffic is rarely Pois son. Instead it is selfsimilar or bursty over range of time scales . What this means is that averaging over long periods of time does not smooth out the traffic. As well as using question able models many of the analyses focus on the interesting performance cases of abnormally high load. Boggs et al. showed by experimentation that Eth ernet works well in reality even at moderately high load. Ethernet soon began to evolve away from the single long cable architecture of tions drove it toward different kind of wiring pattern in which each station has dedicated cable running to central hub. hub simply connects all the attached wires electrically as if they were soldered together. This configuration is shown The wires were telephone company twisted pairs since most office buildings were already wired this way and normally plenty of spares were available. This meters . Adding or removing station is simpler in this configuration and cable breaks can be de tected easily. With the advantages of being able to use existing wiring and ease of maintenance twistedpair hubs quickly became the dominant form of Ethernet. However hubs do not increase capacity because they are logically equivalent to the single long cable of classic Ethernet. As more and more stations are added each station gets decreasing share of the fixed capacity. Eventually the LAN Mbps 1 Gbps or even higher speeds. But with the growth of multimedia and powerful servers even 1Gbps Ethernet can become saturated. Fortunately there is an another way to deal with increased load switched Ethernet. The heart of this system is switch containing highspeed backplane looks just like hub. They are both boxes typically with 4 to 48 ports each with standard RJ45 connector for twistedpair cable. Each cable connects the advantages as hub too. It is easy to add or remove new station by plugging or unplugging wire and it is easy to find most faults since flaky cable or port will usually affect just one station. There is still shared component that can failthe switch itselfbut if all stations lose connectivity the IT folks know what to do to fix the problem replace the whole switch. Inside the switch however something very different is happening. Switches only output frames to the ports for which those frames are destined. When switch port receives an Ethernet frame from station the switch checks the Ether net addresses to see which port the frame is destined for. This step requires the switch to be able to work out which ports correspond to which addresses pro cess that we will describe in Sec. 4.8 when we get to the general case of switches connected to other switches. For now just assume that the switch knows the frames destination port. The switch then forwards the frame over its highspeed backplane to the destination port. The backplane typically runs at many Gbps using proprietary protocol that does not need to be standardized because it is entirely hidden inside the switch. The destination port then transmits the frame on the wire so that it reaches the intended station. None of the other ports even knows the frame exists. What happens if more than one of the stations or ports wants to send frame at the same time Again switches differ from hubs. In hub all stations are in the same collision domain. They must use the CSMACD algorithm to schedule their transmissions. In switch each port is its own independent collision domain. In the common case that the cable is full duplex both the station and the port can send frame on the cable at the same time without worrying about other ports and stations. Collisions are now impossible and CSMACD is not needed. However if the cable is half duplex the station and the port must contend for switch improves performance over hub in two ways. First since there are no collisions the capacity is used more efficiently. Second and more impor tantly with switch multiple frames can be sent simultaneously . These frames will reach the switch ports and travel over the switchs back plane to be output on the proper ports. However since two frames might be sent to the same output port at the same time the switch must have buffering so that it can temporarily queue an input frame until it can be transmitted to the output port. Overall these improvements give large performance win that is not possible with hub. The total system throughput can often be increased by an order of magnitude depending on the number of ports and traffic patterns. The change in the ports on which frames are output also has security benefits. Most LAN interfaces have promiscuous mode in which all frames are given to each computer not just those addressed to it. With hub every computer that is attached can see the traffic sent between all of the other computers. Spies and busybodies love this feature. With switch traffic is forwarded only to the ports where it is destined. This restriction provides better isolation so that traffic will not easily escape and fall into the wrong hands. However it is better to encrypt traffic if security is really needed. Because the switch just expects standard Ethernet frames on each input port the upperright corner is connected not to single station but to 12port hub instead. As frames arrive at the hub they contend for the ether in the usual way including collisions and binary backoff. Successful frames make it through the hub to the switch and are treated there like any other incoming frames. The switch does not know they had to fight their way in. Once in the switch they are sent to the correct output line over the highspeed backplane. It is also possible that the correct destination was one on the lines attached to the hub in which case the frame has already been delivered so the switch just drops it. Hubs are simpler and cheaper than switches but due to falling switch prices they have become an endangered species. Modern networks largely use switched Ethernet. Neverthe less legacy hubs still exist. At the same time that switches were becoming popular the speed of 10Mbps Ethernet was coming under pressure. At first 10 Mbps seemed like heaven just as cable modems seemed like heaven to the users of telephone modems. But the novelty wore off quickly. As kind of corollary to Parkinsons Law it seemed that data expanded to fill the bandwidth available for their transmission. Many installations needed more bandwidth and thus had numerous 10Mbps LANs connected by maze of repeaters hubs and switches although to the net work managers it sometimes felt that they were being held together by bubble gum and chicken wire. But even with Ethernet switches the maximum bandwidth of single computer was limited by the cable that connected it to the switch port. with instructions to come up with faster LAN. One proposal was to keep 802.3 exactly as it was but just make it go faster. Another proposal was to redo it total ly and give it lots of new features such as realtime traffic and digitized voice but just keep the old name . After some wrangling the com mittee decided to keep 802.3 the way it was and just make it go faster. This stra tegy would get the job done before the technology changed and avoid unforeseen compatible with existing Ethernet LANs. The people behind the losing proposal did what any selfrespecting computerindustry people would have done under these circumstances they stomped off and formed their own committee and stand ardized their LAN anyway . It flopped miserably. The work was done quickly and the result standard but an addendum to the existing 802.3 standard . This strategy is used lot. Since practically everyone calls it fast Ethernet rather than 802.3u we will do that too. The basic idea behind fast Ethernet was simple keep all the old frame for nsec. Technically it would have been possible to copy 10Mbps classic Ethernet and still detect collisions on time by just reducing the maximum cable length by factor of 10. However the advantages of twistedpair wiring were so overwhelm ing that fast Ethernet is based entirely on this design. Thus all fast Ethernet sys tems use hubs and switches multidrop cables with vampire taps or BNC connec tors are not permitted. Nevertheless some choices still had to be made the most important being which wire types to support. One contender was Category 3 twisted pair. The argument for it was that practically every office in the Western world had at least four Category 3 twisted pairs running from it to telephone wiring Sometimes two such cables existed. Category 3 twisted pair would make it possible to wire up desktop computers using fast Ethernet without having to rewire the building an enormous advantage for many organizations. The main disadvantage of Category 3 twisted pair is its inability to carry 10Mbps hubs. In contrast Category 5 twisted pair wiring can handle 100 easily and fiber can go much farther. The compromise chosen was to allow all give it the additional carrying capacity needed. The Category 3 UTP scheme called 100BaseT4 used signaling speed of Manchester encoding discussed in Sec. 2.5 requires two clock periods for each of the 10 million bits sent each second. However to achieve the necessary bit rate 100BaseT4 requires four twisted pairs. Of the four pairs one is always to the hub one is always from the hub and the other two are switchable to the current transmission direction. To get 100 Mbps out of the three twisted pairs in the transmission direction fairly involved scheme is used on each twisted pair. It involves sending ternary digits with three different voltage levels. This scheme is not likely to win any prizes for elegance and we will skip the details. How ever since standard telephone wiring for decades has had four twisted pairs per cable most offices are able to use the existing wiring plant. Of course it means giving up your office telephone but that is surely small price to pay for faster 100BaseT4 fell by the wayside as many office buildings were rewired with Category 5 UTP for 100BaseTX Ethernet which came to dominate the market. This design is simpler because the wires can handle clock rates of 125 MHz. Only two twisted pairs per station are used one to the hub and one from it. Nei ther straight binary coding nor Manchester coding is used. Instead the 4B5B encoding we described in Sec 2.5 is used. 4 data bits are encoded as 5 sig nal bits and sent at 125 MHz to provide 100 Mbps. This scheme is simple but has sufficient transitions for synchronization and uses the bandwidth of the wire rela Mbps on one twisted pair and receive at 100 Mbps on another twisted pair at the The last option 100BaseFX uses two strands of multimode fiber one for each direction so it too can run full duplex with 100 Mbps in each direction. In this setup the distance between station and the switch can be up to 2 km. Fast Ethernet allows interconnection by either hubs or switches. To ensure that the CSMACD algorithm continues to work the relationship between the minimum frame size and maximum cable length must be maintained as the net work speed goes up from 10 Mbps to 100 Mbps. So either the minimum frame size of 64 bytes must go up or the maximum cable length of 2500 must come down proportionally. The easy choice was for the maximum distance between any two stations to come down by factor of 10 since hub with 100m cables falls within this new maximum already. However 2km 100BaseFX cables are too long to permit 100Mbps hub with the normal Ethernet collision algorithm. These cables must instead be connected to switch and operate in fullduplex mode so that there are no collisions. Users quickly started to deploy fast Ethernet but they were not about to throw away 10Mbps Ethernet cards on older computers. As consequence virtually all fast Ethernet switches can handle mix of 10Mbps and 100Mbps stations. To make upgrading easy the standard itself provides mechanism called auto negotiation that lets two stations automatically negotiate the optimum speed and duplexity . It works well most of the time but is tiates but the other end does not and is set to fullduplex mode on the periphery. In this configuration all lines are buffered so each computer and switch is free to send frames whenever it wants to. The sender does not have to sense the channel to see if anybody else is using it because contention is impos sible. On the line between computer and switch the computer is the only pos sible sender to the switch and the transmission will succeed even if the switch is currently sending frame to the computer . Since THE MEDIUM ACCESS CONTROL SUBLAYER no contention is possible the CSMACD protocol is not used so the maximum it takes for noise burst to propagate back to the sender in the worst case. Switches are free to mix and match speeds. Autonegotiation is supported just as in fast Ethernet only now the choice is among 10 100 and 1000 Mbps. The other mode of operation halfduplex is used when the computers are connected to hub rather than switch. hub does not buffer incoming frames. Instead it electrically connects all the lines internally simulating the multidrop cable used in classic Ethernet. In this mode collisions are possible so the stan Because 64byte frame can now be transmitted 100 times faster than in classic Ethernet the maximum cable length must be 100 times less or 25 meters to maintain the essential property that the sender is still transmitting when the noise burst gets back to it even in the worst case. With 2500meterlong cable the sender of 64byte frame at 1 Gbps would be long finished before the frame got even tenth of the way to the other end let alone to the end and back. This length restriction was painful enough that two features were added to the standard to increase the maximum cable length to 200 meters which is probably enough for most offices. The first feature called carrier extension essentially tells the hardware to add its own padding after the normal frame to extend the frame to 512 bytes. Since this padding is added by the sending hardware and removed by the receiving hardware the software is unaware of it meaning that no changes are needed to existing software. The downside is that using 512 bytes worth of bandwidth to transmit 46 bytes of user data has line efficiency of only 9. The second feature called frame bursting allows sender to transmit con catenated sequence of multiple frames in single transmission. If the total burst is less than 512 bytes the hardware pads it again. If enough frames are waiting for transmission this scheme is very efficient and preferred over carrier extension. In all fairness it is hard to imagine an organization buying modern computers with gigabit Ethernet cards and then connecting them with an oldfashioned hub to simulate classic Ethernet with all its collisions. Gigabit Ethernet interfaces and switches used to be expensive but their prices fell rapidly as sales volumes picked up. Still backward compatibility is sacred in the computer industry so the com mittee was required to put it in. Today most computers ship with an Ethernet interface that is capable of 10 100 and 1000Mbps operation and compatible with all of them. Signaling at or near 1 Gbps requires encoding and sending bit every nanosecond. This trick was initially accomplished with short shielded copper and 1.3 microns . Signaling at the short wavelength can be achieved with cheaper LEDs. It is used with multimode fiber and is useful for connections within building as it can run up to 500 for 50micron fiber. Signaling at the long wavelength requires more expensive lasers. On the other hand when combined with single mode fiber the cable length can be up to 5 km. This limit allows long distance connections between buildings such as for campus backbone as dedicated pointtopoint link. Later variations of the standard allowed even longer links over singlemode fiber. described in Sec. 2.5 was borrowed from another networking technology called Fibre Channel. That scheme encodes 8 bits of data into 10bit codewords that are sent over the wire or fiber hence the name 8B10B. The codewords were chosen so that they could be balanced with suf ficient transitions for clock recovery. Sending the coded bits with NRZ requires signaling bandwidth of 25 more than that required for the uncoded bits big improvement over the 100 expansion of Manchester coding. However all of these options required new copper or fiber cables to support UTP that had been installed along with fast Ethernet. Within year 1000BaseT THE MEDIUM ACCESS CONTROL SUBLAYER came along to fill this gap and it has been the most popular form of gigabit Ether net ever since. People apparently dislike rewiring their buildings. More complicated signaling is needed to make Ethernet run at 1000 Mbps over Category 5 wires. To start all four twisted pairs in the cable are used and each pair is used in both directions at the same time by using digital signal pro cessing to separate signals. Over each wire five voltage levels that carry 2 bits are used for signaling at 125 Msymbolssec. The mapping to produce the symbols from the bits is not straightforward. It involves scrambling for transitions fol lowed by an error correcting code in which four values are embedded into five speed of 1 Gbps is quite fast. For example if receiver is busy with some other task for even 1 msec and does not empty the input buffer on some line up to 1953 frames may have accumulated in that gap. Also when computer on gigabit Ethernet is shipping data down the line to computer on classic Ether net buffer overruns are very likely. As consequence of these two observations gigabit Ethernet supports flow control. The mechanism consists of one end send ing special control frame to the other end telling it to pause for some period of time. These PAUSE control frames are normal Ethernet frames containing type of 0x8808. Pauses are given in units of the minimum frame time. For gigabit Ethernet the time unit is 512 nsec allowing for pauses as long as 33.6 msec. There is one more extension that was introduced along with gigabit Ethernet. Jumbo frames allow for frames to be longer than 1500 bytes usually up to 9 KB. This extension is proprietary. It is not recognized by the standard because if it is support it anyway. The rationale is that 1500 bytes is short unit at gigabit speeds. By manipulating larger blocks of information the frame rate can be decreased along with the processing associated with it such as interrupting the processor to say that frame has arrived or splitting up and recombining mes sages that were too long to fit in one Ethernet frame. As soon as gigabit Ethernet was standardized the 802 committee got bored and wanted to get back to work. IEEE told them to start on 10gigabit Ethernet. This work followed much the same pattern as the previous Ethernet standards with standards for fiber and shielded copper cable appearing first in 2002 and 2004 followed by the standard for copper twisted pair in 2006. Where could it be needed The answer is inside data centers and exchanges to connect highend routers switches and servers as well as in longdistance high bandwidth trunks between offices that are enabling entire metropolitan area net works based on Ethernet and fiber. The long distance connections use optical fiber while the short connections may use copper or fiber. CSMACD is no longer part of the design and the standards concentrate on the details of physical layers that can run at very high speed. Compatibility still matters though so 10gigabit Ethernet interfaces autonegotiate and fall back to the highest speed supported by both ends of the line. fiber with the 0.85μ wavelength is used for medium distances and single mode fiber at 1.3μ and 1.5μ is used for long distances. 10GBaseER can run for distances of 40 km making it suitable for wide area duced by scrambling the data bits then encoding them with 64B66B code. This Twinaxial copper of twinaxial copper wiring. Each pair uses 8B10B coding and runs at 3.125 early to market but it remains to be seen whether it will be beat out in the long run by 10gigabit Ethernet over more garden variety twisted pair wiring. wiring for shorter runs it can use lower categories to allow some reuse of installed cabling. Not surprisingly the physical layer is quite involved to reach 10 Gbps over twisted pair. We will only sketch some of the highlevel details. Each of the four twisted pairs is used to send 2500 Mbps in both directions. This speed is reached using signaling rate of 800 Msymbolssec with symbols that use 16 voltage levels. The symbols are produced by scrambling the data protecting it with LDPC code and further coding for error correction. 10gigabit Ethernet is still shaking out in the market but the 802.3 committee has already moved on. At the end of 2007 IEEE created group to standardize Ethernet operating at 40 Gbps and 100 Gbps. This upgrade will let Ethernet com pete in very highperformance settings including longdistance connections in backbone networks and short connections over the equipment backplanes. The standard is not yet complete but proprietary products are already available. Ethernet has been around for over 30 years and has no serious competitors in sight so it is likely to be around for many years to come. Few CPU architectures operating systems or programming languages have been king of the mountain for three decades going on strong. Clearly Ethernet did something right. What Probably the main reason for its longevity is that Ethernet is simple and flexi ble. In practice simple translates into reliable cheap and easy to maintain. Once the hub and switch architecture was adopted failures became extremely rare. People hesitate to replace something that works perfectly all the time especially when they know that an awful lot of things in the computer industry work very poorly so that many socalled upgrades are worse than what they replaced. Simple also translates into cheap. Twistedpair wiring is relatively inexpen sive as are the hardware components. They may start out expensive when there is transition for example new gigabit Ethernet NICs or switches but they are merely additions to well established network and the prices fall quickly as the sales volume picks up. Ethernet is easy to maintain. There is no software to install and not much in the way of configuration tables to manage . Also adding new hosts is as simple as just plugging them in. Another point is that Ethernet interworks easily with TCPIP which has become dominant. IP is connectionless protocol so it fits perfectly with Ether net which is also connectionless. IP fits much less well with connectionoriented alternatives such as ATM. This mismatch definitely hurt ATMs chances. Lastly and perhaps most importantly Ethernet has been able to evolve in cer tain crucial ways. Speeds have gone up by several orders of magnitude and hubs and switches have been introduced but these changes have not required changing the software and have often allowed the existing cabling to be reused for time. When network salesman shows up at large installation and says have this fantastic new network for you. All you have to do is throw out all your hardware and rewrite all your software he has problem. Many alternative technologies that you have probably not even heard of were faster than Ethernet when they were introduced. As well as ATM this list includes FDDI and Fibre Channel two ring based optical LANs. Both were incompatible with Ethernet. Neither one made it. They were too complicated which led to complex chips and high prices. The les son that should have been learned here was KISS . Even tually Ethernet caught up with them in terms of speed often by borrowing some of their technology for example the 4B5B coding from FDDI and the 8B10B coding from Fibre Channel. Then they had no advantages left and quietly died off or fell into specialized roles. It is called Fibre Channel and not Fiber Channel because the document editor was British. It looks like Ethernet will continue to expand in its applications for some time. 10gigabit Ethernet has freed it from the distance constraints of CSMACD. Much effort is being put into carriergrade Ethernet to let network providers offer Ethernetbased services to their customers for metropolitan and wide area networks . This application carries Ethernet frames long distances over fiber and calls for better management features to help operators offer reliable highquality services. Very high speed networks are also finding uses in backplanes connecting components in large routers or servers. Both of these uses are in addition to that of sending frames between computers in offices. Wireless LANs are increasingly popular and homes offices cafes libraries airports zoos and other public places are being outfitted with them to connect computers PDAs and smart phones to the Internet. Wireless LANs can also be used to let two or more nearby computers communicate without using the Inter The main wireless LAN standard is 802.11. We gave some background infor mation on it in Sec. 1.5.3. Now it is time to take closer look at the technology. transmission techniques the MAC sublayer protocol the frame structure and the services provided. For more information about 802.11 see Gast . To get the truth from the mouth of the horse consult the published standard IEEE 802.112007 itself. 802.11 networks can be used in two modes. The most popular mode is to con nect clients such as laptops and smart phones to another network such as com mode each client is associated with an AP that is in turn con nected to the other network. The client sends and receives its packets via the AP. Several access points may be connected together typically by wired network called distribution system to form an extended 802.11 network. In this case clients can send frames to other clients via their APs. collection of computers that are associated so that they can directly send frames to each other. There is no access point. Since Internet access is the killer application for wireless ad hoc networks are not very popular. Now we will look at the protocols. All the 802 protocols including 802.11 and Ethernet have certain commonality of structure. partial view of the APs. The physical layer corresponds fairly well to the OSI physical layer but the data link layer in all the 802 protocols is split into two or more sublayers. In 802.11 the MAC sublayer determines how the channel is allocated that is who gets to transmit next. Above it is the LLC that is carried within an 802.11 frame. Several transmission techniques have been added to the physical layer as 802.11 has evolved since it first appeared in 1997. Two of the initial techniques infrared in the manner of television remote controls and frequency hopping in the 2.4GHz band are now defunct. The third initial technique direct sequence spread spectrum at 1 or 2 Mbps in the 2.4GHz band was extended to run at rates up to 11 Mbps and quickly became hit. It is now known as 802.11b. To give wireless junkies muchwanted speed boost new transmission tech niques based on the OFDM scheme we described in Sec. 2.5.3 were introduced in 1999 and 2003. The first is called 802.11a and uses different frequency band 5 GHz. The second stuck with Most recently transmission techniques that simultaneously use multiple an in Oct. 2009. With four antennas and wider channels the 802.11 standard now defines rates up to startling 600 Mbps. We will now examine each of these transmission techniques briefly. We will only cover those that are in use however skipping the legacy 802.11 transmission methods. Technically these belong to the physical layer and should have been examined in Chap. 2 but since they are so closely tied to LANs in general and the Each of the transmission techniques makes it possible to send MAC frame over the air from one station to another. They differ however in the technology used and speeds achievable. detailed discussion of these technologies is far beyond the scope of this book but few words on each one will relate the techni ques to the material we covered in Sec. 2.5 and will provide interested readers with the key terms to search for elsewhere for more information. All of the 802.11 techniques use shortrange radios to transmit signals in ei ther the 2.4GHz or the 5GHz ISM frequency bands both described in Sec. 2.3.3. These bands have the advantage of being unlicensed and hence freely available to any transmitter willing to meet some restrictions such as radiated power of at most 1 . Unfortunate ly this fact is also known to the manufacturers of garage door openers cordless phones microwave ovens and countless other devices all of which compete with laptops for the same spectrum. The 2.4GHz band tends to be more crowded than the 5GHz band so 5 GHz can be better for some applications even though it has shorter range due to the higher frequency. All of the transmission methods also define multiple rates. The idea is that different rates can be used depending on the current conditions. If the wireless signal is weak low rate can be used. If the signal is clear the highest rate can be used. This adjustment is called rate adaptation. Since the rates vary by factor of 10 or more good rate adaptation is important for good performance. Of course since it is not needed for interoperability the standards do not say how rate adap tation should be done. The first transmission method we shall look at is 802.11b. It is spreadspec trum method that supports rates of 1 2 5.5 and 11 Mbps though in practice the operating rate is nearly always 11 Mbps. It is similar to the CDMA system we THE MEDIUM ACCESS CONTROL SUBLAYER examined in Sec. 2.5 except that there is only one spreading code that is shared by all users. Spreading is used to satisfy the FCC requirement that power be spread over the ISM band. The spreading sequence used by 201.11b is Barker sequence. It has the property that its autocorrelation is low except when the se quences are aligned. This property allows receiver to lock onto the start of transmission. To send at rate of 1 Mbps the Barker sequence is used with BPSK modulation to send 1 bit per 11 chips. The chips are transmitted at rate of bits per 11 chips. The higher rates are different. These rates use technique call ed CCK to construct codes instead of the Barker sequence. The 5.5Mbps rate sends 4 bits in every 8chip code and the 11Mbps rate sends 8 bits in every 8chip code. Next we come to 802.11a which supports rates up to 54 Mbps in the 5GHz ISM band. You might have expected that 802.11a to come before 802.11b but that was not the case. Although the 802.11a group was set up first the 802.11b 802.11a products partly because of the difficulty of operating in the higher 5GHz The 802.11a method is based on OFDM because OFDM uses the spectrum efficiently and resists wireless signal degradations such as multipath. Bits are sent over 52 subcarriers in paral lel 48 carrying data and 4 used for synchronization. Each symbol lasts 4μs and sends 1 2 4 or 6 bits. The bits are coded for error correction with binary con volutional code first so only 12 23 or 34 of the bits are not redundant. With different combinations 802.11a can run at eight different rates ranging from 6 to interference in the 5GHz band. However 802.11b has range that is about seven times greater than that of 802.11a which is more important in many situations. Even with the greater range the 802.11b people had no intention of letting this upstart win the speed championship. Fortunately in May 2002 the FCC dropped its longstanding rule requiring all wireless communications equipment operating in the ISM bands in the . to use spread spectrum so it got to work tion methods of 802.11a but operates in the narrow 2.4GHz ISM band along with 802.11b. It offers the same rates as 802.11a plus of course compa tibility with any 802.11b devices that happen to be nearby. All of these different choices can be confusing for customers so it is common for products to support 802.11abg in single NIC. Not content to stop there the IEEE committee began work on highthrough put physical layer called 802.11n. It was ratified in 2009. The goal for 802.11n was throughput of at least 100 Mbps after all the wireless overheads were re moved. This goal called for raw speed increase of at least factor of four. To make it happen the committee doubled the channels from 20 MHz to 40 MHz and reduced framing overheads by allowing group of frames to be sent together. More significantly however 802.11n uses up to four antennas to transmit up to four streams of information at the same time. The signals of the streams interfere at the receiver but they can be separated using MIMO communications techniques. The use of multiple antennas gives large speed boost or better range and reliability instead. MIMO like OFDM is one of those clever communications ideas that is changing wireless designs and which we are all likely to hear lot about in the future. For brief introduction to multi ple antennas in 802.11 see Halperin et al. Let us now return from the land of electrical engineering to the land of com puter science. The 802.11 MAC sublayer protocol is quite different from that of Ethernet due to two factors that are fundamental to wireless communication. First radios are nearly always half duplex meaning that they cannot transmit and listen for noise bursts at the same time on single frequency. The received signal can easily be million times weaker than the transmitted signal so it can not be heard at the same time. With Ethernet station just waits until the ether goes silent and then starts transmitting. If it does not receive noise burst back while transmitting the first 64 bytes the frame has almost assuredly been deliv ered correctly. With wireless this collision detection mechanism does not work. Instead 802.11 tries to avoid collisions with protocol called CSMACA . This protocol is conceptually similar to Ethernets CSMACD with channel sensing before sending and exponential back off after collisions. However station that has frame to send starts with ran dom backoff . It does not wait for collision. The number of slots to backoff is chosen in the range 0 to say 15 in the case of the OFDM physical layer. The sta tion waits until the channel is idle by sensing that there is no signal for short period of time and counts down idle slots pausing when frames are sent. It sends its frame when the counter reaches 0. If the frame gets through the destination immediately sends short acknowledge ment. Lack of an acknowledgement is inferred to indicate an error whether col lision or otherwise. In this case the sender doubles the backoff period and tries again continuing with exponential backoff as in Ethernet until the frame has been successfully transmitted or the maximum number of retransmissions has been frame. While is sending stations and become ready to send. They see that the channel is busy and wait for it to become idle. Shortly after receives an ac knowledgement the channel goes idle. However rather than sending frame right away and colliding and both perform backoff. picks short backoff THE MEDIUM ACCESS CONTROL SUBLAYER and thus sends first. pauses its countdown while it senses that is using the channel and resumes after has received an acknowledgement. soon com Compared to Ethernet there are two main differences. First starting backoffs early helps to avoid collisions. This avoidance is worthwhile because collisions are expensive as the entire frame is transmitted even if one occurs. Second ac knowledgements are used to infer collisions because collisions cannot be detected. This mode of operation is called DCF because each station acts independently without any kind of central control. The standard also includes an optional mode of operation called PCF in which the access point controls all activity in its cell just like cellular base station. However PCF is not used in practice because there is normally no way to prevent stations in another nearby network from transmitting competing traffic. The second problem is that the transmission ranges of different stations may be different. With wire the system is engineered so that all stations can hear each other. With the complexities of RF propagation this situation does not hold for wireless stations. Consequently situations such as the hidden terminal prob stations are within radio range of each other transmissions going on in one part of cell may not be received elsewhere in the same cell. In this example station is transmitting to station . If senses the channel it will not hear anything and will falsely conclude that it may now start transmitting to . This decision leads 26. Here wants to send to so it listens to the channel. When it hears the transmission will fail transmission it falsely concludes that it may not send to even though may in fact be transmitting to . This decision wastes transmission oppor To reduce ambiguities about which station is sending 802.11 defines channel sensing to consist of both physical sensing and virtual sensing. Physical sensing simply checks the medium to see if there is valid signal. With virtual sensing each station keeps logical record of when the channel is in use by tracking the how long the sequence of which this frame is part will take to complete. Stations that overhear this frame know that the channel will be busy for the period indi cated by the NAV regardless of whether they can sense physical signal. For ex ample the NAV of data frame includes the time needed to send an acknowledge ment. All stations that hear the data frame will defer during the acknowledgement period whether or not they can hear the acknowledgement. An optional RTSCTS mechanism uses the NAV to prevent terminals from this example wants to send to . is station within range of . is station within range of but not within range of . The protocol starts when decides it wants to send data to . begins by sending an RTS frame to to request permission to send it frame. If receives send. Upon receipt of the CTS sends its frame and starts an ACK timer. Upon correct receipt of the data frame responds with an ACK frame completing the exchange. If As ACK timer expires before the ACK gets back to it it is treated as collision and the whole protocol is run again after backoff. Now let us consider this exchange from the viewpoints of and . is with in range of so it may receive the RTS frame. If it does it realizes that someone is going to send data soon. From the information provided in the RTS request it good of all it desists from transmitting anything until the exchange is completed. It does so by updating its record of the NAV to indicate that the channel is busy as ternal reminders to keep quiet for certain period of time. However while RTSCTS sounds good in theory it is one of those designs that has proved to be of little value in practice. Several reasons why it is seldom used are known. It does not help for short frames or for the AP . For other situations it only slows down operation. RTSCTS in 802.11 is little different than in the MACA protocol we saw in Sec 4.2 because everyone hearing the RTS or CTS remains quiet for the duration to allow the ACK to get through without collision. Because of this it does not help with exposed terminals as MACA did only with hidden terminals. Most often there are few hidden terminals and CSMACA al ready helps them by slowing down stations that transmit unsuccessfully whatever the cause to make it more likely that transmissions will succeed. CSMACA with physical and virtual sensing is the core of the 802.11 proto col. However there are several other mechanisms that have been developed to go with it. Each of these mechanisms was driven by the needs of real operation so we will look at them briefly. The first need we will look at is reliability. In contrast to wired networks wireless networks are noisy and unreliable in no small part due to interference from other kinds of devices such as microwave ovens which also use the unli censed ISM bands. The use of acknowledgements and retransmissions is of little help if the probability of getting frame through is small in the first place. The main strategy that is used to increase successful transmissions is to lower the transmission rate. Slower rates use more robust modulations that are more likely to be received correctly for given signaltonoise ratio. If too many frames are lost station can lower the rate. If frames are delivered with little loss station can occasionally test higher rate to see if it should be used. Another strategy to improve the chance of the frame getting through undam aged is to send shorter frames. If the probability of any bit being in error is the probability of an nbit frame being received entirely correctly is . For ex ample for 104 the probability of receiving full Ethernet frame correctly is less than 30. Most frames will be lost. But if the frames are only third as long two thirds of them will be received correctly. Now most frames will get through and fewer retransmissions will be needed. Shorter frames can be implemented by reducing the maximum size of the message that is accepted from the network layer. Alternatively 802.11 allows frames to be split into smaller pieces called fragments each with its own check sum. The fragment size is not fixed by the standard but is parameter that can be adjusted by the AP. The fragments are individually numbered and acknowledged until it has received the acknowledgement for fragment . Once the channel has been acquired multiple fragments are sent as burst. They go one after the other with an acknowledgement in between until either the whole frame has been successfully sent or the transmission time reaches the maximum allowed. The NAV mechanism keeps other stations quiet only until the next acknowledgement but another mechanism is used to allow burst of fragments to be sent without other stations sending frame in the middle. The second need we will discuss is saving power. Battery life is always an neither information to send nor to receive. The basic mechanism for saving power builds on beacon frames. Beacons are periodic broadcasts by the AP . The frames advertise the presence of the AP to clients and carry system parameters such as the identi fier of the AP the time how long until the next beacon and security settings. Clients can set powermanagement bit in frames that they send to the AP to tell it that they are entering powersave mode. In this mode the client can doze and the AP will buffer traffic intended for it. To check for incoming traffic the client wakes up for every beacon and checks traffic map that is sent as part of the beacon. This map tells the client if there is buffered traffic. If so the client sends poll message to the AP which then sends the buffered traffic. The client can then go back to sleep until the next beacon is sent. Another powersaving mechanism called APSD was also added to 802.11 in 2005. With this new mechanism the AP buffers frames and sends them to client just after the client sends frames to the This mechanism works well for applications such as VoIP that have frequent traf fic in both directions. For example VoIP wireless phone might use it to send and receive frames every 20 msec much more frequently than the beacon interval of 100 msec while dozing in between. The third and last need we will examine is quality of service. When the VoIP traffic in the preceding example competes with peertopeer traffic the VoIP traf fic will suffer. It will be delayed due to contention with the highbandwidth peertopeer traffic even though the VoIP bandwidth is low. These delays are likely to degrade the voice calls. To prevent this degradation we would like to let the VoIP traffic go ahead of the peertopeer traffic as it is of higher priority. IEEE 802.11 has clever mechanism to provide this kind of quality of service that was introduced as set of extensions under the name 802.11e in 2005. It works by extending CSMACA with carefully defined intervals between frames. After frame has been sent certain amount of idle time is required before any station may send frame to check that the channel is no longer in use. The trick is to define different time intervals for different kinds of frames. frames is called the DIFS . Any station may attempt to acquire the channel to send new frame after the medium has been idle for DIFS. The usual contention rules apply and binary exponential backoff may be needed if collision occurs. The shortest interval is SIFS . It is used to allow the parties in single dialog the chance to go first. Examples include letting the receiver send an ACK other control frame sequences like RTS and CTS or letting sender transmit burst of fragments. Sending the next fragment after waiting only SIFS is what prevents another station from jump ing in with frame in the middle of the exchange. The two AIFS intervals show examples of two different priority levels. The short interval AIFS1 is smaller than DIFS but longer than SIFS. It can be used by the AP to move voice or other highpriority traffic to the head of the line. The AP will wait for shorter interval before it sends the voice traffic and thus send it before regular traffic. The long interval AIFS4 is larger than DIFS. It is used for background traffic that can be deferred until after regular traffic. The AP will wait for longer interval before it sends this traffic giving regular traffic the opportunity to transmit first. The complete quality of service mechanism defines four different priority levels that have dif ferent backoff parameters as well as different idle parameters. The last time interval EIFS is used only The idea is that since the receiver may have no idea of what is going on it should wait while to avoid interfering with an ongoing dialog between two stations. further part of the quality of service extensions is the notion of TXOP or transmission opportunity. The original CSMACA mechanism let stations send one frame at time. This design was fine until the range of rates increased. With 802.11ag one station might be sending at 6 Mbps and another station be sending at 54 Mbps. They each get to send one frame but the 6Mbps station takes nine times as long as the 54Mbps station to send its frame. This disparity has the unfortunate side effect of slowing down fast sender who is competing with slow sender to roughly the rate of the slow sender. For example again ignoring fixed overheads when sending alone the 6Mbps and 54Mbps senders will get their own rates but when sending together they will both get 5.4 the rate anomaly . With transmission opportunities each station gets an equal amount of airtime not an equal number of frames. Stations that send at higher rate for their airtime will get higher throughput. In our example when sending together the 6Mbps and 54Mbps senders will now get 3 Mbps and 27 Mbps respectively. The 802.11 standard defines three different classes of frames in the air data cal layer but these mostly deal with the modulation techniques used so we will not discuss them here. We will look at the format of the data frame as an example. It is shown in sions of 802.11 to operate at the same time in the same cell. Then come the Type and Subtype fields . For regu lar data frame they are set to 10 and 0000 in binary. The To DS and From DS bits are set to indicate whether the frame is going to or coming from the network connected to the APs which is called the distribution THE MEDIUM ACCESS CONTROL SUBLAYER system. The More fragments bit means that more fragments will follow. The Retry bit marks retransmission of frame sent earlier. The Power management bit indicates that the sender is going into powersave mode. The More data bit in dicates that the sender has additional frames for the receiver. The Protected Frame bit indicates that the frame body has been encrypted for security. We will that the higher layer expects the sequence of frames to arrive strictly in order. The second field of the data frame the Duration field tells how long the frame and its acknowledgement will occupy the channel measured in microsec onds. It is present in all types of frames including control frames and is what stations use to manage the NAV mechanism. Next come addresses. Data frames sent to or from an AP have three ad dresses all in standard IEEE 802 format. The first address is the receiver and the second address is the transmitter. They are obviously needed but what is the third address for Remember that the AP is simply relay point for frames as they travel between client and another point on the network perhaps distant client or portal to the Internet. The third address gives this distant endpoint. The Sequence field numbers frames so that duplicates can be detected. Of the 16 bits available 4 identify the fragment and 12 carry number that is advanced bytes. The first bytes of this payload are in format known as LLC . This layer is the glue that identifies the higherlayer protocol to which the payloads should be passed. Last comes the Frame check sequence which is the same 32bit CRC we saw in Sec. 3.2.2 and elsewhere. Management frames have the same format as data frames plus format for the data portion that varies with the subtype . Control frames are short. Like all frames they have the Frame control Duration and Frame check sequence fields. However they may have only one address and no data portion. Most of the key information is conveyed with the Subtype field . The 802.11 standard defines the services that the clients the access points and the network connecting them must be conformant wireless LAN. These ser vices cluster into several groups. The association service is used by mobile stations to connect themselves to APs. Typically it is used just after station moves within radio range of the AP. Upon arrival the station learns the identity and capabilities of the AP either from beacon frames or by directly asking the AP. The capabilities include the data rates supported security arrangements powersaving capabilities quality of service support and more. The station sends request to associate with the AP. The AP may accept or reject the request. Reassociation lets station change its preferred AP. This facility is useful for mobile stations moving from one AP to another AP in the same extended data will be lost as consequence of the handover. Either the station or the AP may also disassociate breaking their relationship. station should use this service before shutting down or leaving the network. The AP may use it before going down for maintenance. Stations must also authenticate before they can send frames via the AP but authentication is handled in different ways depending on the choice of security scheme. If the 802.11 network is open anyone is allowed to use it. Otherwise credentials are needed to authenticate. The recommended scheme called WPA2 implements security as defined in the 802.11i stan dard. With WPA2 the AP can talk to an authentication server that has username and password database to deter mine if the station is allowed to access the network. Alternatively preshared frames are exchanged between the station and the AP with challenge and re sponse that lets the station prove it has the right credentials. This exchange hap pens after association. The scheme that was used before WPA is called WEP . For this scheme authentication with preshared key happens before association. However its use is discouraged because of design flaws that make WEP easy to compromise. The first practical demonstration that WEP was bro ken came when Adam Stubblefield was summer intern at ATT . He was able to code up and test an attack in one week much of which was spent getting permission from management to buy the WiFi cards needed for experiments. Software to crack WEP passwords is now freely available. Once frames reach the AP the distribution service determines how to route them. If the destination is local to the AP the frames can be sent out directly over the air. Otherwise they will have to be forwarded over the wired network. The THE MEDIUM ACCESS CONTROL SUBLAYER integration service handles any translation that is needed for frame to be sent outside the 802.11 LAN or to arrive from outside the 802.11 LAN. The common case here is connecting the wireless LAN to the Internet. Data transmission is what it is all about so 802.11 naturally provides data delivery service. This service lets stations transmit and receive data using the net and transmission over Ethernet is not guaranteed to be 100 reliable trans mission over 802.11 is not guaranteed to be reliable either. Higher layers must deal with detecting and correcting errors. Wireless is broadcast signal. For information sent over wireless LAN to be kept confidential it must be encrypted. This goal is accomplished with pri vacy service that manages the details of encryption and decryption. The encryp tion algorithm for WPA2 is based on AES cryption are determined during the authentication procedure. To handle traffic with different priorities there is QOS traffic scheduling service. It uses the protocols we described to give voice and video traffic pre ferential treatment compared to besteffort and background traffic. companion service also provides higherlayer timer synchronization. This lets stations coordi nate their actions which may be useful for media processing. trum. The transmit power control service gives stations the information they need to meet regulatory limits on transmit power that vary from region to region. The dynamic frequency selection service give stations the information they need to avoid transmitting on frequencies in the 5GHz band that are being used for radar in the proximity. With these services 802.11 provides rich set of functionality for connecting nearby mobile clients to the Internet. It has been huge success and the standard has repeatedly been amended to add more functionality. For perspective on where the standard has been and where it is heading see Hiertz et al. We have been indoors too long. Let us go outdoors where there is quite bit of interesting networking over the socalled last mile. With the deregulation of the telephone systems in many countries competitors to the entrenched telephone companies are now often allowed to offer local voice and highspeed Internet ser vice. There is certainly plenty of demand. The problem is that running fiber or coax to millions of homes and businesses is prohibitively expensive. What is The answer is broadband wireless. Erecting big antenna on hill just out side of town is much easier and cheaper than digging many trenches and stringing cables. Thus companies have begun to experiment with providing multimegabit wireless communication services for voice Internet movies on demand etc. To stimulate the market IEEE formed group to standardize broadband wireless metropolitan area network. The next number available in the 802 num bering space was 802.16 so the standard got this number. Informally the technol ogy is called WiMAX . We will use the terms 802.16 and WiMAX interchangeably. provided wireless local loop between fixed points with line of sight to each other. This design soon changed to make WiMAX more competitive alternative to cable and DSL for Internet access. By January 2003 802.16 had been revised to support nonlineofsight links by using OFDM technology at frequencies be tween 2 GHz and 10 GHz. This change made deployment much easier though by promising high data rates and mobility. In response 802.16 was enhanced again to allow mobility at vehicular speeds by December 2005. Mobile broad band Internet access is the target of the current standard IEEE 802.162009. Like the other 802 standards 802.16 was heavily influenced by the OSI model including the layers terminology service primitives and more. Un fortunately also like OSI it is fairly complicated. In fact the WiMAX Forum was created to define interoperable subsets of the standard for commercial offer highlights of the common forms of 802.16 air interface but this treatment is far from complete and leaves out many details. For additional information about WiMAX and broadband wireless in general see Andrews et al. At this point you may be thinking why devise new standard Why not just use 802.11 or 3G In fact WiMAX combines aspects of both 802.11 and 3G making it more like 4G technology. Like 802.11 WiMAX is all about wirelessly connecting devices to the Inter net at megabitsec speeds instead of using cable or DSL. The devices may be mobile or at least portable. WiMAX did not start by adding lowrate data on the side of voicelike cellular networks 802.16 was designed to carry IP packets over the air and to connect to an IPbased wired network with minimum of fuss. The packets may carry peertopeer traffic VoIP calls or streaming media to support range of applications. Also like 802.11 it is based on OFDM technology to ensure good performance in spite of wireless signal degradations such as mul tipath fading and on MIMO technology to achieve high levels of throughput. However WiMAX is more like 3G in several key re spects. The key technical problem is to achieve high capacity by the efficient use of spectrum so that large number of subscribers in coverage area can all get THE MEDIUM ACCESS CONTROL SUBLAYER high throughput. The typical distances are at least 10 times larger than for an 802.11 network. Consequently WiMAX base stations are more powerful than base station uses more power and better antennas and it performs more proc essing to handle errors. To maximize throughput transmissions are carefully scheduled by the base station for each particular subscriber spectrum use is not left to chance with CSMACA which may waste capacity with collisions. Licensed spectrum is the expected case for WiMAX typically around 2.5 GHz in the . The whole system is substantially more optimized than 802.11. This complexity is worth it considering the large amount of money involved for licensed spectrum. Unlike 802.11 the result is managed and reliable service with good support for quality of service. With all of these features 802.16 most closely resembles the 4G cellular net works that are now being standardized under the name LTE . While 3G cellular networks are based on CDMA and support voice and data 4G cellular networks will be based on OFDM with MIMO and they will tar get data with voice as just one application. It looks like WiMAX and 4G are on collision course in terms of technology and applications. Perhaps this conver gence is unsurprising given that the Internet is the killer application and OFDM and MIMO are the bestknown technologies for efficiently using the spectrum. to the providers backbone network which is in turn connected to the Internet. The base stations communicate with stations over the wireless air interface. Two broadband Internet access for homes. Mobile stations can receive service while they are moving for example car equipped with WiMAX. The 802.16 protocol stack that is used across the air interface is shown in with more sublayers. The bottom layer deals with transmission and here we have shown only the popular offerings of 802.16 fixed and mobile WiMAX. There is different physical layer for each offering. Both layers operate in licensed spec trum below 11 GHz and use OFDM but in different ways. Above the physical layer the data link layer consists of three sublayers. The bottom one deals with privacy and security which is far more crucial for public outdoor networks than for private indoor networks. It manages encryption de cryption and key management. Next comes the MAC common sublayer part. This part is where the main protocols such as channel management are located. The model here is that the base station completely controls the system. It can schedule the downlink channels very efficiently and plays major role in managing Security sublayer the uplink channels as well. An unusual feature of this MAC sublayer is that unlike those of the other 802 protocols it is completely connection oriented in order to provide quality of service guarantees for tele phony and multimedia communication. The servicespecific convergence sublayer takes the place of the logical link sublayer in the other 802 protocols. Its function is to provide an interface to the network layer. Different convergence layers are defined to integrate seamlessly with different upper layers. The important choice is IP though the standard defines mappings for protocols such as Ethernet and ATM too. Since IP is con nectionless and the 802.16 MAC sublayer is connectionoriented this layer must Most WiMAX deployments use licensed spectrum around either 3.5 GHz or GHz. Channels of different sizes are supported for example 3.5 MHz for fixed WiMAX and from 1.25 MHz to 20 MHz for mobile WiMAX. Transmissions are sent over these channels with OFDM the technique we de scribed in Sec. 2.5.3. Compared to 802.11 the 802.16 OFDM design is optimized to make the most out of licensed spectrum and wide area transmissions. The channel is divided into more subcarriers with longer symbol duration to tolerate larger wireless signal degradations WiMAX parameters are around 20 times larg er than comparable 802.11 parameters. For example in mobile WiMAX there are 512 subcarriers for 5MHz channel and the time to send symbol on each subcarrier is roughly 100 μsec. Symbols on each subcarrier are sent with QPSK QAM16 or QAM64 mod ulation schemes we described in Sec. 2.5.3. When the mobile or subscriber sta tion is near the base station and the received signal has high signaltonoise ratio QAM64 can be used to send 6 bits per symbol. To reach distant stations with low SNR QPSK can be used to deliver 2 bits per symbol. The data is first coded for error correction with the convolutional coding that we described in Sec. 3.2.1. This coding is common on noisy channels to tolerate some bit errors without needing to send retransmissions. In fact the modulation and coding methods should sound familiar by now as they are used for many net works we have studied including 802.11 cable and DSL. The net result is that base station can support up to 12.6 Mbps of downlink traffic and 6.2 Mbps of uplink traffic per 5MHz channel and pair of antennas. One thing the designers of 802.16 did not like was certain aspect of the way GSM and DAMPS work. Both of those systems use equal frequency bands for upstream and downstream traffic. That is they implicitly assume there is as much upstream traffic as downstream traffic. For voice traffic is symmetric for the most part but for Internet access there is often more downstream traffic than upstream traffic. The ratio is often 21 31 or more1. So the designers chose flexible scheme for dividing the channel between stations called OFDMA . With OFDMA different sets of subcarriers can be assigned to different stations so that more than one station can send or receive at once. If this were 802.11 all subcarriers would be used by one station to send at any given moment. The added flexibility in how bandwidth is assigned can increase performance because given subcarrier might be faded at one receiver due to multipath effects but clear at another. Subcarriers can be assigned to the stations that can use them best. As well as having asymmetric traffic stations usually alternate between send ing and receiving. This method is called TDD . The alternative method in which station sends and receives at the same time is called FDD . WiMAX allows both methods but TDD is preferred because it is easier to imple ment and more flexible. It starts with preamble to synchronize all stations followed by downlink trans missions from the base station. First the base station sends maps that tell all sta tions how the downlink and uplink subcarriers are assigned over the frame. The base station controls the maps so it can allocate different amounts of bandwidth to stations from frame to frame depending on the needs of each station. Next the base station sends bursts of traffic to different subscriber and mobile stations on the subcarriers at the times given in the map. The downlink transmis sions end with guard time for stations to switch from receiving to transmitting. station in the uplink positions that were reserved for them in the map. One of these uplink bursts is reserved for ranging which is the process by which new stations adjust their timing and request initial bandwidth to connect to the base station. Since no connection is set up at this stage new stations just transmit and hope there is no collision. Since we will not study cryptography until Chap. 8 it is difficult to explain now how the security sublayer works. Suffice it to say that encryption is used to keep THE MEDIUM ACCESS CONTROL SUBLAYER are not. This property means that snooper can see who is talking to whom but cannot tell what they are saying to each other. If you already know something about cryptography what follows is one paragraph explanation of the security sublayer. If you know nothing about crypto graphy you are not likely to find the next paragraph terribly enlightening . When subscriber connects to base station they perform mutual authentica tion with RSA publickey cryptography using .509 certificates. The payloads themselves are encrypted using symmetrickey system either AES or DES with cipher block chaining. Integrity checking uses SHA1. Now that was Let us now look at the MAC common sublayer part. The MAC sublayer is connectionoriented and pointtomultipoint which means that one base station communicates with multiple subscriber stations. Much of this design is borrowed from cable modems in which one cable headend controls the transmissions of multiple cable modems at the customer premises. The downlink direction is fairly straightforward. The base station controls the physicallayer bursts that are used to send information to the different subscriber stations. The MAC sublayer simply packs its frames into this structure. To reduce overhead there are several different options. For example MAC frames may be sent individually or packed backtoback into group. The uplink channel is more complicated since there are competing subscribers 4. Besteffort service. All service in 802.16 is connectionoriented. Each connection gets one of these service classes determined when the connection is set up. This design is different from that of 802.11 or Ethernet which are connectionless in the MAC sublayer. Constant bit rate service is intended for transmitting uncompressed voice. This service needs to send predetermined amount of data at predetermined time this type. Once the bandwidth has been allocated the bursts are available auto matically without the need to ask for each one. Realtime variable bit rate service is for compressed multimedia and other soft realtime applications in which the amount of bandwidth needed at each in fixed interval to ask how much bandwidth is needed this time. Nonrealtime variable bit rate service is for heavy transmissions that are not real time such as large file transfers. For this service the base station polls the subscriber often but not at rigidly prescribed time intervals. Connections with this service can also use besteffort service described next to request bandwidth. Besteffort service is for everything else. No polling is done and the sub scriber must contend for bandwidth with other besteffort subscribers. Requests for bandwidth are sent in bursts marked in the uplink map as available for con tention. If request is successful its success will be noted in the next downlink map. If it is not successful the unsuccessful subscriber have to try again later. To minimize collisions the Ethernet binary exponential backoff algorithm is used. The payload is not needed in control frames for example those requesting chan nel slots. The checksum is also optional due to the error correction in the physical layer and the fact that no attempt is ever made to retransmit real time frames. If no retransmissions will be attempted why even bother with checksum But if there is checksum it is the standard IEEE 802 CRC and ac knowledgements and retransmissions are used for reliability. The Type field identifies the frame type mostly telling whether packing and fragmentation are present. The CI field indi the encryption keys is being used . The Length field gives the complete The 802.16 protocol has many kinds of frames. An example of different THE MEDIUM ACCESS CONTROL SUBLAYER except that the second and third bytes form 16bit number telling how much bandwidth is needed to carry the specified number of bytes. Bandwidth request frames do not carry payload or fullframe CRC. great deal more could be said about 802.16 but this is not the place to say it. For more information please consult the IEEE 802.162009 standard itself. In 1994 the . Ericsson company became interested in connecting its mobile phones to other devices without cables. Together with four other companies it formed SIG in 1998 to develop wireless standard for intercon necting computing and communication devices and accessories using shortrange lowpower inexpensive wireless radios. The project was named Bluetooth after Harald Blaatand II Viking king who unified Denmark and Norway also without cables. Bluetooth 1.0 was released in July 1999 and since then the SIG has never looked back. All manner of consumer electronic devices now use Bluetooth from mobile phones and laptops to headsets printers keyboards mice gameboxes watches music players navigation units and more. The Bluetooth protocols let these devices find and connect to each other an act called pairing and securely The protocols have evolved over the past decade too. After the initial proto cols stabilized higher data rates were added to Bluetooth 2.0 in 2004. With the 3.0 release in 2009 Bluetooth can be used for device pairing in combination with 802.11 for highthroughput data transfer. The 4.0 release in December 2009 spec ified lowpower operation. That will be handy for people who do not want to change the batteries regularly in all of those devices around the house. We will cover the main aspects of Bluetooth below. it contains and what it is intended to do. The basic unit of Bluetooth system is piconet which consists of master node and up to seven active slave nodes with in distance of 10 meters. Multiple piconets can exist in the same room and can even be connected via bridge node that takes part in multiple piconets parked nodes in the net. These are devices that the master has switched to low power state to reduce the drain on their batteries. In parked state device cannot do anything except respond to an activation or beacon signal from the master. Two intermediate power states hold and sniff also exist but these will not con The reason for the masterslave design is that the designers intended to facili tate the implementation of complete Bluetooth chips for under 5. The conse quence of this decision is that the slaves are fairly dumb basically just doing whatever the master tells them to do. At its heart piconet is centralized TDM system with the master controlling the clock and determining which device gets to communicate in which time slot. All communication is between the master and slave direct slaveslave communication is not possible. Most network protocols just provide channels between communicating enti example 802.11 does not specify whether users should use their notebook com puters for reading email surfing the Web or something else. In contrast the Bluetooth SIG specifies particular applications to be supported and provides dif ferent protocol stacks for each one. At the time of writing there are 25 applica tions which are called profiles. Unfortunately this approach leads to very large amount of complexity. We will omit the complexity here but will briefly look at the profiles to see more clearly what the Bluetooth SIG is trying to accomplish. Six of the profiles are for different uses of audio and video. For example the intercom profile allows two telephones to connect as walkietalkies. The headset and handsfree profiles both provide voice communication between headset and its base station as might be used for handsfree telephony while driving car. Other profiles are for streaming stereoquality audio and video say from port able music player to headphones or from digital camera to TV. The human interface device profile is for connecting keyboards and mice to computers. Other profiles let mobile phone or other computer receive images from camera or send images to printer. Perhaps of more interest is profile to use mobile phone as remote control for TV. Still other profiles enable networking. The personal area network profile lets Bluetooth devices form an ad hoc network or remotely access another network such as an 802.11 LAN via an access point. The dialup networking profile was actually the original motivation for the whole project. It allows notebook com puter to connect to mobile phone containing builtin modem without using Profiles for higherlayer information exchange have also been defined. The synchronization profile is intended for loading data into mobile phone when it leaves home and collecting data from it when it returns. We will skip the rest of the profiles except to mention that some profiles serve as building blocks on which the above profiles are built. The generic access profile on which all of the other profiles are built provides way to establish and maintain secure links between the master and the slaves. The other generic profiles define the basics of object exchange and audio and video tran sport. Utility profiles are used widely for functions such as emulating serial line which is especially useful for many legacy applications. Was it really necessary to spell out all these applications in detail and provide different protocol stacks for each one Probably not but there were number of different working groups that devised different parts of the standard and each one just focused on its specific problem and generated its own profile. Think of this Melvin Conway observed that if you assign people to write compiler you will get an npass compiler or more generally the software structure mirrors the struc ture of the group that produced it. It would probably have been possible to get away with two protocol stacks instead of 25 one for file transfer and one for streaming realtime communication. The Bluetooth standard has many protocols grouped loosely into the layers follow the OSI model the TCPIP model the 802 model or any other model. The bottom layer is the physical radio layer which corresponds fairly well to the physical layer in the OSI and 802 models. It deals with radio transmission and modulation. Many of the concerns here have to do with the goal of making the system inexpensive so that it can become massmarket item. The link control layer is somewhat analogous to the MAC sub layer but also includes elements of the physical layer. It deals with how the mas ter controls time slots and how these slots are grouped into frames. Next come two protocols that use the link control protocol. The link manager handles the establishment of logical channels between devices including power management pairing and encryption and quality of service. It lies below the host controller interface line. This interface is convenience for implementation typi cally the protocols below the line will be implemented on Bluetooth chip and the protocols above the line will be implemented on the Bluetooth device that The link protocol above the line is L2CAP . It frames variablelength messages and provides reliability if needed. Many protocols use L2CAP such as the two utility protocols that are shown. The service discovery protocol is used to locate services within the net work. The RFcomm protocol emulates the standard serial port found on PCs for connecting the keyboard mouse and modem among other devices. The top layer is where the applications are located. The profiles are repres ented by vertical boxes because they each define slice of the protocol stack for particular purpose. Specific profiles such as the headset profile usually contain only those protocols needed by that application and no others. For example pro files may include L2CAP if they have packets to send but skip L2CAP if they have only steady flow of audio samples. ous link protocols since these roughly correspond to the physical and MAC sublayers in the other procotol stacks we have studied. The radio layer moves the bits from master to slave or vice versa. It is lowpower system with range of 10 meters operating in the same 2.4GHz ISM band as 802.11. The band is divided into 79 channels of 1 MHz each. To coexist with other networks using the ISM band frequency hopping spread spectrum is used. There can be up to 1600 hopssec over slots with dwell time of 625 μsec. All the nodes in piconet hop frequencies simultaneously following the slot tim ing and pseudorandom hop sequence dictated by the master. fered enough to ruin each others transmissions. Some companies responded by banning Bluetooth altogether but eventually technical solution was devised. The solution is for Bluetooth to adapt its hop sequence to exclude channels on which there are other RF signals. This process reduces the harmful interference. It is called adaptive frequency hopping. Three forms of modulation are used to send bits on channel. The basic scheme is to use frequency shift keying to send 1bit symbol every microsecond giving gross data rate of 1 Mbps. Enhanced rates were introduced with the 2.0 per symbol for gross data rates of 2 or 3 Mbps. The enhanced rates are only used in the data portion of frames. The link control layer is the closest thing Bluetooth has to MAC sublayer. It turns the raw bit stream into frames and defines some key for mats. In the simplest form the master in each piconet defines series of 625 μsec time slots with the masters transmissions starting in the even slots and the slaves transmissions starting in the odd ones. This scheme is traditional time di vision multiplexing with the master getting half the slots and the slaves sharing the other half. Frames can be 1 3 or 5 slots long. Each frame has an overhead of hop to allow the inexpensive radio circuits to become stable. The payload of the frame can be encrypted for confidentiality with key that is chosen when the master and slave connect. Hops only happen between frames not during frame. The result is that 5slot frame is much more efficient than 1slot frame because the overhead is constant but more data is sent. The link manager protocol sets up logical channels called links to carry frames between the master and slave device that have discovered each other. pairing procedure is followed to make sure that the two devices are allowed to communicate before the link is used. The old pairing method is that both devices ber. The matching PIN is how each device would know that it was connecting to the right remote device. However unimaginative users and devices default to PINs such as 0000 and 1234 meant that this method provided very little se curity in practice. The new secure simple pairing method enables users to confirm that both de vices are displaying the same passkey or to observe the passkey on one device and enter it into the second device. This method is more secure because users do not have to choose or set PIN. They merely confirm longer devicegenerated passkey. Of course it cannot be used on some devices with limited inputoutput such as handsfree headset. Once pairing is complete the link manager protocol sets up the links. Two main kinds of links exist to carry user data. The first is the SCO link. It is used for realtime data such as telephone con nections. This type of link is allocated fixed slot in each direction. slave may have up to three SCO links with its master. Each SCO link can transmit one 64000bps PCM audio channel. Due to the timecritical nature of SCO links frames sent over them are never retransmitted. Instead forward error correction can be used to increase reliability. The other kind is the ACL link. This type of link is used for packetswitched data that is available at irregular intervals. ACL traffic is delivered on besteffort basis. No guarantees are given. Frames can be lost and may have to be retransmitted. slave may have only one ACL link to its master. The data sent over ACL links come from the L2CAP layer. This layer has four major functions. First it accepts packets of up to 64 KB from the upper lay ers and breaks them into frames for transmission. At the far end the frames are reassembled into packets. Second it handles the multiplexing and demultiplexing of multiple packet sources. When packet has been reassembled the L2CAP layer determines which upperlayer protocol to hand it to for example RFcomm or service discovery. Third L2CAP handles error control and retransmission. It enforces quality of service requirements between multiple links. Bluetooth defines several frame formats the most important of which is fies the master so that slaves within radio range of two masters can tell which traf fields. If the frame is sent at the basic rate the data field comes next. It has up to 2744 bits for fiveslot transmission. For single time slot the format is the same except that the data field is 240 bits. If the frame is sent at the enhanced rate the data portion may have up to two 5 675 microsec slots bit. These data are preceded by guard field and synchronization pattern that is ried at the basic rate and only the data portion is carried at the faster rate. Enhancedrate frames end with short trailer. which of the eight active devices the frame is intended for. The Type field identi fies the frame type the type of error correction used in the data field and how many slots long the frame is. The Flow bit is asserted by slave when its buffer is full and cannot receive any more data. This bit enables primitive form of flow control. The Acknowledgement bit is used to piggyback an ACK onto frame. The Sequence bit is used to number the frames to detect re transmissions. The protocol is stopandwait so 1 bit is enough. Then comes the amines all three copies of each bit. If all three are the same the bit is accepted. If not the majority opinion wins. Thus 54 bits of transmission capacity are used to ment using cheap lowpowered devices with little computing capacity great deal of redundancy is needed. Various formats are used for the data field for ACL and SCO frames. The bits. Three variants are defined permitting 80 160 or 240 bits of actual payload We can work out the capacity with this frame as follows. Since the slave may use only the odd slots it gets 800 slotssec just as the master does. With an 80bit payload the channel capacity from the slave is 64000 bps as is the channel ca pacity from the master. This capacity is exactly enough for single fullduplex PCM voice channel . That is despite raw bandwidth of 1 Mbps single fullduplex uncompressed voice channel can completely saturate the piconet. The efficiency of 13 is the result repetition coding. This shortcoming highlights the value of the enhanced rates and frames of more than single slot. There is much more to be said about Bluetooth but no more space to say it here. For the curious the Bluetooth 4.0 specification contains all the details. We have looked at MAC designs from LANs up to MANs and down to PANs. As last example we will study category of lowend wireless devices that peo ple may not recognize as forming computer network the RFID tags and readers that we described in Sec. 1.5.4. RFID technology takes many forms used in smartcards implants for pets passports library books and more. The form that we will look at was developed in the quest for an EPC that started with the AutoID Center at the Massachusetts Institute of Technology in 1999. An EPC is re placement for barcode that can carry larger amount of information and is elec tronically readable over distances up to 10 even when it is not visible. It is dif ferent technology than for example the RFID used in passportswhich must be placed quite close to reader to perform transaction. The ability to communi cate over distance makes EPCs more relevant to our studies. EPCglobal was formed in 2003 to commercialize the RFID technology devel oped by the AutoID Center. The effort got boost in 2005 when Walmart re quired its top 100 suppliers to label all shipments with RFID tags. Widespread deployment has been hampered by the difficulty of competing with cheap printed barcodes but new uses such as in drivers licenses are now growing. We will de scribe the second generation of this technology which is informally called EPC Gen 2 . two key components tags and readers. RFID tags are small inexpensive devices that have unique 96bit EPC identifier and small amount of memory that can be read and written by the RFID reader. The memory might be used to record the Often the tags look like stickers that can be placed on for example pairs of jeans on the shelves in store. Most of the sticker is taken up by an antenna that is printed onto it. tiny dot in the middle is the RFID integrated circuit. Alterna tively the RFID tags can be integrated into an object such as drivers license. In both cases the tags have no battery and they must gather power from the radio transmissions of nearby RFID reader to run. This kind of tag is called Class 1 tag to distinguish it from more capable tags that have batteries. The readers are the intelligence in the system analogous to base stations and access points in cellular and WiFi networks. Readers are much more powerful than tags. They have their own power sources often have multiple antennas and are in charge of when tags send and receive messages. As there will commonly be multiple tags within the reading range the readers must solve the multiple ac cess problem. There may be multiple readers that can contend with each other in the same area too. The main job of the reader is to inventory the tags in the neighborhood that is to discover the identifiers of the nearby tags. The inventory is accomplished with the physical layer protocol and the tagidentification protocol that are out The physical layer defines how bits are sent between the RFID reader and tags. Much of it uses methods for sending wireless signals that we have seen pre viously. In the . transmissions are sent in the unlicensed 902928 MHz ISM band. This band falls in the UHF range so the tags are referred to as UHF RFID tags. The reader performs frequency hopping at least every 400 msec to spread its signal across the channel to limit interference and satisfy regulatory requirements. The reader and tags use forms of ASK modulation that we described in Sec. 2.5.2 to encode bits. They take turns to send bits so the link is half duplex. There are two main differences from other physical layers that we have stud ied. The first is that the reader is always transmitting signal regardless of whether it is the reader or tag that is communicating. Naturally the reader trans mits signal to send bits to tags. For the tags to send bits to the reader the reader transmits fixed carrier signal that carries no bits. The tags harvest this signal to get the power they need to run otherwise tag would not be able to transmit in the first place. To send data tag changes whether it is reflecting the signal from the reader like radar signal bouncing off target or absorbing it. This method is called backscatter. It differs from all the other wireless situa tions we have seen so far in which the sender and receiver never both transmit at the same time. Backscatter is lowenergy way for the tag to create weak sig nal of its own that shows up at the reader. For the reader to decode the incoming signal it must filter out the outgoing signal that it is transmitting. Because the tag signal is weak tags can only send bits to the reader at low rate and tags cannot receive or even sense transmissions from other tags. The second difference is that very simple forms of modulation are used so that they can be implemented on tag that runs on very little power and costs only few cents to make. To send data to the tags the reader uses two amplitude levels. Bits are determined to be either 0 or 1 depending on how long the reader waits before lowpower period. The tag measures the time between lowpower periods and compares this time to reference measured during preamble. As Tag responses consist of the tag alternating its backscatter state at fixed inter vals to create series of pulses in the signal. Anywhere from one to eight pulse periods can be used to encode each 0 or 1 depending on the need for reliability. 1s have fewer transitions than 0s as is shown with an example of twopulse To inventory the nearby tags the reader needs to receive message from each tag that gives the identifier for the tag. This situation is multiple access problem for which the number of tags is unknown in the general case. The reader might THE MEDIUM ACCESS CONTROL SUBLAYER broadcast query to ask all tags to send their identifiers. However tags that re plied right away would then collide in much the same way as stations on classic We have seen many ways of tackling the multiple access problem in this hear each others transmissions is slotted ALOHA one of the earliest protocols we studied. This protocol is adapted for use in Gen 2 RFID. first slot the reader sends Query message to start the process. Each QRepeat message advances to the next slot. The reader also tells the tags the range of slots over which to randomize transmissions. Using range is necessary because the reader synchronizes tags when it starts the process unlike stations on an Ethernet tags do not wake up with message at time of their choosing. QRepeat 2. However tags do not send their identifiers when they first reply. Instead tag sends short 16bit random number in an RN16 message. If there is no collision the reader receives this message and sends an ACK message of its own. At this stage the tag has acquired the slot and sends its EPC identifier. The reason for this exchange is that EPC identifiers are long so collisions on these messages would be expensive. Instead short exchange is used to test whether the tag can safely use the slot to send its identifier. Once its identifier has been successfully transmitted the tag temporarily stops responding to new Query messages so that all the remaining tags can be identified. key problem is for the reader to adjust the number of slots to avoid collis ions but without using so many slots that performance suffers. This adjustment is analogous to binary exponential backoff in Ethernet. If the reader sees too many slots with no responses or too many slots with collisions it can send QAdjust message to decrease or increase the range of slots over which the tags are re The RFID reader can perform other operations on the tags. For example it can select subset of tags before running an inventory allowing it to collect re sponses from say tagged jeans but not tagged shirts. The reader can also write data to tags as they are identified. This feature could be used to record the point of sale or other relevant information. readertotag message. The message is compact because the downlink rates are The next flags DR and TR determine the physical layer parameters for reader transmissions and tag responses. For example the response rate may be set to between 5 kbps and 640 kbps. We will skip over the details of these flags. Then come three fields Sel Session and Target that select the tags to re spond. As well as the readers being able to select subset of identifiers the tags keep track of up to four concurrent sessions and whether they have been identified in those sessions. In this way multiple readers can operate in overlapping cover age areas by using different sessions. Next is the most important parameter for this command . This field defines CRC to protect the message fields. At 5 bits it is shorter than most CRCs we have seen but the Query message is much shorter than most packets too. Tagtoreader messages are simpler. Since the reader is in control it knows what message to expect in response to each of its transmissions. The tag re sponses simply carry data such as the EPC identifier. Originally the tags were just for identification purposes. However they have grown over time to resemble very small computers. Some research tags have sen sors and are able to run small programs to gather and process data . One vision for this technology is the Internet of things that connects ob jects in the physical world to the Internet . Many organizations have multiple LANs and wish to connect them. Would it not be convenient if we could just join the LANs together to make larger LAN In fact we can do this when the connections are made with devices called bridges. The Ethernet switches we described in Sec. 4.3.4 are modern name for bridges they provide functionality that goes beyond classic Ethernet and Ethernet hubs to make it easy to join multiple LANs into larger and faster network. We shall use the terms bridge and switch interchangeably. Bridges operate in the data link layer so they examine the data link layer ad dresses to forward frames. Since they are not supposed to examine the payload field of the frames they forward they can handle IP packets as well as other kinds of packets such as AppleTalk packets. In contrast routers examine the addresses in packets and route based on them so they only work with the protocols that they were designed to handle. physical LANs into single logical LAN. We will also look at how to do the re verse and treat one physical LAN as multiple logical LANs called VLANs . Both technologies provide useful flexibility for managing networks. For comprehensive treatment of bridges switches and related topics see Seifert and Edwards and Perlman . Before getting into the technology of bridges let us take look at some com mon situations in which bridges are used. We will mention three reasons why single organization may end up with multiple LANs. First many university and corporate departments have their own LANs to connect their own personal computers servers and devices such as printers. Since the goals of the various departments differ different departments may set up different LANs without regard to what other departments are doing. Sooner or later though there is need for interaction so bridges are needed. In this ex ample multiple LANs come into existence due to the autonomy of their owners. Second the organization may be geographically spread over several buildings separated by considerable distances. It may be cheaper to have separate LANs in each building and connect them with bridges and few longdistance fiber optic links than to run all the cables to single central switch. Even if laying the cables is easy to do there are limits on their lengths . The network would not work for longer cables due to the excessive signal attenuation or roundtrip delay. The only solution is to partition the LAN and install bridges to join the pieces to increase the total physical distance that can Third it may be necessary to split what is logically single LAN into sepa versities for example thousands of workstations are available for student and faculty computing. Companies may also have thousands of employees. The scale of this system precludes putting all the workstations on single LANthere are more computers than ports on any Ethernet hub and more stations than allowed on single classic Ethernet. Even if it were possible to wire all the workstations together putting more stations on an Ethernet hub or classic Ethernet would not add capacity. All of the stations share the same fixed amount of bandwidth. The more stations there are the less average bandwidth per station. However two separate LANs have twice the capacity of single LAN. Bridges let the LANs be joined together while keeping this capacity. The key is not to send traffic onto ports where it is not needed so that each LAN can run at full speed. This behavior also increases reliability since on single LAN defec tive node that keeps outputting continuous stream of garbage can clog up the en tire LAN. By deciding what to forward and what not to forward bridges act like fire doors in building preventing single node that has gone berserk from bring ing down the entire system. To make these benefits easily available ideally bridges should be completely transparent. It should be possible to go out and buy bridges plug the LAN cables into the bridges and have everything work perfectly instantly. There should be no hardware changes required no software changes required no setting of address switches no downloading of routing tables or parameters nothing at all. Just plug in the cables and walk away. Furthermore the operation of the existing LANs should not be affected by the bridges at all. As far as the stations are concerned there should be no observable difference whether or not they are part of bridged LAN. It should be as easy to move stations around the bridged LAN as it is to move them around single LAN. Surprisingly enough it is actually possible to create bridges that are transpar ent. Two algorithms are used backward learning algorithm to stop traffic being sent where it is not needed and spanning tree algorithm to break loops that may be formed when switches are cabled together willynilly. Let us now take look at these algorithms in turn to learn how this magic is accomplished. THE MEDIUM ACCESS CONTROL SUBLAYER cases. On the lefthand side two multidrop LANs such as classic Ethernets are joined by special stationthe bridgethat sits on both LANs. On the righthand side LANs with pointtopoint cables including one hub are joined together. The bridges are the devices to which the stations and hub are attached. If the LAN technology is Ethernet the bridges are better known as Ethernet switches. hub connecting seven pointtopoint stations. Bridges were developed when classic Ethernets were in use so they are often topologies that are encountered today are comprised of pointtopoint cables and switches. The bridges work the same way in both settings. All of the stations at tached to the same port on bridge belong to the same collision domain and this is different than the collision domain for other ports. If there is more than one sta tion as in classic Ethernet hub or halfduplex link the CSMACD protocol is used to send frames. There is difference however in how the bridged LANs are built. To bridge multidrop LANs bridge is added as new station on each of the multidrop nected to bridge or preferably replaced with bridge to increase performance. Different kinds of cables can also be attached to one bridge. For example the fiber optic link while the cable connecting the bridges to stations might be shorthaul twistedpair line. This arrangement is useful for bridging LANs in dif ferent buildings. Now let us consider what happens inside the bridges. Each bridge operates in promiscuous mode that is it accepts every frame transmitted by the stations attached to each of its ports. The bridge must decide whether to forward or dis card each frame and if the former on which port to output the frame. This decis ion is made by using the destination address. As an example consider the topo the frame on port 1. This frame can be immediately discarded without further ado suppose that sends frame to . Bridge B1 will receive the frame on port 1 and output it on port 4. Bridge B2 will then receive the frame on its port 4 and output it on its port 1. simple way to implement this scheme is to have big table inside the bridge. The table can list each possible destination and which output port it be to port 4 since all B1 has to know is which port to put frames on to reach . That in fact more forwarding will happen later when the frame hits B2 is not of When the bridges are first plugged in all the hash tables are empty. None of the bridges know where any of the destinations are so they use flooding algo rithm every incoming frame for an unknown destination is output on all the ports to which the bridge is connected except the one it arrived on. As time goes on the bridges learn where destinations are. Once destination is known frames destined for it are put only on the proper port they are not flooded. The algorithm used by the bridges is backward learning. As mentioned above the bridges operate in promiscuous mode so they see every frame sent on any of their ports. By looking at the source addresses they can tell which ma sees frame on port 3 coming from it knows that must be reachable via port 3 so it makes an entry in its hash table. Any subsequent frame addressed to coming in to B1 on any other port will be forwarded to port 3. The topology can change as machines and bridges are powered up and down and moved around. To handle dynamic topologies whenever hash table entry is made the arrival time of the frame is noted in the entry. Whenever frame time. Thus the time associated with every entry tells the last time frame from that machine was seen. Periodically process in the bridge scans the hash table and purges all entries more than few minutes old. In this way if computer is unplugged from its LAN moved around the building and plugged in again somewhere else within few minutes it will be back in normal operation without any manual intervention. This algorithm also means that if machine is quiet for few minutes any traffic sent to it will have to be flooded until it next sends frame itself. The routing procedure for an incoming frame depends on the port it arrives on and the address to which it is destined . THE MEDIUM ACCESS CONTROL SUBLAYER 1. If the port for the destination address is the same as the source port discard the frame. 2. If the port for the destination address and the source port are dif ferent forward the frame on to the destination port. 3. If the destination port is unknown use flooding and send the frame on all ports except the source port. You might wonder whether the first case can occur with pointtopoint links. The answer is that it can occur if hubs are used to connect group of computers to to hub 1 which is in turn connected to bridge B2. If sends frame to the hub will relay it to B2 as well as to . That is what hubs dothey wire all ports together so that frame input on one port is simply output on all other ports. The frame will arrive at B2 on port 4 which is already the right output port to reach the destination. Bridge B2 need only discard the frame. As each frame arrives this algorithm must be applied so it is usually imple table entry all in few microseconds. Because bridges only look at the MAC ad dresses to decide how to forward frames it is possible to start forwarding as soon . This design reduces the latency of passing through the bridge as well as the number of frames that the bridge must be able to buffer. It is referred to as cutthrough switching or wormhole routing and is usually handled in hardware. We can look at the operation of bridge in terms of protocol stacks to under stand what it means to be link layer device. Consider frame sent from station The frame will pass through one bridge. The protocol stack view of processing is The packet comes from higher layer and descends into the Ethernet MAC This unit is passed to the physical layer goes out over the cable and is picked up In the bridge the frame is passed up from the physical layer to the Ethernet MAC layer. This layer has extended processing compared to the Ethernet MAC layer at station. It passes the frame to relay still within the MAC layer. The handle the frame. In this case it passes the frame to the Ethernet MAC layer of the port used to reach station and the frame continues on its way. layer. VLANs will provide an example shortly. In no case should the bridge look inside the frame and learn that it is carrying an IP packet that is irrelevant to the bridge processing and would violate protocol layering. Also note that bridge for our simple example. To increase reliability redundant links can be used between bridges. In the This design ensures that if one link is cut the network will not be partitioned into two sets of computers that cannot talk to each other. ing at how frame sent by to previously unobserved destination is handled in tions which is to flood the frame. Call the frame from that reaches bridge B1 frame 0. The bridge sends copies of this frame out all of its other ports. We THE MEDIUM ACCESS CONTROL SUBLAYER will only consider the bridge ports that connect B1 to B2 . Since there are two links from B1 to B2 two copies Shortly thereafter bridge B2 receives these frames. However it does not know that they are copies of the same frame rather than two different frames sent one after the other. So bridge B2 takes 1 and sends copies of it out all the other ports and it also takes 2 and sends copies of it out all the other ports. This produces frames 3 and 4 that are sent along the two links back to B1. Bridge B1 then sees two new frames with unknown destinations and copies them again. This cycle goes on forever. The solution to this difficulty is for the bridges to communicate with each other and overlay the actual topology with spanning tree that reaches every bridge. In effect some potential connections between bridges are ignored in the interest of constructing fictitious loopfree topology that is subset of the actual have stations connected to them. Each station connects to only one bridge. There are some redundant connections between the bridges so that frames will be for warded in loops if all of the links are used. This topology can be thought of as graph in which the bridges are the nodes and the pointtopoint links are the edges. The graph can be reduced to spanning tree which has no cycles by defi ning tree there is exactly one path from every station to every other station. Once the bridges have agreed on the spanning tree all forwarding between stations fol lows the spanning tree. Since there is unique path from each source to each links that are not part of the spanning tree. To build the spanning tree the bridges run distributed algorithm. Each bridge periodically broadcasts configuration message out all of its ports to its neighbors and processes the messages it receives from other bridges as described next. These messages are not forwarded since their purpose is to build the tree which can then be used for forwarding. The bridges must first choose one bridge to be the root of the spanning tree. To make this choice they each include an identifier based on their MAC address in the configuration message as well as the identifier of the bridge they believe to be the root. MAC addresses are installed by the manufacturer and guaranteed to be unique worldwide which makes these identifiers convenient and unique. The bridges choose the bridge with the lowest identifier to be the root. After enough messages have been exchanged to spread the news all bridges will agree on Next tree of shortest paths from the root to every bridge is constructed. In hop that is shortest path. Bridge B4 can be reached in two hops via either B2 or B3. To break this tie the path via the bridge with the lowest identifier is chosen so B4 is reached via B2. Bridge B5 can be reached in two hops via B3. To find these shortest paths bridges include the distance from the root in their configuration messages. Each bridge remembers the shortest path it finds to the root. The bridges then turn off ports that are not part of the shortest path. Although the tree spans all the bridges not all the links are necessarily present in the tree. This happens because turning off the ports prunes some links from the network to prevent loops. Even after the spanning tree has been established the algorithm continues to run during normal operation to auto The algorithm for constructing the spanning tree was invented by Radia Perl man. Her job was to solve the problem of joining LANs without loops. She was given week to do it but she came up with the idea for the spanning tree algo rithm in day. Fortunately this left her enough time to write it as poem amplify the incoming signals and are de signed for multiple input lines but the differences are slight. Like repeaters hubs are physical layer devices that do not examine the link layer addresses or use them Now let us move up to the data link layer where we find bridges and switch es. We just studied bridges at some length. bridge connects two or more input lines of certain type. Unlike in hub each port is isolated to be its own collision domain if the port has fullduplex pointtopoint line the CSMACD algorithm is not needed. When frame arrives the bridge extracts the destination For Ethernet this address is the 48bit destination address shown in can forward multiple frames at the same time. Bridges offer much better performance than hubs and the isolation between bridge ports also means that the input lines may run at different speeds possibly even with different network types. common example is bridge with ports that connect to 10 100 and 1000Mbps Ethernet. Buffering within the bridge is needed to accept frame on one port and transmit the frame out on different port. If frames come in faster than they can be retransmitted the bridge may run out of buffer space and have to start discarding frames. For example if gigabit Ethernet is pouring bits into 10Mbps Ethernet at top speed the bridge will have to buffer them hoping not to run out of memory. This problem still exists even if all the ports run at the same speed because more than one port may be sending frames to given destination port. Bridges were originally intended to be able to join different kinds of LANs for example an Ethernet and Token Ring LAN. However this never worked well because of differences between the LANs. Different frame formats require copying and reformatting which takes CPU time requires new checksum calcu lation and introduces the possibility of undetected errors due to bad bits in the bridges memory. Different maximum frame lengths are also serious problem with no good solution. Basically frames that are too large to be forwarded must be discarded. So much for transparency. Two other areas where LANs can differ are security and quality of service. Some LANs have linklayer encryption for example 802.11 and some do not for example Ethernet. Some LANs have quality of service features such as priorities for example 802.11 and some do not for example Ethernet. Consequently when THE MEDIUM ACCESS CONTROL SUBLAYER frame must travel between these LANs the security or quality of service expect ed by the sender may not be able to be provided. For all of these reasons modern bridges usually work for one network type and routers which we will come to soon are used instead to join networks of different types. Switches are modern bridges by another name. The differences are more to Bridges were developed when classic Ethernet was in use so they tend to join rel atively few LANs and thus have relatively few ports. The term switch is more popular nowadays. Also modern installations all use pointtopoint links such as twistedpair cables so individual computers plug directly into switch and thus eral term. With bridge the functionality is clear. On the other hand switch may refer to an Ethernet switch or completely different kind of device that makes forwarding decisions such as telephone switch. So far we have seen repeaters and hubs which are actually quite similar as well as bridges and switches which are even more similar to each other. Now we move up to routers which are different from all of the above. When packet but not 48bit IEEE 802 address. The routing software does not see the frame addresses and does not even know whether the packet came in on LAN or pointtopoint line. We will study routers and routing in Chap. 5. Up another layer we find transport gateways. These connect two computers that use different connectionoriented transport protocols. For example suppose computer using the connectionoriented TCPIP protocol needs to talk to com puter using different connectionoriented transport protocol called SCTP. The transport gateway can copy the packets from one connection to the other refor matting them as need be. and can translate messages from one format to another. An email gateway could translate Internet messages into SMS messages for mobile phones for example. Like switch gateway is somewhat of general term. It refers to for warding process that runs at high layer. In the early days of local area networking thick yellow cables snaked through Every computer they passed was plugged in. No thought was given to which computer belonged on which LAN. All the people in adjacent offices were put on the same LAN whether they be longed together or not. Geography trumped corporate organization charts. With the advent of twisted pair and hubs in the 1990s all that changed. Buildings were rewired to rip out all the yellow garden hoses and install twisted pairs from every office to central wiring closets at the the Vice President in Charge of Wiring was visionary Category 5 twisted pairs were installed if he was bean counter the existing telephone wir ing was used . Today the cables have changed and hubs have become switches but the wir logically rather than physically. For example if company wants LANs it could buy switches. By carefully choosing which connectors to plug into which switches the occupants of LAN can be chosen in way that makes organiza tional sense without too much regard to geography. Does it matter who is on which LAN After all in nearly all organizations all the LANs are interconnected. In short yes it often matters. Network adminis trators like to group users on LANs to reflect the organizational structure rather curity. One LAN might host Web servers and other computers intended for public use. Another LAN might host computers containing the records of the Human Re sources department that are not to be passed outside of the department. In such situation putting all the computers on single LAN and not letting any of the ser vers be accessed from off the LAN makes sense. Management tends to frown when hearing that such an arrangement is impossible. THE MEDIUM ACCESS CONTROL SUBLAYER may be desirable to separate them. For example if the folks in research are run ning all kinds of nifty experiments that sometimes get out of hand and saturate their LAN the folks in management may not be enthusiastic about donating some of the capacity they were using for videoconferencing to help out. Then again this might impress on management the need to install faster network. of the destination is unknown and upperlayer protocols use broadcasting as well. For example when user wants to send packet to an IP address how does it know which MAC address to put in the frame We will study this question in Chap. 5 but briefly summarized the answer is that it broadcasts frame con taining the question who owns IP address Then it waits for an answer. As the number of computers in LAN grows so does the number of broadcasts. Each broadcast consumes more of the LAN capacity than regular frame because it is delivered to every computer on the LAN. By keeping LANs no larger than they need to be the impact of broadcast traffic is reduced. Related to broadcasts is the problem that once in while network interface broadcast frames. If the network is really unlucky some of these frames will elicit responses that lead to ever more traffic. The result of this broadcast storm is that the entire LAN capacity is occupied by these frames and all the machines on all the interconnected LANs are crippled just processing and discarding all the frames being broadcast. At first it might appear that broadcast storms could be limited in scope by separating the LANs with bridges or switches but if the goal is to achieve tran sparency then bridges have to forward broadcast frames. Having seen why companies might want multiple LANs with restricted scopes let us get back to the problem of decoupling the logical topology from the physical topology. Building physical topology to reflect the organizational structure can add work and cost even with centralized wiring and switches. For example if two people in the same department work in different buildings it may be easier to wire them to different switches that are part of different LANs. Even if this is not the case user might be shifted within the company from one depart ment to another without changing offices or might change offices without chang ing departments. This might result in the user being on the wrong LAN until an administrator changes the users connector from one switch to another. Fur thermore the number of computers that belong to different departments may not be good match for the number of ports on switches some departments may be too small and others so big that they require multiple switches. This results in wasted switch ports that are not used. In many companies organizational changes occur all the time meaning that system administrators spend lot of time pulling out plugs and pushing them back in somewhere else. Also in some cases the change cannot be made at all be cause the twisted pair from the users machine is too far from the correct switch or the available switch ports are on the wrong LAN. In response to customer requests for more flexibility network vendors began working on way to rewire buildings entirely in software. The resulting concept committee and is now widely deployed in many organizations. Let us now take look at it. For additional information about VLANs see Seifert and Edwards VLANs are based on VLANaware switches. To set up VLANbased net work the network administrator decides how many VLANs there will be which computers will be on which VLAN and what the VLANs will be called. Often the VLANs are named by colors since it is then possible to print color diagrams showing the physical layout of the machines with the members of the red LAN in red members of the green LAN in green and so on. In this way both the physical and logical layouts are visible in single view. machines belong to the VLAN and five belong to the VLAN. Machines from the gray VLAN are spread across two switches including two ma chines that connect to switch via hub. To make the VLANs function correctly configuration tables have to be set up in the bridges. These tables tell which VLANs are accessible via which ports. When frame comes in from say the gray VLAN it must be forwarded on all the ports marked with . This holds for ordinary traffic for which cast and broadcast traffic. Note that port may be labeled with multiple VLAN As an example suppose that one of the gray stations plugged into bridge B1 in Bridge B1 will receive the frame and see that it came from machine on the gray VLAN so it will flood the frame on all ports labeled . The frame will be sent to the five other gray stations attached to B1 as well as over the link from B1 to bridge B2. At bridge B2 the frame is similarly for warded on all ports labeled . This sends the frame to one further station and the hub . The hub has both labels because it connects to machines from both VLANs. The frame is not sent on other ports without in the label because the bridge knows that there are no ma chines on the gray VLAN that can be reached via these ports. In our example the frame is only sent from bridge B1 to bridge B2 because there are machines on the gray VLAN that are connected to B2. Looking at the white VLAN we can see that the bridge B2 port that connects to bridge B1 is not labeled . This means that frame on the white VLAN will not be forwarded from bridge B2 to bridge B1. This behavior is correct because no stations on the To implement this scheme bridges need to know to which VLAN an incom ing frame belongs. Without this information for example when bridge B2 gets on the gray or white VLAN. If we were designing new type of LAN it would Ethernet which is the dominant LAN and did not have any spare fields lying The IEEE 802 committee had this problem thrown into its lap in 1995. After format contains VLAN tag we will examine it shortly. Not surprisingly chang few questions that come to mind are 1. Need we throw out several hundred million existing Ethernet cards 2. If not who generates the new fields 3. What happens to frames that are already the maximum size The key to the solution is to realize that the VLAN fields are only actually 47 it is not really essential that they are present on the lines going out to the end stations as long as they are on the line between the bridges. Also to use VLANs the bridges have to be VLAN aware. This fact makes the design feasible. As to throwing out all existing Ethernet cards the answer is no. Remember that the 802.3 committee could not even get people to change the Type field into Length field. You can imagine the reaction to an announcement that all existing Ethernet cards had to be thrown out. However new Ethernet cards are 802.1Q compliant and can correctly fill in the VLAN fields. Because there can be computers that are not VLAN aware the first VLANaware bridge to touch frame adds VLAN fields and the last one down the road removes them. An example of mixed topology is shown in frames directly and further switching uses these tags. The shaded symbols are ols are VLAN aware. The empty ones are not. With 802.1Q frames are colored depending on the port on which they are re ceived. For this method to work all machines on port must belong to the same for all ports where an individual computer connects to bridge but not for the port where the hub connects to bridge B2. Additionally the bridge can use the higherlayer protocol to select the color. In this way frames arriving on port might be placed in different VLANs de pending on whether they carry IP packets or PPP frames. Other methods are possible but they are not supported by 802.1Q. As one ex ample the MAC address can be used to select the VLAN color. This might be useful for frames coming in from nearby 802.11 LAN in which laptops send frames via different ports as they move. One MAC address would then be mapped to fixed VLAN regardless of which port it entered the LAN on. As to the problem of frames longer than 1518 bytes 802.1Q just raised the limit to 1522 bytes. Luckily only VLANaware computers and switches must support these longer frames. The only change is the addition of pair of 2byte fields. The first one is the VLAN protocol ID. It always has the value 0x8100. Since this number is greater than 1500 all Ethernet cards interpret it as type rather than length. What legacy card does with such frame is moot since such frames are not supposed to be sent to legacy cards. The second 2byte field contains three subfields. The main one is the VLAN identifier occupying the loworder 12 bits. This is what the whole thing is aboutthe color of the VLAN to which the frame belongs. The 3bit Priority is onceinadecade event taking three years and featuring hundred people why not put in some other good things while you are at it This field makes it possible to distinguish hard realtime traffic from soft realtime traffic from time insensitive traffic in order to provide better quality of service over Ethernet. It is needed for voice over Ethernet . The last field CFI should have been called the CEI . It was originally intended to indicate the order of the bits in the MAC addresses but that use got lost in other controversies. Its presence now indicates that the payload contains freezedried 802.5 frame that is hoping to find another 802.5 LAN at the destina tion while being carried by Ethernet in between. This whole arrangement of course has nothing whatsoever to do with VLANs. But standards committee politics are not unlike regular politics if you vote for my bit will vote for your As we mentioned above when tagged frame arrives at VLANaware which ports to send it on. But where does the table come from If it is manually constructed we are back to square zero manual configuration of bridges. The beauty of the transparent bridge is that it is plugandplay and does not require any manual configuration. It would be terrible shame to lose that property. For observing the tags that come by. If frame tagged as VLAN 4 comes in on port 3 apparently some machine on port 3 is on VLAN 4. The 802.1Q standard ex plains how to build the tables dynamically mostly by referencing appropriate por tions of the 802.1D standard. Before leaving the subject of VLAN routing it is worth making one last observation. Many people in the Internet and Ethernet worlds are fanatically in favor of connectionless networking and violently opposed to anything smacking of connections in the data link or network layers. Yet VLANs introduce some thing that is surprisingly similar to connection. To use VLANs properly each the switch to look up where the frame is supposed to be sent. That is precisely what happens in connectionoriented networks. In connectionless networks it is the destination address that is used for routing not some kind of connection iden tifier. We will see more of this creeping connectionism in Chap. 5. Some networks have single channel that is used for all communication. In competing stations wishing to use it. FDM and TDM are simple efficient alloca tion schemes when the number of stations is small and fixed and the traffic is con tinuous. Both are widely used under these circumstances for example for divid ing up the bandwidth on telephone trunks. However when the number of stations is large and variable or the traffic is fairly burstythe common case in computer networksFDM and TDM are poor choices. ALOHA protocol with and without slotting is used in many derivatives in real systems for example cable modems and RFID. As an improvement when the state of the channel can be sensed stations can avoid starting transmission while another station is transmitting. This technique carrier sensing has led to variety of CSMA protocols for LANs and MANs. It is the basis for classic Ethernet and 802.11 networks. class of protocols that eliminates contention altogether or at least reduces it considerably is well known. The bitmap protocol topologies such as rings and the binary countdown protocol completely eliminate contention. The tree walk protocol reduces it by dynamically dividing the stations into two disjoint groups of different sizes and allowing contention only within one group ideally that group is chosen so that only one station is ready to send when it is permitted to do so. transmissions and that the coverage regions of stations may differ. In the dom inant wireless LAN IEEE 802.11 stations use CSMACA to mitigate the first problem by leaving small gaps to avoid collisions. The stations can also use the RTSCTS protocol to combat hidden terminals that arise because of the second THE MEDIUM ACCESS CONTROL SUBLAYER problem. IEEE 802.11 is commonly used to connect laptops and other devices to wireless access points but it can also be used between devices. Any of several physical layers can be used including multichannel FDM with and without multi ple antennas and spread spectrum. Like 802.11 RFID readers and tags use random access protocol to commun icate identifiers. Other wireless PANs and MANs have different designs. The Bluetooth system connects headsets and many kinds of peripherals to computers without wires. IEEE 802.16 provides wide area wireless Internet data service for stationary and mobile computers. Both of these networks use centralized connectionoriented design in which the Bluetooth master and the WiMAX base station decide when each station may send or receive data. For 802.16 this design supports different quality of service for realtime traffic like telephone calls and interactive traffic like Web browsing. For Bluetooth placing the complexity in Classic Ethernet used snaked from machine to machine. The architecture has changed as speeds have risen from 10 Mbps to 10 Gbps and continue to climb. Now pointtopoint links such as twisted pair are attached to hubs and switches. With modern switches and fullduplex links there is no contention on the links and the switch can forward frames between different ports in parallel. With buildings full of LANs way is needed to interconnect them all. Plug andplay bridges are used for this purpose. The bridges are built with backward learning algorithm and spanning tree algorithm. Since this functionality is built into modern switches the terms bridge and switch are used interchangeably. To help with the management of bridged LANs VLANs let the physical topology be divided into different logical topologies. The VLAN standard IEEE 802.1Q introduces new format for Ethernet frames. arrive randomly at 100Mbps channel for transmission. If the channel is busy when frame arrives it waits its turn in queue. Frame length is exponentially distributed with mean of 10000 bitsframe. For each of the following frame arrival rates give the delay experienced by the average frame including both queueing time and trans 9000 framessec. 2. group of stations share 56kbps pure ALOHA channel. Each station outputs 1000bit frame on average once every 100 sec even if the previous one has not yet been sent . What is the maximum value 3. Consider the delay of pure ALOHA versus slotted ALOHA at low load. Which one is less Explain your answer. 4. large population of ALOHA users manages to generate 50 requestssec including both originals and retransmissions. Time is slotted in units of 40 msec. What is the chance of success on the first attempt What is the probability of exactly collisions and then success What is the expected number of transmission attempts needed 5. In an infinitepopulation slotted ALOHA system the mean number of slots station waits between collision and retransmission is 4. Plot the delay versus throughput curve for this system. 6. What is the length of contention slot in CSMACD for 2km twinlead cable and 40km multimode fiber optic cable 7. How long does station have to wait in the worst case before it can start trans mitting its frame over LAN that uses the basic bitmap protocol 8. In the binary countdown protocol explain how lowernumbered station may be starved from sending packet. 9. Sixteen stations numbered 1 through 16 are contending for the use of shared chan nel by using the adaptive tree walk protocol. If all the stations whose addresses are prime numbers suddenly become ready at once how many bit slots are needed to resolve the contention 10. Consider five wireless stations and . Station can communicate with all other stations. can communicate with and . can communicate with and . can communicate with and . can communicate and . When is sending to what other communications are possible When is sending to what other communications are possible When is sending to what other communications are possible 11. Six stations through communicate using the MACA protocol. Is it possible for two transmissions to take place simultaneously Explain your answer. 12. sevenstory office building has 15 adjacent offices per floor. Each office contains wall socket for terminal in the front wall so the sockets form rectangular grid in the vertical plane with separation of 4 between sockets both horizontally and vertically. Assuming that it is feasible to run straight cable between any pair of sockets horizontally vertically or diagonally how many meters of cable are needed to connect all sockets using star configuration with single router in the middle THE MEDIUM ACCESS CONTROL SUBLAYER 13. What is the baud rate of classic 10Mbps Ethernet 14. Sketch the Manchester encoding on classic Ethernet for the bit stream 0001110101. 15. 1kmlong 10Mbps CSMACD LAN has propagation speed of 200 mμsec. Repeaters are not allowed in this system. Data frames are 256 bits long successful transmission is reserved for the receiver to capture the channel in order to send 32bit acknowledgement frame. What is the effective data rate excluding overhead assuming that there are no collisions 16. Two CSMACD stations are each trying to transmit long files. After each frame is sent they contend for the channel using the binary exponential backoff algorithm. What is the probability that the contention ends on round and what is the If LLC is not in use is padding needed in the Ethernet frame and if so how many 18. Ethernet frames must be at least 64 bytes long to ensure that the transmitter is still going in the event of collision at the far end of the cable. Fast Ethernet has the same 64byte minimum frame size but can get the bits out ten times faster. How is it pos sible to maintain the same minimum frame size 19. Some books quote the maximum size of an Ethernet frame as 1522 bytes instead of 1500 bytes. Are they wrong Explain your answer. 20. How many frames per second can gigabit Ethernet handle Think carefully and take into account all the relevant cases. Hint the fact that it is gigabit Ethernet matters. 21. Name two networks that allow frames to be packed backtoback. Why is this feature do you think is closest to and why 23. Give an example to show that the RTSCTS in the 802.11 protocol is little different than in the MACA protocol. Mbps four stations have data rates of 18 Mbps and the last two stations have data rates of 54 Mbps. What is the data rate experienced by each station when all ten sta TXOP is used 25. Suppose that an 11Mbps 802.11b LAN is transmitting 64byte frames backtoback over radio channel with bit error rate of 107. How many frames per second will be damaged on average 26. An 802.16 network has channel width of 20 MHz. How many bitssec can be sent to subscriber station 27. Give two reasons why networks might use an errorcorrecting code instead of error detection and retransmission. 28. List two ways in which WiMAX is similar to 802.11 and two ways in which it is dif ferent from 802.11. time. Is there any reason why one device cannot be the master in both of them at the 30. What is the maximum size of the data field for 3slot Bluetooth frame at basic rate Explain your answer. Bluetooth physical layer protocol What is the biggest difference between the two coding is about 13 at basic data rate. What will the efficiency be if 5slot frame with repetition encoding is used at basic data rate instead 33. Beacon frames in the frequency hopping spread spectrum variant of 802.11 contain the dwell time. Do you think the analogous beacon frames in Bluetooth also contain the dwell time Discuss your answer. 34. Suppose that there are 10 RFID tags around an RFID reader. What is the best value of How likely is it that one tag responds with no collision in given slot 35. List some of the security concerns of an RFID system. 36. switch designed for use with fast Ethernet has backplane that can move 10 Gbps. How many framessec can it handle in the worst case 37. Briefly describe the difference between storeandforward and cutthrough switches. pose the hash tables in the two bridges are empty. List all ports on which packet will be forwarded for the following sequence of data transmissions sends packet to . 39. Storeandforward switches have an advantage over cutthrough switches with respect to damaged frames. Explain what it is. ning tree. Outline scenario where bridge may not be present in the spanning tree. 41. To make VLANs work configuration tables are needed in the bridges. What if the THE MEDIUM ACCESS CONTROL SUBLAYER switch. Would it be possible to use legacy switch there If so how would that work If not why not 43. Write program to simulate the behavior of the CSMACD protocol over Ethernet when there are stations ready to transmit while frame is being transmitted. Your frame. Assume that clock tick occurs once every slot time and collis ion detection and sending of jamming sequence takes one slot time. All frames are the maximum length allowed. The network layer is concerned with getting packets from the source all the way to the destination. Getting to the destination may require making many hops at intermediate routers along the way. This function clearly contrasts with that of the data link layer which has the more modest goal of just moving frames from one end of wire to the other. Thus the network layer is the lowest layer that deals with endtoend transmission. To achieve its goals the network layer must know about the topology of the network and choose appropriate paths through it even for large networks. It must also take care when choosing routes to avoid overloading some of the communication lines and routers while leaving others its network layer protocol IP. the service provided to the transport layer and the internal design of the network. Before starting to explain the details of the network layer it is worth restating the context in which the network layer protocols operate. This context can be shown inside the shaded oval and the customers equipment shown outside the oval. Host H1 is directly connected to one of the ISPs routers perhaps as home computer that is plugged into DSL modem. In contrast H2 is on LAN which might be an office Ethernet with router owned and operated by the customer. This router has leased line to the ISPs equipment. We have shown as being outside the oval because on customer premises are considered part of the ISP network because they run the same algorithms as the ISPs routers . This equipment is used as follows. host with packet to send transmits it to the nearest router either on its own LAN or over pointtopoint link to the ISP. The packet is stored there until it has fully arrived and the link has finished its processing by verifying the checksum. Then it is forwarded to the next router along the path until it reaches the destination host where it is delivered. This mechanism is storeandforward packet switching as we have seen in previous The network layer provides services to the transport layer at the network layertransport layer interface. An important question is precisely what kind of services the network layer provides to the transport layer. The services need to be carefully designed with the following goals in mind 1. The services should be independent of the router technology. 2. The transport layer should be shielded from the number type and topology of the routers present. 3. The network addresses made available to the transport layer should use uniform numbering plan even across LANs and WANs. Given these goals the designers of the network layer have lot of freedom in writing detailed specifications of the services to be offered to the transport layer. This freedom often degenerates into raging battle between two warring factions. The discussion centers on whether the network layer should provide connection oriented service or connectionless service. One camp argues that the routers job is moving packets around and nothing else. In this view the network is inherently unreliable no matter how it is designed. Therefore the hosts should accept this fact and do error control and flow control themselves. This viewpoint leads to the conclusion that the network service should be con nectionless with primitives SEND PACKET and RECEIVE PACKET and little else. In particular no packet ordering and flow control should be done because the hosts are going to do that anyway and there is usually little to be gained by doing it twice. This reasoning is an example of the endtoend argument design principle that has been very influential in shaping the Internet . Furthermore each packet must carry the full destination address because each packet sent is carried independently of its predecessors if any. The other camp argues that the net years of successful experience with the worldwide telephone system is an excel lent guide. In this view quality of service is the dominant factor and without connections in the network quality of service is very difficult to achieve espe cially for realtime traffic such as voice and video. Even after several decades this controversy is still very much alive. Early widely used data networks such as .25 in the 1970s and its successor Frame Relay in the 1980s were connectionoriented. However since the days of the ARPANET and the early Internet connectionless network layers have grown tremendously in popularity. The IP protocol is now an everpresent symbol of suc cess. It was undeterred by connectionoriented technology called ATM that was developed to overthrow it in the 1980s instead it is ATM that is now found in niche uses and IP that is taking over telephone networks. Under the covers how ever the Internet is evolving connectionoriented features as quality of service be comes more important. Two examples of connectionoriented technologies are and VLANs which we saw in Chap. 4. Both technologies are widely used. Having looked at the two classes of service the network layer can provide to its users it is time to see how this layer works inside. Two different organizations are possible depending on the type of service offered. If connectionless service is offered packets are injected into the network individually and routed indepen dently of each other. No advance setup is needed. In this context the packets are frequently called datagrams and the network is call ed datagram network. If connectionoriented service is used path from the source router all the way to the destination router must be established before any data packets can be sent. This connection is called VC in an alogy with the physical circuits set up by the telephone system and the network is works in the next one we will examine virtualcircuit networks. Let us now see how datagram network works. Suppose that the process P1 with instructions to deliver it to process P2 on host H2. The transport layer code to the front of the message and hands the result to the network layer probably just Let us assume for this example that the message is four times longer than the maximum packet size so the network layer has to break it into four packets 1 2 3 and 4 and send each of them in turn to router using some pointtopoint pro tocol for example PPP. At this point the ISP takes over. Every router has an in ternal table telling it where to send packets for each of the possible destinations. Each table entry is pair consisting of destination and the outgoing line to use for that destination. Only directly connected lines can be used. For example in must be sent to one of these routers even if the ultimate destination is to some At packets 1 2 and 3 are stored briefly having arrived on the incoming link and had their checksums verified. Then each packet is forwarded according to As table onto the outgoing link to within new frame. Packet 1 is then for warded to and then to . When it gets to it is sent within frame over the LAN to H2. Packets 2 and 3 follow the same route. However something different happens to packet 4. When it gets to it is sent to router even though it is also destined for . For some reason decided to send packet 4 via different route than that of the first three packets. Perhaps it ing table as shown under the label later. The algorithm that manages the tables and makes the routing decisions is called the routing algorithm. Routing algo different kinds of them as we will see. IP which is the basis for the entire Internet is the dom inant example of connectionless network service. Each packet carries destina tion IP address that routers use to individually forward each packet. The addresses are 32 bits in IPv4 packets and 128 bits in IPv6 packets. We will describe IP in For connectionoriented service we need virtualcircuit network. Let us see how that works. The idea behind virtual circuits is to avoid having to choose tablished route from the source machine to the destination machine is chosen as part of the connection setup and stored in tables inside the routers. That route is used for all traffic flowing over the connection exactly the same way that the telephone system works. When the connection is released the virtual circuit is also terminated. With connectionoriented service each packet carries an identi fier telling which virtual circuit it belongs to. established connection 1 with host H2. This connection is remembered as the first entry in each of the routing tables. The first line of As table says that if packet bearing connection identifier 1 comes in from H1 it is to be sent to router and given connection identifier 1. Similarly the first entry at routes the packet to also with connection identifier 1. Now let us consider what happens if H3 also wants to establish connection to H2. It chooses connection identifier 1 and tells the network to establish the virtual circuit. This leads to the second row in the tables. Note that we have conflict here be cause although can easily distinguish connection 1 packets from H1 from con nection 1 packets from H3 cannot do this. For this reason assigns different connection identifier to the outgoing traffic for the second connection. Avoiding conflicts of this kind is why routers need the ability to replace connection identi fiers in outgoing packets. In some contexts this process is called label switching. An example of connectionoriented network service is MPLS . It is used within ISP networks in the Internet with IP packets wrapped in an den from customers with the ISP establishing longterm connections for large amounts of traffic but it is increasingly being used to help when quality of service is important but also with other ISP traffic management tasks. We will have more Both virtual circuits and datagrams have their supporters and their detractors. Inside the network several tradeoffs exist between virtual circuits and data grams. One tradeoff is setup time versus address parsing time. Using virtual cir cuits requires setup phase which takes time and consumes resources. However once this price is paid figuring out what to do with data packet in virtualcir find out where the packet goes. In datagram network no setup is needed but more complicated lookup procedure is required to locate the entry for the destina longer than circuit numbers used in virtualcircuit networks because they have global meaning. If the packets tend to be fairly short including full destination address in every packet may represent significant amount of overhead and hence waste of bandwidth. datagram network needs to have an entry for every possible destination whereas virtualcircuit network just needs an entry for each virtual circuit. However this advantage is somewhat illusory since connection setup packets have to be routed too and they use destination addresses the same as datagrams do. Virtual circuits have some advantages in guaranteeing quality of service and avoiding congestion within the network because resources can be reserved in advance when the connection is estab lished. Once the packets start arriving the necessary bandwidth and router capac ity will be there. With datagram network congestion avoidance is more diffi For transaction processing systems the overhead required to set up and clear virtual circuit may easily dwarf the use of the circuit. If the majority of the traffic is expected to be of this kind the use of virtual circuits inside the network makes little sense. On the other hand for longrunning uses such as VPN traffic between two corporate offices permanent virtual circuits Virtual circuits also have vulnerability problem. If router crashes and loses its memory even if it comes back up second later all the virtual circuits passing through it will have to be aborted. In contrast if datagram router goes down only those users whose packets were queued in the router at the time need suffer . The loss of communication line is fatal to virtual circuits using it but can easily be compensated for if datagrams are used. Datagrams also allow the routers to balance the traffic throughout the network since routes can be changed partway through long sequence of packet transmissions. The main function of the network layer is routing packets from the source ma chine to the destination machine. In most networks packets will require multiple hops to make the journey. The only notable exception is for broadcast networks network segment. The algorithms that choose the routes and the data structures that they use are major area of network layer design. The routing algorithm is that part of the network layer software responsible for deciding which output line an incoming packet should be transmitted on. If the network uses datagrams internally this decision must be made anew for every arriving data packet since the best route may have changed since last time. If the network uses virtual circuits internally routing decisions are made only when new virtual circuit is being set up. Thereafter data packets just follow the already established route. The latter case is sometimes called session routing because route remains in force for an entire session . It is sometimes useful to make distinction between routing which is making the decision which routes to use and forwarding which is what happens when packet arrives. One can think of router as having two processes inside it. One of them handles each packet as it arrives looking up the outgoing line to use for it in the routing tables. This process is forwarding. The other process is responsi ble for filling in and updating the routing tables. That is where the routing algo rithm comes into play. Regardless of whether routes are chosen independently for each packet sent or only when new connections are established certain properties are desirable in routing algorithm correctness simplicity robustness stability fairness and effi ciency. Correctness and simplicity hardly require comment but the need for robustness may be less obvious at first. Once major network comes on the air it may be expected to run continuously for years without systemwide failures. Dur ing that period there will be hardware and software failures of all kinds. Hosts routers and lines will fail repeatedly and the topology will change many times. The routing algorithm should be able to cope with changes in the topology and traffic without requiring all jobs in all hosts to be aborted. Imagine the havoc if the network needed to be rebooted every time some router crashed Stability is also an important goal for the routing algorithm. There exist rout ing algorithms that never converge to fixed set of paths no matter how long they run. stable algorithm reaches equilibrium and stays there. It should converge quickly too since communication may be disrupted until the routing algorithm has reached equilibrium. Fairness and efficiency may sound obvioussurely no reasonable person would oppose thembut as it turns out they are often contradictory goals. As traffic between and between and and between and to saturate the horizontal links. To maximize the total flow the to traffic should be shut off altogether. Unfortunately and may not see it that way. Evidently some compromise between global efficiency and fairness to individual connections is Before we can even attempt to find tradeoffs between fairness and efficiency we must decide what it is we seek to optimize. Minimizing the mean packet delay maximizing total network throughput. Furthermore these two goals are also in conflict since operating any queueing system near capacity implies long queue ing delay. As compromise many networks attempt to minimize the distance packet must travel or simply reduce the number of hops packet must make. Ei ther choice tends to improve the delay and also reduce the amount of bandwidth consumed per packet which tends to improve the overall network throughput as Routing algorithms can be grouped into two major classes nonadaptive and adaptive. Nonadaptive algorithms do not base their routing decisions on any measurements or estimates of the current topology and traffic. Instead the choice of the route to use to get from to is computed in advance off line and downloaded to the routers when the network is booted. This procedure is sometimes called static routing. Because it does not respond to failures static routing is mostly useful for situations in which the routing choice is clear. For ex regardless of the ultimate destination. Adaptive algorithms in contrast change their routing decisions to reflect changes in the topology and sometimes changes in the traffic as well. These dynamic routing algorithms differ in where they get their information when they change the routes and what metric is used for optimization . It states that if router is on the optimal path from router to router then the optimal path from to also falls along the same route. To see this call the part of the route from to 1 and the rest of the route 2. If route better than 2 existed from to it could be concatenated with 1 to improve the route from to contradicting our statement that 1r 2 is optimal. As direct consequence of the optimality principle we can see that the set of optimal routes from all sources to given destination form tree rooted at the where the distance metric is the number of hops. The goal of all routing algo rithms is to discover and use the sink trees for all routers. Note that sink tree is not necessarily unique other trees with the same path lengths may exist. If we allow all of the possible paths to be chosen the tree be comes more general structure called DAG . DAGs have no loops. We will use sink trees as convenient shorthand for both cases. Both cases also depend on the technical assumption that the paths do not interfere with each other so for example traffic jam on one path will not cause another Since sink tree is indeed tree it does not contain any loops so each packet will be delivered within finite and bounded number of hops. In practice life is not quite this easy. Links and routers can go down and come back up during oper ation so different routers may have different ideas about the current topology. acquire the information on which to base its sink tree computation or whether this shortly. Nevertheless the optimality principle and the sink tree provide bench mark against which other routing algorithms can be measured. Let us begin our study of routing algorithms with simple technique for com puting optimal paths given complete picture of the network. These paths are the ones that we want distributed routing algorithm to find even though not all rout ers may know all of the details of the network. The idea is to build graph of the network with each node of the graph representing router and each edge of the graph representing communication line or link. To choose route between given pair of routers the algorithm just finds the shortest path between them on the graph. The concept of shortest path deserves some explanation. One way of measuring path length is the number of hops. Using this metric the paths ABC in kilometers in which case ABC is clearly much longer than ABE with its distance from the source node along the best known path. The distances must be nonnegative as they will be if they are based on real quan tities like bandwidth and delay. Initially no paths are known so all nodes are labeled with infinity. As the algorithm proceeds and paths are found the labels may change reflecting better paths. label may be either tentative or permanent. Initially all labels are tentative. When it is discovered that label represents the shortest possible path from the source to that node it is made permanent and never changed thereafter. To illustrate how the labeling algorithm works look at the weighted tance. We want to find the shortest path from to . We start out by marking node as permanent indicated by filledin circle. Then we examine in turn each of the nodes adjacent to relabeling each one with the distance to . Whenever node is relabeled we also label it with the node from network had more than one shortest path from to and we wanted to find all of them we would need to remember all of the probe nodes that could reach node with the same distance. Having examined each of the nodes adjacent to we examine all the tenta tively labeled nodes in the whole graph and make the one with the smallest label We now start at and examine all nodes adjacent to it. If the sum of the label on and the distance from to the node being considered is less than the label on that node we have shorter path so the node is relabeled. After all the nodes adjacent to the working node have been inspected and the tentative labels changed if possible the entire graph is searched for the tentatively labeled node with the smallest value. This node is made permanent and becomes just made permanent. Suppose that there were shorter path than ABE say AXYZE . There are two possibilities either node has already been made permanent or it has not been. If it has then has already been probed so the AXYZE path has not escaped our attention and thus cannot be shorter path. Now consider the case where is still tentatively labeled. If the label at is greater than or equal to that at then AXYZE cannot be shorter path than ABE. If the label is less than that of then and not will become permanent first al lowing to be probed from . the graph and are initialized before shortest path is called. The only difference compute the shortest path starting at the terminal node rather than at the source Since the shortest paths from to in an undirected graph are the same as the shortest paths from to it does not matter at which end we begin. The reason for searching backward is that each node is labeled with its predecessor rather the path is thus reversed. The two reversal effects cancel and the answer is pro duced in the correct order. When routing algorithm is implemented each router must make decisions based on local knowledge not the complete picture of the network. simple local technique is flooding in which every incoming packet is sent out on every outgoing line except the one it arrived on. Flooding obviously generates vast numbers of duplicate packets in fact an infinite number unless some measures are taken to damp the process. One such decremented at each hop with the packet being discarded when the counter reaches zero. Ideally the hop counter should be initialized to the length of the path from source to destination. If the sender does not know how long the path is it can initialize the counter to the worst case namely the full diameter of the net Flooding with hop count can produce an exponential number of duplicate packets as the hop count grows and routers duplicate packets they have seen be fore. better technique for damming the flood is to have routers keep track of which packets have been flooded to avoid sending them out second time. One way to achieve this goal is to have the source router put sequence number in each packet it receives from its hosts. Each router then needs list per source router telling which sequence numbers originating at that source have already been seen. If an incoming packet is on the list it is not flooded. this graph has nodes if if state.length state.length dist Find the tentatively labeled node with the smallest label. for if Copy the path into the output array. do path state.predecessor while To prevent the list from growing without bound each list should be aug mented by counter meaning that all sequence numbers through have been seen. When packet comes in it is easy to check if the packet has already been flooded flooding will find path if one exists to get packet to its destination. Flooding also requires little in the way of setup. The routers only need to know their neighbors. This means that flooding can be used as building block for other routing algorithms that are more efficient but need more in the way of setup. Flooding can also be used as metric against which other routing algorithms can be compared. Flooding always chooses the shortest path because it chooses every possible path in parallel. Con sequently no other algorithm can produce shorter delay . Computer networks generally use dynamic routing algorithms that are more complex than flooding but more efficient because they find shortest paths for the current topology. Two dynamic algorithms in particular distance vector routing distance vector routing algorithm operates by having each router maintain table giving the best known distance to each destination and mation with the neighbors. Eventually every router knows the best link to reach each destination. The distance vector routing algorithm is sometimes called by other names most commonly the distributed BellmanFord routing algorithm after the re searchers who developed it . It was the original ARPANET routing algorithm and was also used in the Internet under the name RIP. and containing one entry for each router in the network. This entry has two parts the preferred outgoing line to use for that destination and an estimate of the dis tance to that destination. The distance might be measured as the number of hops or using another metric as we discussed for computing shortest paths. The router is assumed to know the distance to each of its neighbors. If the metric is hops the distance is just one hop. If the metric is propagation delay the router can measure it directly with special ECHO packets that the receiver just timestamps and sends back as fast as it can. As an example assume that delay is used as metric and that the router knows the delay to each of its neighbors. Once every msec each router sends to each neighbor list of its estimated delays to each destination. It also receives similar list from each neighbor. Imagine that one of these tables has just come in from neighbor with Xi being Xs estimate of how long it takes to get to router . If the router knows that the delay to is msec it also knows that it can reach router via in Xi msec. By performing this calculation for each neighbor router can find out which estimate seems the best and use that estimate and the corresponding link in its new routing table. Note that the old routing table is not used in the calculation. first four columns of part show the delay vectors received from the neighbors of router . claims to have 12msec delay to 25msec delay to 40 msec delay to etc. Suppose that has measured or estimated its delay to its neighbors and as 8 10 12 and 6 msec respectively. Consider how computes its new route to router . It knows that it can get to in 8 msec and furthermore claims to be able to get to in 18 msec so knows it can count on delay of 26 msec to if it forwards packets bound for and 37 msec respectively. The best of these values is 18 so it makes an entry in its routing table that the delay to is 18 msec and that the route to use is via . The same calculation is performed for all the other destinations The settling of routes to best paths across the network is called convergence. Distance vector routing is useful as simple technique by which routers can col lectively compute shortest paths but it has serious drawback in practice al though it converges to the correct answer it may do so slowly. In particular it reacts rapidly to good news but leisurely to bad news. Consider router whose best route to destination is long. If on the next exchange neighbor suddenly send traffic to . In one vector exchange the good news is processed. To see how fast good news propagates consider the fivenode net down initially and all the other routers know this. In other words they have all When comes up the other routers learn about it via the vector exchanges. For simplicity we will assume that there is gigantic gong somewhere that is struck periodically to initiate vector exchange at all routers simultaneously. At the time of the first exchange learns that its lefthand neighbor has zero delay to . now makes an entry in its routing table indicating that is one hop away to the left. All the other routers still think that is down. At this point the rout to indicate path of length 2 but and do not hear the good news until later. Clearly the good news is spreading at the rate of one hop per exchange. In net work whose longest path is of length hops within exchanges everyone will know about newly revived links and routers. routers are initially up. Routers and have distances to of 1 2 3 and 4 hops respectively. Suddenly either goes down or the link between and is cut . At the first packet exchange does not hear anything from . Fortunately says Do not worry have path to of length 2. Little does suspect that Cs path runs through itself. For all knows might have ten links all with sepa rate paths to of length 2. As result thinks it can reach via with path On the second exchange notices that each of its neighbors claims to have path to of length 3. It picks one of them at random and makes its new distance ever has value more than one higher than the minimum of all its neighbors. Gradually all routers work their way up to infinity but the number of exchanges required depends on the numerical value used for infinity. For this reason it is wise to set infinity to the longest path plus 1. Not entirely surprisingly this problem is known as the counttoinfinity prob lem. There have been many attempts to solve it for example preventing routers from advertising their best paths back to the neighbors from which they heard them with the split horizon with poisoned reverse rule discussed in RFC 1058. However none of these heuristics work well in practice despite the colorful names. The core of the problem is that when tells that it has path some where has no way of knowing whether it itself is on the path. Distance vector routing was used in the ARPANET until 1979 when it was replaced by link state routing. The primary problem that caused its demise was that the algorithm often took too long to converge after the network topology changed . Consequently it was replaced by an entirely new algorithm now called link state routing. Variants of link state routing called ISIS and OSPF are the routing algorithms that are most widely used inside large networks and the Internet today. The idea behind link state routing is fairly simple and can be stated as five parts. Each router must do the following things to make it work 1. Discover its neighbors and learn their network addresses. 2. Set the distance or cost metric to each of its neighbors. 3. Construct packet telling all it has just learned. 4. Send this packet to and receive packets from all other routers. 5. Compute the shortest path to every other router. In effect the complete topology is distributed to every router. Then Dijkstras al gorithm can be run at each router to find the shortest path to every other router. Below we will consider each of these five steps in more detail. When router is booted its first task is to learn who its neighbors are. It accomplishes this goal by sending special HELLO packet on each pointtopoint line. The router on the other end is expected to send back reply giving its name. These names must be globally unique because when distant router later hears that three routers are all connected to it is essential that it can determine wheth er all three mean the same . When two or more routers are connected by broadcast link and list of neighbors. The cost to each neighbor is also given. An Building the link state packets is easy. The hard part is determining when to build them. One possibility is to build them periodically that is at regular inter vals. Another possibility is to build them when some significant event occurs such as line or neighbor going down or coming back up again or changing its The trickiest part of the algorithm is distributing the link state packets. All of the routers must get all of the link state packets quickly and reliably. If different First we will describe the basic distribution algorithm. After that we will give some refinements. The fundamental idea is to use flooding to distribute the link state packets to all routers. To keep the flood in check each packet contains sequence number that is incremented for each new packet sent. Routers keep track of all the pairs they see. When new link state packet comes in it is checked against the list of packets already seen. If it is new it is forwarded on all lines except the one it arrived on. If it is duplicate it is discarded. If packet with sequence number lower than the highest one seen so far ever arrives it is rejected as being obsolete as the router has more recent data. quence numbers wrap around confusion will reign. The solution here is to use years to wrap around so this possibility can be ignored. Second if router ever crashes it will lose track of its sequence number. If it starts again at 0 the next packet it sends will be rejected as duplicate. Third if sequence number is ever corrupted and 65540 is received instead of 4 packets 5 through 65540 will be rejected as obsolete since the current sequence number will be thought to be 65540. the sequence number and decrement it once per second. When the age hits zero the information from that router is discarded. Normally new packet comes in say every 10 sec so router information only times out when router is down . The Age field is also decremented by each router during the initial flooding process to make sure no packet can get lost and live for an indefinite period of time . Some refinements to this algorithm make it more robust. When link state packet comes in to router for flooding it is not queued for transmission im mediately. Instead it is put in holding area to wait short while in case more links are coming up or going down. If another link state packet from the same source comes in before the first packet is transmitted their sequence numbers are compared. If they are equal the duplicate is discarded. If they are different the older one is thrown out. To guard against errors on the links all link state packets are acknowledged. not fully processed link state packet. The table records where the packet ori ginated its sequence number and age and the data. In addition there are send and acknowledgement flags for each of Bs three links . The send flags mean that the packet must be sent on the indicated link. The acknowledgement flags mean that it must be acknowledged there. and and acknowledged to as indicated by the flag bits. Similarly the pack et from has to be forwarded to and and acknowledged to . However the situation with the third packet from is different. It arrives twice once via EAB and once via EFB. Consequently it has to be sent only to but must be acknowledged to both and as indicated by the bits. If duplicate arrives while the original is still in the buffer bits have to be changed. For example if copy of Cs state arrives from before the fourth entry in the table has been forwarded the six bits will be changed to 100011 to in dicate that the packet must be acknowledged to but not sent there. Once router has accumulated full set of link state packets it can construct the entire network graph because every link is represented. Every link is in fact represented twice once for each direction. The different directions may even have different costs. The shortestpath computations may then find different paths from router to than from router to . Now Dijkstras algorithm can be run locally to construct the shortest paths to all possible destinations. The results of this algorithm tell the router which link to use to reach each destination. This information is installed in the routing tables and normal operation is resumed. Compared to distance vector routing link state routing requires more memory and computation. For network with routers each of which has neighbors the memory required to store the input data is proportional to kn which is at least as large as routing table listing all the destinations. Also the computation time networks. Nevertheless in many practical situations link state routing works Link state routing is widely used in actual networks so few words about some example protocols are in order. Many ISPs use the ISIS link state protocol . It was designed for an early network called DECnet later adopted by ISO for use with the OSI protocols and then modified to handle other protocols as well most notably IP. OSPF is the other main link state protocol. It was designed by IETF several years after ISIS and adopted many of the innovations designed for ISIS. These innovations include selfstabilizing method of flood od of computing and supporting path splitting and multiple metrics. As conse quence there is very little difference between ISIS and OSPF. The most impor tant difference is that ISIS can carry information about multiple network layer protocols at the same time . OSPF does not have this feature and it is an advantage in large multiprotocol environments. We will go over OSPF in Sec. 5.6.6. general comment on routing algorithms is also in order. Link state dis tance vector and other algorithms rely on processing at all the routers to compute can wreak havoc across the network. For example if router claims to have link it does not have or forgets link it does have the network graph will be incorrect. If router fails to forward packets or corrupts them while forwarding does the routing calculation wrong bad things will happen. As the network grows into the range of tens or hundreds of thousands of nodes the probability of some router failing occasionally becomes nonnegligible. The trick is to try to arrange to limit the damage when the inevitable happens. Perlman discusses these As networks grow in size the router routing tables grow proportionally. Not only is router memory consumed by everincreasing tables but more CPU time is them. At certain point the network may grow to the point where it is no longer feasible for every router to have an entry for every other router so the routing will have to be done hierarchically as it is in the telephone network. When hierarchical routing is used the routers are divided into what we will call regions. Each router knows all the details about how to route packets to dest inations within its own region but knows nothing about the internal structure of other regions. When different networks are interconnected it is natural to regard each one as separate region to free the routers in one network from having to know the topological structure of the other ones. For huge networks twolevel hierarchy may be insufficient it may be nec essary to group the regions into clusters the clusters into zones the zones into groups and so on until we run out of names for aggregations. As an example of multilevel hierarchy consider how packet might be routed from Berkeley Cali fornia to Malindi Kenya. The Berkeley router would know the detailed topology within California but would send all outofstate traffic to the Los Angeles router. The Los Angeles router would be able to route traffic directly to other domestic routers but would send all foreign traffic to New York. The New York router would be programmed to direct all traffic to the router in the destination country work its way down the tree in Kenya until it got to Malindi. with five regions. The full routing table for router 1A has 17 entries as shown in tries for all the local routers as before but all other regions are condensed into single router so all traffic for region 2 goes via the 1B2A line but the rest of the remote traffic goes via the 1C3B line. Hierarchical routing has reduced the table from 17 to 7 entries. As the ratio of the number of regions to the number of rout ers per region grows the savings in table space increase. Unfortunately these gains in space are not free. There is penalty to be paid increased path length. For example the best route from 1A to 5C is via region 2 but with hierarchical routing all traffic to region 5 goes via region 3 because that is better for most destinations in region 5. When single network becomes very large an interesting question is how many levels should the hierarchy have For example consider network with 720 routers. If there is no hierarchy each router needs 720 routing table entries. If the network is partitioned into 24 regions of 30 routers each each router needs 30 local entries plus 23 remote entries for total of 53 entries. If threelevel hierarchy is chosen with 8 clusters each containing 9 regions of 10 routers each router needs 10 entries for local routers 8 entries for routing to other regions within its own cluster and 7 entries for distant clusters for total of 25 entries. Kamoun and Kleinrock discovered that the optimal number of levels for an router network is ln requiring total of ln entries per router. They have also shown that the increase in effective mean path length caused by hierarchical routing is sufficiently small that it is usually acceptable. In some applications hosts need to send messages to many or all other hosts. radio programs might work best by sending to all machines and letting those that are interested read the data. Sending packet to all destinations simultaneously is called broadcasting. Various methods have been proposed for doing it. One broadcasting method that requires no special features from the network is for the source to simply send distinct packet to each destination. Not only is the method wasteful of bandwidth and slow but it also requires the source to have complete list of all destinations. This method is not desirable in practice even though it is widely applicable. An improvement is multidestination routing in which each packet contains either list of destinations or bit map indicating the desired destinations. When packet arrives at router the router checks all the destinations to determine the set of output lines that will be needed. The router generates new copy of the packet for each output line to be used and includes in each packet only those dest inations that are to use the line. In effect the destination set is partitioned among the output lines. After sufficient number of hops each packet will carry only one destination like normal packet. Multidestination routing is like using sepa rately addressed packets except that when several packets must follow the same route one of them pays full fare and the rest ride free. The network bandwidth is therefore used more efficiently. However this scheme still requires the source to know all the destinations plus it is as much work for router to determine where to send one multidestination packet as it is for multiple distinct packets. We have already seen better broadcast routing technique flooding. When implemented with sequence number per source flooding uses links efficiently with decision rule at routers that is relatively simple. Although flooding is ill suited for ordinary pointtopoint communication it rates serious consideration for broadcasting. However it turns out that we can do better still once the shortest path routes for regular packets have been computed. The idea for reverse path forwarding is elegant and remarkably simple once it has been pointed out . When broadcast packet ar rives at router the router checks to see if the packet arrived on the link that is normally used for sending packets toward the source of the broadcast. If so there is an excellent chance that the broadcast packet itself followed the best route from the router and is therefore the first copy to arrive at the router. This being the case the router forwards copies of it onto all links except the one it arrived on. If however the broadcast packet arrived on link other than the preferred one for reaching the source the packet is discarded as likely duplicate. tree built by reverse path forwarding. network part shows sink tree for router of that network and part shows how the reverse path algorithm works. On the first hop sends packets to and as indicated by the second row of the tree. Each of these packets arrives on the preferred path to and is so indicated by circle around the letter. On the second hop eight packets are generated two by each of the routers that received packet on the first hop. As it turns out all eight of these arrive at previously unvisited rout ers and five of these arrive along the preferred line. Of the six packets generated on the third hop only three arrive on the preferred path the oth ers are duplicates. After five hops and 24 packets the broadcasting terminates compared with four hops and 14 packets had the sink tree been followed exactly. The principal advantage of reverse path forwarding is that it is efficient while being easy to implement. It sends the broadcast packet over each link only once in each direction just as in flooding yet it requires only that routers know how to reach all destinations without needing to remember sequence numbers or list all destinations in the packet. Our last broadcast algorithm improves on the behavior of reverse path for warding. It makes explicit use of the sink treeor any other convenient spanning treefor the router initiating the broadcast. spanning tree is subset of the network that includes all the routers but contains no loops. Sink trees are spanning trees. If each router knows which of its lines belong to the spanning tree it can copy an incoming broadcast packet onto all the spanning tree lines except the one it arrived on. This method makes excellent use of bandwidth generating the example when the sink tree of part is used as the spanning tree the broadcast packet is sent with the minimum 14 packets. The only problem is that each router must have knowledge of some spanning tree for the method to be applicable. Sometimes this information is available but sometimes it is not . Some applications such as multiplayer game or live video of sports event the group is very small sending distinct packet to each receiver is expensive. On the other hand broadcasting packet is wasteful if the group consists of say 1000 machines on millionnode network so that most receivers are not inter ested in the message . Thus we need way to send messages to welldefined groups that are numerically large in size but small compared to the network as whole. Sending message to such group is called multicasting and the routing al gorithm used is called multicast routing. All multicasting schemes require some way to create and destroy groups and to identify which routers are members of group. How these tasks are accomplished is not of concern to the routing algo rithm. For now we will assume that each group is identified by multicast ad dress and that routers know the groups to which they belong. We will revisit group membership when we describe the network layer of the Internet in Sec. 5.6. Multicast routing schemes build on the broadcast routing schemes we have al ready studied sending packets along spanning trees to deliver the packets to the members of the group while making efficient use of bandwidth. However the best spanning tree to use depends on whether the group is dense with receivers scattered over most of the network or sparse with much of the network not be If the group is dense broadcast is good start because it efficiently gets the packet to all parts of the network. But broadcast will reach some routers that are not members of the group which is wasteful. The solution explored by Deering and Cheriton is to prune the broadcast spanning tree by removing links that do not lead to members. The result is an efficient multicast spanning tree. As an example consider the two groups 1 and 2 in the network shown in have been removed. The result is the multicast spanning tree for the leftmost router to send to group 1. Packets are forwarded only along this spanning tree which is more efficient than the broadcast tree because there are 7 links instead of efficient too with only five links this time. It also shows that different multicast groups have different spanning trees. Various ways of pruning the spanning tree are possible. The simplest one can be used if link state routing is used and each router is aware of the complete topo logy including which hosts belong to which groups. Each router can then con struct its own pruned spanning tree for each sender to the group in question by constructing sink tree for the sender as usual and then removing all links that do not connect group members to the sink node. MOSPF is an example of link state protocol that works in this way . With distance vector routing different pruning strategy can be followed. The basic algorithm is reverse path forwarding. However whenever router with no hosts interested in particular group and no connections to other routers re ceives multicast message for that group it responds with PRUNE message tel ling the neighbor that sent the message not to send it any more multicasts from the sender for that group. When router with no group members among its own hosts has received such messages on all the lines to which it sends the multicast it too can respond with PRUNE message. In this way the spanning tree is recursively pruned. DVMRP is an example of multicast routing protocol that works this way . Pruning results in efficient spanning trees that use only the links that are actu ally needed to reach members of the group. One potential disadvantage is that it is lots of work for routers especially for large networks. Suppose that network multicast tree for group 1. multicast tree for group 2. has groups each with an average of nodes. At each router and for each group pruned spanning trees must be stored for total of mn trees. For exam The spanning tree for the rightmost router to send to group 1 will look quite different as packets will head directly for group members rather than via the left side of the graph. This in turn means that routers must forward pack ets destined to group 1 in different directions depending on which node is sending to the group. When many large groups with many senders exist considerable storage is needed to store all the trees. An alternative design uses corebased trees to compute single spanning tree for the group . All of the routers agree on root and build the tree by sending packet from each member to the root. The tree is the union of the paths traced by these packets. sends packet to the core. When the packet reaches the core it is forwarded down network. As performance optimization packets destined for the group do not need to reach the core before they are multicast. As soon as packet reaches the tree it can be forwarded up toward the root as well as down all the other 17 the packet from the sender on the righthand side reaches the topright group member via the core in three hops instead of directly. The inefficiency depends on where the core and senders are located but often it is reasonable when the core is in the middle of the senders. When there is only single sender as in video that is streamed to group using the sender as the core is optimal. Also of note is that shared trees can be major savings in storage costs mes sages sent and computation. Each router has to keep only one tree per group in stead of trees. Further routers that are not part of the tree do no work at all to support the group. For this reason shared tree approaches like corebased trees are used for multicasting to sparse groups in the Internet as part of popular proto cols such as PIM . So far we have covered delivery models in which source sends to single destination to all destinations and to group of destinations . Another delivery model called anycast is sometimes also useful. In anycast packet is delivered to the nearest member of group . Schemes that find these paths are called anycast Why would we want anycast Sometimes nodes provide service such as time of day or content distribution for which it is getting the right information all that matters not the node that is contacted any node will do. For example any cast is used in the Internet as part of DNS as we will see in Chap. 7. Luckily we will not have to devise new routing schemes for anycast because regular distance vector and link state routing can produce anycast routes. Suppose we want to anycast to the members of group 1. They will all be given the address 1 instead of different addresses. Distance vector routing will distribute vectors as usual and nodes will choose the shortest path to destination 1. This will result in nodes sending to the nearest instance of destination 1. The routes are shown in that there are multiple instances of destination 1. That is it believes that all the This procedure works for link state routing as well although there is the added consideration that the routing protocol must not find seemingly short paths that pass through node 1. This would result in jumps through hyperspace since the instances of node 1 are really nodes located in different parts of the network. However link state protocols already make this distinction between routers and hosts. We glossed over this fact earlier because it was not needed for our dis Millions of people use computers while on the go from truly mobile situa tions with wireless devices in moving cars to nomadic situations in which laptop hosts to mean either category as distinct from stationary hosts that never move. Increasingly people want to stay connected wherever in the world they may be as easily as if they were at home. These mobile hosts introduce new complication to route packet to mobile host the network first has to find it. The model of the world that we will consider is one in which all hosts are as analogous to the way the telephone number 12125551212 indicates the United States and Manhattan . The routing goal in systems with mobile hosts is to make it possible to send packets to mobile hosts using their fixed home addresses and have the packets efficiently reach them wherever they may be. The trick of course is to find them. Some discussion of this model is in order. different model would be to recompute routes as the mobile host moves and the topology changes. We could with growing number of mobile hosts this model would soon lead to the entire network endlessly computing new routes. Using the home addresses greatly re duces this burden. Another alternative would be to provide mobility above the network layer which is what typically happens with laptops today. When they are moved to new between the old and new addresses the network does not know that they belonged to the same laptop. In this model laptop can be used to browse the Web but other hosts cannot send packets to it without after moving. Moreover connections cannot be maintained while the host is mov ing new connections must be started up instead. Networklayer mobility is useful The basic idea used for mobile routing in the Internet and cellular networks is which acts on behalf of the mobile host is called the home agent. Once it knows where the mobile host is currently located it can forward packets so that they are Seattle wants to send packet to host normally located across the United States in New York. The case of interest to us is when the mobile host is not at home. Instead it is temporarily in San Diego. The mobile host in San Diego must acquire local network address before it can use the network. This happens in the normal way that hosts obtain network local address is called care of address. Once the mobile host has this address it can tell its home agent where it is now. It does this by sending registration message to the home agent with the care of address. The message is Next the sender sends data packet to the mobile host using its permanent because that is where the home address belongs. In New York the home agent intercepts this packet because the mobile host is away from home. It then wraps address . This mechanism is called tunneling. It is very important in the Internet so we will look at it in more detail later. When the encapsulated packet arrives at the care of address the mobile host unwraps it and retrieves the packet from the sender. The mobile host then sends its reply packet directly to the sender . The overall route is called triangle sequent packets can be routed directly to the mobile host by tunneling them to the lost for any reason as the mobile moves the home address can always be used to reach the mobile. An important aspect that we have omitted from this description is security. In general when host or router gets message of the form Starting right now please send all of Stephanys mail to me it might have couple of questions about whom it is talking to and whether this is good idea. Security information is included in the messages so that their validity can be checked with crypto graphic protocols that we will study in Chap. 8. There are many variations on mobile routing. The scheme above is modeled on IPv6 mobility the form of mobility used in the Internet and as part of IPbased cellular networks such as UMTS. We showed the sender to be stationary node for simplicity but the designs let both nodes be mobile hosts. Alternatively the host may be part of mobile network for example computer in plane. Extensions of the basic scheme support mobile networks with no work on the part of the hosts . Some schemes make use of foreign agent similar to the home ister in cellular networks. However in more recent schemes the foreign agent is not needed mobile hosts act as their own foreign agents. In either case know hosts so that the many routers in Snoeren and Balakrishnan . We have now seen how to do routing when the hosts are mobile but the rout ers are fixed. An even more extreme case is one in which the routers themselves are mobile. Among the possibilities are emergency workers at an earthquake site military vehicles on battlefield fleet of ships at sea or gathering of people with laptop computers in an area lacking 802.11. In all these cases and others each node communicates wirelessly and acts as both host and router. Networks of nodes that just happen to be near each other are called ad hoc networks or MANETs . Let us now examine them briefly. More information can be found in Perkins . What makes ad hoc networks different from wired networks is that the topo logy is suddenly tossed out the window. Nodes can come and go or appear in new places at the drop of bit. With wired network if router has valid path to some destination that path continues to be valid barring failures which are hope fully rare. With an ad hoc network the topology may be changing all the time so the desirability and even the validity of paths can change spontaneously without warning. Needless to say these circumstances make routing in ad hoc networks more challenging than routing in their fixed counterparts. Many many routing algorithms for ad hoc networks have been proposed. However since ad hoc networks have been little used in practice compared to mobile networks it is unclear which of these protocols are most useful. As an ex ample we will look at one of the most popular routing algorithms AODV . It is relative of the distance vector algorithm that has been adapted to work in mobile environ ment in which nodes often have limited bandwidth and battery lifetimes. Let us now see how it discovers and maintains routes. In AODV routes to destination are discovered on demand that is only when somebody wants to send packet to that destination. This saves much work that would otherwise be wasted when the topology changes before the route is used. At any instant the topology of an ad hoc network can be described by graph of connected nodes. Two nodes are connected if they can communicate directly using their radios. basic but adequate model that is sufficient for our purposes is that each node can com municate with all other nodes that lie within its coverage circle. Real networks are more complicated with buildings hills and other obstacles that block communi cation and nodes for which is connected to but is not connected to be cause has more powerful transmitter than . However for simplicity we will assume all connections are symmetric. To describe the algorithm consider the newly formed ad hoc network of AODV algorithm maintains distance vector table at each node keyed by desti nation giving information about that destination including the neighbor to which to send packets to reach the destination. First looks in its table and does not find an entry for . It now has to discover route to . This property of discover ing routes only when they are needed is what makes this algorithm on demand. and receive it. After and receive it. The shaded nodes are new recipients. The dashed lines show possible reverse routes. The solid lines show the discovered route. To locate constructs ROUTE REQUEST packet and broadcasts it using flooding as described in Sec. 5.2.3. The transmission from reaches and as sequence number set at the source is used to weed out duplicates during the flood. ready forwarded the request. Eventually the request reaches node which constructs ROUTE REPLY packet. This packet is unicast to the sender along the reverse of the path followed by the request. For this to work each intermediate node must remember the node information that is stored. Each intermediate node also increments hop count as it forwards the reply. This tells the nodes how far they are from the destination. The replies tell each intermediate node which neighbor to use to reach the destina tion it is the node that sent them the reply. Intermediate nodes and put the best route they hear into their routing tables as they process the reply. When the reply reaches new route ADGI has been created. In large network the algorithm generates many broadcasts even for destina tions that are close by. To reduce overhead the scope of the broadcasts is limited using the IP packets Time to live field. This field is initialized by the sender and decremented on each hop. If it hits 0 the packet is discarded instead of being broadcast. The route discovery process is then modified as follows. To locate destination the sender broadcasts ROUTE REQUEST packet with Time to live set to 1. If no response comes back within reasonable time another one is sent this time with Time to live set to 2. Subsequent attempts use 3 4 5 etc. In this way the search is first attempted locally then in increasingly wider rings. Because nodes can move or be switched off the topology can change spon the route it was using to is no longer valid. The algorithm needs to be able to deal with this. Periodically each node broadcasts Hello message. Each of its neighbors is expected to respond to it. If no response is forthcoming the broadcaster knows that that neighbor has moved out of range or failed and is no longer connected to it. Similarly if it tries to send packet to neighbor that does not respond it learns that the neighbor is no longer available. This information is used to purge routes that no longer work. For each pos sible destination each node keeps track of its active neighbors that have fed it packet for that destination during the last ΔT seconds. When any of Ns neigh bors becomes unreachable it checks its routing table to see which destinations have routes using the nowgone neighbor. For each of these routes the active neighbors are informed that their route via is now invalid and must be purged from their routing tables. In our example purges its entries for and from its routing table and notifies which purges its entry for . In the general case the active neighbors tell their active neighbors and so on recursively until all routes depending on the nowgone node are purged from all routing tables. At this stage the invalid routes have been purged from the network and send ers can find new valid routes by using the discovery mechanism that we de scribed. However there is complication. Recall that distance vector protocols change in which they confuse old invalid routes with new valid routes. To ensure rapid convergence routes include sequence number that is con trolled by the destination. The destination sequence number is like logical clock. The destination increments it every time that it sends fresh ROUTE REPLY. Senders ask for fresh route by including in the ROUTE REQUEST the destination sequence number of the last route they used which will either be the sequence number of the route that was just purged or 0 as an initial value. The request will be broadcast until route with higher sequence number is found. Intermediate nodes store the routes that have higher sequence number or the fewest hops for the current sequence number. In the spirit of an on demand protocol intermediate nodes only store the routes that are in use. Other route information learned during broadcasts is timed out after short delay. Discovering and storing only the routes that are used helps to save bandwidth and battery life compared to standard distance vector protocol So far we have considered only single route from to . To further save resources route discovery and maintenance are shared when routes overlap. For instance if also wants to send packets to it will perform route discovery. However in this case the request will first reach which already has route to . Node can then generate reply to tell the route without any additional work There are many other ad hoc routing schemes. Another wellknown on de mand scheme is DSR . dif ferent strategy based on geography is explored by GPSR . If all nodes know their geographic posi tions forwarding to destination can proceed without route computation by sim ply heading in the right direction and circling back to escape any dead ends. Which protocols win out will depend on the kinds of ad hoc networks that prove useful in practice. Too many packets present in the network causes packet delay and loss that degrades performance. This situation is called congestion. The network and transport layers share the responsibility for handling congestion. Since con gestion occurs within the network it is the network layer that directly experiences it and must ultimately determine what to do with the excess packets. However the most effective way to control congestion is to reduce the load that the tran sport layer is placing on the network. This requires the network and transport lay gestion. In Chap. 6 we will complete the topic by covering the transport aspects hosts send into the network is well within its carrying capacity the number deliv ered is proportional to the number sent. If twice as many are sent twice as many are delivered. However as the offered load approaches the carrying capacity bursts of traffic occasionally fill up the buffers inside routers and some packets are lost. These lost packets consume some of the capacity so the number of de livered packets falls below the ideal curve. The network is now congested. Unless the network is well designed it may experience congestion collapse in which performance plummets as the offered load increases beyond the capaci ty. This can happen because packets can be sufficiently delayed inside the net work that they are no longer useful when they leave the network. For example in the early Internet the time packet spent waiting for backlog of packets ahead of it to be sent over slow 56kbps link could reach the maximum time it was al lowed to remain in the network. It then had to be thrown away. different failure mode occurs when senders retransmit packets that are greatly delayed thinking that they have been lost. In this case copies of the same packet will be delivered by the network again wasting its capacity. To capture these factors the yaxis of ered by the network. We would like to design networks that avoid congestion where possible and do not suffer from congestion collapse if they do become congested. Unfortunate ly congestion cannot wholly be avoided. If all of sudden streams of packets begin arriving on three or four input lines and all need the same output line queue will build up. If there is insufficient memory to hold all of them packets will be lost. Adding more memory may help up to point but Nagle real ized that if routers have an infinite amount of memory congestion gets worse not better. This is because by the time packets get to the front of the queue they have already timed out and duplicates have been sent. This makes matters worse not betterit leads to congestion collapse. Lowbandwidth links or routers that process packets more slowly than the line rate can also become congested. In this case the situation can be improved by directing some of the traffic away from the bottleneck to other parts of the net work. Eventually however all regions of the network will be congested. In this situation there is no alternative but to shed load or build faster network. It is worth pointing out the difference between congestion control and flow control as the relationship is very subtle one. Congestion control has to do with volving the behavior of all the hosts and routers. Flow control in contrast relates to the traffic between particular sender and particular receiver. Its job is to make sure that fast sender cannot continually transmit data faster than the re ceiver is able to absorb it. To see the difference between these two concepts consider network made up of 100Gbps fiber optic links on which supercomputer is trying to force feed large file to personal computer that is capable of handling only 1 Gbps. Al though there is no congestion flow control is needed to force the supercomputer to stop frequently to give the personal com puter chance to breathe. At the other extreme consider network with 1Mbps lines and 1000 large computers half of which are trying to transfer files at 100 kbps to the other half. Here the problem is not that of fast senders overpowering slow receivers but that the total offered traffic exceeds what the network can handle. The reason congestion control and flow control are often confused is that the can get slow down message either because the receiver cannot handle the load or because the network cannot handle it. We will come back to this point in We will start our study of congestion control by looking at the approaches that can be used at different time scales. Then we will look at approaches to pre venting congestion from occurring in the first place followed by approaches for coping with it once it has set in. The presence of congestion means that the load is greater than tions are usually applied on different time scales to either prevent congestion or react to it once it has occurred. The most basic way to avoid congestion is to build network that is well matched to the traffic that it carries. If there is lowbandwidth link on the path along which most traffic is directed congestion is likely. Sometimes resources can be added dynamically when there is serious congestion for example turning on spare routers or enabling lines that are normally used only as backups or purchasing bandwidth on the open market. More often links and routers that are regularly heavily utilized are upgraded at the earli est opportunity. This is called provisioning and happens on time scale of months driven by longterm traffic trends. To make the most of the existing network capacity routes can be tailored to traffic patterns that change during the day as network users wake and sleep in dif ferent time zones. For example routes may be changed to shift traffic away from heavily used paths by changing the shortest path weights. Some local radio sta make it possible for their mobile listeners to route their packets around hotspots. This is called trafficaware routing. Splitting traffic across multiple paths is also helpful. However sometimes it is not possible to increase capacity. The only way then to beat back the congestion is to decrease the load. In virtualcircuit net work new connections can be refused if they would cause the network to become congested. This is called admission control. At finer granularity when congestion is imminent the network can deliver feedback to the sources whose traffic flows are responsible for the problem. The network can request these sources to throttle their traffic or it can slow down the Two difficulties with this approach are how to identify the onset of conges tion and how to inform the source that needs to slow down. To tackle the first cases rising numbers indicate growing congestion. the sources. For scheme to work correctly the time scale must be adjusted care fully. If every time two packets arrive in row router yells STOP and every time router is idle for 20 μsec it yells GO the system will oscillate wildly and never converge. On the other hand if it waits 30 minutes to make sure before saying anything the congestioncontrol mechanism will react too sluggishly to be of any use. Delivering timely feedback is nontrivial matter. An added concern is having routers send more messages when the network is already congested. cannot deliver. The general name for this is load shedding. good policy for choosing which packets to discard can help to prevent congestion collapse. The first approach we will examine is trafficaware routing. The routing schemes we looked at in Sec 5.2 used fixed link weights. These schemes adapted to changes in topology but not to changes in load. The goal in taking load into account when computing routes is to shift traffic away from hotspots that will be the first places in the network to experience congestion. The most direct way to do this is to set the link weight to be function of the link bandwidth and propagation delay plus the measured load or average queuing delay. Leastweight paths will then favor paths that are more lightly loaded all else being equal. Trafficaware routing was used in the early Internet according to this model . However there is peril. Consider the network of CF and EI. Suppose that most of the traffic between East and West is using link CF and as result this link is heavily loaded with long delays. Including queue ing delay in the weight used for the shortest path calculation will make EI more attractive. After the new routing tables have been installed most of the EastWest CF will appear to be the shortest path. As result the routing tables may oscil If load is ignored and only bandwidth and propagation delay are considered this problem does not occur. Attempts to include load but change weights within narrow range only slow down routing oscillations. Two techniques can contri bute to successful solution. The first is multipath routing in which there can be multiple paths from source to destination. In our example this means that the traffic can be spread across both of the East to West links. The second one is for the routing scheme to shift traffic across routes slowly enough that it is able to converge as in the scheme of Gallagher . Given these difficulties in the Internet routing protocols do not generally ad just their routes depending on the load. Instead adjustments are made outside the routing protocol by slowly changing its inputs. This is called traffic engineering. One technique that is widely used in virtualcircuit networks to keep conges tion at bay is admission control. The idea is simple do not set up new virtual circuit unless the network can carry the added traffic without becoming congest ed. Thus attempts to set up virtual circuit may fail. This is better than the alter native as letting more people in when the network is busy just makes matters worse. By analogy in the telephone system when switch gets overloaded it practices admission control by not giving dial tones. The trick with this approach is working out when new virtual circuit will lead to congestion. The task is straightforward in the telephone network because of the fixed bandwidth of calls . However vir tual circuits in computer networks come in all shapes and sizes. Thus the circuit must come with some characterization of its traffic if we are to apply admission Traffic is often described in terms of its rate and shape. The problem of how to describe it in simple yet meaningful way is difficult because traffic is typi cally burstythe average rate is only half the story. For example traffic that varies while browsing the Web is more difficult to handle than streaming movie with the same longterm throughput because the bursts of Web traffic are more likely to congest routers in the network. commonly used descriptor that cap tures this effect is the leaky bucket or token bucket. leaky bucket has two pa rameters that bound the average rate and the instantaneous burst size of traffic. Since leaky buckets are widely used for quality of service we will go over them in detail in Sec. 5.4. Armed with traffic descriptions the network can decide whether to admit the new virtual circuit. One possibility is for the network to reserve enough capacity along the paths of each of its virtual circuits that congestion will not occur. In this case the traffic description is service agreement for what the network will guar antee its users. We have prevented congestion but veered into the related topic of Even without making guarantees the network can use traffic descriptions for admission control. The task is then to estimate how many circuits will fit within the carrying capacity of the network without congestion. Suppose that virtual cir cuits that may blast traffic at rates up to 10 Mbps all pass through the same 100 Mbps physical link. How many circuits should be admitted Clearly 10 circuits can be admitted without risking congestion but this is wasteful in the normal case since it may rarely happen that all 10 are transmitting full blast at the same time. In real networks measurements of past behavior that capture the statistics of transmissions can be used to estimate the number of circuits to admit to trade bet ter performance for acceptable risk. Admission control can also be combined with trafficaware routing by consid ering routes around traffic hotspots as part of the setup procedure. For example ed as indicated. congested. virtual circuit from to is also shown. Suppose that host attached to router wants to set up connection to host attached to router . Normally this connection would pass through one of the congested routers. To avoid this situation we can redraw the network as shown in shows possible route for the virtual circuit that avoids the congested routers. Shaikh et al. give design for this kind of loadsensitive routing. In the Internet and many other computer networks senders adjust their trans missions to send as much traffic as the network can readily deliver. In this setting the network aims to operate just before the onset of congestion. When congestion is imminent it must tell the senders to throttle back their transmissions and slow down. This feedback is business as usual rather than an exceptional situation. The term congestion avoidance is sometimes used to contrast this operating point with the one in which the network has become congested. Let us now look at some approaches to throttling traffic that can be used in both datagram networks and virtualcircuit networks. Each approach must solve ideally before it has arrived. To do so each router can continuously monitor the resources it is using. Three possibilities are the utilization of the output links the buffering of queued packets inside the router and the number of packets that are lost due to insufficient buffering. Of these possibilities the second one is the most useful. Averages of utilization do not directly account for the burstiness of most traffica utilization of 50 may be low for smooth traffic and too high for highly variable traffic. Counts of packet losses come too late. Congestion has al ready set in by the time that packets are lost. The queueing delay inside routers directly captures any congestion experi enced by packets. It should be low most of time but will jump when there is burst of traffic that generates backlog. To maintain good estimate of the queueing delay sample of the instantaneous queue length can be made per new αd old where the constant α determines how fast the router forgets recent history. This is called an EWMA . It smoothes out fluctuations and is equivalent to lowpass filter. Whenever moves above the threshold the router notes the onset of congestion. The second problem is that routers must deliver timely feedback to the send ers that are causing the congestion. Congestion is experienced in the network but relieving congestion requires action on behalf of the senders that are using the net work. To deliver feedback the router must identify the appropriate senders. It must then warn them carefully without sending many more packets into the al ready congested network. Different schemes use different feedback mechanisms as we will now describe. The most direct way to notify sender of congestion is to tell it directly. In this approach the router selects congested packet and sends choke packet back to the source host giving it the destination found in the packet. The original more choke packets farther along the path and then forwarded in the usual way. To avoid increasing load on the network during time of congestion the router may only send choke packets at low rate. When the source host gets the choke packet it is required to reduce the traffic sent to the specified destination for example by 50. In datagram network simply picking packets at random when there is congestion is likely to cause choke packets to be sent to fast senders because they will have the most packets in the queue. The feedback implicit in this protocol can help prevent congestion yet not throttle any sender unless it causes trouble. For the same reason it is like ly that multiple choke packets will be sent to given host and destination. The host should ignore these additional chokes for the fixed time interval until its reduction in traffic takes effect. After that period further choke packets indicate that the network is still congested. An example of choke packet used in the early Internet is the SOURCE QUENCH message . It never caught on though partly because the circumstances in which it was generated and the effect it had were not clearly specified. The modern Internet uses an alternative notification design that we will Instead of generating additional packets to warn of congestion router can is experiencing congestion. When the network delivers the packet the destination can note that there is congestion and inform the sender when it sends reply pack et. The sender can then throttle its transmissions as before. This design is called ECN and is used in the Internet . It is refinement of early congestion signaling protocols notably the binary feedback scheme of Ramakrishnan and Jain that was used in the DECNET architecture. Two bits in the IP packet they pass through is congested that router will then mark the packet as having experienced congestion as it is forwarded. The destination will then echo any marks back to the sender as an explicit congestion signal in its next reply packet. IP level . The sender must then throttle its transmissions as in the At high speeds or over long distances many new packets may be transmitted after congestion has been signaled because of the delay before the signal takes ef msec for choke packet to get back to San Francisco to tell it to slow down. An ECN indication will take even longer because it is delivered via the destination. Choke packet propagation is illustrated as the second third and fourth steps in the host in San Francisco completely shuts down immediately the 6.2 megabits in the pipe will continue to pour in and have to be dealt with. Only in the seventh An alternative approach is to have the choke packet take effect at every hop it choke packet reaches is required to reduce the flow to . Doing so will re quire to devote more buffers to the connection since the source is still sending away at full blast but it gives immediate relief like headache remedy in television commercial. In the next step the choke packet reaches which tells to reduce the flow to . This action puts greater demand on Es buffers but genuinely slows down. The net effect of this hopbyhop scheme is to provide quick relief at the point of congestion at the price of using up more buffers upstream. In this way con gestion can be nipped in the bud without losing any packets. The idea is dis cussed in detail by Mishra et al. When none of the above methods make the congestion disappear routers can bring out the heavy artillery load shedding. Load shedding is fancy way of they just throw them away. The term comes from the world of electrical power generation where it refers to the practice of utilities intentionally blacking out certain areas to save the entire grid from collapsing on hot summer days when the demand for electricity greatly exceeds the supply. The key question for router drowning in packets is which packets to drop. The preferred choice may depend on the type of applications that use the network. For file transfer an old packet is worth more than new one. This is because dropping packet 6 and keeping packets 7 through 10 for example will only force the receiver to do more work to buffer data that it cannot yet use. In contrast for realtime media new packet is worth more than an old one. This is because packets become useless if they are delayed and miss the time at which they must be played out to the user. The former policy is often called wine and the latter is often called milk because most people would rather drink new milk and old wine than the alternative. More intelligent load shedding requires cooperation from the senders. An ex ample is packets that carry routing information. These packets are more important than regular data packets because they establish routes if they are lost the net work may lose connectivity. Another example is that algorithms for compressing video like MPEG periodically transmit an entire frame and then send subsequent et that affects each hop it passes through. frames as differences from the last full frame. In this case dropping packet that is part of difference is preferable to dropping one that is part of full frame be cause future packets depend on the full frame. To implement an intelligent discard policy applications must mark their pack ets to indicate to the network how important they are. Then when packets have to be discarded routers can first drop packets from the least important class then the next most important class and so on. Of course unless there is some significant incentive to avoid marking every packet as VERY IMPORTANTNEVER EVER DISCARD nobody will do it. Often accounting and money are used to discourage frivolous marking. For ex ample the network might let senders send faster than the service they purchased allows if they mark excess packets as low priority. Such strategy is actually not bad idea because it makes more efficient use of idle resources allowing hosts to use them as long as nobody else is interested but without establishing right to Dealing with congestion when it first starts is more effective than letting it gum up the works and then trying to deal with it. This observation leads to an in teresting twist on load shedding which is to discard packets before all the buffer space is really exhausted. The motivation for this idea is that most Internet hosts do not yet get conges tion signals from routers in the form of ECN. Instead the only reliable indication of congestion that hosts get from the network is packet loss. After all it is diffi cult to build router that does not drop packets when it is overloaded. Transport protocols such as TCP are thus hardwired to react to loss as congestion slowing down the source in response. The reasoning behind this logic is that TCP was de signed for wired networks and wired networks are very reliable so lost packets are mostly due to buffer overruns rather than transmission errors. Wireless links must recover transmission errors at the link layer to work well with TCP. This situation can be exploited to help reduce congestion. By having routers drop packets early before the situation has become hopeless there is time for the source to take action before it is too late. popular algorithm for doing this is called RED . To deter mine when to start discarding routers maintain running average of their queue lengths. When the average queue length on some link exceeds threshold the link is said to be congested and small fraction of the packets are dropped at ran dom. Picking packets at random makes it more likely that the fastest senders will see packet drop this is the best option since the router cannot tell which source is causing the most trouble in datagram network. The affected sender will notice the loss when there is no acknowledgement and then the transport protocol will slow down. The lost packet is thus delivering the same message as choke packet but implicitly without the router sending any explicit signal. RED routers improve performance compared to routers that drop packets only when their buffers are full though they may require tuning to work well. For ex ample the ideal number of packets to drop depends on how many senders need to be notified of congestion. However ECN is the preferred option if it is available. It works in exactly the same manner but delivers congestion signal explicitly rather than as loss RED is used when hosts cannot receive explicit signals. congestion and improve network performance. However there are applications that demand stronger performance guarantees from the network than the best that could be done under the circumstances. Multimedia applica tions in particular often need minimum throughput and maximum latency to now with sharper focus on ways to provide quality of service that is matched to application needs. This is an area in which the Internet is undergoing longterm An easy solution to provide good quality of service is to build network with enough capacity for whatever traffic will be thrown at it. The name for this solu tion is overprovisioning. The resulting network will carry application traffic without significant loss and assuming decent routing scheme will deliver pack ets with low latency. Performance doesnt get any better than this. To some extent the telephone system is overprovisioned because it is rare to pick up tele phone and not get dial tone instantly. There is simply so much capacity avail able that demand can almost always be met. The trouble with this solution is that it is expensive. It is basically solving problem by throwing money at it. Quality of service mechanisms let network with less capacity meet application requirements just as well at lower cost. Moreover overprovisioning is based on expected traffic. All bets are off if the traffic pattern changes too much. With quality of service mechanisms the net work can honor the performance guarantees that it makes even when traffic spikes at the cost of turning down some requests. 1. What applications need from the network. 2. How to regulate the traffic that enters the network. 3. How to reserve resources at routers to guarantee performance. 4. Whether the network can safely accept more traffic. techniques have been developed for use at the network layer. Integrated Services and Differentiated Services. stream of packets from source to destination is called flow . flow might be all the packets of connection in connectionoriented network or all the packets sent from one process to another process in con nectionless network. The needs of each flow can be characterized by four pri mary parameters bandwidth delay jitter and loss. Together these determine the QoS the flow requires. Several common applications and the stringency of their network re manding than application requirements in those cases that the application can im prove on the service provided by the network. In particular networks do not need to be lossless for reliable file transfer and they do not need to deliver packets with identical delays for audio and video playout. Some amount of loss can be repaired with retransmissions and some amount of jitter can be smoothed by buffering packets at the receiver. However there is nothing applications can do to remedy the situation if the network provides too little bandwidth or too much delay. The applications differ in their bandwidth needs with email audio in all forms and remote login not needing much but file sharing and video in all forms needing great deal. More interesting are the delay requirements. File transfer applications in cluding email and video are not delay sensitive. If all packets are delayed uni formly by few seconds no harm is done. Interactive applications such as Web surfing and remote login are more delay sensitive. Realtime applications such as telephony and videoconferencing have strict delay requirements. If all the words in telephone call are each delayed by too long the users will find the con nection unacceptable. On the other hand playing audio or video files from ser ver does not require low delay. The variation in the delay or packet arrival times is ets arriving with irregular time intervals between them. Remote login is some connection suffers much jitter. Video and especially audio are extremely sensi tive to jitter. If user is watching video over the network and the frames are all delayed by exactly 2.000 seconds no harm is done. But if the transmission time varies randomly between 1 and 2 seconds the result will be terrible unless the ap plication hides the jitter. For audio jitter of even few milliseconds is clearly The first four applications have more stringent requirements on loss than aud io and video because all bits must be delivered correctly. This goal is usually chieved with retransmissions of packets that are lost in the network by the tran sport layer. This is wasted work it would be better if the network refused packets it was likely to lose in the first place. Audio and video applications can tolerate some lost packets without retransmission because people do not notice short pauses or occasional skipped frames. categories of QoS. An influential example comes from ATM networks which were once part of grand vision for networking but have since become niche 1. Constant bit rate . 2. Realtime variable bit rate . 3. Nonrealtime variable bit rate . 4. Available bit rate . These categories are also useful for other purposes and other networks. Constant bit rate is an attempt to simulate wire by providing uniform bandwidth and uniform delay. Variable bit rate occurs when video is compressed with some frames compressing more than others. Sending frame with lot of detail in it may require sending many bits whereas shot of white wall may compress ex tremely well. Movies on demand are not actually real time because few seconds of video can easily be buffered at the receiver before playback starts so jitter on the network merely causes the amount of storedbutnotplayed video to vary. Available bit rate is for applications such as email that are not sensitive to delay or jitter and will take what bandwidth they can get. Before the network can make QoS guarantees it must know what traffic is being guaranteed. In the telephone network this characterization is simple. For example voice call needs 64 kbps and consists of one 8bit sample every 125 μsec. However traffic in data networks is bursty. It typi cally arrives at nonuniform rates as the traffic rate varies users interact with applications agree on certain traffic pattern for that flow. In effect the customer says to the provider My transmission pattern will look like this can Sometimes this agreement is called an SLA es pecially when it is made over aggregate flows and long periods of time such as all of the traffic for given customer. As long as the customer fulfills her part of the bargain and only sends packets according to the agreedon contract the pro vider promises to deliver them all in timely fashion. Traffic shaping reduces congestion and thus helps the network live up to its can tell if the customer is following the agreement and what to do if the customer is not. Packets in excess of the agreed pattern might be dropped by the network or they might be marked as having lower priority. Monitoring traffic flow is called traffic policing. Shaping and policing are not so important for peertopeer and other transfers that will consume any and all available bandwidth but they are of great impor tance for realtime data such as audio and video connections which have We have already seen one way to limit the amount of data an application sends the sliding window which uses one parameter to limit how much data is in transit at any given time which indirectly limits the rate. Now we will look at more general way to characterize traffic with the leaky bucket and token bucket algorithms. The formulations are slightly different but give an equivalent result. Try to imagine bucket with small hole in the bottom as illustrated in constant rate when there is any water in the bucket and zero when the bucket is empty. Also once the bucket is full to capacity any additional water enter ing it spills over the sides and is lost. This bucket can be used to shape or police packets entering the network as interface containing leaky bucket. To send packet into the network it must be possible to put more water into the bucket. If packet arrives when the bucket is full the packet must either be queued until enough water leaks out to hold it or be discarded. The former might happen at host shaping its traffic for the network as part of the operating system. The latter might happen in hardware at provider network interface that is policing traffic entering the network. This technique was proposed by Turner and is called the leaky bucket algorithm. different but equivalent formulation is to imagine the network interface as and the bucket has capacity of as before. Now to send packet we must be bucket . No more than fixed number of tokens can accumulate in the bucket and if the bucket is empty we must wait until more tokens arrive before we can send another packet. This algorithm is called the token bucket algorithm. Leaky and token buckets limit the longterm rate of flow but allow short term bursts up to maximum regulated length to pass through unaltered and without suffering any artificial delays. Large bursts will be smoothed by leaky bucket traffic shaper to reduce congestion in the network. As an example imag ine that computer can produce data at up to 1000 Mbps and that the first link of the network also runs at this speed. The pattern of traffic rate over one second is 200 Mbps even though the host sends burst of 16000 KB at the top speed of 1000 Mbps . Bucket always empty ing with rate 200 Mbps and capacity 16000 KB 9600 KB and 0 KB. Now suppose that the routers can accept data at the top speed only for short intervals until their buffers fill up. The buffer size is 9600 KB smaller than the Mbps . The implica tion is that if traffic is sent in this pattern some of it will be dropped in the net work because it does not fit into the buffers at routers. To avoid this packet loss we can shape the traffic at the host with token bucket. If we use rate of 200 Mbps and capacity of 9600 KB the traffic will fall within what the network can handle. The output of this token bucket is while until it has drained the bucket. Then it has to cut back to 200 Mbps until the burst has been sent. The effect is to spread out the burst over time because it was 29. It starts off full and is depleted by the initial burst. When it reaches zero new packets can be sent only at the rate at which the buffer is filling there can be no more bursts until the bucket has recovered. The bucket fills when no traffic is being sent and stays flat when traffic is being sent at the fill rate. of token bucket with 200 Mbps and capacity of 0. This is the extreme case in which the traffic has been completely smoothed. No bursts are allowed and the traffic enters the network at steady rate. The corresponding bucket level shown into the network and there is always packet waiting to be sent when it is allow Mbps and capacity of 16000 KB. This is the smallest token bucket through which the traffic passes unaltered. It might be used at router in the network to police the traffic that the host sends. If the host is sending traffic that conforms to the token bucket on which it has agreed with the network the traffic will fit through that same token bucket run at the router at the edge of the network. If the host sends at faster or burstier rate the token bucket will run out of water. If this happens traffic policer will know that the traffic is not as described. It will then either drop the excess packets or lower their priority depending on the design of the network. In our example the bucket empties only momentarily at the end of the initial burst then recovers enough for the next burst. Leaky and token buckets are easy to implement. We will now describe the operation of token bucket. Even though we have described water flowing con tinuously into and out of the bucket real implementations must work with discrete quantities. token bucket is implemented with counter for the level of the bucket. The counter is advanced by ΔT units at every clock tick of ΔT seconds. This would be 200 Kbit every 1 msec in our example above. Every time unit of traffic is sent into the network the counter is decremented and traffic may be sent until the counter reaches zero. When the packets are all the same size the bucket level can just be counted in packets . However often variable sized packets are being used. In this case the bucket level is counted in bytes. If the residual byte count is too low to send large packet the packet must wait until the next tick . Calculating the length of the maximum burst is slightly tricky. It is longer than just 9600 KB divided by 125 MBsec because while the burst is being output more tokens arrive. If we call the burst length sec. the maximum output rate bytessec the token bucket capacity bytes and the token arrival rate bytessec we can see that an output burst contains maxi mum of RS bytes. We also know that the number of bytes in maximum speed burst of length seconds is MS. Hence we have KB 125 MBsec and 25 MBsec we get burst time of about 94 msec. potential problem with the token bucket algorithm is that it reduces large bursts down to the longterm rate . It is frequently desirable to reduce the peak rate but without going down to the longterm rate . One way to get smoother traffic is to insert second token bucket after the first one. The rate of the second bucket should be much higher than the first one. Basically the first bucket charac terizes the traffic fixing its average rate but allowing some bursts. The second bucket reduces the peak rate at which the bursts are sent into the network. For ex ample if the rate of the second token bucket is set to be 500 Mbps and the capaci ty is set to 0 the initial burst will enter the network at peak rate of 500 Mbps which is lower than the 1000 Mbps rate we had previously. Using all of these buckets can be bit tricky. When token buckets are used for traffic shaping at hosts packets are queued and delayed until the buckets permit them to be sent. When token buckets are used for traffic policing at routers in the network the algorithm is simulated to make sure that no more packets are sent than permitted. Nevertheless these tools provide ways to shape the network traf fic into more manageable forms to assist in meeting qualityofservice re Being able to regulate the shape of the offered traffic is good start. Howev er to provide performance guarantee we must reserve sufficient resources along the route that the packets take through the network. To do this we are as suming that the packets of flow follow the same route. Spraying them over rout ers at random makes it hard to guarantee anything. As consequence something similar to virtual circuit has to be set up from the source to the destination and all the packets that belong to the flow must follow this route. Algorithms that allocate router resources among the packets of flow and be tween competing flows are called packet scheduling algorithms. Three different kinds of resources can potentially be reserved for different flows 2. Buffer space. The first one bandwidth is the most obvious. If flow requires 1 Mbps and the outgoing line has capacity of 2 Mbps trying to direct three flows through that line is not going to work. Thus reserving bandwidth means not oversubscribing any output line. second resource that is often in short supply is buffer space. When packet arrives it is buffered inside the router until it can be transmitted on the chosen outgoing line. The purpose of the buffer is to absorb small bursts of traffic as the flows contend with each other. If no buffer is available the packet has to be dis carded since there is no place to put it. For good quality of service some buffers might be reserved for specific flow so that flow does not have to compete for buffers with other flows. Up to some maximum value there will always be buffer available when the flow needs one. to process packet so router can process only certain number of packets per second. While modern routers are able to process most packets quickly some kinds of packets require greater CPU processing such as the ICMP packets we will describe in Sec. 5.6. Making sure that the CPU is not overloaded is needed to ensure timely processing of these packets. Packet scheduling algorithms allocate bandwidth and other router resources by determining which of the buffered packets to send on the output line next. We already described the most straightforward scheduler when explaining how rout ers work. Each router buffers packets in queue for each output line until they can be sent and they are sent in the same order that they arrived. This algorithm is known as FIFO or equivalently FCFS . One of the first ones was the fair queueing algorithm devised by Nagle . The essence of this algorithm is that routers have separate queues one for each flow for given output line. When the line becomes idle the router scans next queue. In this way with hosts competing for the output line each host gets to send one out of every packets. It is fair in the sense that all flows get to send packets at the same rate. Sending more packets will not improve this rate. Although start the algorithm has flaw it gives more bandwidth to hosts that use large packets than to hosts that use small packets. Demers et al. suggested an improvement in which the roundrobin is done in such way as to simulate bytebybyte roundrobin instead of packetbypacket roundrobin. The trick is to compute virtual time that is the number of the round at which each packet would finish being sent. Each round drains byte from all of the queues that have data to send. The packets are then sorted in order of their fin ishing times and sent in that order. This algorithm and an example of finish times for packets arriving in three will finish is simply rounds after the start time. The start time is either the fin ish time of the previous packet or the arrival time of the packet if the queue is empty when it arrives. top two queues packets arrive in the order and . Packet arrives at round 0 and is 8 bytes long so its finish time is round 8. Similarly the finish time byterounds after it starts when finishes or 20. Similarly the finish time for is 16. In the absence of new arrivals the relative sending order is even though arrived after . It is possible that another small packet will arrive on the top flow and obtain finish time before . It will only jump ahead of if the transmission of that packet has not started. Fair queueing does not preempt pack ets that are currently being transmitted. Because packets are sent in their entirety fair queueing is only an approximation of the ideal bytebybyte scheme. But it is very good approximation staying within one packet transmission of the ideal scheme at all times. One shortcoming of this algorithm in practice is that it gives all hosts the same priority. In many situations it is desirable to give for example video ser vers more bandwidth than say file servers. This is easily possible by giving the video server two or more bytes per round. This modified algorithm is called WFQ . Letting the number of bytes per round be the weight of flow we can now give the formula for computing the finish time Fi maxLi where Ai is the arrival time Fi is the finish time and Li is the length of packet . Another practical consideration is implementation complexity. WFQ requires that packets be inserted by their finish time into sorted queue. With flows this is at best an operation per packet which is difficult to achieve for many flows in highspeed routers. Shreedhar and Varghese describe an approxi mation called deficit round robin that can be implemented very efficiently with only operations per packet. WFQ is widely used given this approximation. Other kinds of scheduling algorithms exist too. simple example is priority scheduling in which each packet is marked with priority. Highpriority packets are always sent before any lowpriority packets that are buffered. Within prior ity packets are sent in FIFO order. However priority scheduling has the disad vantage that burst of highpriority packets can starve lowpriority packets which may have to wait indefinitely. WFQ often provides better alternative. By giving the highpriority queue large weight say 3 highpriority packets will often go through short line yet some fraction of low priority packets will continue to be sent even when there is high priority traffic. high and low priority system is essentially twoqueue WFQ system in which the high priority has infinite weight. in timestamp order. Clark et al. describe design in which the timestamp records how far the packet is behind or ahead of schedule as it is sent through sequence of routers on the path. Packets that have been queued behind other packets at router will tend to be behind schedule and the packets that have been serviced first will tend to be ahead of schedule. Sending packets in order of their timestamps has the beneficial effect of speeding up slow packets while at the same time slowing down fast packets. The result is that all packets are delivered by the network with more consistent delay. We have now seen all the necessary elements for QoS and it is time to put them together to actually provide it. QoS guarantees are established through the process of admission control. We first saw admission control used to control con gestion which is performance guarantee albeit weak one. The guarantees we are considering now are stronger but the model is the same. The user offers flow with an accompanying QoS requirement to the network. The network then decides whether to accept or reject the flow based on its capacity and the commit ments it has made to other flows. If it accepts the network reserves capacity in advance at routers to guarantee QoS when traffic is sent on the new flow. The reservations must be made at all of the routers along the route that the packets take through the network. Any routers on the path without reservations might become congested and single congested router can break the QoS guaran tee. Many routing algorithms find the single best path between each source and each destination and send all traffic over the best path. This may cause some flows to be rejected if there is not enough spare capacity along the best path. QoS route for the flow that has excess capacity. This is called QoS routing. Chen and the traffic for each destination over multiple paths to more easily find excess ca pacity. simple method is for routers to choose equalcost paths and to divide the traffic equally or in proportion to the capacity of the outgoing links. However more sophisticated algorithms are also available . Given path the decision to accept or reject flow is not simple matter of comparing the resources requested by the flow with the routers excess capacity in those three dimensions. It is little more compli cated than that. To start with although some applications may know about their bandwidth requirements few know about buffers or CPU cycles so at the mini mum different way is needed to describe flows and translate this description to router resources. We will get to this shortly. Next some applications are far more tolerant of an occasional missed dead line than others. The applications must choose from the type of guarantees that the network can make whether hard guarantees or behavior that will hold most of the time. All else being equal everyone would like hard guarantees but the diffi culty is that they are expensive because they constrain worst case behavior. Guar antees for most of the packets are often sufficient for applications and more flows with this guarantee can be supported for fixed capacity. framessec may be willing to drop back to 25 framessec if there is not enough free bandwidth to support 30 framessec. Similarly the number of pixels per frame audio bandwidth and other properties may be adjustable. Because many parties may be involved in the flow negotiation flows must be de scribed accurately in terms of specific parameters that can be negotiated. set of such parameters is called flow specification. Typically the sender produces flow specification proposing the parameters it would like to use. As the specification propagates along the route each router examines it and modifies the parameters as need be. The modifications can only reduce the flow not increase it . When it gets to the other end the parameters can be established. As an example of what can be in flow specification consider the example of rameters the token bucket rate and token bucket size use token bucket to give the maximum sustained rate the sender may transmit averaged over long time interval and the largest burst it can send over short time interval. The third parameter the peak data rate is the maximum transmission rate tolerated even for brief time intervals. The sender must never exceed this rate even for short bursts. The last two parameters specify the minimum and maximum packet sizes in size is useful because processing each packet takes some fixed time no matter how short. router may be prepared to handle 10000 packetssec of 1 KB each but not be prepared to handle 100000 packetssec of 50 bytes each even though this represents lower data rate. The maximum packet size is important due to internal network limitations that may not be exceeded. For example if part of the path goes over an Ethernet the maximum packet size will be restricted to no more than 1500 bytes no matter what the rest of the network can handle. An interesting question is how router turns flow specification into set of specific resource reservations. At first glance it might appear that if router has link that runs at say 1 Gbps and the average packet is 1000 bits it can process 1 million packetssec. This observation is not the case though because there will always be idle periods on the link due to statistical fluctuations in the load. If the link needs every bit of capacity to get its work done idling for even few bits creates backlog it can never get rid of. Even with load slightly below the theoretical capacity queues can build up and delays can occur. Consider situation in which packets arrive at random with mean arrival rate of λ packetssec. The packets have random lengths and can be sent on the link with mean service rate of μ packetssec. Under the assumption that both the arrival and service distributions are Poisson distributions it can be proven using queueing theory that the mean delay experienced by where ρ λμ is the CPU utilization. The first factor 1μ is what the service time would be in the absence of competition. The second factor is the slowdown due to competition with other flows. For example if λ 950000 packetssec and μ 1000000 packetssec then ρ 0.95 and the mean delay experienced by each packet will be 20 μsec instead of 1 μsec. This time accounts for both the queue ing time and the service time as can be seen when the load is very low . If there are say 30 routers along the flows route queueing delay alone will ac count for 600 μsec of delay. One method of relating flow specifications to router resources that correspond to bandwidth and delay performance guarantees is given by Parekh and Gallagher . It is based on traffic sources shaped by token buckets and WFQ at routers. Each flow is given WFQ weight large enough to drain its Mbps and the router and output link have capacity of 1 Gbps the weight for the flow must be greater than 11000th of the total of the weights for all of the flows at that router for the output link. This guarantees the flow minimum bandwidth. If it cannot be given large enough rate the flow cannot be admitted. The largest queueing delay the flow will see is function of the burst size of the token bucket. Consider the two extreme cases. If the traffic is smooth without any bursts packets will be drained from the router just as quickly as they arrive. There will be no queueing delay . On the other hand if the traffic is saved up in bursts then maximumsize burst may arrive at the router all at once. In this case the maximum queueing delay will be the time taken to drain this burst at the guaranteed bandwidth or BR . If this delay is too large the flow must request more band width from the network. These guarantees are hard. The token buckets bound the burstiness of the source and fair queueing isolates the bandwidth given to different flows. This means that the flow will meet its bandwidth and delay guarantees regardless of how the other competing flows behave at the router. Those other flows cannot break the guarantee even by saving up traffic and all sending at once. Moreover the result holds for path through multiple routers in any network topology. Each flow gets minimum bandwidth because that bandwidth is guar anteed at each router. The reason each flow gets maximum delay is more sub tle. In the worst case that burst of traffic hits the first router and competes with the traffic of other flows it will be delayed up to the maximum delay of . How ever this delay will also smooth the burst. In turn this means that the burst will incur no further queueing delays at later routers. The overall queueing delay will Between 1995 and 1997 IETF put lot of effort into devising an architecture for streaming multimedia. This work resulted in over two dozen RFCs starting with RFCs 22052212. The generic name for this work is integrated services. It was aimed at both unicast and multicast applications. An example of the former is single user streaming video clip from news site. An example of the latter is collection of digital television stations broadcasting their programs as streams on multicast since unicast is special case of multicast. In many multicast applications groups can change membership dynamically for example as people enter video conference and then get bored and switch to soap opera or the croquet channel. Under these conditions the approach of hav ing the senders reserve bandwidth in advance does not work well since it would require each sender to track all entries and exits of its audience. For system de signed to transmit television with millions of subscribers it would not work at all. The main part of the integrated services architecture that is visible to the users of the network is RSVP. It is described in RFCs 22052210. This protocol is used for making the reservations other protocols are used for sending the data. RSVP allows multiple senders to transmit to multiple groups of receivers permits individual receivers to switch channels freely and optimizes bandwidth use while at the same time eliminating congestion. In its simplest form the protocol uses multicast routing using spanning trees as discussed earlier. Each group is assigned group address. To send to group sender puts the groups address in its packets. The standard multicast routing al gorithm then builds spanning tree covering all group members. The routing al gorithm is not part of RSVP. The only difference from normal multicasting is little extra information that is multicast to the group periodically to tell the routers along the tree to maintain certain data structures in their memories. cast senders and hosts 3 4 and 5 are multicast receivers. In this example the senders and receivers are disjoint but in general the two sets may overlap. The multicast spanning tree for host 2. To get better reception and eliminate congestion any of the receivers in group can send reservation message up the tree to the sender. The message is propagated using the reverse path forwarding algorithm discussed earlier. At each hop the router notes the reservation and reserves the necessary bandwidth. We make this reservation. failure. By the time the message gets back to the source bandwidth has been re served all the way from the sender to the receiver making the reservation request along the spanning tree. requested channel to host 1. Once it has been established packets can flow from 1 to 3 without congestion. Now consider what happens if host 3 next reserves channel to the other sender host 2 so the user can watch two television that two separate channels are needed from host 3 to router because two inde Bandwidth reserved second channel to host 2. Host 5 requests channel to host 1. by host 1 and also makes reservation. First dedicated bandwidth is reserved as far as router . However this router sees that it already has feed from host 1 so if the necessary bandwidth has already been reserved it does not have to reserve any more. Note that hosts 3 and 5 might have asked for different amounts of bandwidth so the capacity reserved must be large enough to satisfy the greediest receiver. When making reservation receiver can specify one or more sources that it wants to receive from. It can also specify whether these choices are fixed for the duration of the reservation or whether the receiver wants to keep open the option of changing sources later. The routers use this information to op timize bandwidth planning. In particular two receivers are only set up to share path if they both agree not to change sources later on. The reason for this strategy in the fully dynamic case is that reserved band width is decoupled from the choice of source. Once receiver has reserved band width it can switch to another source and keep that portion of the existing path that is valid for the new source. If host 2 is transmitting several video streams in real time for example TV broadcaster with multiple channels host 3 may switch between them at will without changing its reservation the routers do not care what program the receiver is watching. Flowbased algorithms have the potential to offer good quality of service to one or more flows because they reserve whatever resources are needed along the route. However they also have downside. They require an advance setup to es tablish each flow something that does not scale well when there are thousands or millions of flows. Also they maintain internal perflow state in the routers mak code are substantial and involve complex routertorouter exchanges for setting up the flows. As consequence while work continues to advance integrated ser vices few deployments of it or anything like it exist yet. For these reasons IETF has also devised simpler approach to quality of ser vice one that can be largely implemented locally in each router without advance setup and without having the whole path involved. This approach is known as classbased quality of service. IETF has standardized an architecture for it called differentiated services which is described in RFCs 2474 2475 and numerous others. We will now describe it. Differentiated services can be offered by set of routers forming an adminis trative domain . The administration defines set of service classes with corresponding forwarding rules. If customer subscribes to dif ferentiated services customer packets entering the domain are marked with the class to which they belong. This information is carried in the Differentiated ser vices field of IPv4 and IPv6 packets . The classes are de fined as per hop behaviors because they correspond to the treatment the packet will receive at each router not guarantee across the network. Better service is provided to packets with some perhop behaviors than to others . Traffic within class may be required to conform to some specific shape such as leaky bucket with some specified drain rate. An operator with nose for business might charge extra for each premium packet transported or might allow up to premium packets per month for fixed addi tional monthly fee. Note that this scheme requires no advance setup no resource reservation and no timeconsuming endtoend negotiation for each flow as with integrated services. This makes differentiated services relatively easy to imple Classbased service also occurs in other industries. For example package de livery companies often offer overnight twoday and threeday service. Airlines offer first class business class and cattleclass service. Longdistance trains often have multiple service classes. Even the Paris subway has two different ser vice classes. For packets the classes may differ in terms of delay jitter and probability of being discarded in the event of congestion among other possibili ties . To make the difference between flowbased quality of service and classbased quality of service clearer consider an example Internet telephony. With flow based scheme each telephone call gets its own resources and guarantees. With classbased scheme all the telephone calls together get the resources reserved for the class telephony. These resources cannot be taken away by packets from the Web browsing class or other classes but no telephone call gets any private re The choice of service classes is up to each operator but since packets are often forwarded between networks run by different operators IETF has defined some networkindependent service classes. The simplest class is expedited for warding so let us start with that one. It is described in RFC 3246. The idea behind expedited forwarding is very simple. Two classes of service are available regular and expedited. The vast majority of the traffic is expected to be regular but limited fraction of the packets are expedited. The expedited packets should be able to transit the network as though no other packets were present. In this way they will get low loss low delay and low jitter servicejust what is needed for VoIP. symbolic representation of this twotube system is classes of service not second physical line. One way to implement this strategy is as follows. Packets are classified as expedited or regular and marked accordingly. This step might be done on the sending host or in the ingress router. The advantage of doing classification on the sending host is that more information is available about which packets be long to which flows. This task may be performed by networking software or even the operating system to avoid having to change existing applications. For ex ample it is becoming common for VoIP packets to be marked for expedited ser vice by hosts. If the packets pass through corporate network or ISP that sup ports expedited service they will receive preferential treatment. If the network does not support expedited service no harm is done. Of course if the marking is done by the host the ingress router is likely to police the traffic to make sure that customers are not sending more expedited traf fic than they have paid for. Within the network the routers may have two output queues for each outgoing line one for expedited packets and one for regular pack ets. When packet arrives it is queued accordingly. The expedited queue is given priority over the regular one for example by using priority scheduler. In this way expedited packets see an unloaded network even when there is in fact somewhat more elaborate scheme for managing the service classes is called assured forwarding. It is described in RFC 2597. Assured forwarding specifies that there shall be four priority classes each class having its own resources. The top three classes might be called gold silver and bronze. In addition it defines three discard classes for packets that are experiencing congestion low medium and high. Taken together these two factors define 12 service classes. warding. The first step is to classify the packets into one of the four priority classes. As before this step might be done on the sending host which we saw in Sec. 5.3.5. RED will start to drop packets as congestion builds but before the router has run out of buffer space. At this stage there is still buffer space with which to accept low discard packets while dropping high discard packets. Until now we have implicitly assumed that there is single homogeneous network with each machine using the same protocol in each layer. Unfortunately this assumption is wildly optimistic. Many different networks exist including PANs LANs MANs and WANs. We have described Ethernet Internet over cable the fixed and mobile telephone networks 802.11 802.16 and more. Num erous protocols are in widespread use across these networks in every layer. In the more networks are connected to form an internetwork or more simply an inter It would be much simpler to join networks together if everyone used single networking technology and it is often the case that there is dominant kind of network such as Ethernet. Some pundits speculate that the multiplicity of technol ogies will go away as soon as everyone realizes how wonderful is. Do not count on it. History shows this to be wishful thinking. Dif net and satellite networks are always likely to differ. Reusing existing systems such as running data networks on top of cable the telephone network and power lines adds constraints that cause the features of the networks to diverge. Hetero geneity is here to stay. If there will always be different networks it would be simpler if we did not need to interconnect them. This also is unlikely. Bob Metcalfe postulated that the value of network with nodes is the number of connections that may be made between the nodes or 2 . This means that large networks are much more valuable than small networks because they allow many more con nections so there always will be an incentive to combine smaller networks. The Internet is the prime example of this interconnection. The purpose of joining all these networks is to allow users on any of them to communicate with users on all the other ones. When you pay an ISP for Internet service you may be charged depending on the bandwidth of your line but what you are really paying for is the ability to exchange packets with any other host that is also connected to the Internet. After all the Internet would not be very popular if you could only send packets to other hosts in the same city. Since networks often differ in important ways getting packets from one net will begin by looking at how networks can differ to see what we are up against. Then we shall see the approach used so successfully by IP the network layer protocol of the Internet including techniques for tunneling through networks routing in internetworks and packet fragmentation. Networks can differ in many ways. Some of the differences such as different modulation techniques or frame formats are internal to the physical and data link some of the differences that can be exposed to the network layer. It is papering over these differences that makes internetworking more difficult than operating within single network. When packets sent by source on one network must transit one or more for at the interfaces between networks. To start with the source needs to be able to address the destination. What do we do if the source is on an Ethernet network and the destination is on WiMAX network Assuming we can even specify WiMAX destination from an Ethernet network packets would cross from con nectionless network to connectionoriented one. This may require that new connection be set up on short notice which injects delay and much overhead if the connection is not used for many more packets. we multicast packet to group with some members on network that does not By connect time packet byte or not at all support multicast The differing max packet sizes used by different networks can be major nuisance too. How do you pass an 8000byte packet through net work whose maximum size is 1500 bytes If packets on connectionoriented network transit connectionless network they may arrive in different order than they were sent. That is something the sender likely did not expect and it might come as an surprise to the receiver as well. These kinds of differences can be papered over with some effort. For ex ample gateway joining two networks might generate separate packets for each destination in lieu of better network support for multicasting. large packet might be broken up sent in pieces and then joined back together. Receivers might buffer packets and deliver them in order. Networks also can differ in large respects that are more difficult to reconcile. The clearest example is quality of service. If one network has strong QoS and the other offers best effort service it will be impossible to make bandwidth and delay guarantees for realtime traffic end to end. In fact they can likely only be made while the besteffort network is operated at low utilization or hardly used which is unlikely to be the goal of most ISPs. Security mechanisms are prob lematic but at least encryption for confidentiality and data integrity can be lay counting can lead to unwelcome bills when normal usage suddenly becomes ex pensive as roaming mobile phone users with data plans have discovered. There are two basic choices for connecting different networks we can build devices that translate or convert packets from each kind of network into packets for each other network or like good computer scientists we can try to solve the problem by adding layer of indirection and building common layer on top of the different networks. In either case the devices are placed at the boundaries be Early on Cerf and Kahn argued for common layer to hide the dif ferences of existing networks. This approach has been tremendously successful and the layer they proposed was eventually separated into the TCP and IP proto cols. Almost four decades later IP is the foundation of the modern Internet. For this accomplishment Cerf and Kahn were awarded the 2004 Turing Award infor mally known as the Nobel Prize of computer science. IP provides universal packet format that all routers recognize and that can be passed through almost every network. IP has extended its reach from computer networks to take over the telephone network. It also runs on sensor networks and other tiny devices that were once presumed too resourceconstrained to support it. We have discussed several different devices that connect networks including repeaters hubs switches bridges routers and gateways. Repeaters and hubs just move bits from one wire to another. They are mostly analog devices and do not understand anything about higher layer protocols. Bridges and switches operate at the link layer. They can be used to build networks but only with minor protocol translation in the process for example between 10 100 and 1000 Mbps Ethernet network layer namely the routers. We will leave gateways which are higher layer interconnection devices until later. Let us first explore at high level how interconnection with common net work layer can be used to interconnect dissimilar networks. Suppose that the source machine on the 802.11 network wants to send packet to the destination machine on the Ethernet network. Since these technologies are dif ferent and they are further separated by another kind of network some added processing is needed at the boundaries between the networks. Because different networks may in general have different forms of ad dressing the packet carries network layer address that can identify any host cross the three networks. The first boundary the packet reaches is when it tran sitions from an 802.11 network to an MPLS network. 802.11 provides con nectionless service but MPLS provides connectionoriented service. This means that virtual circuit must be set up to cross that network. Once the packet has traveled along the virtual circuit it will reach the Ethernet network. At this boundary the packet may be too large to be carried since 802.11 can work with larger frames than Ethernet. To handle this problem the packet is divided into fragments and each fragment is sent separately. When the fragments reach the destination they are reassembled. Then the packet has completed its journey. accepts data from the transport layer and generates packet with the common net layer protocol processing. ultimate destination address which is used to determine that the packet should be sent via the first router. So the packet is encapsulated in an 802.11 frame whose destination is the first router and transmitted. At the router the packet is removed now examines the IP address in the packet and looks up this address in its routing table. Based on this address it decides to send the packet to the second router next. For this part of the path an MPLS virtual circuit must be established to the is again consulted to find the next network layer hop. It is the destination itself. Since the packet is too long to be sent over Ethernet it is split into two portions. Each of these portions is put into the data field of an Ethernet frame and sent to Observe that there is an essential difference between the routed case and the switched case. With router the packet is extracted from the frame and the network address in the packet is used for deciding where to send it. With switch the entire frame is transported on the basis of its MAC ad dress. Switches do not have to understand the network layer protocol being used to switch packets. Routers do. Unfortunately internetworking is not as easy as we have made it sound. In fact when bridges were introduced it was intended that they would join different types of networks or at least different types of LANs. They were to do this by translating frames from one LAN into frames from another LAN. However this did not work well for the same reason that internetworking is difficult the dif ferences in the features of LANs such as different maximum packet sizes and LANs with and without priority classes are hard to mask. Today bridges are predominantly used to connect the same kind of network at the link layer and routers connect different networks at the network layer. Internetworking has been very successful at building large networks but it only works when there is common network layer. There have in fact been many network protocols over time. Getting everybody to agree on single format is difficult when companies perceive it to their commercial advantage to have proprietary format that they control. Examples besides IP which is now the nearuniversal network protocol were IPX SNA and AppleTalk. None of these protocols are still in widespread use but there will always be other protocols. The most relevant example now is probably IPv4 and IPv6. While these are both ver sions of IP they are not compatible . Then it limits usage across the networks to applications that use TCP . The alternative is to translate packets between the networks. However unless the packet formats are close relatives with the same information fields such IPv6 addresses are 128 bits long. They will not fit in 32bit IPv4 address field no matter how hard the router tries. Getting IPv4 and IPv6 to run in the same net work has proven to be major obstacle to the deployment of IPv6. ferent protocols such as connectionless and connectionoriented network proto even IP has only worked so well by serving as kind of lowest common denomi nator. It requires little of the networks on which it runs but offers only besteffort service as result. Handling the general case of making two different networks interwork is exceedingly difficult. However there is common special case that is man ageable even for different network protocols. This case is where the source and destination hosts are on the same type of network but there is different network in between. As an example think of an international bank with an IPv6 network in Paris an IPv6 network in London and connectivity between the offices via the The solution to this problem is technique called tunneling. To send an IP packet to host in the London office host in the Paris office constructs the packet containing an IPv6 address in London and sends it to the multiprotocol router that connects the Paris IPv6 network to the IPv4 Internet. When this router the IPv4 side of the multiprotocol router that connects to the London IPv6 net work. That is the router puts packet inside packet. When this wrapped packet arrives the London router removes the original IPv6 packet and sends it onward to the destination host. The path through the IPv4 Internet can be seen as big tunnel extending from one multiprotocol router to the other. The IPv6 packet just travels from one end of the tunnel to the other snug in its nice box. It does not have to worry about dealing with IPv4 at all. Neither do the hosts in Paris or London. Only the multi protocol routers have to understand both IPv4 and IPv6 packets. In effect the en tire trip from one multiprotocol router to the other is like hop over single link. An analogy may make tunneling clearer. Consider person driving her car from Paris to London. Within France the car moves under its own power but when it hits the English Channel it is loaded onto highspeed train and tran sported to England through the Chunnel . Effectively the car is being carried as freight as depicted in continues to move under its own power. Tunneling of packets through foreign network works the same way. Tunneling is widely used to connect isolated hosts and networks using other networks. The network that results is called an overlay since it has effectively been overlaid on the base network. Deployment of network protocol with new feature is common reason as our IPv6 over IPv4 example shows. The disad vantage of tunneling is that none of the hosts on the network that is tunneled over can be reached because the packets cannot escape in the middle of the tunnel. However this limitation of tunnels is turned into an advantage with VPNs . VPN is simply an overlay that is used to provide measure of security. We will explore VPNs when we get to Chap. 8. Routing through an internet poses the same basic problem as routing within single network but with some added complications. To start the networks may internally use different routing algorithms. For example one network may use link state routing and another distance vector routing. Since link state algorithms need to know the topology but distance vector algorithms do not this difference alone would make it unclear how to find the shortest paths across the internet. tors may have different ideas about what is good path through the network. One operator may want the route with the least delay while another may want the most inexpensive route. This will lead the operators to use different quantities to set the shortestpath costs . The weights will not be comparable across networks so shortest paths on the internet will not be well defined. Worse yet one operator may not want another operator to even know the de tails of the paths in its network perhaps because the weights and paths may reflect sensitive information that represents competitive business advantage. comprise it. It may therefore require routing algorithms that scale well by using hierarchy even if none of the individual networks need to use hierarchy. All of these considerations lead to twolevel routing algorithm. Within each network an intradomain or interior gateway protocol is used for routing. It might be link state protocol of the kind we have already described. Across the networks that make up the internet an interdomain or exterior gateway protocol is used. The networks may all use different intradomain protocols but they must use the same interdomain protocol. In the Internet the interdomain routing protocol is called BGP . good mental model for an AS is an ISP network. In fact an ISP net work may be comprised of more than one AS if it is managed or has been ac quired as multiple networks. But the difference is usually not significant. The two levels are usually not strictly hierarchical as highly suboptimal paths might result if large international network and small regional network were both abstracted to be single network. However relatively little information about routes within the networks is exposed to find routes across the internetwork. This helps to address all of the complications. It improves scaling and lets opera tors freely select routes within their own networks using protocol of their choos ing. It also does not require weights to be compared across networks or expose sensitive information outside of networks. However we have said little so far about how the routes across the networks of the internet are determined. In the Internet large determining factor is the business arrangements between ISPs. Each ISP may charge or receive money from the other ISPs for carrying traffic. Another factor is that if internetwork routing requires crossing international boundaries various laws may suddenly come into play such as Swedens strict privacy laws about exporting personal data about Swedish citizens from Sweden. All of these nontechnical factors are wrapped up in the concept of routing policy that governs the way autonomous networks select the routes that they use. We will return to routing policies when we describe BGP. Each network or link imposes some maximum size on its packets. These lim its have various causes among them 1. Hardware . 2. Operating system . 3. Protocols . 4. Compliance with some national standard. 5. Desire to reduce errorinduced retransmissions to some level. 6. Desire to prevent one packet from occupying the channel too long. The result of all these factors is that the network designers are not free to choose any old maximum packet size they wish. Maximum payloads for some common technologies are 1500 bytes for Ethernet and 2272 bytes for 802.11. IP is more generous allows for packets as big as 65515 bytes. Hosts usually prefer to transmit large packets because this reduces packet ing problem appears when large packet wants to travel through network whose One solution is to make sure the problem does not occur in the first place. However this is easier said than done. source does not usually know the path packet will take through the network to destination so it certainly does not know how small packets must be to get there. This packet size is called the Path MTU . Even if the source did know the path MTU packets are routed independently in connectionless network such as the Internet. This routing means that paths may suddenly change which can unexpectedly change the path MTU. The alternative solution to the problem is to allow routers to break up packets into fragments sending each fragment as separate network layer packet. How ever as every parent of small child knows converting large object into small fragments is considerably easier than the reverse process. Packetswitching networks too have trouble putting the fragments back together again. Two opposing strategies exist for recombining the fragments back into the original packet. The first strategy is to make fragmentation caused by small packet network transparent to any subsequent networks through which the pack 42. In this approach when an oversized packet arrives at G1 the router breaks it up into fragments. Each fragment is addressed to the same exit router G2 where the pieces are recombined. In this way passage through the smallpacket network is made transparent. Subsequent networks are not even aware that frag mentation has occurred. thing the exit router must know when it has received all the pieces so either count field or an end of packet bit must be provided. Also because all packets must exit via the same router so that they can be reassembled the routes are con strained. By not allowing some fragments to follow one route to the ultimate dest ination and other fragments disjoint route some performance may be lost. More significant is the amount of work that the router may have to do. It may need to buffer the fragments as they arrive and decide when to throw them away if not all of the fragments arrive. Some of this work may be wasteful too as the packet may pass through series of small packet networks and need to be repeatedly fragmented and reassembled. The other fragmentation strategy is to refrain from recombining fragments at any intermediate routers. Once packet has been fragmented each fragment is The fragments are not reassembled treated as though it were an original packet. The routers pass the fragments as The main advantage of nontransparent fragmentation is that it requires routers to do less work. IP works this way. complete design requires that the fragments be numbered in such way that the original data stream can be reconstructed. The design used by IP is to give every fragment packet number an absolute byte offset within the packet and flag indicating whether it design has some attractive properties. Fragments can be placed in buffer at the destination in the right place for reassembly even if they arrive out of order. Fragments can also be fragmented if they pass over network with yet smaller cases the destination simply uses the packet number and fragment offset to place the data in the right position and the endofpacket flag to determine when it has the complete packet. over some links where they may not be needed. But the real problem is the exist ence of fragments in the first place. Kent and Mogul argued that frag whole packet is lost if any of its fragments are lost and because fragmentation is more of burden for hosts than was originally realized. Number of the first elementary fragment in this packet nal packet containing 10 data bytes. Fragments after passing through net after passing through size 5 gateway. This leads us back to the original solution of getting rid of fragmentation in the network the strategy used in the modern Internet. The process is called path MTU discovery . It works as follows. Each IP packet performed. If router receives packet that is too large it generates an error When the source receives the error packet it uses the information inside to refrag ment the packet into pieces that are small enough for the router to handle. If router further down the path has an even smaller MTU the process is repeated. The advantage of path MTU discovery is that the source now knows what length packet to send. If the routes and path MTU change new error packets will be triggered and the source will adapt to the new path. However fragmentation is still needed between the source and the destination unless the higher layers learn the path MTU and pass the right amount of data to IP. TCP and IP are typically implemented together to be able to pass this sort of information. Even if this is not done for other protocols fragmentation has still been moved out of the network and into the hosts. The disadvantage of path MTU discovery is that there may be added startup delays simply to send packet. More than one roundtrip delay may be needed to probe the path and find the MTU before any data is delivered to the destination. This begs the question of whether there are better designs. The answer is proba bly Yes. Consider the design in which each router simply truncates packets that exceed its MTU. This would ensure that the destination learns the MTU as rapidly as possible and receives some of the It is now time to discuss the network layer of the Internet in detail. But before getting into specifics it is worth taking look at the principles that drove its de sign in the past and made it the success that it is today. All too often nowadays people seem to have forgotten them. These principles are enumerated and dis cussed in RFC 1958 which is well worth reading in the 14th century. Put in modern terms fight features. If feature is not absolutely es sential leave it out especially if the same effect can be achieved by combining other features. 3. Make clear choices. If there are several ways of doing the same thing choose one. Having two or more ways to do the same thing is looking for trouble. Standards often have multiple options or modes THE NETWORK LAYER IN THE INTERNET or parameters because several powerful parties insist that their way is best. Designers should strongly resist this tendency. Just say no. 4. Exploit modularity. This principle leads directly to the idea of hav ing protocol stacks each of whose layers is independent of all the other ones. In this way if circumstances require one module or layer to be changed the other ones will not be affected. Different types of hardware transmission facilities and applications will occur on any large network. handle them the network design must be simple general and flexi 6. Avoid static options and parameters. If parameters are unavoid able it is best to have the sender and re ceiver negotiate value rather than defining fixed choices. 7. Look for good design it need not be perfect. Often the de signers have good design but it cannot handle some weird special case. Rather than messing up the design the designers should go with the good design and put the burden of working around it on the people with the strange requirements. 8. Be strict when sending and tolerant when receiving. In other words send only packets that rigorously comply with the standards but expect incoming packets that may not be fully conformant and try to deal with them. 9. Think about scalability. If the system is to handle millions of hosts and billions of users effectively no centralized databases of any kind are tolerable and load must be spread as evenly as possible over the formance or outrageous costs nobody will use it. Let us now leave the general principles and start looking at the details of the Internets network layer. In the network layer the Internet can be viewed as collection of networks or ASes that are interconnected. There is no real structure but several major backbones exist. These are con structed from highbandwidth lines and fast routers. The biggest of these back bones to which everyone else connects to reach the rest of the Internet are called Tier 1 networks. Attached to the backbones are ISPs that provide Internet access to homes and businesses data centers and The data centers serve much of the content that is sent over the Internet. Attached to the regional networks are more ISPs LANs at many universities and com panies and other edge networks. sketch of this quasihierarchical organization The glue that holds the whole Internet together is the network layer protocol IP . Unlike most older network layer protocols IP was de signed from the beginning with internetworking in mind. good way to think of the network layer is this its job is to provide besteffort way to transport packets from source to destination without regard to whether these machines are on the same network or whether there are other networks in Communication in the Internet works as follows. The transport layer takes data streams and breaks them up so that they may be sent as IP packets. In theory packets can be up to 64 KB each but in practice they are usually not more than 1500 bytes . IP routers forward each packet through the Internet along path from one router to the next until the destination is reached. At the destination the network layer hands the data to the transport the destination machine they are reassembled by the network layer into the origi nal datagram. This datagram is then handed to the transport layer. work has to traverse four networks and large number of IP routers before even getting to the company network on which the destination host is located. This is THE NETWORK LAYER IN THE INTERNET not unusual in practice and there are many longer paths. There is also much redundant connectivity in the Internet with backbones and ISPs connecting to between two hosts. It is the job of the IP routing protocols to decide which paths An appropriate place to start our study of the network layer in the Internet is with the format of the IP datagrams themselves. An IPv4 datagram consists of are transmitted from left to right and top to bottom with the highorder bit of the on both transmission and reception. In retrospect little endian would have been better choice but at the time IP was designed no one knew it would come to Options use will eventually be forced when each of Chinas almost 231 people has desk top PC laptop and an IP phone. As an aside on numbering IPv5 was an exper imental realtime stream protocol that was never widely used. applies when no options are present. The maximum value of this 4bit field is 15 some options such as one that records the route packet has taken 40 bytes is far too small making those options useless. The Differentiated services field is one of the few fields that has changed its meaning over the years. Originally it was called the Type of service field. It was and still is intended to distinguish between different classes of ser vice. Various combinations of reliability and speed are possible. For digitized voice fast delivery beats accurate delivery. For file transfer errorfree transmis sion is more important than fast transmission. The Type of service field provided 3 bits to signal priority and 3 bits to signal whether host cared more about delay throughput or reliability. However no one really knew what to do with these bits at routers so they were left unused for many years. When differentiated services were designed IETF threw in the towel and reused this field. Now the top 6 bits are used to mark the packet with its service class we described the expedited and cit congestion notification information such as whether the packet has experi enced congestion we described explicit congestion notification as part of conges The maximum length is 65535 bytes. At present this upper limit is tolerable but with future networks larger datagrams may be needed. The Identification field is needed to allow the destination host to determine which packet newly arrived fragment belongs to. All the fragments of packet contain the same Identification value. Next comes an unused bit which is surprising as available real estate in the using this bit to detect malicious traffic. This would greatly simplify security as packets with the evil bit set would be known to have been sent by attackers and could just be discarded. Unfortunately network security is not this simple. Then come two 1bit fields related to fragmentation. DF stands for Dont Fragment. It is an order to the routers not to fragment the packet. Originally it was intended to support hosts incapable of putting the pieces back together again. Now it is used as part of the process to discover the path MTU which is the larg est packet that can travel along path without being fragmented. By marking the datagram with the DF bit the sender knows it will either arrive in one piece or an error message will be returned to the sender. MF stands for More Fragments. All fragments except the last one have this bit set. It is needed to know when all fragments of datagram have arrived. The Fragment offset tells where in the current packet this fragment belongs. All fragments except the last one in datagram must be multiple of 8 bytes the THE NETWORK LAYER IN THE INTERNET fragments per datagram supporting maximum packet length up to the limit of the Total length field. Working together the Identification MF and Fragment offset fields are used to implement fragmentation as described in Sec. 5.5.5. The TtL field is counter used to limit packet lifetimes. It was sec. It must be decremented on each hop and is supposed to be decremented mul tiple times when packet is queued for long time in router. In practice it just counts hops. When it hits zero the packet is discarded and warning packet is sent back to the source host. This feature prevents packets from wandering around forever something that otherwise might happen if the routing tables ever become corrupted. When the network layer has assembled complete packet it needs to know what to do with it. The Protocol field tells it which transport process to give the packet to. TCP is one possibility but so are UDP and some others. The num bering of protocols is global across the entire Internet. Protocols and other assign ed numbers were formerly listed in RFC 1700 but nowadays they are contained in an online database located at and then take the ones complement of the result. For purposes of this algorithm useful for detecting errors while the packet travels through the network. Note that it must be recomputed at each hop because at least one field always changes but tricks can be used to speed up the computation. The Source address and Destination address indicate the IP address of the source and destination network interfaces. We will discuss Internet addresses in The Options field was designed to provide an escape to allow subsequent ver sions of the protocol to include information not present in the original design to information that is rarely needed. The options are of variable length. Each begins with 1byte code identifying the option. Some options are followed by 1byte option length field and then one or more data bytes. The Options field is padded The Security option tells how secret the information is. In theory military router might use this field to specify not to route packets through certain countries the military considers to be bad guys. In practice all routers ignore it so its only practical function is to help spies find the good stuff more easily. The Strict source routing option gives the complete path from source to desti nation as sequence of IP addresses. The datagram is required to follow that Makes each router append its address and timestamp exact route. It is most useful for system managers who need to send emergency packets when the routing tables have been corrupted or for making timing meas The Loose source routing option requires the packet to traverse the list of routers specified in the order specified but it is allowed to pass through other routers on the way. Normally this option will provide only few routers to force particular path. For example to force packet from London to Sydney to go west instead of east this option might specify routers in New York Los Angeles and Honolulu. This option is most useful when political or economic consid erations dictate passing through or avoiding certain countries. The Record route option tells each router along the path to append its IP ad dress to the Options field. This allows system managers to track down bugs in the routing algorithms . When the ARPANET was first set up no packet ever passed through more than nine routers so 40 bytes of options was plenty. As mentioned above now it is too small. addition to recording its 32bit IP address each router also records 32bit time stamp. This option too is mostly useful for network measurement. Today IP options have fallen out of favor. Many routers ignore them or do not process them efficiently shunting them to the side as an uncommon case. That is they are only partly supported and they are rarely used. defining feature of IPv4 is its 32bit addresses. Every host and router on the Internet has an IP address that can be used in the Source address and Destina tion address fields of IP packets. It is important to note that an IP address does not actually refer to host. It really refers to network interface so if host is on two networks it must have two IP addresses. However in practice most hosts are on one network and thus have one IP address. In contrast routers have multi IP addresses are hierarchical unlike Ethernet addresses. Each 32bit address is comprised of variablelength network portion in the top bits and host portion in the bottom bits. The network portion has the same value for all hosts on sin gle network such as an Ethernet LAN. This means that network corresponds to contiguous block of IP address space. This block is called prefix. IP addresses are written in dotted decimal notation. In this format each of the 4 bytes is written in decimal from 0 to 255. For example the 32bit hexade cimal address 80D00297 is written as 128.208.2.151. Prefixes are written by giv ing the lowest IP address in the block and the size of the block. The size is deter mined by the number of bits in the network portion the remaining bits in the host portion can vary. This means that the size must be power of two. By conven tion it is written after the prefix IP address as slash followed by the length in bits of the network portion. In our example if the prefix contains 28 addresses and so leaves 24 bits for the network portion it is written as 128.208.0.024. Since the prefix length cannot be inferred from the IP address alone routing protocols must carry the prefixes to routers. Sometimes prefixes are simply de scribed by their length as in 16 which is pronounced slash 16. The length of the prefix corresponds to binary mask of 1s in the network portion. When written out this way it is called subnet mask. It can be ANDed with the IP ad dress to extract only the network portion. For our example the subnet mask is Hierarchical addresses have significant advantages and disadvantages. The key advantage of prefixes is that routers can forward packets based on only the network portion of the address as long as each of the networks has unique ad dress block. The host portion does not matter to the routers because all hosts on the same network will be sent in the same direction. It is only when the packets reach the network for which they are destined that they are forwarded to the cor rect host. This makes the routing tables much smaller than they would otherwise be. Consider that the number of hosts on the Internet is approaching one billion. That would be very large table for every router to keep. However by using hierarchy routers need to keep routes for only around 300000 prefixes. While using hierarchy lets Internet routing scale it has two disadvantages. First the IP address of host depends on where it is located in the network. An Ethernet address can be used anywhere in the world but every IP address belongs to specific network and routers will only be able to deliver packets destined to that address to the network. Designs such as mobile IP are needed to support hosts that move between networks but want to keep the same IP addresses. The second disadvantage is that the hierarchy is wasteful of addresses unless it is carefully managed. If addresses are assigned to networks in large blocks there will be addresses that are allocated but not in use. This al However it was realized more than two decades ago that the tremendous growth of the Internet was rapidly depleting the free address space. IPv6 is the solution to this shortage but until it is widely deployed there will be great pressure to allocate IP addresses so that they are used very efficiently. Network numbers are managed by nonprofit corporation called ICANN to avoid conflicts. In turn ICANN has delegated parts of the address space to various regional authorities which dole out IP addresses to ISPs and other companies. This is the process by which company is allocated block of IP addresses. However this process is only the start of the story as IP address assignment is ongoing as companies grow. We have said that routing by prefix requires all the hosts in network to have the same network number. This property can cause with our example 16 prefix for use by the Computer Science Dept. for the com puters on its Ethernet. year later the Electrical Engineering Dept. wants to get on the Internet. The Art Dept. soon follows suit. What IP addresses should these departments use Getting further blocks requires going outside the university and may be expensive or inconvenient. Moreover the 16 already allocated has enough addresses for over 60000 hosts. It might be intended to allow for signifi cant growth but until that happens it is wasteful to allocate further blocks of IP addresses to the same university. different organization is required. The solution is to allow the block of addresses to be split into several parts for internal use as multiple networks while still acting like single network to the outside world. This is called subnetting and the networks that result from dividing up larger network are called subnets. As we mentioned in Chap. 1 you should be aware that this new usage of the term con flicts with older usage of subnet to mean the set of all routers and communica tion lines in network. been split into pieces. This split does not need to be even but each piece must be THE NETWORK LAYER IN THE INTERNET aligned so that any bits can be used in the lower host portion. In this case half of the block is allocated to the Computer Science Dept quarter is allocated to the Electrical Engineering Dept. and one eighth to the Art Dept. The remaining eighth is unallocated. different way to see how the block was di vided is to look at the resulting prefixes when written in binary notation Here the vertical bar shows the boundary between the subnet number and the When packet comes into the main router how does the router know which subnet to give it to This is where the details of our prefixes come in. One way would be for each router to have table with 65536 entries telling it which out going line to use for each host on campus. But this would undermine the main scaling benefit we get from using hierarchy. Instead the routers simply need to know the subnet masks for the networks on campus. When packet arrives the router looks at the destination address of the pack et and checks which subnet it belongs to. The router can do this by ANDing the destination address with the mask for each subnet and checking to see if the result is the corresponding prefix. For example consider packet destined for IP ad dress 128.208.2.151. To see if it is for the Computer Science Dept. we AND with 255.255.128.0 to take the first 17 bits and see if they match the prefix address . They do not match. Checking the first 18 bits for the Electrical Engineering Dept. we get 128.208.0.0 when ANDing with the subnet mask. This does match the prefix address so the packet is forwarded onto the interface which leads to the Electrical Engineering network. The subnet divisions can be changed later if necessary by updating all subnet masks at routers inside the university. Outside the network the subnetting is not visible so allocating new subnet does not require contacting ICANN or chang Even if blocks of IP addresses are allocated so that the addresses are used ef ficiently there is still problem that remains routing table explosion. Routers in organizations at the edge of network such as university need to have an entry for each of their subnets telling the router which line to use to get to that network. For routes to destinations outside of the organization they can use the simple default rule of sending the packets on the line toward the ISP that connects the organization to the rest of the Internet. The other destination ad dresses must all be out there somewhere. Routers in ISPs and backbones in the middle of the Internet have no such lux fault will work. These core routers are said to be in the defaultfree zone of the Internet. No one really knows how many networks are connected to the Internet any more but it is large number probably at least million. This can make for very large table. It may not sound large by computer standards but realize that routers must perform lookup in this table to forward every packet and routers at large ISPs may forward up to millions of packets per second. Specialized hard ware and fast memory are needed to process packets at these rates not general purpose computer. In addition routing algorithms require each router to exchange information about the addresses it can reach with other routers. The larger the tables the more information needs to be communicated and processed. The processing grows at least linearly with the table size. Greater communication increases the likelihood that some parts will get lost at least temporarily possibly leading to routing insta The routing table problem could have been solved by going to deeper hier archy like the telephone network. For example having each IP address contain country stateprovince city network and host field might work. Then each router would only need to know how to get to each country the states or pro vinces in its own country the cities in its state or province and the networks in its city. Unfortunately this solution would require considerably more than 32 bits for IP addresses and would use addresses inefficiently . Fortunately there is something we can do to reduce routing table sizes. We about given IP address as belonging to prefixes of different sizes. However in stead of splitting an address block into subnets here we combine multiple small THE NETWORK LAYER IN THE INTERNET prefixes into single larger prefix. This process is called route aggregation. The resulting larger prefix is sometimes called supernet to contrast with subnets as the division of blocks of addresses. With aggregation IP addresses are contained in prefixes of varying sizes. The same IP address that one router treats as part of 22 may be treated by another router as part of larger 20 . It is up to each router to have the corresponding prefix infor mation. This design works with subnetting and is called CIDR which is pronounced cider as in the drink. The most recent the contrast with addresses that encode hierarchy with classes which we will de All of the routers in the defaultfree zone are now told about the IP addresses in the three networks. Routers close to the universities may need to send on dif ferent outgoing line for each of the prefixes so they need an entry for each of the Now let us look at these three universities from the point of view of distant router in New York. All of the IP addresses in the three prefixes should be sent from New York to London. The routing process in London notices this and combines the three prefixes into single aggregate entry for the prefix 194.24.0.019 that it passes to the New York router. This prefix contains 8K addresses and covers the three universities and the otherwise unallocated 1024 ad dresses. By using aggregation three prefixes have been reduced to one reducing the prefixes that the New York router must be told about and the routing table en tries in the New York router. When aggregation is turned on it is an automatic process. It depends on which prefixes are located where in the Internet not on the actions of an adminis trator assigning addresses to networks. Aggregation is heavily used throughout the Internet and can reduce the size of router tables to around 200000 prefixes. As further twist prefixes are allowed to overlap. The rule is that packets are sent in the direction of the most specific route or the longest matching prefix that has the fewest IP addresses. Longest matching prefix routing provides use ful degree of flexibility as seen in the behavior of the router at New York in three universities to London. However the previously available block of ad dresses within this prefix has now been allocated to network in San Francisco. One possibility is for the New York router to keep four prefixes sending packets for three of them to London and packets for the fourth to San Francisco. Instead longest matching prefix routing can handle this forwarding with the two prefixes that are shown. One overall prefix is used to direct traffic for the entire block to London. One more specific prefix is also used to direct portion of the larger prefix to San Francisco. With the longest matching prefix rule IP addresses with in the San Francisco network will be sent on the outgoing line to San Francisco and all other IP addresses in the larger prefix will be sent to London. Conceptually CIDR works as follows. When packet comes in the routing table is scanned to determine if the destination lies within the prefix. It is possible that multiple entries with different prefix lengths will match in which case the entry with the longest prefix is used. Thus if there is match for 20 mask and 24 mask the 24 entry is used to look up the outgoing line for the packet. How ever this process would be tedious if the table were really scanned entry by entry. Instead complex algorithms have been devised to speed up the address matching process . Commercial routers use custom VLSI chips To help you better appreciate why CIDR is so useful we will briefly relate The class and formats allow for up to 128 networks with 16 million hosts each 16384 networks with up to 65536 hosts each and 2 million networks with up to 256 hosts each . Also supported is multicast in which datagram is directed to multiple hosts. Addresses beginning with 1111 are reserved for use in the future. They would be valuable to use now given the depletion of the IPv4 address space. Unfortunately many hosts will not accept these addresses as valid because they have been offlimits for so long and it is hard to teach old hosts new tricks. This is hierarchical design but unlike CIDR the sizes of the address blocks are fixed. Over 2 billion addresses exist but organizing the address space by classes wastes millions of them. In particular the real villain is the class net work. For most organizations class network with 16 million addresses is too big and class network with 256 addresses is too small. class net work with 65536 is just right. In Internet folklore this situation is known as the three bears problem . In reality though class address is far too large for most organizations. hosts. class network would have done the job but no doubt every organiza tion that asked for class address thought that one day it would outgrow the 8 bit host field. In retrospect it might have been better to have had class net works use 10 bits instead of 8 for the host number allowing 1022 hosts per net work. Had this been the case most organizations would probably have settled for class network and there would have been half million of them . It is hard to fault the Internets designers for not having provided more class addresses. At the time the decision was made to create the three classes the Internet was research network connecting the major research univer sities in the . No one then perceived the Internet becoming mass market communication system rivaling the telephone network. At the time some one no doubt said The . has about 2000 colleges and universities. Even if all of them connect to the Internet and many universities in other countries join too we are never going to hit 16000 since there are not that many universities in the whole world. Furthermore having the host number be an integral number of bytes speeds up packet processing . Perhaps some day people will look back and fault the folks who designed the tele phone number scheme and say What idiots. Why didnt they include the planet number in the phone number But at the time it did not seem necessary. of addresses within an organization. Later CIDR was added to reduce the size of the global routing table. Today the bits that indicate whether an IP address be classes in the literature are still common. To see how dropping the classes made forwarding more complicated consider how simple it was in the old classful system. When packet arrived at router copy of the IP address was shifted right 28 bits to yield 4bit class number. 16way branch then sorted packets into classes with eight of the cases for class four of the cases for class and two of the cases for class . The code for each class then masked off the 8 16 or 24bit network THE NETWORK LAYER IN THE INTERNET number and right aligned it in 32bit word. The network number was then hashing for networks. Once the entry was found the outgoing line could be looked up and the packet forwarded. This is much simpler than the longest matching prefix operation which can no longer use simple table lookup because an IP address may have any length prefix. Class addresses continue to be used in the Internet for multicast. Actually it might be more accurate to say that they are starting to be used for multicast since Internet multicast has not been widely deployed in the past. There are also several other addresses that have special meanings as shown in are being booted. It means this network or this host. IP addresses with 0 as the network number refer to the current network. These addresses allow machines to refer to their own network without knowing its number . The address consisting of all 1s or 255.255.255.255the highest addressis used to mean all hosts on the in dicated network. It allows broadcasting on the local network typically LAN. The addresses with proper network number and all 1s in the host field allow ma chines to send broadcast packets to distant LANs anywhere in the Internet. How ever many network administrators disable this feature as it is mostly security testing. Packets sent to that address are not put out onto the wire they are proc essed locally and treated as incoming packets. This allows packets to be sent to the host without the sender knowing its number which is useful for testing. IP addresses are scarce. An ISP might have 16 address giving it 65534 usable host numbers. If it has more customers than that it has problem. This scarcity has led to techniques to use IP addresses sparingly. One ap proach is to dynamically assign an IP address to computer when it is on and using the network and to take the IP address back when the host becomes inac tive. The IP address can then be assigned to another computer that becomes ac tive. In this way single 16 address can handle up to 65534 active users. This strategy works well in some cases for example for dialup networking and mobile and other computers that may be temporarily absent or powered off. However it does not work very well for business customers. Many PCs in busi nesses are expected to be on continuously. Some are employee machines backed up at night and some are servers that may have to serve remote request at moments notice. These businesses have an access line that always provides con nectivity to the rest of the Internet. Increasingly this situation also applies to home users subscribing to ADSL or Internet over cable since there is no connection charge . Many of these users have two or more computers at home often one for each family member and they all want to be online all the time. The solution is to connect all the computers into home network via LAN and put router on it. The router then connects to the ISP. From the ISPs point of view the family is now the same as small business with handful of computers. Wel come to Jones Inc. With the techniques we have seen so far each computer must have its own IP address all day long. For an ISP with many thousands of custom ers particularly business customers and families that are just like small busi nesses the demand for IP addresses can quickly exceed the block that is available. The problem of running out of IP addresses is not theoretical one that might occur at some point in the distant future. It is happening right here and right now. The longterm solution is for the whole Internet to migrate to IPv6 which has 128bit addresses. This transition is slowly occurring but it will be years before the process is complete. To get by in the meantime quick fix was needed. The quick fix that is widely used today came in the form of NAT which is described in RFC 3022 and which we will summarize below. For additional information see Dutcher . The basic idea behind NAT is for the ISP to assign each home or business single IP address for Internet traffic. Within the customer network every computer gets unique IP address which is used for routing intramural traffic. However just before packet exits the customer net work and goes to the ISP an address translation from the unique internal IP ad dress to the shared public IP address takes place. This translation makes use of three ranges of IP addresses that have been declared as private. Networks may use them internally as they wish. The only rule is that no packets containing these addresses may appear on the Internet itself. The three reserved ranges are The first range provides for 16777216 addresses and is the usual choice even if the network is not large. every machine has unique address of the form 10. However before pack et leaves the customer premises it passes through NAT box that converts the in 198.60.42.12 in this example. The NAT box is often combined in single device with firewall which provides security by carefully controlling what goes into the customer network and what comes out of it. We will study firewalls in Chap. 8. It is also possible to integrate the NAT box into router or ADSL modem. So far we have glossed over one tiny but crucial detail when the reply comes back it is naturally addressed to 198.60.42.12 so how does the NAT box know which internal address to replace it with Herein lies the used to keep track of who the real sender was but only 1 bit is still unused. In principle new option could be created to hold the true source address but doing so would require changing the IP code on all the machines on the entire Internet to handle the new option. This is not promising alternative for quick fix. What actually happens is as follows. The NAT designers observed that most IP packets carry either TCP or UDP payloads. When we study TCP and UDP in destination port. Below we will just discuss TCP ports but exactly the same story holds for UDP ports. The ports are 16bit integers that indicate where the TCP connection begins and ends. These ports provide the field needed to make NAT When process wants to establish TCP connection with remote process it attaches itself to an unused TCP port on its own machine. This is called the source port and tells the TCP code where to send incoming packets belonging to this connection. The process also supplies destination port to tell who to give the packets to on the remote side. Ports 01023 are reserved for wellknown ser vices. For example port 80 is the port used by Web servers so remote clients can locate them. Each outgoing TCP message contains both source port and desti nation port. Together these ports serve to identify the processes using the con nection on both ends. An analogy may make the use of ports clearer. Imagine company with single main telephone number. When people call the main number they reach an operator who asks which extension they want and then puts them through to that extension. The main number is analogous to the customers IP address and the extensions on both ends are analogous to the ports. Ports are effectively an extra 16 bits of addressing that identify which process gets which incoming packet. Using the Source port field we can solve our mapping problem. Whenever an outgoing packet enters the NAT box the 10. source address is replaced by the customers true IP address. In addition the TCP Source port field is replaced necessary to replace the Source port because connections from machines 10.0.0.1 and 10.0.0.2 may both happen to use port 5000 for example so the Source port alone is not enough to identify the sending process. When packet arrives at the NAT box from the ISP the Source port in the From the entry located the internal IP address and original TCP Source port are extracted and inserted into the packet. Then both the IP and TCP checksums are recomputed and inserted into the packet. The packet is then passed to the custo mer router for normal delivery using the 10. address. Although this scheme sort of solves the problem networking purists in the IP community have tendency to regard it as an abominationonthefaceofthe earth. Briefly summarized here are some of the objections. First NAT violates the architectural model of IP which states that every IP address uniquely identi fies single machine worldwide. The whole software structure of the Internet is built on this fact. With NAT thousands of machines may use address Second NAT breaks the endtoend connectivity model of the Internet which says that any host can send packet to any other host at any time. Since the map ping in the NAT box is set up by outgoing packets incoming packets cannot be accepted until after outgoing ones. In practice this means that home user with NAT can make TCPIP connections to remote Web server but remote user cannot make connections to game server on the home network. Special configu ration or NAT traversal techniques are needed to support this kind of situation. Third NAT changes the Internet from connectionless network to peculiar kind of connectionoriented network. The problem is that the NAT box must maintain information for each connection passing through it. Having the network maintain connection state is property of connectionoriented networks not connectionless ones. If the NAT box crashes and its mapping table is lost all its TCP connections are destroyed. In the absence of NAT router can crash and restart with no longterm effect on TCP connections. The sending proc ess just times out within few seconds and retransmits all unacknowledged pack ets. With NAT the Internet becomes as vulnerable as circuitswitched network. Fourth NAT violates the most fundamental rule of protocol layering layer may not make any assumptions about what layer 1 has put into the payload field. This basic principle is there to keep the layers independent. If TCP is later fail. The whole idea of layered protocols is to ensure that changes in one layer do not require changes in other layers. NAT destroys this independence. Fifth processes on the Internet are not required to use TCP or UDP. If user on machine decides to use some new transport protocol to talk to user on ma chine introduction of NAT box will cause the application to fail because the NAT box will not be able to locate the TCP Source port correctly. sixth and related problem is that some applications use multiple TCPIP connections or UDP ports in prescribed ways. For example FTP the standard File Transfer Protocol inserts IP addresses in the body of packet for the receiver to extract and use. Since NAT knows nothing about these arrangements it cannot rewrite the IP addresses or otherwise account for them. This lack of under standing means that FTP and other applications such as the .323 Internet tele phony protocol will fail in the presence of NAT unless special precautions are taken. It is often possible to patch NAT for these cases but having to patch the code in the NAT box every time new application comes along is not good idea. can be mapped onto an IP address. Actually the number is slightly less because the first 4096 ports are reserved for special uses. However if multiple IP ad dresses are available each one can handle up to 61440 machines. networks as the only expedient technique to deal with the IP address shortage. It has become wrapped up with firewalls and privacy because it blocks unsolicited incoming packets by default. For this reason it is unlikely to go away even when IPv6 is widely deployed. IP has been in heavy use for decades. It has worked extremely well as demonstrated by the exponential growth of the Internet. Unfortunately IP has be come victim of its own popularity it is close to running out of addresses. Even with CIDR and NAT using addresses more sparingly the last IPv4 addresses are expected to be assigned by ICANN before the end of 2012. This looming disaster was recognized almost two decades ago and it sparked great deal of discussion and controversy within the Internet community about what to do about it. tions. The only longterm solution is to move to larger addresses. IPv6 is replacement design that does just that. It uses 128bit addresses shortage of these addresses is not likely any time in the foreseeable future. How ever IPv6 has proved very difficult to deploy. It is different network layer pro tocol that does not really interwork with IPv4 despite many similarities. Also companies and users are not really sure why they should want IPv6 in any case. The result is that IPv6 is deployed and used on only tiny fraction of the Internet despite having been an Internet Standard since 1998. The next several years will be an interesting time as the few remaining IPv4 addresses are allocated. Will people start to auction off their IPv4 addresses on eBay Will black market in them spring up Who knows. its early years the Internet was largely used by universities hightech industries and the . Government . With the explosion of interest in the Internet starting in the mid1990s it began to be used by dif ferent group of people often with different requirements. For one thing numer ous people with smart phones use it to keep in contact with their home bases. For another with the impending convergence of the computer communication and entertainment industries it may not be that long before every telephone and tele vision set in the world is an Internet node resulting in billion machines being used for audio and video on demand. Under these circumstances it became apparent that IP had to evolve and become more flexible. 2. Reduce the size of the routing tables. 3. Simplify the protocol to allow routers to process packets faster. 4. Provide better security . 5. Pay more attention to the type of service particularly for realtime data. 6. Aid multicasting by allowing scopes to be specified. 7. Make it possible for host to roam without changing its address. 8. Allow the protocol to evolve in the future. 9. Permit the old and new protocols to coexist for years. The design of IPv6 presented major opportunity to improve all of the fea tures in IPv4 that fall short of what is now wanted. To develop protocol that met 1550. Twentyone responses were initially received. By December 1992 seven serious proposals were on the table. They ranged from making minor patches to IP to throwing it out altogether and replacing it with completely different proto One proposal was to run TCP over CLNP the network layer protocol de signed for OSI. With its 160bit addresses CLNP would have provided enough address space forever as it could give every molecule of water in the oceans enough addresses to set up small network. This choice would also have unified two major network layer protocols. However many people felt that this would have been an admission that something in the OSI world was actually done right statement considered Politically Incorrect in Internet circles. CLNP was patterned closely on IP so the two are not really that different. In fact the protocol ultimately chosen differs from IP far more than CLNP does. Another strike against CLNP was its poor support for service types something required to transmit multimedia efficiently. Three of the better proposals were published in IEEE Network was se lected and given the designation IPv6. IPv6 meets IETFs goals fairly well. It maintains the good features of IP dis cards or deemphasizes the bad ones and adds new ones where needed. In gener al IPv6 is not compatible with IPv4 but it is compatible with the other auxiliary Internet protocols including TCP UDP ICMP IGMP OSPF BGP and DNS with small modifications being required to deal with longer addresses. The main features of IPv6 are discussed below. More information about it can be found in RFCs 2460 through 2466. First and foremost IPv6 has longer addresses than IPv4. They are 128 bits long which solves the problem that IPv6 set out to solve providing an effectively unlimited supply of Internet addresses. We will have more to say about addresses contains only seven fields . This change allows routers to process packets faster and thus improves throughput and delay. We will discuss The third major improvement is better support for options. This change was now optional . In addition the way options are represented is different making it simple for routers to skip over options not intended for them. This feature speeds up packet processing time. fourth area in which IPv6 represents big advance is in security. IETF had its fill of newspaper stories about precocious 12yearolds using their personal computers to break into banks and military bases all over the Internet. There was strong feeling that something had to be done to improve security. Authentica tion and privacy are key features of the new IP. These were later retrofitted to IPv4 however so in the area of security the differences are not so great any more. hearted efforts to improve QoS have been made in the past but now with the growth of multimedia on the Internet the sense of urgency is greater. During the transition period from IPv4 which has already taken more than decade routers will be able to examine this field to tell what kind of packet they have. As an aside making this test wastes few instructions in the for demultiplexing so some routers may skip the check. For example the Ether net Type field has different values to indicate an IPv4 or an IPv6 payload. The discussions between the Do it right and Make it fast camps will no doubt be The Differentiated services field is used to distinguish the class of service for packets with different realtime delivery THE NETWORK LAYER IN THE INTERNET requirements. It is used with the differentiated service architecture for quality of service in the same manner as the field of the same name in the IPv4 packet. Also the loworder 2 bits are used to signal explicit congestion indications again in the same way as with IPv4. The Flow label field provides way for source and destination to mark groups of packets that have the same requirements and should be treated in the same way by the network forming pseudoconnection. For example stream of packets from one process on certain source host to process on specific desti nation host might have stringent delay requirements and thus need reserved band width. The flow can be set up in advance and given an identifier. When packet with nonzero Flow label shows up all the routers can look it up in internal tables to see what kind of special treatment it requires. In effect flows are an at tempt to have it both ways the flexibility of datagram network and the guaran tees of virtualcircuit network. Each flow for quality of service purposes is designated by the source address destination address and flow number. This design means that up to 220 flows may be active at the same time between given pair of IP addresses. It also means that even if two flows coming from different hosts but with the same flow label pass through the same router the router will be able to tell them apart using the source and destination addresses. It is expected that flow labels will be cho sen randomly rather than assigned sequentially starting at 1 so routers are ex pected to hash them. of the length . This change means the payload can now be 65535 bytes instead of mere 65515 bytes. tocol handler to pass the packet to. The Hop limit field is used to keep packets from living forever. It is in prac tice the same as the Time to live field in IPv4 namely field that is decremented on each hop. In theory in IPv4 it was time in seconds but no router used it that way so the name was changed to reflect the way it is actually used. Next come the Source address and Destination address fields. Deerings original proposal SIP used 8byte addresses but during the review process many people felt that with 8byte addresses IPv6 would run out of addresses within few decades whereas with 16byte addresses it would never run out. Other peo ple argued that 16 bytes was overkill whereas still others favored using 20byte addresses to be compatible with the OSI datagram protocol. Still another faction wanted variablesized addresses. After much debate and more than few words unprintable in an academic textbook it was decided that fixedlength 16byte ad dresses were the best compromise. new notation has been devised for writing 16byte addresses. They are written as eight groups of four hexadecimal digits with colons between the groups Since many addresses will have many zeros inside them three optimizations have been authorized. First leading zeros within group can be omitted so 0123 can be written as 123. Second one or more groups of 16 zero bits can be replaced by pair of colons. Thus the above address now becomes Perhaps it is unnecessary to be so explicit about it but there are lot of 16 Specifically there are 2128 of them which is approximately 3 1038. If the entire earth land and water were covered with computers IPv6 would allow 7 1023 IP addresses per square meter. Students of chemistry will notice that this number is larger than Avogadros number. While it was not the intention to give every molecule on the surface of the earth its own IP address we are not that far off. In practice the address space will not be used efficiently just as the telephone number address space is not . In RFC 3194 Durand and Huitema cal most pessimistic scenario there will still be well over 1000 IP addresses per square meter of the entire earths surface . In any likely scenario there will be trillions of them per square meter. In short it seems unlikely that we will run out in the foreseeable future. All the fields relating to fragmentation were removed because IPv6 takes different approach to fragmentation. To start with all IPv6conformant hosts are expected to dynamically determine the packet size to use. They do this using the path MTU discovery procedure we described in Sec. 5.5.5. In brief when host sends an IPv6 packet that is too large instead of fragmenting it the router that is unable to forward it drops the packet and sends an error message back to the THE NETWORK LAYER IN THE INTERNET sending host. This message tells the host to break up all future packets to that destination. Having the host send packets that are the right size in the first place is ultimately much more efficient than having the routers fragment them on the fly. Also the minimumsize packet that routers must be able to forward has been formance. With the reliable networks now used combined with the fact that the data link layer and transport layers normally have their own checksums the value of yet another checksum was deemed not worth the performance price it extracted. Removing all these features has resulted in lean and mean network layer protocol. Thus the goal of IPv6a fast yet flexible protocol with plenty of address spaceis met by this design. Some of the missing IPv4 fields are occasionally still needed so IPv6 intro plied to provide extra information but encoded in an efficient way. Six kinds of tional but if more than one is present they must appear directly after the fixed Encrypted security payload variablelength options. For these each item is encoded as tuple. The Type is 1byte field telling which option this is. The Type values have been chosen so that the first 2 bits tell routers that do not know how to process the option what to do. The choices are skip the option discard the packet discard the packet and send back an ICMP packet and discard the packet but do not send ICMP packets for multicast addresses . The Value is any information required up to 255 bytes. must examine. So far one option has been defined support of datagrams exceed sions begin this way. The next 2 bytes indicate that this option defines the datagram size and that the size is 4byte number. The last 4 bytes give the size of the data gram. Sizes less than 65536 bytes are not permitted and will result in the first router discarding the packet and sending back an ICMP error message. Data grams is important for supercomputer applications that must transfer gigabytes of data efficiently across the Internet. tially it will not be used. It was included to make sure that new routing and host software can handle it in case someone thinks of destination option some day. the destination. It is very similar to the IPv4 loose source routing in that all ad dresses listed must be visited in order but other routers not listed may be visited THE NETWORK LAYER IN THE INTERNET served 32bit word follows the first word followed by some number of IPv6 ad ments left field keeps track of how many of the addresses in the list have not yet been visited. It is decremented every time one is visited. When it hits 0 the packet is on its own with no more guidance about what route to follow. Usually at this point it is so close to the destination that the best route is obvious. whether more fragments will follow. In IPv6 unlike in IPv4 only the source host can fragment packet. Routers along the way may not do this. This change is major philosophical break with the original IP but in keeping with current prac tice for IPv4. Plus it simplifies the routers work and makes routing go faster. As mentioned above if router is confronted with packet that is too big it discards the packet and sends an ICMP error packet back to the source. This information packet can be sure of who sent it. The encrypted security payload makes it pos Chap. 8 to accomplish their missions. Given the open design process and the strongly held opinions of many of the people involved it should come as no surprise that many choices made for IPv6 were highly controversial to say the least. We will summarize few of these briefly below. For all the gory details see the RFCs. We have already mentioned the argument about the address length. The result was compromise 16byte fixedlength addresses. Another fight developed over the length of the Hop limit field. One camp felt strongly that limiting the maximum number of hops to 255 was gross mistake. After all paths of 32 hops are common now and 10 years from now much longer paths may be common. These people argued that using huge address size was farsighted but using tiny hop count was short sighted. In their view the greatest sin computer scientist can commit is to pro vide too few bits somewhere. The response was that arguments could be made to increase every field lead ets from wandering around for too long time and 65535 hops is far far too long. making it possible to get from any country to any other country in half dozen hops at most. If it takes more than 125 hops to get from the source and the desti nation to their respective international gateways something is wrong with the na tional backbones. The 8bitters won this one. Another hot potato was the maximum packet size. The supercomputer com munity wanted packets in excess of 64 KB. When supercomputer gets started KB. The argument against large packets is that if 1MB packet hits 1.5Mbps T1 line that packet will tie the line up for over 5 seconds producing very noticeable delay for interactive users sharing the line. compromise was reached can be used to permit jumbograms. third hot topic was removing the IPv4 checksum. Some people likened this move to removing the brakes from car. Doing so makes the car lighter so it can go faster but if an unexpected event happens you have problem. The argument against checksums was that any application that really cares about data integrity has to have transport layer checksum anyway so having an other one in IP is overkill. Fur thermore experience showed that computing the IP checksum was major expense in IPv4. The antichecksum camp won this one and IPv6 does not have Mobile hosts were also point of contention. If portable computer flies halfway around the world can it continue operating there with the same IPv6 ad dress or does it have to use scheme with home agents Some people wanted to build explicit support for mobile hosts into IPv6. That effort failed when no con sensus could be found for any specific proposal. Probably the biggest battle was about security. Everyone agreed it was essen tial. The war was about where to put it and how. First where. The argument for putting it in the network layer is that it then becomes standard service that all applications can use without any advance planning. The argument against it is that really secure applications generally want nothing less than endtoend en cryption where the source application does the encryption and the destination ap plication undoes it. With anything less the user is at the mercy of potentially buggy network layer implementations over which he has no control. The response to this argument is that these applications can just refrain from using the IP securi ty features and do the job themselves. The rejoinder to that is that the people who do not trust the network to do it right do not want to pay the price of slow bulky IP implementations that have this capability even if it is disabled. Another aspect of where to put security relates to the fact that many countries have very stringent export laws concerning cryptography. Some notably France and Iraq also restrict its use domestically so that people cannot have secrets from the government. As result any IP implementation that used THE NETWORK LAYER IN THE INTERNET cryptographic system strong enough to be of much value could not be exported from the United States to customers worldwide. Hav ing to maintain two sets of software one for domestic use and one for export is something most computer vendors vigorously oppose. One point on which there was no controversy is that no one expects the IPv4 Internet to be turned off on Sunday evening and come back up as an IPv6 Inter net Monday morning. Instead isolated islands of IPv6 will be converted ini tially communicating via tunnels as we showed in Sec. 5.5.3. As the IPv6 islands grow they will merge into bigger islands. Eventually all the islands will merge and the Internet will be fully converted. At least that was the plan. Deployment has proved the Achilles heel of IPv6. It remains little used even though all major operating systems fully support it. Most deployments are new situations in which network operatorfor example mobile phone operator needs large number of IP addresses. Many strategies have been defined to help ease the transition. Among them are ways to automat hosts to automatically find the tunnel endpoints. Dualstack hosts have an IPv4 and an IPv6 implementation so that they can select which protocol to use depend ing on the destination of the packet. These strategies will streamline the substan tial deployment that seems inevitable when IPv4 addresses are exhausted. For more information about IPv6 see Davies . In addition to IP which is used for data transfer the Internet has several com panion control protocols that are used in the network layer. They include ICMP ARP is called NDP for IPv6. The operation of the Internet is monitored closely by the routers. When some to the sender by the ICMP . ICMP is also used to test the Internet. About dozen types of ICMP messages are defined. Each ICMP message type is carried encapsulated in an IP packet. The most im The DESTINATION UNREACHABLE message is used when the router cannot locate the destination or when packet with the DF bit cannot be delivered be cause smallpacket network stands in the way. The TIME EXCEEDED message is sent when packet is dropped because its TtL counter has reached zero. This event is symptom that packets are looping or that the counter values are being set too low. One clever use of this error message is the traceroute utility that was devel oped by Van Jacobson in 1987. Traceroute finds the routers along the path from the host to destination IP address. It finds this information without any kind of privileged network support. The method is simply to send sequence of packets to the destination first with TtL of 1 then TtL of 2 3 and so on. The counters on these packets will reach zero at successive routers along the path. These rout ers will each obediently send TIME EXCEEDED message back to the host. From those messages the host can determine the IP addresses of the routers along the path as well as keep statistics and timings on parts of the path. It is not what the TIME EXCEEDED message was intended for but it is perhaps the most useful net work debugging tool of all time. The PARAMETER PROBLEM message indicates that an illegal value has been software or possibly in the software of router transited. The SOURCE QUENCH message was long ago used to throttle hosts that were sending too many packets. When host received this message it was expected to slow down. It is rarely used anymore because when congestion occurs these packets tend to add more fuel to the fire and it is unclear how to respond to them. Congestion control in the Internet is now done largely by taking action in the tran sport layer using packet losses as congestion signal we will study it in detail in The REDIRECT message is used when router notices that packet seems to The ECHO and ECHO REPLY messages are sent by hosts to see if given destination is reachable and currently alive. Upon receiving the ECHO message THE NETWORK LAYER IN THE INTERNET the destination is expected to send back an ECHO REPLY message. These mes sages are used in the ping utility that checks if host is up and on the Internet. The TIMESTAMP REQUEST and TIMESTAMP REPLY messages are similar except that the arrival time of the message and the departure time of the reply are recorded in the reply. This facility can be used to measure network performance. The ROUTER ADVERTISEMENT and ROUTER SOLICITATION messages are used to let hosts find nearby routers. host needs to learn the IP address of at least one router to be able to send packets off the local network. In addition to these messages others have been defined. The online list is Although every machine on the Internet has one or more IP addresses these addresses are not sufficient for sending packets. Data link layer NICs such as Ethernet cards do not understand Internet addresses. In the case of Ethernet every NIC ever manufactured comes equipped with unique Ethernet addresses from IEEE to ensure that no two NICs have the same address . The NICs send and receive frames based on 48bit Ethernet addresses. They know nothing at all about 32bit IP addresses. The question now arises how do IP addresses get mapped onto data link layer addresses such as Ethernet To explain how this works let us use the example of network is switched Ethernet in the Computer Science Dept. It has the prefix 192.32.65.024. The other LAN also switched Ethernet is in Electri cal Engineering and has the prefix 192.32.63.024. The two LANs are connected by an IP router. Each machine on an Ethernet and each interface on the router has unique Ethernet address labeled E1 through E6 and unique IP address on the CS or EE network. Let us start out by seeing how user on host 1 sends packet to user on host 2 on the CS network. Let us assume the sender knows the name of the intended receiver possibly something like eagle.cs.uni.edu. The first step is to find the IP address for host 2. This lookup is performed by DNS which we will study in Chap. 7. For the moment we will just assume that DNS returns the IP address for The upper layer software on host 1 now builds packet with 192.32.65.5 in the Destination address field and gives it to the IP software to transmit. The IP software can look at the address and see that the destination is on the CS network . However it still needs some way to find the destinations Ethernet address to send the frame. One solution is to have configuration file somewhere in the system that maps IP addresses onto Ethernet addresses. While Host 1 to 4 on EE net this solution is certainly possible for organizations with thousands of machines better solution is for host 1 to output broadcast packet onto the Ethernet asking who owns IP address 192.32.65.5. The broadcast will arrive at every ma chine on the CS Ethernet and each one will check its IP address. Host 2 alone will respond with its Ethernet address . In this way host 1 learns that IP ad dress 192.32.65.5 is on the host with Ethernet address E2. The protocol used for asking this question and getting the reply is called ARP . Almost every machine on the Internet runs it. ARP is defined in RFC The advantage of using ARP over configuration files is the simplicity. The system manager does not have to do much except assign each machine an IP ad dress and decide about subnet masks. ARP does the rest. At this point the IP software on host 1 builds an Ethernet frame addressed to E2 puts the IP packet in the payload field and dumps it onto the Ethernet. The IP and Ethernet addresses of this packet are given in for itself scoops it up and causes an interrupt. The Ethernet driver extracts the IP packet from the payload and passes it to the IP software which sees that it is cor rectly addressed and processes it. Various optimizations are possible to make ARP work more efficiently. To start with once machine has run ARP it caches the result in case it needs to contact the same machine shortly. Next time it will find the mapping in its own THE NETWORK LAYER IN THE INTERNET will need to send back reply forcing it too to run ARP to determine the send ers Ethernet address. This ARP broadcast can be avoided by having host 1 in clude its IPtoEthernet mapping in the ARP packet. When the ARP broadcast ar rives at host 2 the pair is entered into host 2s ARP cache. In fact all machines on the Ethernet can enter this mapping into their ARP caches. new IP address entries in the ARP cache should time out after few minutes. clever way to help keep the cached infor mation current and to optimize performance is to have every machine broadcast an ARP looking for its own IP address. There should not be response but side This is known as gratuitous ARP. If response does arrive two machines have been assigned the same IP address. The error must be resolv ed by the network manager before both machines can use the network. send packet to host 4 on the EE network. Host 1 will see that the destination IP address is not on the CS network. It knows to send all such offnet work traffic to the router which is also known as the default gateway. By con vention the default gateway is the lowest address on the network . To send frame to the router host 1 must still know the Ethernet address of the router interface on the CS network. It discovers this by sending an ARP broadcast for 198.31.65.1 from which it learns E3. It then sends the frame. The same lookup mechanisms are used to send packet from one router to the next over sequence of routers in an Internet path. When the Ethernet NIC of the router gets this frame it gives the packet to the IP software. It knows from the network masks that the packet should be sent onto the EE network where it will reach host 4. If the router does not know the Ether source and destination Ethernet and IP addresses that are present in the frames as observed on the CS and EE networks. Observe that the Ethernet addresses change with the frame on each network while the IP addresses remain constant . It is also possible to send packet from host 1 to host 4 without host 1 know ing that host 4 is on different network. The solution is to have the router answer ARPs on the CS network for host 4 and give its Ethernet address E3 as the re sponse. It is not possible to have host 4 reply directly because it will not see the ARP request . The router will then receive frames sent to 192.32.63.8 and forward them onto the EE network. This solution is called proxy ARP. It is used in special cases in which host wants to appear on network even though it actually resides on another network. common situation for example is mobile computer that wants some other node to pick up packets for it when it is not on its home network. ARP makes the assumption that hosts are but that is tedious and errorprone. There is better way and it is called DHCP . With DHCP every network must have DHCP server that is responsible for configuration. When computer is started it has builtin Ethernet or other link layer address embedded in the NIC but no IP address. Much like ARP the com puter broadcasts request for an IP address on its network. It does this by using DHCP DISCOVER packet. This packet must reach the DHCP server. If that server DHCP broadcasts and relay them to the DHCP server wherever it is located. When the server receives the request it allocates free IP address and sends it to the host in DHCP OFFER packet . To be able to do this work even when hosts do not have IP addresses the server identifies host using its Ethernet address for how long an IP address should be allocated. If host leaves the network and does not return its IP address to the DHCP server that address will be perma nently lost. After period of time many addresses may be lost. To prevent that from happening IP address assignment may be for fixed period of time tech nique called leasing. Just before the lease expires the host must ask for DHCP renewal. If it fails to make request or the request is denied the host may no longer use the IP address it was given earlier. DHCP is described in RFCs 2131 and 2132. It is widely used in the Internet dresses. As well as in business and home networks DHCP is used by ISPs to set the parameters of devices over the Internet access link so that customers do not need to phone their ISPs to get this information. Common examples of the infor gateway and the IP addresses of DNS and time servers. DHCP has largely re placed earlier protocols with more limited func So far on our tour of the network layer of the Internet we have focused exclusively on packets as datagrams that are forwarded by IP routers. There is also another kind of technology that is starting to be widely used especially by ISPs in order to move Internet traffic across their networks. This technology is THE NETWORK LAYER IN THE INTERNET called MPLS and it is perilously close to cir cuit switching. Despite the fact that many people in the Internet community have an intense dislike for connectionoriented networking the idea seems to keep coming back. As Yogi Berra once put it it is like deja vu all over again. Howev er there are essential differences between the way the Internet handles route con struction and the way connectionoriented networks do it so the technique is cer tainly not traditional circuit switching. MPLS adds label in front of each packet and forwarding is based on the ternal table makes finding the correct output line just matter of table lookup. Using this technique forwarding can be done very quickly. This advantage was the original motivation behind MPLS which began as proprietary technology known by various names including tag switching. Eventually IETF began to standardize the idea. It is described in RFC 3031 and many other RFCs. The main benefits over time have come to be routing that is flexible and forwarding that is suited to quality of service as well as fast. The first question to ask is where does the label go Since IP packets were not designed for virtual circuits there is no field available for virtualcircuit num vice. The field relates to stacking multiple labels . The TtL field indicates how many more times the packet may be forwarded. It is decremented at each router and if it hits 0 the packet is discarded. This feature prevents infinite looping in the case of routing instability. MPLS falls between the IP network layer protocol and the PPP link layer pro tocol. It is not really layer 3 protocol because it depends on IP or other network layer addresses to set up label paths. It is not really layer 2 protocol either be cause it forwards packets across multiple hops not single link. For this reason MPLS is sometimes described as layer 2.5 protocol. It is an illustration that real protocols do not always fit neatly into our ideal layered protocol model. layer packet or the data link layer frame MPLS is to large extent independent of both layers. Among other things this property means it is possible to build MPLS switches that can forward both IP packets and nonIP packets depending on what shows up. This feature is where the multiprotocol in the name MPLS came from. MPLS can also carry IP packets over nonIP networks. When an MPLSenhanced packet arrives at LSR also the new label to use. This label swapping is used in all virtualcircuit net works. Labels have only local significance and two different routers can feed un related packets with the same label into another router for transmission on the same outgoing line. To be distinguishable at the other end labels have to be uses the same technique. As an aside some people distinguish between forwarding and switching. For warding is the process of finding the best match for destination address in table to decide where to send packets. An example is the longest matching prefix algorithm used for IP forwarding. In contrast switching uses label taken from nitions are far from universal however. Since most hosts and routers do not understand MPLS we should also ask when and how the labels are attached to packets. This happens when an IP packet reaches the edge of an MPLS network. The LER inspects the destination IP address and other fields to see which MPLS path the packet should follow and puts the right label on the front of the packet. Within the MPLS network this label is used to forward the packet. At the other edge of the MPLS network the label has served its purpose and is removed revealing the IP ference from traditional virtual circuits is the level of aggregation. It is certainly possible for each flow to have its own set of labels through the MPLS network. However it is more common for routers to group multiple flows that end at par ticular router or LAN and use single label for them. The flows that are grouped together under single label are said to belong to the same FEC . This class covers not only where the packets are going but also their service class because all the pack ets are treated the same way for forwarding purposes. With traditional virtualcircuit routing it is not possible to group several dis tinct paths with different endpoints onto the same virtualcircuit identifier because THE NETWORK LAYER IN THE INTERNET continue the usual way using the network layer destination address. Actually MPLS goes even further. It can operate at multiple levels at once by adding more than one label to the front of packet. For example suppose that there are many packets that already have different labels that should follow com mon path to some destination. Instead of setting up many label switching paths one for each of the different labels we can set up single path. When the al readylabeled packets reach the start of this path another label is added to the front. This is called stack of labels. The outermost label guides the packets along the path. It is removed at the end of the path and the labels revealed if any removing label to know if there are any additional labels left. It is set to 1 for the bottom label and 0 for all the other labels. that packets follow them. This is one area of major difference between MPLS and conventional virtualcircuit designs. In traditional virtualcircuit networks when user wants to establish connection setup packet is launched into the network to create the path and make the forwarding table entries. MPLS does not involve users in the setup phase. Requiring users to do anything other than send datagram would break too much existing Internet software. Instead the forwarding information is set up by protocols that are combina tion of routing protocols and connection setup protocols. These control protocols are cleanly separated from label forwarding which allows multiple different con trol protocols to be used. One of the variants works like this. When router is fixes belong to its interfaces. It then creates one or more FECs for them allo cates label for each one and passes the labels to its neighbors. They in turn enter the labels in their forwarding tables and send new labels to their neighbors until all the routers have acquired the path. Resources can also be reserved as the path is constructed to guarantee an appropriate quality of service. Other variants can set up different paths such as traffic engineering paths that take unused ca pacity into account and create paths ondemand to support service offerings such as quality of service. Although the basic ideas behind MPLS are straightforward the details are complicated with many variations and use cases that are being actively devel oped. For more information see Davie and Farrel and Davie and Rekhter We have now finished our study of how packets are forwarded in the Internet. It is time to move on to the next topic routing in the Internet. As we mentioned earlier the Internet is made up of large number of independent networks or ASes that are operated by different organizations usually company university or ISP. Inside of its own network an organization can use its own algorithm for internal routing or intradomain routing as it is more com monly known. Nevertheless there are only handful of standard protocols that look at the OSPF protocol that is widely used in practice. An intradomain routing study the problem of routing between independently operated networks or inter domain routing. For that case all networks must use the same interdomain rout ing protocol or exterior gateway protocol. The protocol that is used in the Inter net is BGP . Early intradomain routing protocols used distance vector design based on the distributed BellmanFord algorithm inherited from the ARPANET. RIP is the main example that is used to this day. It works well in small systems but less well as networks get larger. It also suffers from the counttoinfinity problem and generally slow convergence. in 1988 IETF began work on link state protocol for intradomain routing. That protocol called OSPF became standard in 1990. It drew on protocol called ISIS which became an ISO standard. Because of their shared heritage the two protocols are much more alike than different. For the complete story see RFC 2328. They are the dominant intradomain routing protocols and most router ven dors now support both of them. OSPF is more widely used in company networks and ISIS is more widely used in ISP networks. Of the two we will give sketch of how OSPF works. Given the long experience with other routing protocols the group designing OSPF had long list of requirements that had to be met. First the algorithm had to be published in the open literature hence the in OSPF. proprietary THE NETWORK LAYER IN THE INTERNET solution owned by one company would not do. Second the new protocol had to support variety of distance metrics including physical distance delay and so on. Third it had to be dynamic algorithm one that adapted to changes in the topology automatically and quickly. Fourth and new for OSPF it had to support routing based on type of service. The new protocol had to be able to route realtime traffic one way and other traf fic different way. At the time IP had Type of service field but no existing routing protocol used it. This field was included in OSPF but still nobody used it and it was eventually removed. Perhaps this requirement was ahead of its time as it preceded IETFs work on differentiated services which has rejuvenated classes Fifth and related to the above OSPF had to do load balancing splitting the load over multiple lines. Most previous protocols sent all packets over single best route even if there were two routes that were equally good. The other route was not used at all. In many cases splitting the load over multiple routes gives better performance. Sixth support for hierarchical systems was needed. By 1988 some networks had grown so large that no router could be expected to know the entire topology. OSPF had to be designed so that no router would have to. Seventh some modicum of security was required to prevent funloving stu provision was needed for dealing with routers that were connected to the Internet via tunnel. Previous protocols did not handle this well. OSPF supports both pointtopoint links and broadcast net works . Actually it is able to support networks with multiple routers each of which can communicate directly with the others even if they do not have broadcast capability. Earlier protocols did not handle this case well. are omitted because they do not generally play role in OSPF while routers and connected to other routers by pointtopoint links and to networks to reach the hosts on those networks. However routers R3 R4 and R5 are connected by broadcast LAN such as switched Ethernet. OSPF operates by abstracting the collection of actual networks routers and links into directed graph in which each arc is assigned weight . pointtopoint connection between two routers is represented by pair of arcs one in each direction. Their weights may be different. broadcast network is represented by node for the network itself plus node for each router. The arcs from that network node to the routers have weight 0. They are important nonetheless as without them there is no path through the network. Other net works which have only hosts have only an arc reaching them and not one re turning. This structure gives routes to hosts but not through them. What OSPF fundamentally does is represent the actual network as graph like this and then use the link state method to have every router compute the shortest path from itself to all other nodes. Multiple paths may be found that are equally short. In this case OSPF remembers the set of shortest paths and during packet forwarding traffic is split across them. This helps to balance load. It is called ECMP . Many of the ASes in the Internet are themselves large and nontrivial to man age. To work at this scale OSPF allows an AS to be divided into numbered areas where an area is network or set of contiguous networks. Areas do not overlap but need not be exhaustive that is some routers may belong to no area. Routers that lie wholly within an area are called internal routers. An area is generalization of an individual network. Outside an area its destinations are visi ble but not its topology. This characteristic helps routing to scale. Every AS has backbone area called area 0. The routers in this area are called backbone routers. All areas are connected to the backbone possibly by tunnels so it is possible to go from any area in the AS to any other area in the AS via the backbone. tunnel is represented in the graph as just another arc with cost. As with other areas the topology of the backbone is not visible outside the Each router that is connected to two or more areas is called an area border router. It must also be part of the backbone. The job of an area border router is THE NETWORK LAYER IN THE INTERNET the details of the topology within an area. Passing cost information allows hosts in other areas to find the best area border router to use to enter an area. Not passing topology information reduces traffic and simplifies the shortestpath computations of routers in other areas. However if there is only one border router out of an area is called stub area. The last kind of router is the AS boundary router. It injects routes to exter nal destinations on other ASes into the area. The external routes then appear as destinations that can be reached via the AS boundary router with some cost. An external route can be injected at one or more AS boundary routers. The relation One router may play multiple roles for example border router is also back During normal operation each router within an area has the same link state database and runs the same shortest path algorithm. Its main job is to calculate the shortest path from itself to every other router and network in the entire AS. An area border router needs the databases for all the areas to which it is connected and must run the shortest path algorithm for each area separately. For source and destination in the same area the best intraarea route is chosen. For source and destination in different areas the interarea route must go from the source to the backbone across the backbone to the destination area and then to the destination. This algorithm forces star configuration on OSPF with the backbone being the hub and the other areas being spokes. Because the route with the lowest cost is chosen rout ers in different parts of the network may use different area border routers to enter the backbone and destination area. Packets are routed from source to destination as is. They are not encapsulated or tunneled . Also routes to external destinations may include the external cost from the AS boundary router over the external path if desired or just the cost internal to the AS. When router boots it sends HELLO messages on all of its pointtopoint lines and multicasts them on LANs to the group consisting of all the other routers. From the responses each router learns who its neighbors are. Routers on the same LAN are all neighbors. OSPF works by exchanging information between adjacent routers which is not the same as between neighboring routers. In particular it is inefficient to have every router on LAN talk to every other router on the LAN. To avoid this situa tion one router is elected as the designated router. It is said to be adjacent to all the other routers on its LAN and exchanges information with them. In effect it is acting as the single node that represents the LAN. Neighboring routers that are not adjacent do not exchange information with each other. backup de designated router crash and need to be replaced immediately. During normal operation each router periodically floods LINK STATE and provide the costs used in the topological database. The flooding messages are acknowledged to make them reliable. Each message has sequence number so what it currently has. Routers also send these messages when link goes up or down or its cost changes. DATABASE DESCRIPTION messages give the sequence numbers of all the link state entries currently held by the sender. By comparing its own values with those of the sender the receiver can determine who has the most recent values. These messages are used when link is brought up. Either partner can request link state information from the other one by using LINK STATE REQUEST messages. The result of this algorithm is that each pair of adjacent routers checks to see who has the most recent data and new information is spread throughout the area this way. All these messages are sent directly in IP Using flooding each router informs all the other routers in its area of its links to other routers and networks and the cost of these links. This information allows each router to construct the graph for its area and compute the shortest paths. The backbone area does this work too. In addition the backbone routers accept information from the area border routers in order to compute the best route from each backbone router to every other router. This information is propagated back to the area border routers which advertise it within their areas. Using this information internal routers can select the best route to destination outside their area including the best exit router to the backbone. Within single AS OSPF and ISIS are the protocols that are commonly used. Between ASes different protocol called BGP is used. different protocol is needed because the goals of an intradomain protocol and an interdomain protocol are not the same. All an intradomain proto col has to do is move packets as efficiently as possible from the source to the dest ination. It does not have to worry about politics. In contrast interdomain routing protocols have to worry about politics great deal . For example corporate AS might want the ability to send packets to any Internet site and receive packets from any Internet site. However it might be unwilling to carry transit packets originating in foreign AS and end ing in different foreign AS even if its own AS is on the shortest path between the two foreign ASes . On the other hand it might be willing to carry transit traffic for its neighbors or even for specific other ASes that paid it for this service. Telephone companies for example might be happy to act as carriers for their customers but not for others. Exterior gateway protocols in general and BGP in particular have been designed to allow many kinds of routing policies to be enforced in the interAS traffic. Typical policies involve political security or economic considerations. few examples of possible routing constraints are 1. Do not carry commercial traffic on the educational network. 2. Never send traffic from the Pentagon on route through Iraq. 3. Use TeliaSonera instead of Verizon because it is cheaper. 4. Dont use ATT in Australia because performance is poor. 5. Traffic starting or ending at Apple should not transit Google. As you might imagine from this list routing policies can be highly individual. They are often proprietary because they contain sensitive business information. However we can describe some patterns that capture the reasoning of the com pany above and that are often used as starting point. routing policy is implemented by deciding what traffic can flow over which of the links between ASes. One common policy is that customer ISP pays anoth er provider ISP to deliver packets to any other destination on the Internet and re ceive packets sent from any other destination. The customer ISP is said to buy transit service from the provider ISP. This is just like customer at home buying Internet access service from an ISP. To make it work the provider should adver tise routes to all destinations on the Internet to the customer over the link that con nects them. In this way the customer will have route to use to send packets anywhere. Conversely the customer should advertise routes only to the destina tions on its network to the provider. This will let the provider send traffic to the customer only for those addresses the customer does not want to handle traffic in tended for other destinations. that are connected. The connection is often made with link at IXPs facilities to which many ISPs have link for the purpose of connecting with other ISPs. AS2 AS3 and AS4 are customers of AS1. They buy transit service from it. Thus when source sends to destination the packets the opposite direction to the packets. AS4 advertises as destination to its tran sit provider AS1 to let sources reach via AS1. Later AS1 advertises route to to its other customers including AS2 to let the customers know that they can advertisements them with connectivity so they can interact with any host on the Internet. Howev er they have to pay for this privilege. Suppose that AS2 and AS3 exchange lot of traffic. Given that their networks are connected already if they want to they THE NETWORK LAYER IN THE INTERNET can use different policythey can send traffic directly to each other for free. This will reduce the amount of traffic they must have AS1 deliver on their behalf and hopefully it will reduce their bills. This policy is called peering. To implement peering two ASes send routing advertisements to each other for the addresses that reside in their networks. Doing so makes it possible for AS2 to send AS3 packets from destined to and vice versa. However note that peering allows traffic from destined for to be sent directly to AS4. What hap pens if sends packet to AS3 is only advertising route to to AS4. It is not advertising route to . The consequence is that traffic will not pass from AS4 to AS3 to AS2 even though physical path exists. This restriction is exactly what AS3 wants. It peers with AS4 to exchange traffic but does not want to carry traffic from AS4 to other parts of the Internet since it is not being paid to so do. In stead AS4 gets transit service from AS1. Thus it is AS1 who will carry the packet Now that we know about transit and peering we can also see that and have transit arrangements. For example must buy Internet access from AS2. might be single home computer or company network with many LANs. How ever it does not need to run BGP because it is stub network that is connected to the rest of the Internet by only one link. So the only place for it to send packets destined outside of the network is over the link to AS2. There is nowhere else to go. This path can be arranged simply by setting up default route. For this rea son we have not shown and as ASes that participate in interdomain rout On the other hand some company networks are connected to multiple ISPs. This technique is used to improve reliability since if the path through one ISP fails the company can use the path via the other ISP. This technique is called multihoming. In this case the company network is likely to run an interdomain routing protocol to tell other ASes which addresses should be reached via which ISP links. Many variations on these transit and peering policies are possible but they al ready illustrate how business relationships and control over where route advertise ments go can implement different kinds of policies. Now we will consider in more detail how routers running BGP advertise routes to each other and select paths over which to forward packets. BGP is form of distance vector protocol but it is quite unlike intradomain distance vector protocols such as RIP. We have already seen that policy instead of minimum distance is used to pick which routes to use. Another large dif ference is that instead of maintaining just the cost of the route to each destination each BGP router keeps track of the path used. This approach is called path vec tor protocol. The path consists of the next hop router and the sequence of ASes or AS path that the route with each other by establishing TCP connections. Operating this way provides re liable communication and also hides all the details of the network being passed are three ASes and the middle one is providing transit to the left and right ISPs. route advertisement to prefix starts in AS3. When it is propagated across the hop router of R3a. At the bottom it has the same AS path but different next hop because it came across different link. This advertisement continues to propagate Carrying the complete path with the route makes it easy for the receiving router to detect and break routing loops. The rule is that each router that sends route outside of the AS prepends its own AS number to the route. When router receives route it checks to see if its own AS number is already in the AS path. If it is loop has been detected and the advertisement is discarded. However and somewhat ironically it was realized in counttoinfinity problem . There are no longlived loops but routes can sometimes be slow to converge and have transient loops. Giving list of ASes is very coarse way to specify path. An AS might be small company or an international backbone network. There is no way of telling from the route. BGP does not even try because different ASes may use different intradomain protocols whose costs cannot be compared. Even if they could be compared an AS may not want to reveal its internal metrics. This is one of the ways that interdomain routing protocols differ from intradomain protocols. So far we have seen how route advertisement is sent across the link between two ISPs. We still need some way to propagate BGP routes from one side of the ISP to the other so they can be sent on to the next ISP. This task could be handled by the intradomain protocol but because BGP is very good at scaling to large net works variant of BGP is often used. It is called iBGP to distin guish it from the regular use of BGP as eBGP . The rule for propagating routes inside an ISP is that every router at the bound ary of the ISP learns of all the routes seen by all the other boundary routers for If one boundary router on the ISP learns of prefix to IP 128.208.0.016 all the other routers will learn of this prefix. The prefix will then be reachable from all parts of the ISP no matter how packets enter the ISP from ample router R2b will know that it can reach via either router R2c at top or so that routers on the far side of the ISP know which router to use to exit the ISP on the other side. This can be seen in the leftmost routes in which the next hop points to router in the same ISP and not router in the next ISP. We can now describe the key missing piece which is how BGP routers choose which route to use for each destination. Each BGP router may learn route for given destination from the router it is connected to in the next ISP and from all of the other boundary routers . Each router must decide which route in this set of routes is the best one to use. Ultimately the answer is that it is up to the ISP to write some policy to pick the preferred route. However this explana tion is very general and not at all satisfying so we can at least describe some common strategies. The first strategy is that routes via peered networks are chosen in preference to routes via transit providers. The former are free the latter cost money. simi lar strategy is that customer routes are given the highest preference. It is only good business to send traffic directly to the paying customers. different kind of strategy is the default rule that shorter AS paths are better. This is debatable given that an AS could be network of any size so path through three small ASes could actually be shorter than path through one big AS. However shorter tends to be better on average and this rule is common at the top router R1a. Packets sent from exit via the bottom router R1b. The reason is that both and are taking the lowestcost path or quickest route out of AS1. Because they are located in different parts of the ISP the quickest exit for each one is different. The same thing happens as the packets pass through AS2. On the last leg AS3 has to carry the packet from through its own network. This strategy is known as early exit or hotpotato routing. It has the curious side effect of tending to make routes asymmetric. For example consider the path taken when sends packet back to . The packet will exit AS3 quickly at the top router to avoid wasting its resources. Similarly it will stay at the top when AS2 passes it to AS1 as quickly as possible. Then the packet will have longer journey in AS1. This is mirror image of the path taken from to . The above discussion should make clear that each BGP router chooses its own best route from the known possibilities. It is not the case as might naively be ex pected that BGP chooses path to follow at the AS level and OSPF chooses paths within each of the ASes. BGP and the interior gateway protocol are integrated much more deeply. This means that for example BGP can find the best exit point from one ISP to the next and this point will vary across the ISP as in the case of the hotpotato policy. It also means that BGP routers in different parts of one AS may choose different AS paths to reach the same destination. compatible choices given all of this freedom but this can be done in practice. Amazingly we have only scratched the surface of BGP. For more infor However realize that much of its complexity lies with policies which are not de scribed in the specification of the BGP protocol. Normal IP communication is between one sender and one receiver. However for some applications it is useful for process to be able to send to large num ber of receivers simultaneously. Examples are streaming live sports event to handling digital conference telephone calls. IP supports onetomany communication or multicasting using class IP ad dresses. Each class address identifies group of hosts. Twentyeight bits are available for identifying groups so over 250 million groups can exist at the same time. When process sends packet to class address besteffort attempt is made to deliver it to all the members of the group addressed but no guarantees are given. Some members may not get the packet. The range of IP addresses 224.0.0.024 is reserved for multicast on the local network. In this case no routing protocol is needed. The packets are multicast by simply broadcasting them on the LAN with multicast address. All hosts on the LAN receive the broadcasts and hosts that are members of the group process the packet. Routers do not forward the packet off the LAN. Some examples of local Other multicast addresses may have members on different networks. In this case routing protocol is needed. But first the multicast routers need to know which hosts are members of group. process asks its host to join in specific group. It can also ask its host to leave the group. Each host keeps track of which groups its processes currently belong to. When the last process on host leaves group the host is no longer member of that group. About once minute each multicast router sends query packet to all the hosts on its LAN . It is described in Any of several multicast routing protocols may be used to build multicast spanning trees that give paths from senders to all of the members of the group. The algorithms that are used are the ones we described in Sec. 5.2.8. Within an AS the main protocol used is PIM . PIM comes in several flavors. In Dense Mode PIM pruned reverse path forwarding tree is created. This is suited to situations in which members are everywhere in the network such as distributing files to many servers within data center net work. In Sparse Mode PIM spanning trees that are built are similar to corebased trees. This is suited to situations such as content provider multicasting TV to subscribers on its IP network. variant of this design called SourceSpecific Multicast PIM is optimized for the case that there is only one sender to the group. routes when the group members are in more than one AS. Many users of the Internet have mobile computers and want to stay connected when they are away from home and even on the road in between. Unfortunately the IP addressing system makes working far from home easier said than done as we will describe shortly. When people began demanding the ability anyway IETF set up Working Group to find solution. The Working Group quickly for mulated number of goals considered desirable in any solution. The major ones 1. Each mobile host must be able to use its home IP address anywhere. 2. Software changes to the fixed hosts were not permitted. 3. Changes to the router software and tables were not permitted. 4. Most packets for mobile hosts should not make detours on the way. 5. No overhead should be incurred when mobile host is at home. The solution chosen was the one described in Sec. 5.2.10. In brief every site that wants to allow its users to roam has to create helper at the site called home agent. When mobile host shows up at foreign site it obtains new IP address at the foreign site. The mobile then tells the home agent where it is now by giving it the careof address. When packet for the mobile arrives at the home site and the mobile is elsewhere the home agent grabs the packet and tunnels it to the mobile at the current careof address. The mobile can send reply packets directly to whoever it is communicating with but still using its home address as the source address. This solution meets all the re quirements stated above except that packets for mobile hosts do make detours. Now that we have covered the network layer of the Internet we can go into the solution in more detail. The need for mobility support in the first place comes from the IP addressing scheme itself. Every IP address contains network num ber and host number. For example consider the machine with IP address 160.80.40.2016. The 160.80 gives the network number the 40.20 is the host number. Routers all over the world have routing tables telling which link to use to get to network 160.80. Whenever packet comes in with destination IP address of the form 160.80.xxx.yyy it goes out on that line. If all of sudden the ma chine with that address is carted off to some distant site the packets for it will continue to be routed to its home LAN . At this stage there are two optionsboth unattractive. The first is that we could create route to more specific prefix. That is if the distant site advertises route to 160.80.40.2032 packets sent to the destination will start arriving in the right place again. This option depends on the longest matching prefix algorithm that is used at routers. However we have added route to an IP prefix with sin gle IP address in it. All ISPs in the world will learn about this prefix. If everyone changes global IP routes in this way when they move their computer each router would have millions of table entries at astronomical cost to the Internet. This option is not workable. The second option is to change the IP address of the mobile. True packets sent to the home IP address will no longer be delivered until all the relevant peo ple programs and databases are informed of the change. But the mobile can still This option handles mobility at higher layer. It is what typically happens when user takes laptop to coffee store and uses the Internet via the local wireless network. The disadvantage is that it breaks some applications and it does not keep connectivity as the mobile moves around. As an aside mobility can also be handled at lower layer the link layer. This is what happens when using laptop on single 802.11 wireless network. The IP address of the mobile does not change and the network path remains the same. It is the wireless link that is providing mobility. However the degree of mobility is limited. If the laptop moves too far it will have to connect to the Internet via an The mobile IP solution for IPv4 is given in RFC 3344. It works with the existing Internet routing and allows hosts to stay connected with their own IP ad dresses as they move about. For it to work the mobile must be able to discover when it has moved. This is accomplished with ICMP router advertisement and solicitation messages. Mobiles listen for periodic router advertisements or send solicitation to discover the nearest router. If this router is not the usual address of the router when the mobile is at home it must be on foreign network. If this router has changed since last time the mobile has moved to another foreign net work. This same mechanism lets mobile hosts find their home agents. To get careof IP address on the foreign network mobile can simply use DHCP. Alternatively if IPv4 addresses are in short supply the mobile can send and receive packets via foreign agent that already has an IP address on the net work. The mobile host finds foreign agent using the same ICMP mechanism used to find the home agent. After the mobile obtains an IP address or finds for eign agent it is able to use the network to send message to its home agent The home agent needs way to intercept packets sent to the mobile only when the mobile is not at home. ARP provides convenient mechanism. To send packet over an Ethernet to an IP host the router needs to know the Ethernet ad dress of the host. The usual mechanism is for the router to send an ARP query to ask for example what is the Ethernet address of 160.80.40.20. When the mobile dress. When the mobile is away the home agent responds to this query by giving its Ethernet address. The router then sends packets for 160.80.40.20 to the home agent. Recall that this is called proxy ARP. home or arrives back home another ARP technique called gratuitous ARP can be used. Basically the mobile or home agent send themselves an ARP query for the mobile IP address that supplies the right answer so that the router notices and Tunneling to send packet between the home agent and the mobile host at the tined for the careof address. When the encapsulated packet arrives at the careof As with many Internet protocols the devil is in the details and most often the details of compatibility with other protocols that are deployed. There are two The second complication is that some ISPs check the source IP addresses of packets to see that they match where the routing protocol believes the source should be located. This technique is called ingress filtering and it is security measure intended to discard traffic with seemingly incorrect addresses that may be malicious. However packets sent from the mobile to other Internet hosts when it is on foreign network will have source IP address that is out of place so they will be discarded. To get around this problem the mobile can use the careof ad dress as source to tunnel the packets back to the home agent. From here they that the route is more roundabout. message asking it to please forward all of Robertas packets to some IP address it had better not comply unless it is convinced that Roberta is the source of this re quest and not somebody trying to impersonate her. Cryptographic authentication protocols are used for this purpose. We will study such protocols in Chap. 8. Mobility protocols for IPv6 build on the IPv4 foundation. The scheme above suffers from the triangle routing problem in which packets sent to the mobile take dogleg through distant home agent. In IPv6 route optimization is used to fol low direct path between the mobile and other IP addresses after the initial pack ets have followed the long route. Mobile IPv6 is defined in RFC 3775. There is another kind of mobility that is also being defined for the Internet. Some airplanes have builtin wireless networking that passengers can use to con nect their laptops to the Internet. The plane has router that connects to the rest of the Internet via wireless link. So now we have flying router which means that the whole network is mobile. Network mobility designs support this situation without the laptops realizing that the plane is mobile. As far as they are concerned it is just another network. Of course some of the laptops may be using mobile IP to keep their home addresses while they are on the plane so we have two levels of mobility. Network mobility is de fined for IPv6 in RFC 3963. The network layer provides services to the transport layer. It can be based on either datagrams or virtual circuits. In both cases its main job is routing packets from the source to the destination. In datagram networks routing decision is made on every packet. In virtualcircuit networks it is made when the virtual cir Many routing algorithms are used in computer networks. Flooding is simple algorithm to send packet along all paths. Most algorithms find the shortest path and adapt to changes in the network topology. The main algorithms are distance vector routing and link state routing. Most actual networks use one of these. Other important routing topics are the use of hierarchy in large networks routing for mobile hosts and broadcast multicast and anycast routing. Networks can easily become congested leading to increased delay and lost packets. Network designers attempt to avoid congestion by designing the network to have enough capacity choosing uncongested routes refusing to accept more traffic signaling sources to slow down and shedding load. The next step beyond just dealing with congestion is to actually try to achieve promised quality of service. Some applications care more about throughput whereas others care more about delay and jitter. The methods that can be used to provide different qualities of service include combination of traffic shaping reserving resources at routers and admission control. Approaches that have been designed for good quality of service include IETF integrated services and differentiated services. Networks differ in various ways so when multiple networks are intercon packet sizes fragmentation may be needed. Different networks may run different routing protocols internally but need to run common protocol externally. Some work but if the source and destination networks are different this approach fails. The Internet has rich variety of protocols related to the network layer. These include the datagram protocol IP and associated control protocols such as ICMP ARP and DHCP. connectionoriented protocol called MPLS carries IP packets across some networks. One of the main routing protocols used within net works is OSPF and the routing protocol used across networks is BGP. The Inter developed and is eversoslowly being deployed. 1. Give two example computer applications for which connectionoriented service is ap propriate. Now give two examples for which connectionless service is best. 2. Datagram networks route each packet as separate unit independent of all others. Virtualcircuit networks do not have to do this since each data packet follows prede termined route. Does this observation mean that virtualcircuit networks do not need the capability to route isolated packets from an arbitrary source to an arbitrary destina tion Explain your answer. 3. Give three examples of protocol parameters that might be negotiated when con nection is set up. 4. Assuming that all routers and hosts are working properly and that all software in both is free of all errors is there any chance however small that packet will be delivered to the wrong destination 5. Give simple heuristic for finding two paths through network from given source to given destination that can survive the loss of any communication line . The routers are considered reliable enough so it is not necessary to line to use and the cost. 7. If costs are recorded as 8bit numbers in 50router network and distance vectors are exchanged twice second how much bandwidth per line is chewed up by the distributed routing algorithm Assume that each router has three lines to other just an accident here or does it hold for all networks under all circumstances 9. For hierarchical routing with 4800 routers what region and cluster sizes should be chosen to minimize the size of the routing table for threelayer hierarchy good starting place is the hypothesis that solution with clusters of regions of routers is close to optimal which means that is about the cube root of 4800 . Use trial and error to check out combinations where all three parameters are in the general vicinity of 16. 10. In the text it was stated that when mobile host is not at home packets sent to its home LAN are intercepted by its home agent on that LAN. For an IP network on an the sink tree 13. Compute multicast spanning tree for router in the following network for group with members at routers and . its tables. It suddenly needs route to . It sends out broadcasts with TtL set to 1 2 3 and so on. How many rounds does it take to find route 15. As possible congestion control mechanism in network using virtual circuits inter nally router could refrain from acknowledging received packet until it knows its last transmission along the virtual circuit was received successfully and it has free buffer. For simplicity assume that the routers use stopandwait protocol and that each virtual circuit has one buffer dedicated to it for each direction of traffic. If it takes sec to transmit packet and there are routers on the path what is the rate at which packets are delivered to the destination host As sume that transmission errors are rare and that the hostrouter connection is infinitely 16. datagram network allows routers to drop packets whenever they need to. The probability of router discarding packet is . Consider the case of source host connected to the source router which is connected to the destination router and then to the destination host. If either of the routers discards packet the source host even tually times out and tries again. If both hostrouter and routerrouter lines are counted hops required per received packet 17. Describe two major differences between the ECN method and the RED method of congestion avoidance. 18. token bucket scheme is used for traffic shaping. new token is put into the bucket every 5 μsec. Each token is good for one short packet which contains 48 bytes of data. What is the maximum sustainable data rate 19. computer on 6Mbps network is regulated by token bucket. The token bucket is filled at rate of 1 Mbps. It is initially filled to capacity with 8 megabits. How long can the computer transmit at the full 6 Mbps and another channel of bandwidth 1 MBsec for flow from host 2. At the same time host 4 requests channel of bandwidth 2 MBsec for flow from host 1 and host 5 re quests channel of bandwidth 1 MBsec for flow from host 2. How much total bandwidth will be reserved for these requests at routers and 21. router can process 2 million packetssec. The load offered to it is 1.5 million pack etssec on average. If route from source to destination contains 10 routers how much time is spent being queued and serviced by the router 22. Consider the user of differentiated services with expedited forwarding. Is there guarantee that expedited packets experience shorter delay than regular packets 23. Suppose that host is connected to router 1 1 is connected to another router ery to . Show the Total length Identification DF MF and Fragment offset fields of link R1R2 can support maximum frame size of 512 bytes including an 8byte frame Assuming that packets live for 10 sec what is the maximum line speed the router can operate at without danger of cycling through the IP datagram ID number space 25. An IP datagram using the Strict source routing option has to be fragmented. Do you think the option is copied into each fragment or is it sufficient to just put it in the first fragment Explain your answer. 26. Suppose that instead of using 16 bits for the network part of class address origi nally 20 bits had been used. How many class networks would there have been 27. Convert the IP address whose hexadecimal representation is C22F1582 to dotted decimal notation. 28. network on the Internet has subnet mask of 255.255.240.0. What is the maximum number of hosts it can handle 29. While IP addresses are tried to specific networks Ethernet addresses are not. Can you think of good reason why they are not 30. large number of consecutive IP addresses are available starting at 198.16.0.0. Sup pose that four organizations and request 4000 2000 4000 and 8000 ad dresses respectively and in that order. For each of these give the first IP address as signed the last IP address assigned and the mask in the .zs notation. what can be done instead 33. router has the following entries in its routing table For each of the following IP addresses what does the router do if packet with that address arrives 34. Many companies have policy of having two routers connecting the com pany to the Internet to provide some redundancy in case one of them goes down. Is this policy still possible with NAT Explain your answer. 35. You have just explained the ARP protocol to friend. When you are all done he says Ive got it. ARP provides service to the network layer so it is part of the data link layer. What do you say to him 36. Describe way to reassemble IP fragments at the destination. 37. Most IP datagram reassembly algorithms have timer to avoid having lost fragment tie up reassembly buffers forever. Suppose that datagram is fragmented into four fragments. The first three fragments arrive but the last one is delayed. Eventually the timer goes off and the three fragments in the receivers memory are discarded. little later the last fragment stumbles in. What should be done with it design was chosen 39. person who lives in Boston travels to Minneapolis taking her portable computer with her. To her surprise the LAN at her destination in Minneapolis is wireless IP LAN so she does not have to plug in. Is it still necessary to go through the entire bus iness with home agents and foreign agents to make email and other traffic arrive cor 40. IPv6 uses 16byte addresses. If block of 1 million addresses is allocated every picosecond how long will the addresses last 42. When the IPv6 protocol is introduced does the ARP protocol have to be changed If so are the changes conceptual or technical 43. Write program to simulate routing using flooding. Each packet should contain counter that is decremented on each hop. When the counter gets to zero the packet is discarded. Time is discrete with each line handling one packet per time interval. line are flooded and only the best lines are flooded. Compare flooding with deterministic routing in terms of both delay and the bandwidth 44. Write program that simulates computer network using discrete time. The first packet on each router queue makes one hop per time interval. Each router has only finite number of buffers. If packet arrives and there is no room for it it is discarded and not retransmitted. Instead there is an endtoend protocol complete with time outs and acknowledgement packets that eventually regenerates the packet from the source router. Plot the throughput of the network as function of the endtoend time out interval parameterized by error rate. 45. Write function to do forwarding in an IP router. The procedure has one parameter an IP address. It also has access to global table consisting of an array of triples. Each triple contains three integers an IP address subnet mask and the outline line to use. The function looks up the IP address in the table using CIDR and returns the line to use as its value. 46. Use the traceroute or tracert programs to trace the route from your computer to various universities on other continents. Make list of transoceanic Together with the network layer the transport layer is the heart of the proto col hierarchy. The network layer provides endtoend packet delivery using data grams or virtual circuits. The transport layer builds on the network layer to pro vide data transport from process on source machine to process on destina tion machine with desired level of reliability that is independent of the physical networks currently in use. It provides the abstractions that applications need to use the network. Without the transport layer the whole concept of layered proto ity connections and congestion control protocols such as TCP and UDP and per vice. We look at what kind of service is provided to the application layer. To transport layer primitives. First comes simple one to show the basic ideas. Then comes the interface commonly used in the Internet. The ultimate goal of the transport layer is to provide efficient reliable and costeffective data transmission service to its users normally processes in the ap plication layer. To achieve this the transport layer makes use of the services pro vided by the network layer. The software andor hardware within the transport layer that does the work is called the transport entity. The transport entity can be located in the operating system kernel in library package bound into network applications in separate user process or even on the network interface card. The first two options are most common on the Internet. The relationship Just as there are two types of network service connectionoriented and con nectionless there are also two types of transport service. The connectionoriented transport service is similar to the connectionoriented network service in many ways. In both cases connections have three phases establishment data transfer and release. Addressing and flow control are also similar in both layers. Fur thermore the connectionless transport service is also very similar to the con nectionless network service. However note that it can be difficult to provide connectionless transport service on top of connectionoriented network service since it is inefficient to set up connection to send single packet and then tear it down immediately afterwards. The obvious question is this if the transport layer service is so similar to the network layer service why are there two distinct layers Why is one layer not adequate The answer is subtle but crucial. The transport code runs entirely on the users machines but the network layer mostly runs on the routers which are operated by the carrier . What happens if the network layer offers inadequate service What if it frequently loses packets What happens if routers crash from time to time layer so they cannot solve the problem of poor service by using better routers or putting more error handling in the data link layer because they dont own the rout ers. The only possibility is to put on top of the network layer another layer that improves the quality of the service. If in connectionless network packets are lost or mangled the transport entity can detect the problem and compensate for it by using retransmissions. If in connectionoriented network transport entity is informed halfway through long transmission that its network connection has been abruptly terminated with no indication of what has happened to the data cur rently in transit it can set up new network connection to the remote transport entity. Using this new network connection it can send query to its peer asking which data arrived and which did not and knowing where it was pick up from where it left off. In essence the existence of the transport layer makes it possible for the tran sport service to be more reliable than the underlying network. Furthermore the transport primitives can be implemented as calls to library procedures to make them independent of the network primitives. The network service calls may vary considerably from one network to another . Hiding the network service behind set of transport service primitives ensures that changing the network merely requires replacing one set of library procedures with another one that does the same thing with different underlying Thanks to the transport layer application programmers can write code accord ing to standard set of primitives and have these programs work on wide variety of networks without having to worry about dealing with different network inter faces and levels of reliability. If all real networks were flawless and all had the same service primitives and were guaranteed never ever to change the transport layer might not be needed. However in the real world it fulfills the key function of isolating the upper layers from the technology design and imperfections of the For this reason many people have made qualitative distinction between lay ers 1 through 4 on the one hand and layer above 4 on the other. The bottom four layers can be seen as the transport service provider whereas the upper layer are the transport service user. This distinction of provider versus user has considerable impact on the design of the layers and puts the transport layer in key position since it forms the major boundary between the provider and user of the reliable data transmission service. It is the level that applications see. To allow users to access the transport service the transport layer must provide some operations to application programs that is transport service interface. simple transport service and its interface to see the bare essen The transport service is similar to the network service but there are also some important differences. The main difference is that the network service is intended to model the service offered by real networks warts and all. Real networks can lose packets so the network service is generally unreliable. The connectionoriented transport service in contrast is reliable. Of course real networks are not errorfree but that is precisely the purpose of the transport layerto provide reliable service on top of an unreliable network. As an example consider two processes on single machine connected by pipe in UNIX . They assume the connection between them is 100 perfect. They do not want to know about acknowledgements lost packets congestion or anything at all like that. What they want is 100 reliable connection. Process puts data into one end of the pipe and process takes it out of the other. This is what the connectionoriented transport service is all abouthiding the imperfections of the network service so that user processes can just assume the existence of an errorfree bit stream even when they are on different machines. As an aside the transport layer can also provide unreliable ser vice. However there is relatively little to say about that besides its datagrams so we will mainly concentrate on the connectionoriented transport service in this ing and streaming multimedia that build on connectionless transport service and we will say little bit about that later on. second difference between the network service and transport service is whom the services are intended for. The network service is used only by the tran sport entities. Few users write their own transport entities and thus few users or programs ever see the bare network service. In contrast many programs see the transport primitives. Consequently the transport service must be convenient and easy to use. To get an idea of what transport service might be like consider the five gives the essential flavor of what connectionoriented transport interface has to do. It allows application programs to establish use and then release connections which is sufficient for many applications. To see how these primitives might be used consider an application with ser ver and number of remote clients. To start with the server executes LISTEN primitive typically by calling library procedure that makes system call that Request release of the connection blocks the server until client turns up. When client wants to talk to the server it executes CONNECT primitive. The transport entity carries out this primitive by blocking the caller and sending packet to the server. Encapsulated in the pay load of this packet is transport layer message for the servers transport entity. quick note on terminology is now in order. For lack of better term we will use the term segment for messages sent from transport entity to transport en tity. TCP UDP and other Internet protocols use this term. Some older protocols used the ungainly name TPDU . That term is not used much any more now but you may see it in older papers and books. Thus segments are contained in packets . In turn these packets are contained in frames . When frame arrives the data link layer of the packet payload up to the transport entity. This nesting is illustrated in Getting back to our clientserver example the clients CONNECT call causes CONNECTION REQUEST segment to be sent to the server. When it arrives the transport entity checks to see that the server is blocked on LISTEN . If so it then unblocks the server and sends CON NECTION ACCEPTED segment back to the client. When this segment arrives the client is unblocked and the connection is established. Data can now be exchanged using the SEND and RECEIVE primitives. In the simplest form either party can do RECEIVE to wait for the other party to do SEND. When the segment arrives the receiver is unblocked. It can then process the segment and send reply. As long as both sides can keep track of whose turn it is to send this scheme works fine. Note that in the transport layer even simple unidirectional data exchange is more complicated than at the network layer. Every data packet sent will also be The packets bearing control segments are also acknowledged implicitly or explicitly. These acknowledgements are managed by the transport entities using the network layer protocol and are not visible to the transport users. Similarly the transport entities need to worry about timers and retransmissions. None of this machinery is visible to the transport users. To the transport users connection is reliable bit pipe one user stuffs bits in and they magically appear in the same order at the other end. This ability to hide com plexity is the reason that layered protocols are such powerful tool. When connection is no longer needed it must be released to free up table space within the two transport entities. Disconnection has two variants asymmet DISCONNECT primitive which results in DISCONNECT segment being sent to the remote transport entity. Upon its arrival the connection is released. In the symmetric variant each direction is closed separately independently of the other one. When one side does DISCONNECT that means it has no more data to send but it is still willing to accept data from its partner. In this model con nection is released when both sides have done DISCONNECT. state diagram for connection establishment and release for these simple primitive executed by the local transport user or an incoming packet. For simpli city we assume here that each segment is separately acknowledged. We also as sume that symmetric disconnection model is used with the client going first. Please note that this model is quite unsophisticated. We will look at more realis tic models later on when we describe how TCP works. Let us now briefly inspect another set of transport primitives the socket prim itives as they are used for TCP. Sockets were first released as part of the Berke ley UNIX 4.2BSD software distribution in 1983. They quickly became popular. The primitives are now widely used for Internet programming on many operating Transitions labeled in italics are caused by packet arrivals. The solid lines show the clients state sequence. The dashed lines show the servers state sequence. systems especially UNIXbased systems and there is socketstyle API for Win dows called winsock. del of our first example but offer more features and flexibility. We will not look at the corresponding segments here. That discussion will come later. Announce willingness to accept connections give queue size The first four primitives in the list are executed in that order by servers. The SOCKET primitive creates new endpoint and allocates table space for it within the transport entity. The parameters of the call specify the addressing format to be used the type of service desired and the protocol. successful SOCKET call returns an ordinary file descriptor for use in succeeding calls the same way an OPEN call on file does. Newly created sockets do not have network addresses. These are assigned using the BIND primitive. Once server has bound an address to socket remote clients can connect to it. The reason for not having the SOCKET call create an ad dress directly is that some processes care about their addresses whereas Next comes the LISTEN call which allocates space to queue incoming calls for the case that several clients try to connect at the same time. In contrast to LISTEN in our first example in the socket model LISTEN is not blocking call. To block waiting for an incoming connection the server executes an ACCEPT primitive. When segment asking for connection arrives the transport entity creates new socket with the same properties as the original one and returns file descriptor for it. The server can then fork off process or thread to handle the connection on the new socket and go back to waiting for the next connection on the original socket. ACCEPT returns file descriptor which can be used for read ing and writing in the standard way the same as for files. Now let us look at the client side. Here too socket must first be created using the SOCKET primitive but BIND is not required since the address used does not matter to the server. The CONNECT primitive blocks the caller and actively starts the connection process. When it completes the client process is unblocked and the con nection is established. Both sides can now use SEND and RECEIVE to transmit and receive data over the fullduplex connection. The standard UNIX READ and WRITE system calls can also be used if none of the special options of SEND and RECEIVE Connection release with sockets is symmetric. When both sides have exe cuted CLOSE primitive the connection is released. Sockets have proved tremendously popular and are the de facto standard for abstracting transport services to applications. The socket API is often used with the TCP protocol to provide connectionoriented service called reliable byte stream which is simply the reliable bit pipe that we described. However other protocols could be used to implement this service using the same API. It should all be the same to the transport service users. strength of the socket API is that is can be used by an application for other transport services. For instance sockets can be used with connectionless tran sport service. In this case CONNECT sets the address of the remote transport peer and SEND and RECEIVE send and receive datagrams to and from the remote peer. Sockets can also be used with transport protocols that provide message stream rather than byte stream and that do or do not have congestion control. For example DCCP is sport users to understand what service they are getting. For example applications often work with group of related streams such as Web browser that requests several objects from the same server. With sockets the most natural fit is for application programs to use one stream per object. This structure means that congestion control is applied separately for each stream not across the group which is suboptimal. It punts to the application the burden of managing the set. Newer protocols and interfaces have been devised that support groups of related streams more effectively and simply for the application. Two examples are SCTP defined in RFC 4960 and SST . These protocols must change the socket API slightly to get the benefits of groups of related streams and they also support features such as mix of connectionoriented and connectionless traffic and even multiple network paths. Time will tell if they are As an example of the nittygritty of how real socket calls are made consider server along with an example client that uses it. The code has many limitations but in principle the server code can be compiled and run on any UNIX system connected to the Internet. The client code can be compiled and run on any other UNIX machine on the Internet anywhere in the world. The cli ent code can be executed with appropriate parameters to fetch any file to which the server has access on its machine. The file is written to standard output which of course can be redirected to file or pipe. Let us look at the server code first. It starts out by including some standard data structures. Next comes definition of SERVER PORT as 12345. This num ber was chosen arbitrarily. Any number between 1024 and 65535 will work just as well as long as it is not in use by some other process ports below 1023 are re served for privileged users. The next two lines in the server define two constants needed. The first one determines the chunk size in bytes used for the file transfer. The second one de termines how many pending connections can be held before additional ones are holds IP address if fatal channel.sin family AF INET memcpy channel.sin port htons connect channel sizeof if fatal Connection is now established. Send file name including 0 byte at end. write1 Go get the file and write it to standard output. holds IP address Build address structure to bind to socket. if fatal setsockopt on sizeof bind channel sizeof if fatal Socket is now set up and bound. Wait for connection and process it. if fatal bytes read read from file After the declarations of local variables the server code begins. It starts out by initializing data structure that will hold the servers IP address. This data structure will soon be bound to the servers socket. The call to memset sets the data structure to all 0s. The three assignments following it fill in three of its fields. The last of these contains the servers port. The functions htonl and htons have to do with converting values to standard format so the code runs correctly on both littleendian machines and bigendian machines . Their exact semantics are not relevant here. Next the server creates socket and checks for errors . In tory. The call to setsockopt is needed to allow the port to be reused so the server can run indefinitely fielding request after request. Now the IP address is bound to in the initialization is the call to listen to announce the servers willingness to ac cept incoming calls and tell the system to hold up to QUEUE SIZE of them in case new requests arrive while the server is still processing the current one. If the queue is full and additional requests arrive they are quietly discarded. At this point the server enters its main loop which it never leaves. The only way to stop it is to kill it from outside. The call to accept blocks the server until some client tries to establish connection with it. If the accept call succeeds it returns socket descriptor that can be used for reading and writing analogous to how file descriptors can be used to read from and write to pipes. However unlike pipes which are unidirectional sockets are bidirectional so sa can be used for reading from the connection and also for writing to it. pipe file descriptor is for reading or writing but not both. After the connection is established the server reads the file name from it. If the name is not yet available the server blocks waiting for it. After getting the file name the server opens the file and enters loop that alternately reads blocks from the file and writes them to the socket until the entire file has been copied. Then the server closes the file and the connection and waits for the next con nection to show up. It repeats this loop forever. Now let us look at the client code. To understand how it works it is neces sary to understand how it is invoked. Assuming it is called client typical call is This call only works if the server is already running on flits.cs.vu.nl and the file usrtomfilename exists and the server has read access to it. If the call is suc cessful the file is transferred over the Internet and written to after which the cli ent program exits. Since the server continues after transfer the client can be started again and again to get other files. The client code starts with some includes and declarations. Execution begins by checking to see if it has been called with the right number of arguments . Note that argv contains the name of the server and is converted to an IP address by gethostbyname. This function uses DNS to look up the name. We will study DNS Next socket is created and initialized. After that the client attempts to es tablish TCP connection to the server using connect. If the server is up and run ning on the named machine and attached to SERVER PORT and is either idle or has room in its listen queue the connection will be established. Using the connection the client sends the name of the file by writing on the byte terminating the name must also be sent to tell the server where the name Now the client enters loop reading the file block by block from the socket and copying it to standard output. When it is done it just exits. The procedure fatal prints an error message and exits. The server needs the client and server are compiled separately and normally run on different com puters they cannot share the code of fatal. These two programs can be Just for the record this server is not the last word in serverdom. Its error quests strictly sequentially its performance is poor. It has clearly never heard about security and using bare UNIX system calls is not the way to gain platform independence. It also makes some assumptions that are technically illegal such as assuming that the file name fits in the buffer and is transmitted atomically. These shortcomings notwithstanding it is work more information about programming with sockets see Donahoo and Calvert The transport service is implemented by transport protocol used between the two transport entities. In some ways transport protocols resemble the data link protocols we studied in detail in Chap. 3. Both have to deal with error con However significant differences between the two also exist. ferences are due to major dissimilarities between the environments in which the communicate directly via physical channel whether wired or wireless whereas at the transport layer this physical channel is replaced by the entire network. This difference has many important implications for the protocols. transport layer. For one thing over pointtopoint links such as wires or optical fiber it is usually not necessary for router to specify which router it wants to talk toeach outgoing line leads directly to particular router. In the transport layer explicit addressing of destinations is required. For another thing the process of establishing connection over the wire of case it is not there. Either way there is not much to do. Even on wireless links the process is not much different. Just sending message is sufficient to have it reach all other destinations. If the message is not acknowledged due to an error it can be resent. In the transport layer initial connection establishment is complicat ed as we will see. Another difference between the data link layer and the transport layer is the potential existence of storage capacity in the network. When router sends packet over link it may arrive or be lost but it cannot bounce around for while go into hiding in far corner of the world and sudden ly emerge after other packets that were sent much later. If the network uses data grams which are independently routed inside there is nonnegligible probability that packet may take the scenic route and arrive late and out of the expected order or even that duplicates of the packet will arrive. The consequences of the networks ability to delay and duplicate packets can sometimes be disastrous and can require the use of special protocols to correctly transport information. rather than of kind. Buffering and flow control are needed in both layers but the presence in the transport layer of large and varying number of connections with bandwidth that fluctuates as the connections compete with each other may require different approach than we used in the data link layer. Some of the protocols discussed in Chap. 3 allocate fixed number of buffers to each line so that when frame arrives buffer is always available. In the transport layer the larger num ber of connections that must be managed and variations in the bandwidth each connection may receive make the idea of dedicating many buffers to each one less sues and others. When an application process wishes to set up connection to remote application process it must specify which one to connect to. The method normally used is to define transport addresses to which proc esses can listen for connection requests. In the Internet these endpoints are called ports. We will use the generic term TSAP to mean specific endpoint in the transport layer. The analogous endpoints in the network layer are notsurprisingly called NSAPs . IP addresses are examples of NSAPs. transport connection. Application processes both clients and servers can attach themselves to local TSAP to establish connection to remote TSAP. These connections run through NSAPs on each host as shown. The purpose of having TSAPs is that in some networks each computer has single NSAP so some way is needed to distinguish multiple transport endpoints that share that NSAP. possible scenario for transport connection is as follows 1. mail server process attaches itself to TSAP 1522 on host 2 to wait for an incoming call. How process attaches itself to TSAP is out side the networking model and depends entirely on the local operat ing system. call such as our LISTEN might be used for example. 2. An application process on host 1 wants to send an email message so on host 2 as the destination. This action ultimately results in tran sport connection being established between the application process 3. The application process sends over the mail message. 4. The mail server responds to say that it will deliver the message. 5. The transport connection is released. Note that there may well be other servers on host 2 that are attached to other TSAPs and are waiting for incoming connections that arrive over the same NSAP. The picture painted above is fine except we have swept one little problem under the rug how does the user process on host 1 know that the mail server is at tached to TSAP 1522 One possibility is that the mail server has been attaching itself to TSAP 1522 for years and gradually all the network users have learned this. In this model services have stable TSAP addresses that are listed in files in wellknown places. For example the etcservices file on UNIX systems lists which servers are permanently attached to which ports including the fact that the mail server is found on TCP port 25. While stable TSAP addresses work for small number of key services that never change user processes in general often want to talk to other user processes that do not have TSAP addresses that are known in ad vance or that may exist for only short time. To handle this situation an alternative scheme can be used. In this scheme there exists special process called portmapper. To find the TSAP address corresponding to given service name such as BitTorrent user sets up con nection to the portmapper . The user then sends message specifying the service name and the portmapper sends back the TSAP address. Then the user releases the connection with the portmapper and es tablishes new one with the desired service. In this model when new service is created it must register itself with the portmapper giving both its service name and its TSAP. The portmapper records this information in its internal database so that The function of the portmapper is analogous to that of directory assistance operator in the telephone systemit provides mapping of names onto numbers. Just as in the telephone system it is essential that the address of the wellknown TSAP used by the portmapper is indeed well known. If you do not know the number of the information operator you cannot call the information operator to find it out. If you think the number you dial for information is obvious try it in foreign country sometime. Many of the server processes that can exist on machine will be used only rarely. It is wasteful to have each of them active and listening to stable TSAP form. It is known as the initial connection protocol. Instead of every conceiv able server listening at wellknown TSAP each machine that wishes to offer services to remote users has special process server that acts as proxy for less heavily used servers. This server is called inetd on UNIX systems. It listens to set of ports at the same time waiting for connection request. Potential users of service begin by doing CONNECT request specifying the TSAP address of the service they want. If no server is waiting for them they get connection to the server in host 2 via process server. After it gets the incoming request the process server spawns the requested server allowing it to inherit the existing connection with the user. The new server does the requested work while the process server goes back to listening for new can be created on demand. Establishing connection sounds easy but it is actually surprisingly tricky. At first glance it would seem sufficient for one transport entity to just send CONNECTION REQUEST segment to the destination and wait for CONNECTION ACCEPTED reply. The problem occurs when the network can lose delay corrupt and duplicate packets. This behavior causes serious complications. Imagine network that is so congested that acknowledgements hardly ever get back in time and each packet times out and is retransmitted two or three times. Suppose that the network uses datagrams inside and that every packet follows different route. Some of the packets might get stuck in traffic jam inside the network and take long time to arrive. That is they may be delayed in the net work and pop out much later when the sender thought that they had been lost. The worst possible nightmare is as follows. user establishes connection with bank sends messages telling the bank to transfer large amount of money to the account of notentirelytrustworthy person. Unfortunately the packets de cide to take the scenic route to the destination and go off exploring remote corner of the network. The sender then times out and sends them all again. This time the packets take the shortest route and are delivered quickly so the sender re leases the connection. ing and arrive at the destination in order asking the bank to establish new con nection and transfer money . The bank has no way of telling that these are duplicates. It must assume that this is second independent transaction and transfers the money again. This scenario may sound unlikely or even implausible but the point is this protocols must be designed to be correct in all cases. Only the common cases need be implemented efficiently to obtain good network performance but the protocol must be able to cope with the uncommon cases without breaking. If it cannot we have built fairweather network that can fail without warning when the condi tions get tough. cates with emphasis on algorithms for establishing connections in reliable way so that nightmares like the one above cannot happen. The crux of the problem is that the delayed duplicates are thought to be new packets. We cannot prevent packets from being duplicated and delayed. But if and when this happens the packets must be rejected as duplicates and not processed as fresh packets. The problem can be attacked in various ways none of them very satisfactory. One way is to use throwaway transport addresses. In this approach each time transport address is needed new one is generated. When connection is re leased the address is discarded and never used again. Delayed duplicate packets then never find their way to transport process and can do no damage. However this approach makes it more difficult to connect with process in the first place. Another possibility is to give each connection unique identifier chosen by the ini tiating party and put in each segment including the one requesting the connection. obsolete connections as pairs. When ever connection request comes in it can be checked against the table to see if it belongs to previously released connection. Unfortunately this scheme has basic flaw it requires each transport entity to maintain certain amount of history information indefinitely. This history must persist at both the source and destination machines. Otherwise if machine crashes and loses its memory it will no longer know which connection identifiers have already been used by its peers. Instead we need to take different tack to simplify the problem. Rather than allowing packets to live forever within the network we devise mechanism to kill off aged packets that are still hobbling about. With this restriction the prob lem becomes somewhat more manageable. Packet lifetime can be restricted to known maximum using one of 3. Timestamping each packet. The first technique includes any method that prevents packets from looping com bined with some way of bounding delay including congestion over the longest possible path. It is difficult given that internets may range from single city to international in scope. The second method consists of having the hop count initialized to some appropriate value and decremented each time the packet is forwarded. The network protocol simply discards any packet whose hop counter becomes zero. The third method requires each packet to bear the time it was created with the routers agreeing to discard any packet older than some agreedupon time. This latter method requires the router clocks to be synchron ized which itself is nontrivial task and in practice hop counter is close enough approximation to age. In practice we will need to guarantee not only that packet is dead but also that all acknowledgements to it are dead too so we will now introduce period which is some small multiple of the true maximum packet lifetime. The maxi mum packet lifetime is conservative constant for network for the Internet it is somewhat arbitrarily taken to be 120 seconds. The multiple is protocol dependent and simply has the effect of making longer. If we wait time secs after packet has been sent we can be sure that all traces of it are now gone and that nei ther it nor its acknowledgements will suddenly appear out of the blue to compli With packet lifetimes bounded it is possible to devise practical and fool proof way to reject delayed duplicate segments. The method described below is due to Tomlinson as refined by Sunshine and Dalal . Variants of it are widely used in practice including in TCP. The heart of the method is for the source to label segments with sequence numbers that will not be reused within secs. The period and the rate of pack ets per second determine the size of the sequence numbers. In this way only one packet with given sequence number may be outstanding at any given time. Dup licates of this packet may still occur and they must be discarded by the destina tion. However it is no longer the case that delayed duplicate of an old packet may beat new packet with the same sequence number and be accepted by the destination in its stead. To get around the problem of machine losing all memory of where it was after crash one possibility is to require transport entities to be idle for secs after recovery. The idle period will let all old segments die off so the sender can start again with any sequence number. However in complex internetwork may be large so this strategy is unattractive. Instead Tomlinson proposed equipping each host with timeofday clock. The clocks at different hosts need not be synchronized. Each clock is assumed to take the form of binary counter that increments itself at uniform intervals. Fur thermore the number of bits in the counter must equal or exceed the number of bits in the sequence numbers. Last and most important the clock is assumed to continue running even if the host goes down. When connection is set up the loworder bits of the clock are used as the kbit initial sequence number. Thus unlike our protocols of Chap. 3 each con nection starts numbering its segments with different initial sequence number. The sequence space should be so large that by the time sequence numbers wrap around old segments with the same sequence number are long gone. This linear forbidden region shows the times for which segment sequence numbers are illegal leading up to their use. If any segment is sent with sequence number in this re gion it could be delayed and impersonate different packet with the same se and restarts at time 70 seconds it will use initial sequence numbers based on the clock to pick up after it left off the host does not start with lower sequence number in the forbidden region. Once both transport entities have agreed on the initial sequence number any sliding window protocol can be used for data flow control. This window protocol will correctly find and discard duplicates of packets after they have already been chronization problem. accepted. In reality the initial sequence number curve is not linear but staircase since the clock advances in discrete steps. For sim plicity we will ignore this detail. To keep packet sequence numbers out of the forbidden region we need to take care in two respects. We can get into trouble in two distinct ways. If host sends too much data too fast on newly opened connection the actual sequence number versus time curve may rise more steeply than the initial sequence number versus time curve causing the sequence number to enter the forbidden region. To prevent this from happening the maximum data rate on any connection is one segment per clock tick. This also means that the transport entity must wait until the clock ticks before opening new connection after crash restart lest the same number be used twice. Both of these points argue in favor of short clock tick . But the clock cannot tick too fast relative to the sequence number. For clock rate of and sequence number space of size we must have SCT so that the sequence numbers cannot wrap around too quickly. Entering the forbidden region from underneath by sending too fast is not the than the clock rate the curve of actual sequence numbers used versus time will eventually run into the forbidden region from the left as the sequence numbers wrap around. The greater the slope of the actual sequence numbers the longer this event will be delayed. Avoiding this situation limits how slowly sequence numbers can advance on connection . The clockbased method solves the problem of not being able to distinguish delayed duplicate segments from new segments. However there is practical snag for using it for establishing connections. Since we do not normally remember sequence numbers across connections at the destination we still have no way of knowing if CONNECTION REQUEST segment containing an initial sequence number is duplicate of recent connection. This snag does not exist during connection because the sliding window protocol does remember the current se To solve this specific problem Tomlinson introduced the threeway handshake. This establishment protocol involves one peer checking with the other that the connection request is indeed current. The normal setup procedure replies with an ACK segment acknowledging and announcing its own initial se quence number in the first data segment that it sends. Now let us see how the threeway handshake works in the presence of delayed cate CONNECTION REQUEST from an old connection. This segment arrives at host 2 without host 1s knowledge. Host 2 reacts to this segment by sending host 1 an ACK segment in effect asking for verification that host 1 was indeed trying to set up new connection. When host 1 rejects host 2s attempt to establish connection host 2 realizes that it was tricked by delayed duplicate and abandons the connection. In this way delayed duplicate does no damage. The worst case is when both delayed CONNECTION REQUEST and an ACK previous example host 2 gets delayed CONNECTION REQUEST and replies to it. At this point it is crucial to realize that host 2 has proposed using as the ini tial sequence number for host 2 to host 1 traffic knowing full well that no seg ments containing sequence number or acknowledgements to are still in exist ence. When the second delayed segment arrives at host 2 the fact that has been acknowledged rather than tells host 2 that this too is an old duplicate. The im portant thing to realize here is that there is no combination of old segments that can cause the protocol to fail and have connection set up by accident when no TCP uses this threeway handshake to establish connections. Within con nection timestamp is used to extend the 32bit sequence number so that it will not wrap within the maximum packet lifetime even for gigabitpersecond con nections. This mechanism is fix to TCP that was needed as it was used on faster and faster links. It is described in RFC 1323 and called PAWS . Across connections for the initial se quence numbers and before PAWS can come into play TCP originally used the clockbased scheme just described. However this turned out to have security vulnerability. The clock made it easy for an attacker to predict the next initial se quence number and send packets that tricked the threeway handshake and estab lished forged connection. To close this hole pseudorandom initial sequence numbers are used for connections in practice. However it remains important that ACK threeway handshake. CR denotes CONNECTION REQUEST. Normal opera Old duplicate CONNECTION REQUEST appearing out of nowhere. Duplicate CONNECTION REQUEST and duplicate ACK. the initial sequence numbers not repeat for an interval even though they appear random to an observer. Otherwise delayed duplicates can wreak havoc. Releasing connection is easier than establishing one. Nevertheless there are more pitfalls than one might expect here. As we mentioned earlier there are two styles of terminating connection asymmetric release and symmetric release. Asymmetric release is the way the telephone system works when one party hangs up the connection is broken. Symmetric release treats the connection as two sep arate unidirectional connections and requires each one to be released separately. Asymmetric release is abrupt and may result in data loss. Consider the scen arrives properly at host 2. Then host 1 sends another segment. Unfortunately the connection is released and data are lost. Clearly more sophisticated release protocol is needed to avoid data loss. One way is to use symmetric release in which each direction is released indepen dently of the other one. Here host can continue to receive data even after it has sent DISCONNECT segment. Symmetric release does the job when each process has fixed amount of data to send and clearly knows when it has sent it. In other situations determining that all the work has been done and the connection should be terminated is not so ob vious. One can envision protocol in which host 1 says am done. Are you done too If host 2 responds am done too. Goodbye the connection can be Unfortunately this protocol does not always work. There is famous prob rounding hillsides are blue armies. The white army is larger than either of the blue armies alone but together the blue armies are larger than the white army. If either blue army attacks by itself it will be defeated but if the two blue armies at tack simultaneously they will be victorious. The blue armies want to synchronize their attacks. However their only com munication medium is to send messengers on foot down into the valley where they might be captured and the message lost . The question is does protocol exist that allows the Suppose that the commander of blue army 1 sends message reading propose we attack at dawn on March 29. How about it Now suppose that the message arrives the commander of blue army 2 agrees and his reply gets safely back to blue army 1. Will the attack happen Probably not because commander 2 does not know if his reply got through. If it did not blue army 1 will not at tack so it would be foolish for him to charge into battle. Now let us improve the protocol by making it threeway handshake. The initiator of the original proposal must acknowledge the response. Assuming no messages are lost blue army 2 will get the acknowledgement but the com mander of blue army 1 will now hesitate. After all he does not know if his ac knowledgement got through and if it did not he knows that blue army 2 will not attack. We could now make fourway handshake protocol but that does not In fact it can be proven that no protocol exists that works. Suppose that some protocol did exist. Either the last message of the protocol is essential or it is not. If it is not we can remove it until we are left message does not get through We just said that it was essential so if it is lost sure of its arrival he will not risk attacking. Worse yet the other blue army knows this so it will not attack either. To see the relevance of the twoarmy problem to releasing connections rather than to military affairs just substitute disconnect for attack. If neither side is prepared to disconnect until it is convinced that the other side is prepared to disconnect too the disconnection will never happen. In practice we can avoid this quandary by foregoing the need for agreement and pushing the problem up to the transport user letting each side independently four scenarios of releasing using threeway handshake. While this protocol is not infallible it is usually adequate. segment to initiate the connection release. When it arrives the recipient sends back DR segment and starts timer just in case its DR is lost. When this DR arrives the original sender sends back an ACK segment also releases the connection. Releasing connection means that the transport en tity removes the information about the connection from its table of currently open connections and signals the connections owner somehow. This action is different from transport user issuing DISCONNECT primitive. saved by the timer. When the timer expires the connection is released anyway. Now consider the case of the second DR being lost. The user initiating the disconnection will not receive the expected response will time out and will start time no segments are lost and all segments are delivered correctly and on time. assume all the repeated attempts to retransmit the DR also fail due to lost seg ments. After retries the sender just gives up and releases the connection. Meanwhile the receiver times out and also exits. While this protocol usually suffices in theory it can fail if the initial DR and retransmissions are all lost. The sender will give up and release the connection while the other side knows nothing at all about the attempts to disconnect and is still fully active. This situation results in halfopen connection. We could have avoided this problem by not allowing the sender to give up after retries and forcing it to go on forever until it gets response. However if the other side is allowed to time out the sender will indeed go on forever because no response will ever be forthcoming. If we do not allow the receiving side to One way to kill off halfopen connections is to have rule saying that if no segments have arrived for certain number of seconds the connection is automat ically disconnected. That way if one side ever disconnects the other side will detect the lack of activity and also disconnect. This rule also takes care of the case where the connection is broken without either end disconnecting first. Of course if this rule is introduced it is necessary for each transport entity to have timer that is stopped and then restarted whenever segment is sent. If this timer expires sponse lost and subsequent DRs lost. dummy segment is transmitted just to keep the other side from disconnecting. On the other hand if the automatic disconnect rule is used and too many dummy seg ments in row are lost on an otherwise idle connection first one side then the other will automatically disconnect. We will not belabor this point any more but by now it should be clear that releasing connection without data loss is not nearly as simple as it first appears. The lesson here is that the transport user must be involved in deciding when to disconnectthe problem cannot be cleanly solved by the transport entities them selves. To see the importance of the application consider that while TCP nor mally does symmetric close many Web servers send the client RST packet that causes an abrupt close of the connection that is more like an asymmetric close. This works only because the Web server knows the pat tern of data exchange. First it receives request from the client which is all the data the client will send and then it sends response to the client. When the Web server is finished with its response all of the data has been sent in either direction. The server can send the client warning and abruptly shut the connection. If the client gets this warning it will release its connection state then and there. If the client does not get the warning it will eventually realize that the server is no long er talking to it and release the connection state. The data has been successfully transferred in either case. Having examined connection establishment and release in some detail let us are error control and flow control. Error control is ensuring that the data is deliv ered with the desired level of reliability usually that all of the data is delivered without any errors. Flow control is keeping fast transmitter from overrunning that we studied in Chap. 3. As very brief recap 1. frame carries an errordetecting code that is used to check if the information was correctly received. 2. frame carries sequence number to identify itself and is retrans mitted by the sender until it receives an acknowledgement of suc cessful receipt from the receiver. This is called ARQ . 3. There is maximum number of frames that the sender will allow to be outstanding at any time pausing if the receiver is not acknowledg ing frames quickly enough. If this maximum is one packet the proto col is called stopandwait. Larger windows enable pipelining and improve performance on long fast links. 4. The sliding window protocol combines these features and is also used to support bidirectional data transfer. Given that these mechanisms are used on frames at the link layer it is natural to wonder why they would be used on segments at the transport layer as well. However there is little duplication between the link and transport layers in prac tice. Even though the same mechanisms are used there are differences in function For difference in function consider error detection. The link layer check sum protects frame while it crosses single link. The transport layer checksum protects segment while it crosses an entire network path. It is an endtoend check which is not the same as having check on every link. Saltzer et al. describe situation in which packets were corrupted inside router. The link layer checksums protected the packets only while they traveled across link not while they were inside the router. Thus packets were delivered incorrectly even though they were correct according to the checks on every link. This and other examples led Saltzer et al. to articulate the endtoend argu ment. According to this argument the transport layer check that runs endtoend is essential for correctness and the link layer checks are not essential but nonethe less valuable for improving performance . As difference in degree consider retransmissions and the sliding window protocol. Most wireless links other than satellite links can have only single frame outstanding from the sender at time. That is the bandwidthdelay product for the link is small enough that not even whole frame can be stored inside the link. In this case small window size is sufficient for good performance. For ex ample 802.11 uses stopandwait protocol transmitting or retransmitting each frame and waiting for it to be acknowledged before moving on to the next frame. Having window size larger than one frame would add complexity without im proving performance. For wired and optical fiber links such as Ether net or ISP backbones the errorrate is low enough that linklayer retransmissions can be omitted because the endtoend retransmissions will repair the residual On the other hand many TCP connections have bandwidthdelay product that is much larger than single segment. Consider connection sending data cross the . at 1 Mbps with roundtrip time of 100 msec. Even for this slow connection 200 Kbit of data will be stored at the receiver in the time it takes to send segment and receive an acknowledgement. For these situations large sliding window must be used. Stopandwait will cripple performance. In our ex ample it would limit performance to one segment every 200 msec or 5 seg mentssec no matter how fast the network really is. Given that transport protocols generally use larger sliding windows we will connections each of which is treated separately it may need substantial amount of buffering for the sliding windows. The buffers are needed at both the sender and the receiver. Certainly they are needed at the sender to hold all transmitted but as yet unacknowledged segments. They are needed there because these seg ments may be lost and need to be retransmitted. However since the sender is buffering the receiver may or may not dedicate specific buffers to specific connections as it sees fit. The receiver may for ex ample maintain single buffer pool shared by all connections. When segment comes in an attempt is made to dynamically acquire new buffer. If one is avail able the segment is accepted otherwise it is discarded. Since the sender is pre pared to retransmit segments lost by the network no permanent harm is done by having the receiver drop segments although some resources are wasted. The sender just keeps trying until it gets an acknowledgement. The best tradeoff between source buffering and destination buffering depends on the type of traffic carried by the connection. For lowbandwidth bursty traffic such as that produced by an interactive terminal it is reasonable not to dedicate any buffers but rather to acquire them dynamically at both ends relying on buff ering at the sender if segments must occasionally be discarded. On the other hand for file transfer and other highbandwidth traffic it is better if the receiver does dedicate full window of buffers to allow the data to flow at maximum speed. This is the strategy that TCP uses. There still remains the question of how to organize the buffer pool. If most segments are nearly the same size it is natural to organize the buffers as pool of large packets in peertopeer file transfers pool of fixedsized buffers presents space will be wasted whenever short segment arrives. If the buffer size is cho sen to be less than the maximum segment size multiple buffers will be needed for long segments with the attendant complexity. Another approach to the buffer size problem is to use variablesized buffers more complicated buffer management. third possibility is to dedicate single elegant and does not depend on segment sizes but makes good use of memory only when the connections are heavily loaded. As connections are opened and closed and as the traffic pattern changes the quently the transport protocol should allow sending host to request buffer space at the other end. Buffers could be allocated per connection or collectively for all the connections running between the two hosts. Alternatively the receiver know ing its buffer situation could tell the sender have reserved buffers for you. If the number of open connections should in provide for this possibility. the buffering from the acknowledgements in contrast to the sliding window pro tocols of Chap. 3. Dynamic buffer management means in effect variablesized One large circular buffer per connection. window. Initially the sender requests certain number of buffers based on its expected needs. The receiver then grants as many of these as it can afford. Every work in datagram network with 4bit sequence numbers. In this example data flows in segments from host to host and acknowledgements and buffer alloca tions flow in segments in the reverse direction. Initially wants eight buffers but it is granted only four of these. It then sends three segments of which the third is lost. Segment 6 acknowledges receipt of all segments up to and including sequence number 1 thus allowing to release those buffers and furthermore . knows that it has already sent number 2 so it thinks that it may send segments 3 and 4 which it proceeds to do. At this point it is sions however may occur while blocked since they use buffers that have already been allocated. In line 10 acknowledges receipt of all segments up to and including 4 but refuses to let continue. Such situation is impossible with the fixedwindow protocols of Chap. 3. The next segment from to allocates another buffer and allows to continue. This will happen when has buffer space likely because the transport user has accepted more segment data. The arrows show the direction of transmission. An ellipsis indicates lost segment. networks if control segments can get lostwhich they most certainly can. Look lost. Oops. Since control segments are not sequenced or timed out is now deadlocked. To prevent this situation each host should periodically send control way the deadlock will be broken sooner or later. Until now we have tacitly assumed that the only limit imposed on the senders data rate is the amount of buffer space available in the receiver. This is often not the case. Memory was once expensive but prices have fallen dramatically. Hosts may be equipped with sufficient memory that the lack of buffers is rarely if ever problem even for wide area connections. Of course this depends on the buffer size being set to be large enough which has not always been the case for TCP . When buffer space no longer limits the maximum flow another bottleneck will appear the carrying capacity of the network. If adjacent routers can ex change at most packetssec and there are disjoint paths between pair of hosts there is no way that those hosts can exchange more than kx segmentssec no mat ter how much buffer space is available at each end. If the sender pushes too hard the network will become congested be cause it will be unable to deliver segments as fast as they are coming in. What is needed is mechanism that limits transmissions from the sender based on the networks carrying capacity rather than on the receivers buffering capacity. Belsnes proposed using sliding window flowcontrol scheme in which the sender dynamically adjusts the window size to match the networks carrying capacity. This means that dynamic sliding window can implement both flow control and congestion control. If the network can handle segmentssec and the roundtrip time is the senders win dow should be cr. With window of this size the sender normally operates with the pipeline full. Any small decrease in network performance will cause it to block. Since the network capacity available to any given flow varies over time the window size should be adjusted frequently to track changes in the carrying capacity. As we will see later TCP uses similar scheme. Multiplexing or sharing several conversations over connections virtual cir cuits and physical links plays role in several layers of the network architecture. In the transport layer the need for multiplexing can arise in number of ways. For example if only one network address is available on host all transport con nections on that machine have to use it. When segment comes in some way is needed to tell which process to give it to. This situation called multiplexing is same network connection to the remote host. Multiplexing can also be useful in the transport layer for another reason. Sup pose for example that host has multiple network paths that it can use. If user needs more bandwidth or more reliability than one of the network paths can pro vide way out is to have connection that distributes the traffic among multiple operandi is called inverse multiplexing. With network connections open the effective bandwidth might be increased by factor of . An example of inverse multiplexing is SCTP which can run connection using multiple network interfaces. In contrast TCP uses single net work endpoint. Inverse multiplexing is also found at the link layer when several lowrate links are used in parallel as one highrate link. If hosts and routers are subject to crashes or connections are longlived recovery from these crashes becomes an Transport address and router crashes is straightforward. The transport entities expect lost segments all the time and know how to cope with them by using retransmissions. more troublesome problem is how to recover from host crashes. In particu lar it may be desirable for clients to be able to continue working when servers crash and quickly reboot. To illustrate the difficulty let us assume that one host the client is sending long file to another host the file server using simple stopandwait protocol. The transport layer on the server just passes the incoming segments to the transport user one by one. Partway through the transmission the server crashes. When it comes back up its tables are reinitialized so it no longer knows precisely where it was. segment to all other hosts announcing that it has just crashed and requesting that of two states one segment outstanding S1 or no segments outstanding S0. Based on only this state information the client must decide whether to retransmit the most recent segment. At first glance it would seem obvious the client should retransmit if and only if it has an unacknowledged segment outstanding when it learns of the crash. However closer inspection reveals difficulties with this naive approach. Consider for example the situation in which the servers tran sport entity first sends an acknowledgement and then when the acknowledgement has been sent writes to the application process. Writing segment onto the out put stream and sending an acknowledgement are two distinct events that cannot be done simultaneously. If crash occurs after the acknowledgement has been sent but before the write has been fully completed the client will receive the acknowledgement and thus be in state S0 when the crash recovery announcement arrives. The client will therefore not retransmit thinking that the segment has arrived. This decision by the client leads to missing segment. At this point you may be thinking That problem can be solved easily. All you have to do is reprogram the transport entity to first do the write and then send the acknowledgement. Try again. Imagine that the write has been done but the crash occurs before the acknowledgement can be sent. The client will be in state S1 and thus retransmit leading to an undetected duplicate segment in the output stream to the server application process. No matter how the client and server are programmed there are always situa tions where the protocol fails to recover properly. The server can be programmed in one of two ways acknowledge first or write first. The client can be pro grammed in one of four ways always retransmit the last segment never retrans mit the last segment retransmit only in state S0 or retransmit only in state S1. This gives eight combinations but as we shall see for each combination there is some set of events that makes the protocol fail. Three events are possible at the server sending an acknowledgement writing to the output process and crashing . The three events can occur in six different orderings AC AWC WAC and WC where the parentheses are used to indicate that neither nor can follow as function of the offered load. This curve and matching curve for the delay as function of the offered load are Goodput rate but as the load approaches the capacity goodput rises more gradually. This falloff is because bursts of traffic can occasionally mount up and cause some losses at buffers inside the network. If the transport protocol is poorly designed and retransmits packets that have been delayed but not lost the network can enter congestion collapse. In this state senders are furiously sending packets but in creasingly little useful work is being accomplished. representing the propagation delay across the network. As the load approaches the capacity the delay rises slowly at first and then much more rapidly. This is again because of bursts of traffic that tend to mound up at high load. The delay cannot Instead packets will be lost after experiencing the maximum buffering delay. For both goodput and delay performance begins to degrade at the onset of congestion. Intuitively we will obtain the best performance from the network if we allocate bandwidth up until the delay starts to climb rapidly. This point is be low the capacity. To identify it Kleinrock proposed the metric of power Power will initially rise with offered load as delay remains small and roughly constant but will reach maximum and fall as delay grows rapidly. The load with the highest power represents an efficient load for the transport entity to place on In the preceding discussion we did not talk about how to divide bandwidth between different transport senders. This sounds like simple question to answergive all the senders an equal fraction of the bandwidthbut it involves several considerations. Perhaps the first consideration is to ask what this problem has to do with con gestion control. After all if the network gives sender some amount of bandwidth to use the sender should just use that much bandwidth. However it is often the case that networks do not have strict bandwidth reservation for each flow or connection. They may for some flows if quality of service is supported but many connections will seek to use whatever bandwidth is available or be lumped toget ed services separates traffic into two classes and connections compete for band width within each class. IP routers often have all connections competing for the same bandwidth. In this situation it is the congestion control mechanism that is allocating bandwidth to the competing connections. second consideration is what fair portion means for flows in network. It is simple enough if flows use single link in which case they can all have 1N of the bandwidth . But what happens if the flows have different but overlapping network paths For example one flow may cross three links and the other flows may cross one link. The threelink flow consumes more network resources. It might be fairer in some sense to give it less bandwidth than the onelink flows. It should certainly be possible to support more onelink flows by reducing the band width of the threelink flow. This point demonstrates an inherent tension between fairness and efficiency. However we will adopt notion of fairness that does not depend on the length of the network path. Even with this simple model giving connections an equal fraction of bandwidth is bit complicated because different connections will take different paths through the network and these paths will themselves have different capacities. In this case it is possible for flow to be bottlenecked on downstream link and take smaller portion of an upstream link than other flows reducing the bandwidth of the other flows would slow them down but would not help the bottlenecked flow at all. The form of fairness that is often desired for network usage is maxmin fair increased without decreasing the bandwidth given to another flow with an alloca tion that is no larger. That is increasing the bandwidth of flow will only make the situation worse for flows that are less well off. same capacity taken to be 1 unit though in the general case the links will have different capacities. Three flows compete for the bottomleft link between routers R4 and R5. Each of these flows therefore gets 13 of the link. The remaining 13 gets the remaining 23 of the link. Notice that all of the other links have spare capacity. However this capacity cannot be given to any of the flows without decreasing the capacity of another lower flow. For example if more of the band width on the link between R2 and R3 is given to flow there will be less for flow . This is reasonable as flow already has more bandwidth. However the ca pacity of flow or must be decreased to give more bandwidth to work. An intuitive way to think about them is to imagine that the rate for all of the flows starts at zero and is slowly increased. When the rate reaches bottleneck for any flow then that flow stops increasing. The other flows all continue to increase sharing equally in the available capacity until they too reach their respective bot third consideration is the level over which to consider fairness. network could be fair at the level of connections connections between pair of hosts or in Sec. 5.4 and concluded that each of these definitions will fare no better than mobile phone while defining fairness per connection encourages hosts to open more connections. Given that there is no clear answer fairness is often considered per connection but precise fairness is usually not concern. It is more important in practice that no connection be starved of band width than that all connections get precisely the same amount of bandwidth. In fact with TCP it is possible to open multiple connections and compete for band width more aggressively. This tactic is used by bandwidthhungry applications such as BitTorrent for peertopeer file sharing. ing point above assumes static network environment. However connections are always coming and going in network and the bandwidth needed by given con occasionally downloads large videos. Because of the variation in demand the ideal operating point for the network varies over time. good congestion control algorithm should rapidly converge to the ideal operating point and it should track that point as it changes over time. If the convergence is too slow the algorithm will never be close to the changing op erating point. If the algorithm is not stable it may fail to converge to the right point in some cases or even oscillate around the right point. ond later flow 2 starts. It needs bandwidth as well. changes to give each of these flows half the bandwidth. At 4 seconds third flow joins. However this flow uses only 20 of the bandwidth which is less than its fair share . Flows 1 and 2 quickly adjust dividing the available bandwidth to each have 40 of the bandwidth. At 9 seconds the second flow leaves and the third flow remains unchanged. The first flow quickly captures 80 of the bandwidth. At all times the total allocated bandwidth is approximately 100 so that the network is fully used and competing flows get equal treatment . Now it is time for the main course. How do we regulate the sending rates to factors. The first is flow control in the case that there is insufficient buffering at the receiver. The second is congestion in the case that there is insufficient capaci flowcontrol limited situation. As long as the sender does not send more water factor is not the bucket capacity but the internal carrying capacity of the network. If too much water comes in too fast it will back up and some will be lost . These cases may appear similar to the sender as transmitting too fast causes packets to be lost. However they have different causes and call for different solu tions. We have already talked about flowcontrol solution with variablesized window. Now we will consider congestion control solution. Since either of The way that transport protocol should regulate the sending rate depends on the form of the feedback returned by the network. Different network layers may return different kinds of feedback. The feedback may be explicit or implicit and it may be precise or imprecise. An example of an explicit precise design is when routers tell the sources the rate at which they may send. Designs in the literature such as XCP operate in this manner . An explicit impre cise design is the use of ECN with TCP. In this design routers set bits on packets that experience congestion to warn the senders to slow down but they do not tell them how much to slow down. network feeding highcapacity receiver. In other designs there is no explicit signal. FAST TCP measures the round trip delay and uses that metric as signal to avoid congestion . TCP with droptail or RED routers packet loss is inferred and used to signal that the network has become congested. There are many variants of this form of TCP including CUBIC TCP which is used in Linux . Combinations are also possible. For example Windows includes Compound TCP that uses both packet loss and delay as feedback signals . These designs are If an explicit and precise signal is given the transport entity can use that sig nal to adjust its rate to the new operating point. For example if XCP tells senders the rate to use the senders may simply use that rate. In the other cases however some guesswork is involved. In the absence of congestion signal the senders should decrease their rates. When congestion signal is given the senders should decrease their rates. The way in which the rates are increased or decreased is given by control law. These laws have major effect on performance. Chiu and Jain studied the case of binary congestion feedback and con cluded that AIMD is the appropr iate control law to arrive at the efficient and fair operating point. To argue this case they constructed graphical argument for the simple case of two con shows the bandwidth allocated to user 1 on the xaxis and to user 2 on the yaxis. ciency line. congestion signal is given by the network to both users when the sired operating point when both users have the same bandwidth and all of the net Multiplicative increase additively increase their respective bandwidths over time. For example the users may each increase their sending rate by 1 Mbps every second. Eventually the operating point crosses the efficiency line and both users receive congestion sig an additive decrease would simply cause them to oscillate along an additive line. close to efficient but it will not necessarily be fair. Similarly consider the case when both users multiplicatively increase their bandwidth over time until they receive congestion signal. For example the users may increase their sending rate by 10 every second. If they then multiplica tively decrease their sending rates the operating point of the users will simply The multiplicative line has different slope than the additive line. But it is otherwise no better. In neither case will the users converge to the optimal sending rates that are both fair and efficient. Now consider the case that the users additively increase their bandwidth al seen that the path traced by this behavior does converge to the optimal point that is both fair and efficient. This convergence happens no matter what the starting point making AIMD broadly useful. By the same argument the only other com bination multiplicative increase and additive decrease would diverge from the AIMD is the control law that is used by TCP based on this argument and an other stability argument . It is not quite fair since TCP connections adjust their window size by given amount every roundtrip time. Different connections will have dif ferent roundtrip times. This leads to bias in which connections to closer hosts receive more bandwidth than connections to distant hosts all else being equal. In Sec. 6.5 we will describe in detail how TCP implements an AIMD control law to adjust the sending rate and provide congestion control. This task is more difficult than it sounds because rates are measured over some interval and traffic is bursty. Instead of adjusting the rate directly strategy that is often used in practice is to adjust the size of sliding window. TCP uses this strategy. If the window size is and the roundtrip time is RTT the equivalent rate is WRTT. This strategy is easy to combine with flow control which already uses window and has the advantage that the sender paces packets using acknowledgements and traffic into the network. What will happen if the different protocols compete with what. Since TCP is the dominant form of congestion control in the Internet there is significant community pressure for new transport protocols to be designed so that they compete fairly with it. The early streaming media protocols caused prob lems by excessively reducing TCP throughput because they did not compete fairly. This led to the notion of TCPfriendly congestion control in which TCP and nonTCP transport protocols can be freely mixed with no ill effects driven by loss link layer frame retransmissions and transport layer congestion control. The puzzle is how these two mechanisms can coexist without getting confused. After all loss should cause only one mechanism to take action be cause it is either transmission error or congestion signal. It cannot be both. If both mechanisms take action then we are back to the original problem of transports that run far too slowly over wireless links. Consider this puzzle for moment and see if you The solution is that the two mechanisms act at different timescales. Link layer retransmissions happen on the order of microseconds to milliseconds for wireless links such as 802.11. Loss timers in transport protocols fire on the order of milliseconds to seconds. The difference is three orders of magnitude. This al lows wireless links to detect frame losses and retransmit frames to repair trans mission errors long before packet loss is inferred by the transport entity. The masking strategy is sufficient to let most transport protocols run well across most wireless links. However it is not always fitting solution. Some wireless links have long roundtrip times such as satellites. For these links other techniques must be used to mask loss such as FEC or the transport protocol must use nonloss signal for congestion control. ty. That is the capacity of wireless link changes over time sometimes abruptly as nodes move and the signaltonoise ratio varies with the changing channel con ditions. This is unlike wired links whose capacity is fixed. The transport protocol must adapt to the changing capacity of wireless links otherwise it will either con gest the network or fail to use the available capacity. One possible solution to this problem is simply not to worry about it. This strategy is feasible because congestion control algorithms must already handle the case of new users entering the network or existing users changing their sending rates. Even though the capacity of wired links is fixed the changing behavior of other users presents itself as variability in the bandwidth that is available to given user. Thus it is possible to simply run TCP over path with an 802.11 wire less link and obtain reasonable performance. However when there is much wireless variability transport protocols de signed for wired links may have trouble keeping up and deliver poor performance. The solution in this case is transport protocol that is designed for wireless links. particularly challenging setting is wireless mesh network in which multiple interfering wireless links must be crossed routes change due to mobility and there is lots of loss. Research in this area is ongoing. See Li et al. for an example of wireless transport protocol design. The Internet has two main protocols in the transport layer connectionless protocol and connectionoriented one. The protocols complement each other. The connectionless protocol is UDP. It does almost nothing beyond sending pack ets between applications letting applications build their own protocols on top as needed. The connectionoriented protocol is TCP. It does almost everything. It makes connections and adds reliability with retransmissions along with flow con trol and congestion control all on behalf of the applications that use it. UDP because it is simplest. We will also look at two uses of UDP. Since UDP is transport layer protocol that typically runs in the operating system and protocols that use UDP typically run in user space these uses might be considered applica tions. However the techniques they use are useful for many applications and are better considered to belong to transport service so we will cover them here. The Internet protocol suite supports connectionless transport protocol called UDP . UDP provides way for applications to send encapsulated IP datagrams without having to establish connection. UDP is de scribed in RFC 768. points within the source and destination machines. When UDP packet arrives its payload is handed to the process attached to the destination port. This attach ment occurs when the BIND primitive or something similar is used as we saw in mailboxes that applications can rent to receive packets. We will have more to say about them when we describe TCP which also uses ports. In fact the main value of UDP over just using raw IP is the addition of the source and destination ports. Without the port fields the transport layer would not know what to do with each incoming packet. With them it delivers the embedded segment to the correct ap The source port is primarily needed when reply must be sent back to the source. By copying the Source port field from the incoming segment into the Destination port field of the outgoing segment the process sending the reply can specify which process on the sending machine is to get it. is lower than the largest number that will fit in 16 bits because of the size limit on An optional Checksum is also provided for extra reliability. It checksums the putation the Checksum field is set to zero and the data field is padded out with an additional zero byte if its length is an odd number. The checksum algorithm is simply to add up all the 16bit words in ones complement and to take the ones complement of the sum. As consequence when the receiver performs the calcu lation on the entire segment including the Checksum field the result should be 0. If the checksum is not computed it is stored as 0 since by happy coincidence of ones complement arithmetic true computed 0 is stored as all 1s. However turning it off is foolish unless the quality of the data does not matter than with protocol requiring an initial setup like TCP. An application that uses UDP this way is DNS which we will study in Chap. 7. In brief program that needs to look up the IP address of some host name for example can send UDP packet containing the host name to DNS server. The server replies with UDP packet containing the hosts IP address. No setup is needed in advance and no re lease is needed afterward. Just two messages go over the network. In certain sense sending message to remote host and getting reply back is lot like making function call in programming language. In both cases you start with one or more parameters and you get back result. This observation has led people to try to arrange requestreply interactions on networks to be cast in the form of procedure calls. Such an arrangement makes network applications much easier to program and more familiar to deal with. For example just imagine procedure named get IP address that works by sending UDP packet to DNS server and waiting for the reply timing out and trying again if one is not forthcoming quickly enough. In this way all the details of networking can be hidden from the programmer. The key work in this area was done by Birrell and Nelson . In nut shell what Birrell and Nelson suggested was allowing programs to call proce dures located on remote hosts. When process on machine 1 calls procedure on machine 2 the calling process on 1 is suspended and execution of the called pro cedure takes place on 2. Information can be transported from the caller to the cal lee in the parameters and can come back in the procedure result. No message pas sing is visible to the application programmer. This technique is known as RPC and has become the basis for many networking appli cations. Traditionally the calling procedure is known as the client and the called procedure is known as the server and we will use those names here too. The idea behind RPC is to make remote procedure call look as much as pos sible like local one. In the simplest form to call remote procedure the client program must be bound with small library procedure called the client stub that represents the server procedure in the clients address space. Similarly the server is bound with procedure called the server stub. These procedures hide the fact that the procedure call from the client to the server is not local. ent calling the client stub. This call is local procedure call with the parameters pushed onto the stack in the normal way. Step 2 is the client stub packing the pa rameters into message and making system call to send the message. Packing the parameters is called marshaling. Step 3 is the operating system sending the message from the client machine to the server machine. Step 4 is the operating stub calling the server procedure with the unmarshaled parameters. The reply traces the same path in the other direction. The key item to note here is that the client procedure written by the user just makes normal procedure call to the client stub which has the same name as the server procedure. Since the client procedure and client stub are in the same address space the parameters are passed in the usual way. Similarly the server procedure is called by procedure in its address space with the parameters it expects. To the server procedure nothing is unusual. In this way instead of IO being done on sockets network communication is done by faking normal Despite the conceptual elegance of RPC there are few snakes hiding under the grass. big one is the use of pointer parameters. Normally passing pointer to procedure is not problem. The called procedure can use the pointer in the same way the caller can because both procedures live in the same virtual address Operating system space. With RPC passing pointers is impossible because the client and server are in different address spaces. In some cases tricks can be used to make it possible to pass pointers. Sup pose that the first parameter is pointer to an integer . The client stub can marshal and send it along to the server. The server stub then creates pointer to and passes it to the server procedure just as it expects. When the server proce dure returns control to the server stub the latter sends back to the client where the new is copied over the old one just in case the server changed it. In effect the standard calling sequence of callbyreference has been replaced by callby copyrestore. Unfortunately this trick does not always work for example if the pointer points to graph or other complex data structure. For this reason some restrictions must be placed on parameters to procedures called remotely as we second problem is that in weakly typed languages like it is perfectly legal to write procedure that computes the inner product of two vectors without specifying how large either one is. Each could be terminated by special value known only to the calling and called procedures. Under these circum stances it is essentially impossible for the client stub to marshal the parameters it has no way of determining how large they are. third problem is that it is not always possible to deduce the types of the pa rameters not even from formal specification or the code itself. An example is printf which may have any number of parameters and the parame ters can be an arbitrary mixture of integers shorts longs characters strings float ingpoint numbers of various lengths and other types. Trying to call printf as remote procedure would be practically impossible because is so permissive. However rule saying that RPC can be used provided that you do not program in would not be popular with lot of programmers. fourth problem relates to the use of global variables. Normally the calling and called procedure can communicate by using global variables in addition to communicating via parameters. But if the called procedure is moved to remote machine the code will fail because the global variables are no longer shared. widely used but some restrictions are needed to make it work well in practice. In terms of transport layer protocols UDP is good base on which to imple ment RPC. Both requests and replies may be sent as single UDP packet in the simplest case and the operation can be fast. However an implementation must in clude other machinery as well. Because the request or the reply may be lost the client must keep timer to retransmit the request. Note that reply serves as an implicit acknowledgement for request so the request need not be separately acknowledged. Sometimes the parameters or results may be larger than the maxi mum UDP packet size in which case some protocol is needed to deliver large messages. If multiple requests and replies can overlap an identifier is needed to match the request with the reply. higherlevel concern is that the operation may not be idempotent . The simple case is idempotent operations such as DNS requests and replies. The client can safely retransmit these requests again and again if no replies are forthcoming. It does not matter whether the server never received the ever not all operations are idempotent for example because they have important sideeffects such as incrementing counter. RPC for these operations requires stronger semantics so that when the programmer calls procedure it is not exe cuted multiple times. In this case it may be necessary to set up TCP connection and send the request over it rather than using UDP. Clientserver RPC is one area in which UDP is widely used. Another one is for realtime multimedia applications. In particular as Internet radio Internet te lephony musicondemand videoconferencing videoondemand and other mul timedia applications became more commonplace people have discovered that each application was reinventing more or less the same realtime transport proto col. It gradually became clear that having generic realtime transport protocol for multiple applications would be good idea. Thus was RTP born. It is described in RFC 3550 and is now in widespread use for multimedia applications. We will describe two aspects of realtime transport. The first is the RTP protocol for transporting audio and video data in packets. The second is the processing that takes place mostly at the receiver to play out the audio and video at the right time. These RTP normally runs in user space over UDP . It oper ates as follows. The multimedia application consists of multiple audio video text and possibly other streams. These are fed into the RTP library which is in user space along with the application. This library multiplexes the streams and encodes them in RTP packets which it stuffs into socket. On the operating sys tem side of the socket UDP packets are generated to wrap the RTP packets and handed to IP for transmission over link such as Ethernet. The reverse process happens at the receiver. The multimedia application eventually receives multi media data from the RTP library. It is responsible for playing out the media. The As consequence of this design it is little hard to say which layer RTP is in. Since it runs in user space and is linked to the application program it certainly looks like an application protocol. On the other hand it is generic application independent protocol that just provides transport facilities so it also looks like transport protocol. Probably the best description is that it is transport protocol that just happens to be implemented in the application layer which is why we are The basic function of RTP is to multiplex several realtime data streams onto single stream of UDP packets. The UDP stream can be sent to single destina tion or to multiple destinations . Because RTP just uses normal UDP its packets are not treated specially by the routers unless some nor mal IP qualityofservice features are enabled. In particular there are no special guarantees about delivery and packets may be lost delayed corrupted etc. The RTP format contains several features to help receivers work with multi media information. Each packet sent in an RTP stream is given number one higher than its predecessor. This numbering allows the destination to determine if any packets are missing. If packet is missing the best action for the destination to take is up to the application. It may be to skip video frame if the packets are carrying video data or to approximate the missing value by interpolation if the packets are carrying audio data. Retransmission is not practical option since the retransmitted packet would probably arrive too late to be useful. As conse quence RTP has no acknowledgements and no mechanism to request retransmis Each RTP payload may contain multiple samples and they may be coded any way that the application wants. To allow for interworking RTP defines several profiles and for each profile multiple encoding for mats may be allowed. For example single audio stream may be encoded as 8 bit PCM samples at 8 kHz using delta encoding predictive encoding GSM en source can specify the encoding but is otherwise not involved in how encoding is Another facility many realtime applications need is timestamping. The idea here is to allow the source to associate timestamp with the first sample in each packet. The timestamps are relative to the start of the stream so only the dif ferences between timestamps are significant. The absolute values have no mean ing. As we will describe shortly this mechanism allows the destination to do small amount of buffering and play each sample the right number of milliseconds after the start of the stream independently of when the packet containing the sam Not only does timestamping reduce the effects of variation in network delay but it also allows multiple streams to be synchronized with each other. For ex ample digital television program might have video stream and two audio streams. The two audio streams could be for stereo broadcasts or for handling films with an original language soundtrack and soundtrack dubbed into the local language giving the viewer choice. Each stream comes from different physi cal device but if they are timestamped from single counter they can be played back synchronously even if the streams are transmitted andor received somewhat there is only one code point left . The bit is an applicationspecific marker bit. It can be used to mark the start of video frame the start of word in an audio channel or some thing else that the application understands. The Payload type field tells which en coding algorithm has been used . Since every packet carries this field the encoding can change during transmission. The Sequence number is just counter that is incremented on each RTP packet sent. It is used to detect lost packets. The Timestamp is produced by the streams source to note when the first sam ple in the packet was made. This value can help reduce timing variability called jitter at the receiver by decoupling the playback from the packet arrival time. The Synchronization source identifier tells which stream the packet belongs to. It is the method used to multiplex and demultiplex multiple data streams onto single used when mixers are present in the studio. In that case the mixer is the syn chronizing source and the streams being mixed are listed here. RTP has little sister protocol called RTCP when the network is functioning well and to cut back the data rate when there is trouble in the network. By providing continuous feedback the encoding algorithms can be continuously adapted to provide the best quality pos sible under the current circumstances. For example if the bandwidth increases or decreases during the transmission the encoding may switch from MP3 to 8bit PCM to delta encoding as required. The Payload type field is used to tell the dest ination what encoding algorithm is used for the current packet making it possible to vary it on demand. ticipants. For multicast application with large group the bandwidth used by RTCP would quickly grow large. To prevent this from happening RTCP senders of the media bandwidth. To do this each participant needs to know the media bandwidth which it learns from the sender and the number of participants which RTCP also handles interstream synchronization. The problem is that different streams may use different clocks with different granularities and different drift rates. RTCP can be used to keep them in sync. text. This information can be displayed on the receivers screen to indicate who is talking at the moment. More information about RTP can be found in Perkins . Once the media information reaches the receiver it must be played out at the right time. In general this will not be the time at which the RTP packet arrived at the receiver because packets will take slightly different amounts of time to transit the network. Even if the packets are injected with exactly the right intervals be tween them at the sender they will reach the receiver with different relative times. This variation in delay is called jitter. Even small amount of packet jitter can cause distracting media artifacts such as jerky video frames and unintelligible audio if the media is simply played out as it arrives. The solution to this problem is to buffer packets at the receiver before they packets being delivered with substantial amount of jitter. Packet 1 is sent from the server at 0 sec and arrives at the client at 1 sec. Packet 2 undergoes more delay and takes 2 sec to arrive. As the packets arrive they are buffered on the client machine. At 10 sec playback begins. At this time packets 1 through 6 have been buffered so that they can be removed from the buffer at uniform intervals for smooth play. In the general case it is not necessary to use uniform intervals be cause the RTP timestamps tell when the media should be played. Unfortunately we can see that packet 8 has been delayed so much that it is not available when its play slot comes up. There are two options. Packet 8 can be skipped and the player can move on to subsequent packets. Alternatively play back can stop until packet 8 arrives creating an annoying gap in the music or movie. In live media application like voiceoverIP call the packet will typi cally be skipped. Live applications do not work well on hold. In streaming me dia application the player might pause. This problem can be alleviated by delay ing the starting time even more by using larger buffer. For streaming audio or video player buffers of about 10 seconds are often used to ensure that the player receives all of the packets in time. For live applications like videoconferencing short buffers are needed for responsiveness. key consideration for smooth playout is the playback point or how long to wait at the receiver for media before playing it out. Deciding how long to wait depends on the jitter. The difference between lowjitter and highjitter con the two but if there is high jitter the playback point may need to be much further out to capture 99 of the packets than if there is low jitter. To pick good playback point the application can measure the jitter by look ing at the difference between the RTP timestamps and the arrival time. Each dif ference gives sample of the delay . However the delay can change over time due to other competing traffic and changing routes. they are running. However if not done well changing the playback point can pro duce an observable glitch to the user. One way to avoid this problem for audio is to adapt the playback point between talkspurts in the gaps in conversation. No one will notice the difference between short and slightly longer silence. RTP lets applications set the marker bit to indicate the start of new talkspurt for If the absolute delay until media is played out is too long live applications will suffer. Nothing can be done to reduce the propagation delay if direct path is Fraction of packets already being used. The playback point can be pulled in by simply accepting that larger fraction of packets will arrive too late to be played. If this is not ac ceptable the only way to pull in the playback point is to reduce the jitter by using better quality of service for example the expedited forwarding differentiated service. That is better network is needed. UDP is simple protocol and it has some very important uses such as client server interactions and multimedia but for most Internet applications reliable se quenced delivery is needed. UDP cannot provide this so another protocol is re quired. It is called TCP and is the main workhorse of the Internet. Let us now study it in detail. TCP was specifically designed to provide reliable endtoend byte stream over an unreliable internetwork. An internet work differs from single network because different parts may have wildly dif ferent topologies bandwidths delays packet sizes and other parameters. TCP was designed to dynamically adapt to properties of the internetwork and to be robust in the face of many kinds of failures. TCP was formally defined in RFC 793 in September 1981. As time went on many improvements have been made and various errors and inconsistencies have been fixed. To give you sense of the extent of TCP the important RFCs are THE INTERNET TRANSPORT PROTOCOLS TCP now RFC 793 plus clarifications and bug fixes in RFC 1122 extensions for highperformance in RFC 1323 selective acknowledgements in RFC 2018 con RFC 2873 improved retransmission timers in RFC 2988 and explicit congestion notification in RFC 3168. The full collection is even larger which led to guide to the many RFCs published of course as another RFC document RFC 4614. Each machine supporting TCP has TCP transport entity either library pro cedure user process or most commonly part of the kernel. In all cases it man ages TCP streams and interfaces to the IP layer. TCP entity accepts user data streams from local processes breaks them up into pieces not exceeding 64 KB or the TCP protocol . From the context it will be clear which is meant. For example in The user gives TCP the data the TCP transport entity is clear The IP layer gives no guarantee that datagrams will be delivered properly nor any indication of how fast datagrams may be sent. It is up to TCP to send data grams fast enough to make use of the capacity but not cause congestion and to time out and retransmit any datagrams that are not delivered. Datagrams that do arrive may well do so in the wrong order it is also up to TCP to reassemble them into messages in the proper sequence. In short TCP must furnish good per formance with the reliability that most applications want and that IP does not pro TCP service is obtained by both the sender and the receiver creating end points called sockets as discussed in Sec. 6.1.3. Each socket has socket num ber consisting of the IP address of the host and 16bit number local to that host called port. port is the TCP name for TSAP. For TCP service to be obtained connection must be explicitly established between socket on one socket may be used for multiple connections at the same time. In other words two or more connections may terminate at the same socket. Connections are identified by the socket identifiers at both ends that is . No virtual circuit numbers or other identifiers are used. Port numbers below 1024 are reserved for standard services that can usually only be started by privileged users . They are called wellknown ports. For example any process wishing to remotely retrieve mail from host can connect to the destination hosts port 143 to contact its IMAP daemon. The list of wellknown ports is given at Over 700 have Other ports from 1024 through 49151 can be registered with IANA for use by unprivileged users but applications can and do choose their own ports. For ex ample the BitTorrent peertopeer filesharing application uses ports 68816887 but may run on other ports as well. at boot time the SSH daemon attach itself to port 22 at boot time and so on. However doing so would clutter up memory with daemons that were idle most of the time. Instead what is commonly done is to have single daemon called inetd in UNIX attach itself to multiple ports and wait for the first incoming connection. When that occurs inetd forks off new process and executes the appropriate daemon in it letting that daemon handle the request. In this way the daemons other than inetd are only active when there is work for them to do. Inetd learns which ports it is to use from configuration file. Conse quently the system administrator can set up the system to have permanent dae mons on the busiest ports and inetd on the rest. All TCP connections are full duplex and pointtopoint. Full duplex means that traffic can go in both directions at the same time. Pointtopoint means that each connection has exactly two end points. TCP does not support multicasting or TCP connection is byte stream not message stream. Message bound aries are not preserved end to end. For example if the sending process does four 512byte writes to TCP stream these data may be delivered to the receiving process as four 512byte chunks two 1024byte chunks one 2048byte chunk in which the data were written no matter how hard it tries. THE INTERNET TRANSPORT PROTOCOLS TCP 2048 bytes of data delivered to the application in single READ call. Files in UNIX have this property too. The reader of file cannot tell whether the file was written block at time byte at time or all in one blow. As with UNIX file the TCP software has no idea of what the bytes mean and no interest in finding out. byte is just byte. When an application passes data to TCP TCP may send it immediately or buffer it at its discretion. However sometimes the application really wants the data to be sent immediately. For example suppose user of an interactive game wants to send stream of there is collection of them. To force data out TCP has the notion of PUSH flag that is carried on packets. The original intent was to let applications tell TCP implementations via the PUSH flag not to delay the transmission. However ap plications cannot literally set the PUSH flag when they send data. Instead dif ferent operating systems have evolved different options to expedite transmission . For Internet archaeologists we will also mention one interesting feature of TCP service that remains in the protocol but is rarely used urgent data. When an application has high priority data that should be processed immediately for ex ample if an interactive user hits the CTRLC key to break off remote computa tion that has already begun the sending application can put some control infor mation in the data stream and give it to TCP along with the URGENT flag. This event causes TCP to stop accumulating data and transmit everything it has for that connection immediately. When the urgent data are received at the destination the receiving application is interrupted so it can stop whatever it was doing and read the data stream to find the urgent data. The end of the urgent data is marked so the application knows when it is over. The start of the urgent data is This scheme provides crude signaling mechanism and leaves everything else up to the application. However while urgent data is potentially useful it found no compelling application early on and fell into disuse. Its use is now discouraged because of implementation differences leaving applications to handle their own signaling. Perhaps future transport protocols will provide better signaling. key feature of TCP and one that dominates the protocol design is that every byte on TCP connection has its own 32bit sequence number. When the Internet began the lines between routers were mostly 56kbps leased lines so host blasting away at full speed took over 1 week to cycle through the sequence numbers. At modern network speeds the sequence numbers can be consumed at an alarming rate as we will see later. Separate 32bit sequence numbers are car ried on packets for the sliding window position in one direction and for acknowl edgements in the reverse direction as discussed below. The sending and receiving TCP entities exchange data in the form of seg followed by zero or more data bytes. The TCP software decides how big seg ments should be. It can accumulate data from several writes into one segment or can split data from one write over multiple segments. Two limits restrict the seg byte IP payload. Second each link has an MTU . Each segment must fit in the MTU at the sender and receiver so that it can be sent and received in single unfragmented packet. In practice the MTU is generally 1500 bytes and thus defines the upper bound on seg However it is still possible for IP packets carrying TCP segments to be frag mented when passing over network path for which some link has small MTU. Instead modern TCP implementations perform path MTU discovery by using the technique outlined in RFC 1191 that we described in Sec. 5.5.5. This technique uses ICMP error messages to find the smallest MTU for any link on the path. TCP then adjusts the segment size downwards to avoid frag The basic protocol used by TCP entities is the sliding window protocol with dynamic window size. When sender transmits segment it also starts timer. When the segment arrives at the destination the receiving TCP entity sends back segment bearing an acknowledge ment number equal to the next sequence number it expects to receive and the re maining window size. If the senders timer goes off before the acknowledgement is received the sender transmits the segment again. Although this protocol sounds simple there are many sometimes subtle ins and outs which we will cover below. Segments can arrive out of order so bytes 30724095 can arrive but cannot be acknowledged because bytes 20483071 have not turned up yet. Segments can also be delayed so long in transit that the sender times out and retransmits them. The retransmissions may include different byte THE INTERNET TRANSPORT PROTOCOLS TCP ranges than the original transmission requiring careful administration to keep track of which bytes have been correctly received so far. However since each byte in the stream has its own unique offset it can be done. cient way. considerable amount of effort has gone into optimizing the per algorithms used by many TCP implementations will be discussed below. Segments without any data are legal and are commonly used for acknowledge Options port fields identify the local end points of the connection. TCP port plus its hosts IP address forms 48bit unique end point. The source and destination end tuple because it consists of five pieces of information the protocol source IP and source port and destination IP and destination port. The Sequence number and Acknowledgement number fields perform their usual functions. Note that the latter specifies the next inorder byte expected not the last byte correctly received. It is cumulative acknowledgement because it summarizes the received data with single number. It does not go beyond lost data. Both are 32 bits because every byte of data is numbered in TCP stream. length in words so the effect is the same. Next comes 4bit field that is not used. The fact that these bits have remained unused for 30 years is testimony to how well thought out TCP is. Lesser protocols would have needed these bits to fix bugs in the original design. Now come eight 1bit flags. CWR and ECE are used to signal congestion when ECN is used as specified in RFC 3168. ECE is set to signal an ECNEcho to TCP sender to tell it to slow down when the TCP receiver gets congestion indication from the network. CWR is set to signal Congestion Window Reduced from the TCP sender to the TCP receiver so that it knows the sender has slowed down and can stop sending the ECNEcho. We discuss the role of ECN in TCP congestion control in Sec. 6.5.10. URG is set to 1 if the Urgent pointer is in use. The Urgent pointer is used to indicate byte offset from the current sequence number at which urgent data are to be found. This facility is in lieu of interrupt messages. As we mentioned above this facility is barebones way of allowing the sender to signal the re ceiver without getting TCP itself involved in the reason for the interrupt but it is The ACK bit is set to 1 to indicate that the Acknowledgement number is valid. This is the case for nearly all packets. If ACK is 0 the segment does not contain an acknowledgement so the Acknowledgement number field is ignored. The PSH bit indicates PUSHed data. The receiver is hereby kindly requested to deliver the data to the application upon arrival and not buffer it until full buff er has been received . The RST bit is used to abruptly reset connection that has become confused due to host crash or some other reason. It is also used to reject an invalid seg ment or refuse an attempt to open connection. In general if you get segment with the RST bit on you have problem on your hands. The SYN bit is used to establish connections. The connection request has SYN 1 and ACK 0 to indicate that the piggyback acknowledgement field is not in use. The connection reply does bear an acknowledgement however so it has SYN 1 and ACK 1. In essence the SYN bit is used to denote both CONNEC TION REQUEST and CONNECTION ACCEPTED with the ACK bit used to distin The FIN bit is used to release connection. It specifies that the sender has no more data to transmit. However after closing connection the closing process may continue to receive data indefinitely. Both SYN and FIN segments have se quence numbers and are thus guaranteed to be processed in the correct order. Flow control in TCP is handled using variablesized sliding window. The Window size field tells how many bytes may be sent starting at the byte acknow ledged. Window size field of 0 is legal and says that the bytes up to and includ ing Acknowledgement number 1 have been received but that the receiver has not had chance to consume the data and would like no more data for the mo ment thank you. The receiver can later grant permission to send by transmitting segment with the same Acknowledgement number and nonzero Window size In the protocols of Chap. 3 acknowledgements of frames received and per mission to send new frames were tied together. This was consequence of fixed window size for each protocol. In TCP acknowledgements and permission to send additional data are completely decoupled. In effect receiver can say have received bytes up through but do not want any more just now thank you. This decoupling gives additional flexibil mandatory. Please see Sec. 6.4.1 for details. The Options field provides way to add extra facilities not covered by the The options are of variable length fill multiple of 32 bits by using padding with can be specified. Some options are carried when connection is established to ne gotiate or inform the other side of capabilities. Other options are carried on pack ets during the lifetime of the connection. Each option has TypeLengthValue widely used option is the one that allows each host to specify the MSS it is willing to accept. Using large segments is more more data but small hosts may not be able to handle big segments. During con nection setup each side can announce its maximum and see its partners. If host does not use this option it defaults to 536byte payload. All Internet hosts are required to accept TCP segments of 536 20 556 bytes. The maximum seg ment size in the two directions need not be the same. For lines with high bandwidth high delay or both the 64KB window corres ponding to 16bit field is problem. For example on an OC12 line the sender will be idle more than 98 of the time waiting for acknowledge ments. larger window size would allow the sender to keep pumping data out. The window scale option allows the sender and receiver to negotiate window scale factor at the start of connection. Both sides use the scale factor to shift the bytes. Most TCP implementations support this option. The timestamp option carries timestamp sent by the sender and echoed by the receiver. It is included in every packet once its use is established during con nection setup and used to compute roundtrip time samples that are used to esti mate when packet has been lost. It is also used as logical extension of the 32 bit sequence number. On fast connection the sequence number may wrap around quickly leading to possible confusion between old and new data. The PAWS scheme discards ar riving segments with old timestamps to prevent this problem. sender the ranges of sequence numbers that it has received. It supplements the Acknowledgement number and is used after packet has been lost but subsequent data has arrived. The new data is not reflected by the Acknowledge byte that is expected. With SACK the sender is explicitly aware of what data the receiver has and hence can determine what data should be retransmitted. SACK is defined in RFC 2108 and RFC 2883 and is increasingly used. We describe the use of SACK along with congestion control in Sec. 6.5.10. Connections are established in TCP by means of the threeway handshake dis cussed in Sec. 6.2.2. To establish connection one side say the server pas sively waits for an incoming connection by executing the LISTEN and ACCEPT primitives in that order either specifying specific source or nobody in particular. The other side say the client executes CONNECT primitive specifying the IP address and port to which it wants to connect the maximum TCP segment size it is willing to accept and optionally some user data . The CON NECT primitive sends TCP segment with the SYN bit on and ACK bit off and waits for response. When this segment arrives at the destination the TCP entity there checks to see if there is process that has done LISTEN on the port given in the Destination port field. If not it sends reply with the RST bit on to reject the connection. If some process is listening to the port that process is given the incoming TCP segment. It can either accept or reject the connection. If it accepts an ac knowledgement segment is sent back. The sequence of TCP segments sent in the of sequence space so that it can be acknowledged unambiguously. taneous connection establishment on both sides. In the event that two hosts simultaneously attempt to establish connection 37. The result of these events is that just one connection is established not two because connections are identified by their end points. If the first setup re sults in connection identified by and the second one does too only one table entry is made namely for . Recall that the initial sequence number chosen by each host should cycle slowly rather than be constant such as 0. This rule is to protect against delayed duplicate packets as we discussed in Sec 6.2.2. Originally this was accomplished with clockbased scheme in which the clock ticked every 4 μsec. However vulnerability with implementing the threeway handshake is that the listening process must remember its sequence number as soon it responds with its own SYN segment. This means that malicious sender can tie up resources on host by sending stream of SYN segments and never following through to com plete the connection. This attack is called SYN flood and it crippled many Web servers in the 1990s. One way to defend against this attack is to use SYN cookies. Instead of remembering the sequence number host chooses cryptographically generated sequence number puts it on the outgoing segment and forgets it. If the threeway handshake completes this sequence number will be returned to the host. It can then regenerate the correct sequence number by running the same crypto graphic function as long as the inputs to that function are known for example the other hosts IP address and port and local secret. This procedure allows the host to check that an acknowledged sequence number is correct without having to remember the sequence number separately. There are some caveats such as the inability to handle TCP options so SYN cookies may be used only when the host is subject to SYN flood. However they are an interesting twist on connection establishment. For more information see RFC 4987 and Lemon . Although TCP connections are full duplex to understand how connections are released it is best to think of them as pair of simplex connections. Each simplex connection is released independently of its sibling. To release connection either party can send TCP segment with the FIN bit set which means that it has no more data to transmit. When the FIN is acknowledged that direction is shut down for new data. Data may continue to flow indefinitely in the other direction how ever. When both directions have been shut down the connection is released. Normally four TCP segments are needed to release connection one FIN and one ACK for each direction. However it is possible for the first ACK and the sec ond FIN to be contained in the same segment reducing the total count to three. Just as with telephone calls in which both people say goodbye and hang up the phone simultaneously both ends of TCP connection may send FIN segments at the same time. These are each acknowledged in the usual way and the con nection is shut down. There is in fact no essential difference between the two hosts releasing sequentially or simultaneously. To avoid the twoarmy problem timers are used. If response to FIN is not forthcoming within two maximum packet lifetimes the sender of the FIN releases the connection. The other side will eventually notice that nobody seems to be listening to it anymore and will time out as well. While this solution is not perfect given the fact that perfect solution is theoretically The steps required to establish and release connections can be represented in events are legal. When legal event happens some action may be taken. If some Each connection starts in the CLOSED state. It leaves that state when it does either passive open or an active open . If the other side does the opposite one connection is established and the state becomes ESTA BLISHED. Connection release can be initiated by either side. When it is com plete the state returns to CLOSED. client actively connecting to passive server is shown with heavy linessolid for the client dotted for the server. The lightface lines are unusual event sequences. Wait for all packets to die off userinitiated system call segment arrival or in one case timeout of twice the maximum packet lifetime. The action is the sending of control segment or nothing indicated by . Comments are shown in parentheses. One can best understand the diagram by first following the path of client then later following the path of server at the same time on behalf of multiple applications so the state is per connection and recorded in the connection record. When the switches into the ESTABLISHED state. Data can now be sent and received. When an application is finished it executes CLOSE primitive which causes the local TCP entity to send FIN segment and wait for the corresponding ACK . When the ACK arrives transition is made to the state FIN WAIT 2 and one direction of the connection is closed. When the other side closes too FIN comes in which is acknowledged. Now both sides are closed but TCP waits time equal to twice the maximum packet lifetime to guarantee that all packets from the connection have died off just in case the ac knowledgement was lost. When the timer goes off TCP deletes the connection Now let us examine connection management from the servers viewpoint. The server does LISTEN and settles down to see who turns up. When SYN of the 3way handshake solid line is the normal path for client. The heavy dashed line is the normal path for server. The light lines are unusual events. Each transition is labeled with the event causing it and the action resulting from it separated by slash. comes in it is acknowledged and the server goes to the SYN RCVD state. When the servers SYN is itself acknowledged the threeway handshake is complete and the server goes to the ESTABLISHED state. Data transfer can now occur. When the client is done transmitting its data it does CLOSE which causes FIN to arrive at the server . The server is then signaled. When it too does CLOSE FIN is sent to the client. When the THE INTERNET TRANSPORT PROTOCOLS TCP clients acknowledgement shows up the server releases the connection and deletes the connection record. acknowledgement of the correct receipt of segments and receiver buffer alloca tion. For example suppose the receiver has 4096byte buffer as shown in bytes of buffer space it will advertise window of 2048 starting at the next byte expected. Now the sender transmits another 2048 bytes which are acknowledged but the advertised window is of size 0. The sender must stop until the application process on the receiving host has removed some data from the buffer at which time TCP can advertise larger window and more data can be sent. When the window is 0 the sender may not normally send segments with two exceptions. First urgent data may be sent for example to allow the user to kill the process running on the remote machine. Second the sender may send 1byte segment to force the receiver to reannounce the next byte expected and the win dow size. This packet is called window probe. The TCP standard explicitly Senders are not required to transmit data as soon as they come in from the ap plication. Neither are receivers required to send acknowledgements as soon as knowing that it had 4KB window would have been completely correct in just buffering the data until another 2 KB came in to be able to transmit segment with 4KB payload. This freedom can be used to improve performance. Consider connection to remote terminal for example using SSH or telnet that reacts on every keystroke. In the worst case whenever character arrives at the sending TCP entity TCP creates 21byte TCP segment which it gives to IP to send as 41byte IP datagram. At the receiving side TCP immediately sends the remote terminal has processed the character it echoes the character for local display using 41byte packet. In all 162 bytes of bandwidth are used and four segments are sent for each character typed. When bandwidth is scarce this meth od of doing business is not desirable. One approach that many TCP implementations use to optimize this situation is called delayed acknowledgements. The idea is to delay acknowledgements which to hitch free ride. Assuming the terminal echoes within 500 msec only one 41byte packet now need be sent back by the remote side cutting the packet count and bandwidth usage in half. Although delayed acknowledgements reduce the load placed on the network by the receiver sender that sends multiple short packets is still operating inefficiently. way to reduce this usage is known as Nagles algorithm . What Nagle suggested is simple when data come into the sender in small pieces just send the first piece and buffer all the rest until the first piece is acknowledged. Then send all the buffered data in one TCP segment and start buffering again until the next segment is acknowledged. That is only one short packet can be outstanding at any time. If many pieces of data are sent by the application in one roundtrip time Nagles algorithm will put the many pieces in one segment greatly reducing the band width used. The algorithm additionally says that new segment should be sent if enough data have trickled in to fill maximum segment. Nagles algorithm is widely used by TCP implementations but there are times when it is better to disable it. In particular in interactive games that are run over which makes for unhappy users. more subtle problem is that Nagles algorithm can sometimes interact with delayed acknowledgements to cause temporary deadlock the receiver waits for data on which to piggyback an acknowledgement and the sender waits on the acknowledgement to send more data. This interaction rithm can be disabled . Mogul and Another problem that can degrade TCP performance is the silly window syn drome . This problem occurs when data are passed to the sending TCP entity in large blocks but an interactive application on the receiving side the TCP buffer on the receiving side is full and the sender knows this. Then the interactive application reads one character from the TCP stream. This action makes the receiving TCP happy so it sends window and sends 1 byte. The buffer is now full so the receiver acknowledges the 1byte segment and sets the window to 0. This behavior can go on forever. byte. Instead it is forced to wait until it has decent amount of space available and advertise that instead. Specifically the receiver should not send window nection was established or until its buffer is half empty whichever is smaller. Furthermore the sender can also help by not sending tiny segments. Instead it should wait until it can send full segment or at least one containing half of the receivers buffer size. Nagles algorithm and Clarks solution to the silly window syndrome are complementary. Nagle was trying to solve the problem caused by the sending ap plication delivering data to TCP byte at time. Clark was trying to solve the problem of the receiving application sucking the data up from TCP byte at not to send small segments and the receiver not to ask for them. The receiving TCP can go further in improving performance than just doing can block READ request from the application until it has large chunk of data for it. Doing so reduces the number of calls to TCP . It also increases the response time but for noninteractive applications like file transfer efficiency may be more important than response time to individual requests. order. The receiver will buffer the data until it can be passed up to the application Room for one more byte in order. Actually nothing bad would happen if outoforder segments were dis carded since they would eventually be retransmitted by the sender but it would Acknowledgements can be sent only when all the data up to the byte acknow ledged have been received. This is called cumulative acknowledgement. If the receiver gets segments 0 1 2 4 5 6 and 7 it can acknowledge everything up to and including the last byte in segment 2. When the sender times out it then retransmits segment 3. As the receiver has buffered segments 4 through 7 upon receipt of segment 3 it can acknowledge all bytes up to the end of segment 7. TCP uses multiple timers to do its work. The most im portant of these is the RTO . When segment is sent retransmission timer is started. If the segment is acknowledged before the timer expires the timer is stopped. If on the other hand the timer goes off before the acknowledgement comes in the segment is retransmitted . The question that arises is how long should the timeout be This problem is much more difficult in the transport layer than in data link protocols such as 802.11. In the latter case the expected delay is measured in THE INTERNET TRANSPORT PROTOCOLS TCP microseconds and is highly predictable so the timer can be set to go off just slightly after the acknowledgement is expected as shown in the absence of an acknowledgement at the expected time generally means either the frame or the acknowledgement has been lost. Roundtrip time data link layer. Probability density of acknowledgement arrival times for TCP. TCP is faced with radically different environment. The probability density function for the time it takes for TCP acknowledgement to come back looks mining the roundtrip time to the destination is tricky. Even when it is known deciding on the timeout interval is also difficult. If the timeout is set too short ternet with useless packets. If it is set too long performance will suffer due to the long retransmission delay whenever packet is lost. Furthermore the mean and variance of the acknowledgement arrival distribution can change rapid ly within few seconds as congestion builds up or is resolved. The solution is to use dynamic algorithm that constantly adapts the timeout interval based on continuous measurements of network performance. The algo rithm generally used by TCP is due to Jacobson and works as follows. For each connection TCP maintains variable SRTT that is the best current estimate of the roundtrip time to the destination in ques tion. When segment is sent timer is started both to see how long the ac knowledgement takes and also to trigger retransmission if it takes too long. If the acknowledgement gets back before the timer expires TCP measures how long SRTT α SRTT where α is smoothing factor that determines how quickly the old values are for gotten. Typically α 78. This kind of formula is an EWMA or lowpass filter that discards noise in the samples. Even given good value of SRTT choosing suitable retransmission timeout is nontrivial matter. Initial implementations of TCP used 2xRTT but experience showed that constant value was too inflexible because it failed to respond when the variance went up. In particular queueing models of random traffic predict that when the load approaches capacity the delay becomes large and highly variable. This can lead to the retransmission timer firing and copy of the packet being retransmitted although the original packet is still transiting the network. It is all the more likely to happen under conditions of high load which is the worst time at which to send additional packets into the network. To fix this problem Jacobson proposed making the timeout value sensitive to the variance in roundtrip times as well as the smoothed roundtrip time. This change requires keeping track of another smoothed variable RTTVAR but it is close enough in practice. Jacobsons paper is full of clever tricks to compute timeouts using only integer adds subtracts and shifts. This economy is not needed for modern hosts but it has become part of the culture that allows TCP to run on all manner of devices from supercomputers down to tiny devices. So far nobody has put it on an RFID chip but someday Who knows. More details of how to compute this timeout including initial settings of the variables are given in RFC 2988. The retransmission timer is also held to mini mum of 1 second regardless of the estimates. This is conservative value chosen to prevent spurious retransmissions based on measurements . Most TCP implementations use it. The retransmission timer is not the only timer TCP uses. second timer is the persistence timer. It is designed to prevent the following deadlock. The re ceiver sends an acknowledgement with window size of 0 telling the sender to lost. Now the sender and the receiver are each waiting for the other to do some thing. When the persistence timer goes off the sender transmits probe to the re ceiver. The response to the probe gives the window size. If it is still 0 the per sistence timer is set again and the cycle repeats. If it is nonzero data can now be third timer that some implementations use is the keepalive timer. When connection has been idle for long time the keepalive timer may go off to cause one side to check whether the other side is still there. If it fails to respond the con nection is terminated. This feature is controversial because it adds overhead and may terminate an otherwise healthy connection due to transient network parti The last timer used on each TCP connection is the one used in the TIME WAIT state while closing. It runs for twice the maximum packet lifetime to make sure that when connection is closed all packets created by it have died off. We have saved one of the key functions of TCP for last congestion control. When the load offered to any network is more than it can handle congestion builds up. The Internet is no exception. The network layer detects congestion when queues grow large at routers and tries to manage it if only by dropping packets. It is up to the transport layer to receive congestion feedback from the network layer and slow down the rate of traffic that it is sending into the network. In the Internet TCP plays the main role in controlling congestion as well as the main role in reliable transport. That is why it is such special protocol. We covered the general situation of congestion control in Sec. 6.3. One key takeaway was that transport protocol using an AIMD control law in response to binary congestion signals from the gestion control is based on implementing this approach using window and with packet loss as the binary signal. To do so TCP maintains congestion window whose size is the number of bytes the sender may have in the network at any time. The corresponding rate is the window size divided by the roundtrip time of the connection. TCP adjusts the size of the window according to the AIMD rule. Recall that the congestion window is maintained in addition to the flow con trol window which specifies the number of bytes that the receiver can buffer. Both windows are tracked in parallel and the number of bytes that may be sent is the smaller of the two windows. Thus the effective window is the smaller of what the sender thinks is all right and what the receiver thinks is all right. It takes two to tango. TCP will stop sending data if either the congestion or the flow con trol window is temporarily full. If the receiver says send 64 KB but the sender knows that bursts of more than 32 KB clog the network it will send 32 KB. On the other hand if the receiver says send 64 KB and the sender knows that bursts of up to 128 KB get through effortlessly it will send the full 64 KB re quested. The flow control window was described earlier and in what follows we will only describe the congestion window. Modern congestion control was added to TCP largely through the efforts of Van Jacobson . It is fascinating story. Starting in 1986 the growing pop ularity of the early Internet led to the first occurrence of what became known as congestion collapse prolonged period during which goodput dropped precipi tously due to congestion in the network. Jacob son set out to understand what was happening and remedy the The highlevel fix that Jacobson implemented was to approximate an AIMD congestion window. The interesting part and much of the complexity of TCP con gestion control is how he added this to an existing implementation without chang ing any of the message formats which made it instantly deployable. To start he observed that packet loss is suitable signal of congestion. This signal comes little late but it is quite dependable. After all it is difficult to build router that does not drop packets when it is overloaded. This fact is unlikely to change. Even when terabyte memories appear to buffer vast numbers of packets we will probably have terabitsec networks to fill up However using packet loss as congestion signal depends on transmission er rors being relatively rare. This is not normally the case for wireless links such as 802.11 which is why they include their own retransmission mechanism at the link layer. Because of wireless retransmissions network layer packet loss due to transmission errors is normally masked on wireless networks. It is also rare on other links because wires and optical fibers typically have low biterror rates. All the Internet TCP algorithms assume that lost packets are caused by con gestion and monitor timeouts and look for signs of trouble the way miners watch their canaries. good retransmission timer is needed to detect packet loss signals accurately and in timely manner. We have already discussed how the TCP re transmission timer includes estimates of the mean and variation in roundtrip THE INTERNET TRANSPORT PROTOCOLS TCP times. Fixing this timer by including the variation factor was an important step in Jacobsons work. Given good retransmission timeout the TCP sender can track the outstanding number of bytes which are loading the network. It simply looks at the difference between the sequence numbers that are transmitted and acknow Now it seems that our task is easy. All we need to do is to track the conges tion window using sequence and acknowledgement numbers and adjust the con gestion window using an AIMD rule. As you might have expected it is more complicated than that. first consideration is that the way packets are sent into the network even over short periods of time must be matched to the network path. Otherwise the traffic will cause congestion. For example consider host with congestion window of 64 KB attached to 1Gbps switched Ethernet. If the host sends the entire window at once this burst of traffic may travel over slow 1Mbps ADSL line further along the path. The burst that took only half millisecond on the 1Gbps line will clog the 1Mbps line for half second com pletely disrupting protocols such as voice over IP. This behavior might be good idea for protocol designed to cause congestion but not for protocol to control However it turns out that we can use small bursts of packets to our advan link sends small burst of four packets to receiver on slow network that is the bottleneck or slowest part of the path. Initially the four packets travel over the link as quickly as they can be sent by the sender. At the router they are queued while being sent because it takes longer to send packet over the slow link than to receive the next packet over the fast link. But the queue is not large because only small number of packets were sent at once. Note the increased length of the packets on the slow link. The same packet of 1 KB say is now longer because it takes more time to send it on slow link than on fast one. Eventually the packets get to the receiver where they are acknowledged. The times for the acknowledgements reflect the times at which the packets arrived at the receiver after crossing the slow link. They are spread out compared to the original packets on the fast link. As these acknowledgements travel over the net work and back to the sender they preserve this timing. The key observation is this the acknowledgements return to the sender at about the rate that packets can be sent over the slowest link in the path. This is precisely the rate that the sender wants to use. If it injects new packets into the network at this rate they will be sent as fast as the slow link permits but they will not queue up and congest any router along the path. This timing is known as an ack clock. It is an essential part of TCP. By using an ack clock TCP smoothes out traffic and avoids unnecessary queues at routers. second consideration is that the AIMD rule will take very long time to reach good operating point on fast networks if the congestion window is started from small size. Consider modest network path that can support 10 Mbps with an RTT of 100 msec. The appropriate congestion window is the bandwidthdelay product which is 1 Mbit or 100 packets of 1250 bytes each. If the congestion win dow starts at 1 packet and increases by 1 packet every RTT it will be 100 RTTs or 10 seconds before the connection is running at about the right rate. That is long time to wait just to get to the right speed for transfer. We could reduce this startup time by starting with larger initial window say of 50 packets. But this window would be far too large for slow or short links. It would cause congestion if used all at once as we have just described. Instead the solution Jacobson chose to handle both of these considerations is mix of linear and multiplicative increase. When connection is established the sender initializes the congestion window to small initial value of at most four segments the details are described in RFC 3390 and the use of four segments is an increase from an earlier initial value of one segment based on experience. The sender then sends the initial window. The packets will take roundtrip time to be acknowledged. For each segment that is acknowledged before the retransmis sion timer goes off the sender adds one segments worth of bytes to the conges tion window. Plus as that segment has been acknowledged there is now one less segment in the network. The upshot is that every acknowledged segment allows two more segments to be sent. The congestion window is doubling every round This algorithm is called slow start but it is not slow at allit is exponential growthexcept in comparison to the previous algorithm that let an entire flow roundtrip time the sender injects one packet into the network . Two packets are sent in the next roundtrip time then four packets in the third roundtrip time. Slowstart works well over range of link speeds and roundtrip times and uses an ack clock to match the rate of sender transmissions to the network path. Take look at the way acknowledgements return from the sender to the receiver tion window by one and immediately sends two packets into the network. However these two packets will not necessarily arrive at the receiver as closely spaced as when they were sent. For example suppose the sender is on 100Mbps Ethernet. Each packet of 1250 bytes takes 100 μsec to send. So the delay between the packets can be as small as 100 μsec. The situation changes if these packets go across 1 Mbps ADSL link anywhere along the path. It now takes 10 msec to send the same This means that the minimum spacing between the two packets has grown by factor of 100. Unless the packets have to wait together in queue on later link the spacing will remain large. data packets arriving at the receiver. The same spacing is kept when the receiver sends acknowledgements and thus when the sender receives the acknowledge ments. If the network path is slow acknowledgements will come in slowly . If the network path is fast acknowledgements will come in quickly . All the sender has to do is follow the timing of the ack clock as it injects new packets which is what slow start does. Because slow start causes exponential growth eventually it will send too many packets into the network too quickly. When this happens queues will build up in the network. When the queues are full one or more packets will be lost. After this happens the TCP sender will time out when an acknowledgement fails to arrive in time. There is evidence of slow start grow four packets take an entire RTT to arrive at the receiver. That is congestion window of four packets is the right size for this connection. However as these packets are acknowledged slow start continues to grow the congestion window reaching eight packets in another RTT. Only four of these packets can reach the receiver in one RTT no matter how many are sent. That is the network pipe is full. Additional packets placed into the network by the sender will build up in router queues since they cannot be delivered to the receiver quickly enough. Con gestion and packet loss will occur soon. To keep slow start under control the sender keeps threshold for the connect ion called the slow start threshold. Initially this value is set arbitrarily high to the size of the flow control window so that it will not limit the connection. TCP keeps increasing the congestion window in slow start until timeout occurs or the congestion window exceeds the threshold . Whenever packet loss is detected for example by timeout the slow start threshold is set to be half of the congestion window and the entire process is restarted. The idea is that the current window is too large because it caused con gestion previously that is only now detected by timeout. Half of the window which was used successfully at an earlier time is probably better estimate for congestion window that is close to the path capacity but will not cause loss. In cause loss while the congestion window of four packets in the previous RTT was the right value. The congestion window is then reset to its small initial value and slow start resumes. Whenever the slow start threshold is crossed TCP switches from slow start to additive increase. In this mode the congestion window is increased by one seg ment every roundtrip time. Like slow start this is usually implemented with an increase for every segment that is acknowledged rather than an increase once per RTT. Call the congestion window cwnd and the maximum segment size MSS. common approximation is to increase cwnd by cwnd for each of the cwnd MSS packets that may be acknowledged. This increase does not need to be fast. The whole idea is for TCP connection to spend lot of time with its con gestion window close to the optimum valuenot so small that throughput will be low and not so large that congestion will occur. the end of every RTT the senders congestion window has grown enough that it can inject an additional packet into the network. Compared to slow start the linear rate of growth is much slower. It makes little difference for small conges tion windows as is the case here but large difference in the time taken to grow the congestion window to 100 segments for example. There is something else that we can do to improve performance too. The defect in the scheme so far is waiting for timeout. Timeouts are relatively long because they must be conservative. After packet is lost the receiver cannot acknowledge past it so the acknowledgement number will stay fixed and the sender will not be able to send any new packets into the network because its con gestion window remains full. This condition can continue for relatively long period until the timer fires and the lost packet is retransmitted. At that stage TCP slow starts again. There is quick way for the sender to recognize that one of its packets has been lost. As packets beyond the lost packet arrive at the receiver they trigger THE INTERNET TRANSPORT PROTOCOLS TCP acknowledgements that return to the sender. These acknowledgements bear the same acknowledgement number. They are called duplicate acknowledgements. Each time the sender receives duplicate acknowledgement it is likely that an other packet has arrived at the receiver and the lost packet still has not shown up. Because packets can take different paths through the network they can arrive out of order. This will trigger duplicate acknowledgements even though no pack ets have been lost. However this is uncommon in the Internet much of the time. When there is reordering across multiple paths the received packets are usually not reordered too much. Thus TCP somewhat arbitrarily assumes that three dupli cate acknowledgements imply that packet has been lost. The identity of the lost packet can be inferred from the acknowledgement number as well. It is the very next packet in sequence. This packet can then be retransmitted right away before the retransmission timeout fires. This heuristic is called fast retransmission. After it fires the slow start threshold is still set to half the current congestion window just as with timeout. Slow start can be restarted by setting the congestion window to one packet. With this window size new packet will be sent after the one roundtrip time that it takes to acknowledge the retransmitted packet along with all data that had been sent before the loss was detected. An illustration of the congestion algorithm we have built up so far is shown in lease in 1988 in which it was included. The maximum segment size here is 1 KB. Initially the congestion window was 64 KB but timeout occurred so the thres hold is set to 32 KB and the congestion window to 1 KB for transmission 0. The congestion window grows exponentially until it hits the threshold . The window is increased every time new acknowledgement arrives rather than con tinuously which leads to the discrete staircase pattern. After the threshold is pas sed the window grows linearly. It is increased by one segment every RTT. The transmissions in round 13 are unlucky and one of them is lost in the network. This is detected when three duplicate acknowledge ments arrive. At that time the lost packet is retransmitted the threshold is set to half the current window and slow start is ini tiated all over again. Restarting with congestion window of one packet takes one roundtrip time for all of the previously transmitted data to leave the network and be acknowledged including the retransmitted packet. The congestion window KB. At that time the growth becomes linear again. It will continue in this fashion until another packet loss is detected via duplicate acknowledgements or timeout . TCP Tahoe provided working congestion control algorithm that solved the problem of congestion collapse. Jacobson realized that it is possible to do even better. At the time of the fast re transmission the connection is running with congestion window that is too large but it is still running with working ack clock. Every time another dupli cate acknowledgement arrives it is likely that another packet has left the network. Using duplicate acknowledgements to count the packets in the network makes it possible to let some packets exit the network and continue to send new packet for each additional duplicate acknowledgement. Fast recovery is the heuristic that implements this behavior. It is temporary mode that aims to maintain the ack clock running with congestion window that is the new threshold or half the value of the congestion window at the time of the THE INTERNET TRANSPORT PROTOCOLS TCP fast retransmission. To do this duplicate acknowledgements are counted until the number of packets in the network has fallen to the new threshold. This takes about half roundtrip time. From then on new packet can be sent for each duplicate acknowledgement that is received. One roundtrip time after the fast retransmission the lost packet will have been acknowledged. At that time the stream of duplicate acknowledgements will cease and fast recovery mode will be exited. The congestion window will be set to the new slow start threshold and grows by linear increase. The upshot of this heuristic is that TCP avoids slow start except when the connection is first started and when timeout occurs. The latter can still happen when more than one packet is lost and fast retransmission does not recover ade quately. Instead of repeated slow starts the congestion window of running con nection follows sawtooth pattern of additive increase and multiplicative decrease . This is exactly the AIMD rule that we sought to implement. named after the 4.3BSD Reno release in 1990 in which it was included. TCP Reno is essentially TCP Tahoe plus fast recovery. After an initial slow start the congestion window climbs linearly until packet loss is detected by duplicate ac knowledgements. The lost packet is retransmitted and fast recovery is used to keep the ack clock running until the retransmission is acknowledged. At that time the congestion window is resumed from the new slow start threshold rather than from 1. This behavior continues indefinitely and the connection spends most of the time with its congestion window close to the optimum value of the band TCP Reno with its mechanisms for adjusting the congestion window has formed the basis for TCP congestion control for more than two decades. Most of the changes in the intervening years have adjusted these mechanisms in minor ways for example by changing the choices of the initial window and removing various ambiguities. Some improvements have been made for recovering from two or more losses in window of packets. For example the TCP NewReno ver sion uses partial advance of the acknowledgement number after retransmission to find and repair another loss as described in RFC 3782. Since the mid1990s several variations have emerged that follow the principles we have de scribed but use slightly different control laws. For example Linux uses variant called CUBIC TCP and Windows includes variant called Com pound TCP . Two larger changes have also affected TCP implementations. First much of the complexity of TCP comes from inferring from stream of duplicate acknowl edgements which packets have arrived and which packets have been lost. The cumulative acknowledgement number does not provide this information. sim ple fix is the use of SACK which lists up to three ranges of bytes that have been received. With this information the sender can more directly decide what packets to retransmit and track the packets in flight to implement the congestion window. When the sender and receiver set up connection they each send the SACK permitted TCP option to signal that they understand selective acknowledgements. ceiver uses the TCP Acknowledgement number field in the normal manner as cumulative acknowledgement of the highest inorder byte that has been received. When it receives packet 3 out of order it sends SACK option for the received data along with the cumulative acknowl edgement for packet 1. The SACK option gives the byte ranges that have been re ceived above the number given by the cumulative acknowledgement. The first range is the packet that triggered the duplicate acknowledgement. The next ranges if present are older blocks. Up to three ranges are commonly used. By the time packet 6 is received two SACK byte ranges are used to indicate that packet 6 and packets 3 to 4 have been received in addition to all packets up to packet 1. From the information in each SACK option that it receives the sender can decide which packets to retransmit. In this case retransmitting packets 2 and 5 would be good idea. SACK is strictly advisory information. The actual detection of loss using dup licate acknowledgements and adjustments to the congestion window proceed just as before. However with SACK TCP can recover more easily from situations in which multiple packets are lost at roughly the same time since the TCP sender knows which packets have not been received. SACK is now widely deployed. It is described in RFC 2883 and TCP congestion control using SACK is described The second change is the use of ECN in addition to packet loss as congestion signal. ECN is an IP layer mechanism to Retransmit 2 and 5 notify hosts of congestion that we described in Sec. 5.3.4. With it the TCP re ceiver can receive congestion signals from IP. The use of ECN is enabled for TCP connection when both the sender and re ceiver indicate that they are capable of using ECN by setting the ECE and CWR bits during connection establishment. If ECN is used each packet that carries Routers that support ECN will set congestion signal on packets that can carry ECN flags when congestion is approaching instead of dropping those packets after congestion has occurred. The TCP receiver is informed if any packet that arrives carries an ECN con gestion signal. The receiver then uses the ECE flag to signal the TCP sender that its packets have experienced congestion. The sender tells the receiver that it has heard the signal by using the CWR flag. The TCP sender reacts to these congestion notifications in exactly the same way as it does to packet loss that is detected via duplicate acknowledgements. However the situation is strictly better. Congestion has been detected and no packet was harmed in any way. ECN is described in RFC 3168. It requires both host and router support and is not yet widely used on the Internet. For more information on the complete set of congestion control behaviors that are implemented in TCP see RFC 5681. As the workhorse of the Internet TCP has been used for many applications and extended over time to give good performance over wide range of networks. sic algorithms we have described especially for congestion control and robustness against attacks. It is likely that TCP will continue to evolve with the Internet. We The first one is that TCP does not provide the transport semantics that all ap plications want. For example some applications want to send messages or records whose boundaries need to be preserved. Other applications work with group of related conversations such as Web browser that transfers several objects from the same server. Still other applications want better control over the network paths that they use. TCP with its standard sockets interface does not meet these needs well. Essentially the application has the burden of dealing with any problem not solved by TCP. This has led to proposals for new protocols that would provide slightly different interface. Two examples are SCTP defined in RFC 4960 and SST . However whenever someone proposes changing something that has worked so well for so long there is always huge battle between the Users are demanding more features and If it aint broke dont fix it camps. solved problem after our deliberations and the mechanisms that have been devel oped over time. Not so. The form of TCP congestion control that we described and which is widely used is based on packet losses as signal of congestion. When Padhye et al. modeled TCP throughput based on the sawtooth pat tern they found that the packet loss rate must drop off rapidly with increasing byte packets one packet can be lost approximately every 10 minutes. That is packet loss rate of 2 108 which is incredibly small. It is too infrequent to serve as good congestion signal and any other source of loss can easily dominate it limiting the throughput. This relationship has not been problem in the past but networks are getting faster and faster leading many people to revisit congestion control. One possibil ity is to use an alternate congestion control in which the signal is not packet loss at all. We gave several examples in Sec. 6.2. The signal might be roundtrip time which grows when the network becomes congested as is used by FAST TCP . Other approaches are possible too and time will tell which is or thousands of computers are interconnected complex interactions with unfore seen consequences are common. Frequently this complexity leads to poor per and what can be done about them. Unfortunately understanding network performance is more an art than sci ence. There is little underlying theory that is actually of any use in practice. The best we can do is give some rules of thumb gained from hard experience and pres ent examples taken from the real world. We have delayed this discussion until we studied the transport layer because the performance that applications receive depends on the combined performance of the transport network and link layers and to be able to use TCP as an example in various places. 6. Protocols for long fat networks. These aspects consider network performance both at the host and across the net work and as networks are increased in speed and size. source overloads. If more traffic suddenly arrives at router than the router can handle congestion will build up and performance will suffer. We studied conges Performance also degrades when there is structural resource imbalance. For example if gigabit communication line is attached to lowend PC the poor host will not be able to process the incoming packets fast enough and some will be lost. These packets will eventually be retransmitted adding delay wasting bandwidth and generally reducing performance. Overloads can also be synchronously triggered. As an example if segment contains bad parameter in many cases the receiver will thoughtfully send back an error notification. Now consider what could happen if bad segment is broadcast to 1000 machines each one might send back an error message. The resulting broadcast storm could cripple the network. UDP suffered from this problem until the ICMP protocol was changed to cause hosts to refrain from responding to errors in UDP segments sent to broad cast addresses. Wireless networks must be particularly careful to avoid unchecked broadcast responses because broadcast occurs naturally and the wireless band width is limited. second example of synchronous overload is what happens after an electrical power failure. When the power comes back on all the machines simultaneously start rebooting. typical reboot sequence might require first going to some server to learn ones true identity and then to some file server to get copy of the operating system. If hundreds of machines in data center all do this at once the server will probably collapse under the load. Even in the absence of synchronous overloads and the presence of sufficient resources poor performance can occur due to lack of system tuning. For ex ample if machine has plenty of CPU power and memory but not enough of the memory has been allocated for buffer space flow control will slow down segment reception and limit performance. This was problem for many TCP connections as the Internet became faster but the default size of the flow control window stayed fixed at 64 KB. set to guard against loss of the segment. If the timeout is set too short unneces sary retransmissions will occur clogging the wires. If the timeout is set too long unnecessary delays will occur after segment is lost. Other tunable parameters include how long to wait for data on which to piggyback before sending separate acknowledgement and how many retransmissions to make before giving up. Another performance problem that occurs with realtime applications like audio and video is jitter. Having enough bandwidth on average is not sufficient for good performance. Short transmission delays are also required. Consistently achieving short delays demands careful engineering of the load on the network qualityofservice support at the link and network layers or both. When network performs poorly its users often complain to the folks running it demanding improvements. To improve the performance the operators must first determine exactly what is going on. To find out what is really happening the formance measurements. Much of the discussion below is based on the seminal the protocol stack and physically. The most basic kind of measurement is to start timer when beginning some activity and see how long that activity takes. For example knowing how long it takes for segment to be acknowledged is key measurement. Other measurements are made with counters that record how often terested in knowing the amount of something such as the number of bytes proc essed in certain time interval. Measuring network performance and parameters has many potential pitfalls. We list few of them here. Any systematic attempt to measure network per Do not measure the time to send one segment but repeat the measurement say one million times and take the average. Startup effects such as the 802.16 NIC or cable modem getting bandwidth reservation after an idle period can slow the first segment and queueing introduces variability. Having large sam ple will reduce the uncertainty in the measured mean and standard deviation. This uncertainty can be computed using standard statistical formulas. Ideally the whole sequence of one million measurements should be repeated at different times of the day and the week to see the effect of different network conditions on the measured quantity. Measurements of congestion for example are of little use if they are made at moment when there is no congestion. Some . and 1 . but no congestion at noon . propagation. Even measurement node placed close to wireless client may not observe the same packets as the client due to differences in the antennas. It is best to take measurements from the wireless client under study to see what it sees. Failing that it is possible to use techniques to combine the wireless measurements taken at different vantage points to gain more complete picture of what is going Repeating measurement many times will return an unexpectedly fast answer looking up DNS name may involve network exchange the first time and then return the answer from local cache without sending any packets over the network. The results from such measurement are essentially worthless . Buffering can have similar effect. TCPIP performance tests have been network allows. How does this occur call to UDP normally returns control as soon as the message has been accepted by the kernel and added to the transmis sion queue. If there is sufficient buffer space timing 1000 UDP calls does not mean that all the data have been sent. Most of them may still be in the kernel but the performance test program thinks they have all been transmitted. Caution is advised to be absolutely sure that you understand how data can be cached and buffered as part of network operation. Be Sure That Nothing Unexpected Is Going On during Your Tests Making measurements at the same time that some user has decided to run video conference over your network will often give different results than if there is no video conference. It is best to run tests on an idle network and create the entire workload yourself. Even this approach has pitfalls though. While you might think nobody will be using the network at 3 . that might be when the automatic backup program begins copying all the disks to tape. Or there might Wireless networks are challenging in this respect because it is often not pos sible to separate them from all sources of interference. Even if there are no other wireless networks sending traffic nearby someone may microwave popcorn and inadvertently cause interference that degrades 802.11 performance. For these rea sons it is good practice to monitor the overall network activity so that you can at least realize when something unexpected does happen. Computer clocks function by incrementing some counter at regular intervals. For example millisecond timer adds 1 to counter every 1 msec. Using such timer to measure an event that takes less than 1 msec is possible but requires some care. Some computers have more accurate clocks of course but there are always shorter events to measure too. Note that clocks are not always as accurate as the precision with which the time is returned when they are read. To measure the time to make TCP connection for example the clock should be read out when the transport layer code is entered and again when it is exited. If the true connection setup time is 300 μsec the dif ference between the two readings will be either 0 or 1 both wrong. However if the measurement is repeated one million times and the total of all measurements is added up and divided by one million the mean time will be accurate to better Suppose that you make measurements with simulated network loads running from 0 to 0.4 . For example the response time to send voiceoverIP packet over an 802.11 network might be as shown by the data linearly as shown by the dotted line. However many queueing results involve factor of 1 where ρ is the load so the true values may look more like the dashed line which rises much faster than linearly when the load gets high. That is beware contention effects that become much more pronounced at high load. Measuring and tinkering can improve performance considerably but they can not substitute for good design in the first place. poorly designed network can be improved only so much. Beyond that it has to be redesigned from scratch. mentation of network protocols on hosts. Surprisingly experience shows that this is often performance bottleneck on otherwise fast networks for two reasons. First NICs and routers have already been engineered to run at wire speed. This means that they can process packets as quickly as the packets can possibly arrive on the link. Second the relevant performance is that which applications obtain. It is not the link capacity but the throughput and delay after network and transport processing. Reducing software overheads improves performance by increasing throughput and decreasing delay. It can also reduce the energy that is spent on networking which is an important consideration for mobile computers. Most of these ideas have been common knowledge to network designers for years. They were first stated explicitly by Mogul our treatment largely follows his. Another Long experience has shown that in nearly all fast networks operating system and protocol overhead dominate actual time on the wire. For example in theory the minimum RPC time on 1Gbps Ethernet is 1 μsec corresponding to mini mum request followed by minimum reply. In practice overcoming the software overhead and getting the RPC time anywhere near there is substantial achievement. It rarely happens in practice. Similarly the biggest problem in running at 1 Gbps is often getting the bits from the users buffer out onto the network fast enough and having the receiving host process them as fast as they come in. If you double the host speed you often can come close to doubling the throughput. Doubling the network capacity has no effect if the bottleneck is in the hosts. Reduce Packet Count to Reduce Overhead data . Bandwidth is required for both components. Processing is sum. When 1 million bytes are being sent the data cost is the same no matter what the segment size is. However using 128byte segments means 32 times as much persegment overhead as using 4KB segments. The bandwidth and proc essing overheads add up fast to reduce throughput. Perpacket overhead in the lower layers amplifies this effect. Each arriving packet causes fresh interrupt if the host is keeping up. On modern pipelined processor each interrupt breaks the CPU pipeline interferes with the cache re quires change to the memory management context voids the branch prediction table and forces substantial number of CPU registers to be saved. An nfold re duction in segments sent thus reduces the interrupt and packet overhead by fac You might say that both people and computers are poor at multitasking. This observation underlies the desire to send MTU packets that are as large as will pass along the network path without fragmentation. Mechanisms such as Nagles algo rithm and Clarks solution are also attempts to avoid sending small packets. The most straightforward way to implement layered protocol stack is with one module for each layer. Unfortunately this leads to copying as each layer does its own work. For ex ample after packet is received by the NIC it is typically copied to kernel buff er. From there it is copied to network layer buffer for network layer processing ceiving application process. It is not unusual for an incoming packet to be copied three or four times before the segment enclosed in it is delivered. All this copying can greatly degrade performance because memory operations are an order of magnitude slower than registerregister instructions. For example likely when touching incoming packets the average instruction execution time is slowed down by factor of 2.8 . Hardware assistance will not help here. The problem is too much copying by the operating system. clever operating system will minimize copying by combining the proc essing of multiple layers. For example TCP and IP are usually implemented to gether so that it is not necessary to copy the payload of the packet as processing switches from network to transport layer. Another common trick is to perform multiple operations within layer in single pass over the data. For example checksums are often computed while copying the data and the newly computed checksum is appended to the end. related rule is that context switches are deadly. They have the bad properties of interrupts and copying combined. This cost is why transport protocols are often implemented in the kernel. Like reducing packet count context switches can be reduced by having the library pro cedure that sends data do internal buffering until it has substantial amount of them. Similarly on the receiving side small incoming segments should be col lected together and passed to the user in one fell swoop instead of individually to minimize context switches. In the best case an incoming packet causes context switch from the current user to the kernel and then switch to the receiving process to give it the newly Unfortunately with some operating systems additional context switches happen. For example if the network manager runs as special process in user space packet arrival is likely to cause context switch from the current user to the kernel then another one from the kernel to the network manager fol on each packet are wasteful of CPU time and can have devastating effect on net Avoiding Congestion Is Better Than Recovering from It The old maxim that an ounce of prevention is worth pound of cure certainly holds for network congestion. When network is congested packets are lost bandwidth is wasted useless delays are introduced and more. All of these costs are unnecessary and recovering from congestion takes time and patience. Not having it occur in the first place is better. Congestion avoidance is like getting your DTP vaccination it hurts little at the time you get it but it prevents some thing that would hurt lot more in the future. Timers are necessary in networks but they should be used sparingly and time outs should be minimized. When timer goes off some action is generally re peated. If it is truly necessary to repeat the action so be it but repeating it unnecessarily is wasteful. The way to avoid extra work is to be careful that timers are set little bit on the conservative side. timer that takes too long to expire adds small amount of extra delay to one connection in the event of segment being lost. timer that goes off when it should not have uses up host resources wastes band width and puts extra load on perhaps dozens of routers for no good reason. Now that we have covered general rules we will look at some specific meth ods for speeding up segment processing. For more information see Clark et al. and Chase et al. Segment processing overhead has two components overhead per segment and overhead per byte. Both must be attacked. The key to fast segment processing is to separate out the normal successful case and handle it specially. Many protocols tend to emphasize what to do when something goes wrong but to make the protocols run fast the designer should aim to minimize processing time when everything goes right. Minimizing processing time when an error occurs is secondary. Although sequence of special segments is needed to get into the ESTAB LISHED state once there segment processing is straightforward until one side starts to close the connection. Let us begin by examining the sending side in the ESTABLISHED state when there are data to be transmitted. For the sake of clar ity we assume here that the transport entity is in the kernel although the same ideas apply if it is userspace process or library inside the sending process. In the transport entity does is test to see if this is the normal case the state is ESTA BLISHED neither side is trying to close the connection regular full segment is being sent and enough window space is available at the receiver. If all conditions are met no further tests are needed and the fast path through the sending transport entity can be taken. Typically this path is taken The processing steps on this path are shaded. sport entity. At the start of the fast path it is copied as fast as possible to scratch buffer word by word. Those fields that change from segment to segment are overwritten in the buffer. Frequently these fields are easily derived from state plus pointer to the user data are then passed to the network layer. Here the layer gives the resulting packet to the data link layer for transmission. As an example of how this principle works in practice let us consider TCPIP. utive segments on oneway flow are shaded. All the sending transport entity has in the next sequence number compute the checksum and increment the sequence number in memory. It can then hand the buffer fills in the Identification field and computes its checksum. The packet is now ready for transmission. Step 1 is locating the connection record for the incoming segment. For TCP the the prototype without change. connection record can be stored in hash table for which some simple function of the two IP addresses and two ports is the key. Once the connection record has been located both addresses and both ports must be compared to verify that the correct record has been found. An optimization that often speeds up connection record lookup even more is to maintain pointer to the last one used and try that one first. Clark et al. tried this and observed hit rate exceeding 90. The segment is checked to see if it is normal one the state is ESTAB LISHED neither side is trying to close the connection the segment is full one no special flags are set and the sequence number is the one expected. These tests take just handful of instructions. If all conditions are met special fast path TCP procedure is called. While it is copying it also computes the checksum eliminating an extra pass over knowledgement is sent back. The general scheme of first making quick check to it is possible to get TCP to run at 90 of the speed of local memorytomemory copy assuming the network itself is fast enough. Two other areas where major performance gains are possible are buffer man unnecessary copying as mentioned above. Timer management is important be cause nearly all timers set do not expire. They are set to guard against segment loss but most segments and their acknowledgements arrive correctly. Hence it is important to optimize timer management for the case of timers rarely expiring. common scheme is to use linked list of timer events sorted by expiration time. The head entry contains counter telling how many ticks away from expiry it is. Each successive entry contains counter telling how many ticks after the previous entry it is. Thus if timers expire in 3 10 and 12 ticks respectively the three counters are 3 7 and 2 respectively. At every clock tick the counter in the head entry is decremented. When it hits zero its event is processed and the next item on the list becomes the head. Its counter does not have to be changed. This way inserting and deleting timers are expensive operations with execution times proportional to the length of the list. much more efficient approach can be used if the maximum timer interval is bounded and known in advance. Here an array called timing wheel can be time shown is 4. Timers are scheduled to expire at 3 10 and 12 ticks from now. If new timer suddenly is set to expire in seven ticks an entry is just made in slot 11. Similarly if the timer set for 10 has to be canceled the list starting in slot 14 has to be searched and the required entry removed. Note that the array When the clock ticks the current time pointer is advanced by one slot . If the entry now pointed to is nonzero all of its timers are processed. Many variations on the basic idea are discussed by Varghese and Lauck . We have been looking at fast networks for too long. There is more out there. Let us now consider performance on wireless and other networks in which band width is limited. Reducing software overhead can help mobile computers run more efficiently but it does nothing to improve performance when the network links are the bottleneck. the minimum of bits. For payloads this means using compact encodings of infor mation such as images that are in JPEG format rather than bitmap or document formats such as PDF that include compression. It also means applicationlevel caching mechanisms such as Web caches that reduce transfers in the first place. works are typically compact because they were designed with scarce bandwidth in longer addresses. However higher layer protocols such as IP TCP and UDP example voiceoverIP data that is being carried with the combination of IP UDP and 12 for RTP. With IPv6 the situation is even worse 60 bytes including mitted data and consume more than half the bandwidth. compress well individually and decompression requires all prior data to be re ceived. This will not be the case if packet is lost. format. One of the first schemes was designed by Van Jacobson for com to packet. There is no need for example to send the same IP TTL or the same TCP port numbers in each and every packet. They can be omitted on the sending side of the link and filled in on the receiving side. Similarly other fields change in predictable manner. For example barring loss the TCP sequence number advances with the data. In these cases the re ceiver can predict the likely value. The actual number only needs to be carried when it differs from what is expected. Even then it may be carried as small change from the previous value as when the acknowledgement number increases when new data is received in the reverse direction. protocols and compact encodings over low bandwidth links. ROHC and is described in RFC 1323. second problem is that the size of the flow control window must be greatly increased. Consider for example sending 64KB burst of data from San Diego Gbps and the oneway speedoflightinfiber delay is 20 msec. Initially at 0 54 all the segments are out on the fiber. The lead segment will now be some where in the vicinity of Brawley still deep in Southern California. However the Acknowledgements 0. After 500 μsec. After 20 msec. After 40 msec. back to the sender and the second burst can be transmitted. Since the transmission line was used for 1.25 msec out of 100 the efficiency is about 1.25. This situa tion is typical of an older protocols running over gigabit lines. useful quantity to keep in mind when analyzing network performance is the It is obtained by multiplying the bandwidth by the roundtrip delay time . The product is the capacity of the pipe from the sender to the receiver and back . In other words the sender would have to transmit burst of 40 million bits to be able to keep going full speed until the first acknowledgement came back. It takes this many bits to fill the pipe . This is why burst of half million bits only achieves 1.25 efficiency it is only 1.25 of the pipes capac The conclusion that can be drawn here is that for good performance the re ceivers window must be at least as large as the bandwidthdelay product and preferably somewhat larger since the receiver may not respond instantly. For transcontinental gigabit line at least 5 MB are required. third and related problem is that simple retransmission schemes such as the gobackn protocol perform poorly on lines with large bandwidthdelay product. Consider the 1Gbps transcontinental link with roundtrip transmission time of 40 msec. sender can transmit 5 MB in one round trip. If an error is detected it will be 40 msec before the sender is told about it. If gobackn is used the sender will have to retransmit not just the bad packet but also the 5 MB worth of packets that came afterward. Clearly this is massive waste of resources. More complex protocols such as selectiverepeat are needed. fourth problem is that gigabit lines are fundamentally different from mega bit lines in that long gigabit lines are delay limited rather than bandwidth limited. transmission speeds. At speeds up to 1 Mbps the transmission time is dominated by the rate at which the bits can be sent. By 1 Gbps the 40msec roundtrip delay dominates the 1 msec it takes to put the bits on the fiber. Further increases in bandwidth have hardly any effect at all. stopandwait protocols such as RPC have an inherent upper bound on their per formance. This limit is dictated by the speed of light. No amount of technologi cal progress in optics will ever improve matters . Unless some other use can be found for gigabit line while host is waiting for reply the gigabit line is no better than megabit line just more ex fifth problem is that communication speeds have improved faster than com puting speeds. CPU is idle waiting for the second CPU to do the critical work. It is myth to think that the main CPU has other work to do while waiting. Fur thermore when two generalpurpose CPUs communicate race conditions can oc cur so elaborate protocols are needed between the two processors to synchronize them correctly and avoid races. Usually the best approach is to make the proto cols simple and have the main CPU do the work. should contain as few fields as possible to reduce processing time and these fields should be big enough to do the job and be wordaligned for fast processing. wrapping around while old packets still exist receivers being unable to advertise enough window space because the window field is too small etc. do not occur. The maximum data size should be large to reduce software overhead and per mit efficient operation. 1500 bytes is too small for highspeed networks which is why gigabit Ethernet supports jumbo frames of up to 9 KB and IPv6 supports jumbogram packets in excess of 64 KB. long delay loop feedback should be avoided it takes too long for the receiver to signal the sender. One example of feedback is governing the transmis sion rate by using sliding window protocol. Future protocols may switch to ratebased protocols to avoid the delays inherent in the receiver sending to provided it does not send faster than some rate the sender and receiver have agreed upon in advance. second example of feedback is Jacobsons slow start algorithm. This algo rithm makes multiple probes to see how much the network can handle. With highspeed networks making half dozen or so small probes to see how the net work responds wastes huge amount of bandwidth. more efficient scheme is to have the sender receiver and network all reserve the necessary resources at con nection setup time. Reserving resources in advance also has the advantage of ma king it easier to reduce jitter. In short going to high speeds inexorably pushes the design toward connectionoriented operation or something fairly close to it. Another valuable feature is the ability to send normal amount of data along with the connection request. In this way one roundtrip time can be saved. day be an important component of the Internet. TCP and most other transport pro tocols are based on the assumption that the sender and the receiver are continu ously connected by some working path or else the protocol fails and data cannot be delivered. In some networks there is often no endtoend path. An example is space network as LEO satellites pass in and out of range of ground stations. given satellite may be able to communicate to ground station only at particular times and two satellites may never be able to communicate with each other at any time even via ground station because one of the satellites may always be out of range. Other example networks involve submarines buses mobile phones and other devices with computers for which there is intermittent connectivity due to mobility or extreme conditions. In these occasionally connected networks data can still be communicated by storing them at nodes and forwarding them later when there is working link. This technique is called message switching. Eventually the data will be relayed to the destination. network whose architecture is based on this approach is call ed DTN . Work on DTNs started in 2002 when IETF set up research group on the topic. The inspiration for DTNs came from an unlikely source efforts to send packets in space. Space networks must deal with intermittent communication and very long delays. Kevin Fall observed that the ideas for these Interplanetary In ternets could be applied to networks on Earth in which intermittent connectivity was the norm . This model gives useful generalization of the Inter net in which storage and delays can occur during communication. Data delivery is akin to delivery in the postal system or electronic mail rather than packet switching at routers. Since 2002 the DTN architecture has been refined and the applications of the DTN model have grown. As mainstream application consider large datasets of many terabytes that are produced by scientific experiments media events or around the world. Operators would like to send this bulk traffic at offpeak times to make use of bandwidth that has already been paid for but is not being used and are willing to tolerate some delay. It is like doing the backups at night when other applications are not making heavy use of the network. The problem is that for There may be little overlap in the times when datacenters in Boston and Perth have offpeak network bandwidth because night for one city is day for the other. However DTN models allow for storage and delays during transfer. With this model it becomes possible to send the dataset from Boston to Amsterdam using offpeak bandwidth as the cities have time zones that are only 6 hours apart. The dataset is then stored in Amsterdam until there is offpeak bandwidth between Amsterdam and Perth. It is then sent to Perth to complete the transfer. Laoutaris et al. have studied this model and find that it can provide sub stantial capacity at little cost and that the use of DTN model often doubles that capacity compared with traditional endtoend model. In what follows we will describe the IETF DTN architecture and protocols. The main assumption in the Internet that DTNs seek to relax is that an end toend path between source and destination exists for the entire duration of communication session. When this is not the case the normal Internet protocols fail. DTNs get around the lack of endtoend connectivity with an architecture tolerate links with low reliability and large delays. The architecture is specified in In DTN terminology message is called bundle. DTN nodes are equipped with storage typically persistent storage such as disk or flash memory. They store bundles until links become available and then forward the bundles. The links working and two links that are working. working link is called contact. the bundles onward. In this way the bundles are relayed via contacts from the source to their destination. The storing and forwarding of bundles at DTN nodes sounds similar to the queueing and forwarding of packets at routers but there are qualitative dif ferences. In routers in the Internet queueing occurs for milliseconds or at most seconds. At DTN nodes bundles may be stored for hours until bus arrives in town while an airplane completes flight until sensor node harvests enough solar energy to run until sleeping computer wakes up and so forth. These ex amples also point to second difference which is that nodes may move while they hold stored data and this movement may even be key part of data delivery. Routers in the Internet are not allowed to move. The whole process of moving bundles might be better known as storecarryforward. of DTN protocols in space . The source of bundles is an LEO satellite that is recording Earth images as part of the Disaster Monitoring Constel lation of satellites. The images must be returned to the collection point. However the satellite has only intermittent contact with three ground stations as it orbits the Earth. It comes into contact with each ground station in turn. Each of the satellite ground stations and collection point act as DTN node. At each contact bundle is sent to ground station. The bundles are then sent over backhaul terrestrial network to the collection point to complete the The primary advantage of the DTN architecture in this example is that it nat urally fits the situation of the satellite needing to store images because there is no connectivity at the time the image is taken. There are two further advantages. First there may be no single contact long enough to send the images. However they can be spread across the contacts with three ground stations. Second the use of the link between the satellite and ground station is decoupled from the link over the backhaul network. This means that the satellite download is not limited by slow terrestrial link. It can proceed at full speed with the bundle stored at the ground station until it can be relayed to the collection point. routes via DTN nodes. route in this path to use. Good routes depend on the nature of the architecture describes when to send data and also which contacts. Some contacts are known ahead of time. good example is the motion of heavenly bodies in the space example. For the space experiment it was known to 14 minutes per pass with each ground station and that the downlink capacity was 8.134 Mbps. Given this knowledge the transport of bundle of images can be planned ahead of time. In other cases the contacts can be predicted but with less certainty. Examples include buses that make contact with each other in mostly regular ways due to timetable yet with some variation and the times and amount of offpeak band width in ISP networks which are predicted from past data. At the other extreme the contacts are occasional and random. One example is carrying data from user to user on mobile phones depending on which users make contact with each other during the day. When there is unpredictability in contacts one routing strategy is to send copies of the bundle along different paths in the hope that one of the cop ies is delivered to the destination before the lifetime is reached. To take closer look at the operation of DTNs we will now look at the IETF protocols. DTNs are an emerging kind of network and experimental DTNs have used different protocols as there is no requirement that the IETF protocols be used. However they are at least good place to start and highlight many of the dle protocol which is specified in RFC 5050. It is responsible for accepting mes sages from the application and sending them as one or more bundles via store carryforward operations to the destination DTN node. It is also apparent from TCPIP may be used over each contact to move bundles between DTN nodes. layer protocol or an application layer protocol. Just as with RTP we take the position that despite running over transport protocol the Bundle protocol is pro viding transport service to many different applications and so we cover DTNs Convergence layer protocols such as UDP or even other kinds of internets. For example in space network the links may have very long delays. The roundtrip time between Earth and Mars can easily be 20 minutes depending on the relative position of the planets. Imagine how well TCP acknowledgements and retransmissions will work over that link especially for relatively short messages. Not well at all. Instead another protocol that uses errorcorrecting codes might be used. Or in sensor net works that are very resource constrained more lightweight protocol than TCP Since the Bundle protocol is fixed yet it is intended to run over variety of transports there is must be gap in functionality between the protocols. That gap gence layer is just glue layer that matches the interfaces of the protocols that it joins. By definition there is different convergence layer for each different lower layer transport. Convergence layers are commonly found in standards to join new Bundle protocol. Each message consists of primary block which can be thought of as head er payload block for the data and optionally other blocks for example to carry followed by Flags field. Among other functions the flags encode class of ser vice to let source mark its bundles as higher or lower priority and other han dling requests such as whether the destination should acknowledge the bundle. Then come addresses which highlight three interesting parts of the design. As well as Destination and Source identifier field there is Custodian identifier. The custodian is the party responsible for seeing that the bundle is delivered. In the Internet the source node is usually the custodian as it is the node that retrans mits if the data is not ultimately delivered to the destination. However in DTN the source node may not always be connected and may have no way of knowing whether the data has been delivered. DTNs deal with this problem using the notion of custody transfer in which another node closer to the destination can assume responsibility for seeing the data safely delivered. For example if bun may become the custodian of the bundle. The second interesting aspect is that these identifiers are not IP addresses. Be cause the Bundle protocol is intended to work across variety of transports and internets it defines its own identifiers. These identifiers are really more like addresses. They give DTNs an aspect of applicationlevel routing such as email The third interesting aspect is the way the identifiers are encoded. There is In fact much of the message format has been designed with both extensibility and efficiency in mind by using compact representation of variable length fields. The compact representation is important for wireless links and resource constrained nodes such as in sensor network. Next comes Creation field carrying the time at which the bundle was creat ed along with sequence number from the source for ordering plus Lifetime field that tells the time at which the bundle data is no longer useful. These fields exist because data may be stored for long period at DTN nodes and there must be some way to remove stale data from the network. Unlike the Internet they re quire that DTN nodes have loosely synchronized clocks. The primary block is completed with the Dictionary field. Then comes the payload block. This block starts with short Type field that identifies it as pay load followed by small set of Flags that describe processing options. Then tional blocks such as block that carries security parameters. Many aspects of DTNs are being explored in the research community. Good strategies for routing depend on the nature of the contacts as was mentioned must consider storage at nodes as another kind of resource that can be depleted. DTN node takes custody of bundle it may want to know that the sender is authorized to use the network and that the bundle is probably wanted by the desti works are different from sensor networks. The transport layer is the key to understanding layered protocols. It provides various services the most important of which is an endtoend reliable con nectionoriented byte stream from sender to receiver. It is accessed through ser vice primitives that permit the establishment use and release of connections. common transport layer interface is the one provided by Berkeley sockets. Transport protocols must be able to do connection management over unre liable networks. Connection establishment is complicated by the existence of de layed duplicate packets that can reappear at inopportune moments. To deal with them threeway handshakes are needed to establish connections. Releasing connection is easier than establishing one but is still far from trivial due to the twoarmy problem. Even when the network layer is completely reliable the transport layer has plenty of work to do. It must handle all the service primitives manage connec tions and timers allocate bandwidth with congestion control and run variable sized sliding window for flow control. Congestion control should allocate all of the available bandwidth between competing flows fairly and it should track changes in the usage of the network. The Internet has two main transport protocols UDP and TCP. UDP is con nectionless protocol that is mainly wrapper for IP packets with the additional feature of multiplexing and demultiplexing multiple processes using single IP address. UDP can be used for clientserver interactions for example using RPC. It can also be used for building realtime protocols such as RTP. The main Internet transport protocol is TCP. It provides reliable bidirec great deal of work has gone into optimizing TCP performance using algorithms from Nagle Clark Jacobson Karn and others. Network performance is typically dominated by protocol and segment proc essing overhead and this situation gets worse at higher speeds. Protocols should be designed to minimize the number of segments and work for large bandwidth delay paths. For gigabit networks simple protocols and streamlined processing Delaytolerant networking provides delivery service across networks that have occasional connectivity or long delays across links. Intermediate nodes store carry and forward bundles of information so that it is eventually delivered even if there is no working path from sender to receiver at any time. strictly necessary If not explain how nonblocking primitive could be used. What advantage would this have over the scheme described in the text 2. Primitives of transport service assume asymmetry between the two end points during connection establishment one end executes LISTEN while the other end executes CONNECT. However in peer to peer applications such file sharing systems . BitTorrent all end points are peers. There is no server or client func tionality. How can transport service primitives may be used to build such peer to peer work layer and thus must be individually acknowledged. Suppose that the network layer is 100 percent reliable and never loses packets. What changes if any are the same in both client and server. Why is this so important client fail for any reason other than listen queue being full on the server Assume that the network is perfect. 6. One criteria for deciding whether to have server active all the time or have it start on demand using process server is how frequently the service provided is used. Can you think of any other criteria for making this decision 7. Suppose that the clockdriven scheme for generating initial sequence numbers is used with 15bit wide clock counter. The clock ticks once every 100 msec and the max imum packet lifetime is 60 sec. How often need resynchronization take place in the worst case when the data consumes 240 sequence numbersmin 8. Why does the maximum packet lifetime have to be large enough to ensure that not only the packet but also its acknowledgements have vanished 9. Imagine that twoway handshake rather than threeway handshake were used to set up connections. In other words the third message was not required. Are deadlocks now possible Give an example or show that none exist. 10. Imagine generalized narmy problem in which the agreement of any two of the blue armies is sufficient for victory. Does protocol exist that allows blue to win between writing and sending an acknowledgement or vice versa can be made rela tively small what are the two best senderreceiver strategies for minimizing the chance of protocol failure 13. Discuss the advantages and disadvantages of credits versus sliding window protocols. 14. Some other policies for fairness in congestion control are Additive Increase Additive Decrease Multiplicative Increase Additive Decrease and Multipli cative Increase Multiplicative Decrease . Discuss these three policies in terms of convergence and stability. 15. Why does UDP exist Would it not have been enough to just let user processes send 16. Consider simple applicationlevel protocol built on top of UDP that allows client to retrieve file from remote server residing at wellknown address. The client first sends request with file name and the server responds with sequence of data packets containing different parts of the requested file. To ensure reliability and sequenced delivery client and server use stopandwait protocol. Ignoring the obvi about the possibility of processes crashing. 17. client sends 128byte request to server located 100 km away over 1gigabit optical fiber. What is the efficiency of the line during the remote procedure call 18. Consider the situation of the previous problem again. Compute the minimum possible response time both for the given 1Gbps line and for 1Mbps line. What conclusion 19. Both UDP and TCP use port numbers to identify the destination entity when deliver ing message. Give two reasons why these protocols invented new abstract ID instead of using process IDs which already existed when these protocols 20. Several RPC implementations provide an option to the client to use RPC implemented over UDP or RPC implemented over TCP. Under what conditions will client prefer to use RPC over UDP and under what conditions will he prefer to use RPC over TCP 21. Consider two networks 1 and 2 that have the same average delay between source and destination . In 1 the delay experienced by different packets is unformly distributed with maximum delay being 10 seconds while in 2 99 of the packets experience less than one second delay with no limit on maximum delay. Dis cuss how RTP may be used in these two cases to transmit live audiovideo stream. 22. What is the total size of the minimum TCP MTU including TCP and IP overhead but not including data link layer overhead 23. Datagram fragmentation and reassembly are handled by IP and are invisible to TCP. Does this mean that TCP does not have to worry about data arriving in the wrong 24. RTP is used to transmit CDquality audio which makes pair of 16bit samples 44100 timessec one sample for each of the stereo channels. How many packets per second must RTP transmit 25. Would it be possible to place the RTP code in the operating system kernel along with the UDP code Explain your answer. 26. process on host 1 has been assigned port and process on host 2 has been assigned port . Is it possible for there to be two or more TCP connections between ACK bit in the fourth word. Does this really add anything Why or why not 28. The maximum payload of TCP segment is 65495 bytes. Why was such strange 30. Consider the effect of using slow start on line with 10msec roundtrip time and no congestion. The receive window is 24 KB and the maximum segment size is 2 KB. How long does it take before the first full window can be sent 31. Suppose that the TCP congestion window is set to 18 KB and timeout occurs. How big will the window be if the next four transmission bursts are all successful Assume that the maximum segment size is 1 KB. 32. If the TCP roundtrip time RTT is currently 30 msec and the following acknowledge ments come in after 26 32 and 24 msec respectively what is the new RTT estimate using the Jacobson algorithm Use α 0.9. 33. TCP machine is sending full windows of 65535 bytes over 1Gbps channel that has 10msec oneway delay. What is the maximum throughput achievable What is the line efficiency 34. What is the fastest line speed at which host can blast out 1500byte TCP payloads with 120sec maximum packet lifetime without having the sequence numbers wrap around Take TCP IP and Ethernet overhead into consideration. Assume that Ether net frames may be sent continuously. address the limitations of TCP. Explain why this is the case. 36. In network whose max segment is 128 bytes max segment lifetime is 30 sec and has 8bit sequence numbers what is the maximum data rate per connection 37. Suppose that you are measuring the time to receive segment. When an interrupt occurs you read out the system clock in milliseconds. When the segment is fully pro cessed you read out the clock again. You measure 0 msec 270000 times and 1 msec 730000 times. How long does it take to receive segment 38. CPU executes instructions at the rate of 1000 MIPS. Data can be copied 64 bits at time with each word copied costing 10 instructions. If an coming packet has to be copied four times can this system handle 1Gbps line For simplicity assume that all instructions even those instructions that read or write memory run at the full 39. To get around the problem of sequence numbers wrapping around while old packets still exist one could use 64bit sequence numbers. However theoretically an optical fiber can run at 75 Tbps. What maximum packet lifetime is required to make sure that numbers Assume that each byte has its own sequence number as TCP does. 40. In Sec. 6.6.5 we calculated that gigabit line dumps 80000 packetssec on the host giving it only 6250 instructions to process it and leaving half the CPU time for appli cations. This calculation assumed 1500byte packet. Redo the calculation for an ARPANETsized packet . In both cases assume that the packet sizes given include all overhead. 41. For 1Gbps network operating over 4000 km the delay is the limiting factor not the bandwidth. Consider MAN with the average source and destination 20 km apart. At what data rate does the roundtrip delay due to the speed of light equal the transmis sion delay for 1KB packet 42. Calculate the bandwidthdelay product for the following networks T1 What are its implications in light of your calculations 43. What is the bandwidthdelay product for 50Mbps channel on geostationary satel lite If the packets are all 1500 bytes how big should the win Make the following modifications. Give the client third argument that specifies byte range. Add client flag that allows the file to be written to the server. 45. One common function that all network protocols need is to manipulate messages. may break single message into multiple fragments and later join these multiple frag ments back into single message. To this end design and implement message management library that provides support for creating new message attaching messages combining two messages into single message and saving copy of mes sage. Your implementation must minimize data copying from one buffer to another as much as possible. It is critical that the operations that manipulate messages do not touch the data in message but rather only manipulate pointers. 46. Design and implement chat system that allows multiple groups of users to chat. chat coordinator resides at wellknown network address uses UDP for communica tion with chat clients sets up chat servers for each chat session and maintains chat session directory. There is one chat server per chat session. chat server uses TCP for communication with clients. chat client allows users to start join and leave chat session. Design and implement the coordinator server and client code. Having finished all the preliminaries we now come to the layer where all the applications are found. The layers below the application layer are there to provide study some real network applications. However even in the application layer there is need for support protocols to allow the applications to function. Accordingly we will look at an important one of these before starting with the applications themselves. The item in question is DNS which handles naming within the Internet. After that we will examine three real applications electronic mail the World Wide Web and multimedia. peertopeer networks. other resources by using the network addresses of the computers on which they are stored these addresses are hard for people to remember. Also moves the Web server to different machine with different IP address everyone needs to be told the new IP address. Consequently highlevel readable names were introduced in order to decouple machine names from machine addresses. In this way the companys Web server might be known as regardless of its IP address. Nevertheless since the network itself understands only numerical addresses some mechanism is required to convert the names to accomplished in the Internet. Way back in the ARPANET days there was simply file hosts.txt that listed all the computer names and their IP addresses. Every night all the hosts would fetch it from the site at which it was maintained. For network of few hundred large timesharing machines this approach worked reasonably well. However well before many millions of PCs were connected to the Internet everyone involved with it realized that this approach could not continue to work forever. For one thing the size of the file would become too large. However even more importantly host name conflicts would occur constantly unless names were centrally managed something unthinkable in huge international network tem was invented in 1983. It has been key part of the Internet ever since. The essence of DNS is the invention of hierarchical domainbased naming scheme and distributed database system for implementing this naming scheme. It is primarily used for mapping host names to IP addresses but can also be used for other purposes. DNS is defined in RFCs 1034 1035 2181 and further ela borated in many others. Very briefly the way DNS is used is as follows. To map name onto an IP address an application program calls library procedure called the resolver pas sing it the name as parameter. We saw an example of resolver gethost DNS server which looks up the name and returns response containing the IP ad dress to the resolver which then returns it to the caller. The query and response messages are sent as UDP packets. Armed with the IP address the program can then establish TCP connection with the host or send it UDP packets. Managing large and constantly changing set of names is nontrivial prob lem. In the postal system name management is done by requiring letters to speci fy the country state or province city street address and name of the addressee. Using this kind of hierarchical addressing ensures that there is no confusion between the Marvin Anderson on Main St. in White Plains . and the Marvin Anderson on Main St. in Austin Texas. DNS works the For the Internet the top of the naming hierarchy is managed by an organiza tion called ICANN . ICANN was created for this purpose in 1998 as part of the maturing of the Inter net to worldwide economic concern. Conceptually the Internet is divided into over 250 toplevel domains where each domain covers many hosts. Each do main is partitioned into subdomains and these are further partitioned and so on. of the tree represent domains that have no subdomains . leaf domain may contain single host or it may represent com pany and contain thousands of hosts. The toplevel domains come in two flavors generic and countries. The gen mains introduced via applications to ICANN. Other generic toplevel domains will be added in the future. The country domains include one entry for every country as defined in ISO 3166. Internationalized country domain names that use nonLatin alphabets were introduced in 2010. These domains let people name hosts in Arabic Cyrillic Chinese or other languages. Getting secondlevel domain such as nameofcompany.com is easy. The toplevel domains are run by registrars appointed by ICANN. Getting name merely requires going to corresponding registrar to check if the desired name is available and not somebody elses trademark. If there are However as the Internet has become more commercial and more internation al it has also become more contentious especially in matters related to naming. This controversy includes ICANN itself. For example the creation of the xxx do main took several years and court cases to resolve. Is voluntarily placing adult content in its own domain good or bad thing . Some of the domains selforganize while others have restrictions on who can obtain name Nonprofit organizations for example. It is for qualified professionals. But who is professional Doctors and lawyers clearly are professionals. But what about freelance photographers piano teachers magicians plumbers barbers exterminators tattoo artists mer cenaries and prostitutes Are these occupations eligible According to whom There is also money in names. Tuvalu sold lease on its tv do main for 50 million all because the country code is wellsuited to advertising television sites. Virtually every common word has been taken in the com domain along with the most common misspellings. Try household articles animals plants body parts etc. The practice of registering domain only to turn around and sell it off to an interested party at much higher price even has name. It is called cybersquatting. Many companies that were slow off the mark when the Internet era began found their obvious domain names already taken when they tried to acquire them. In general as long as no trademarks are being violated and no fraud is involved it is firstcome firstserved with names. Never theless policies to resolve naming disputes are still being refined. Each domain is named by the path upward from it to the root. The components are separated by periods . Thus the engineering department at Cisco might be eng.cisco.com. rather than UNIXstyle name such as comciscoeng. Notice that this hierarchical naming means that eng.cisco.com. does not conflict with potential use of eng in eng.washington.edu. which might be used by the English department at the University of Washington. Domain names can be either absolute or relative. An absolute domain name always ends with period whereas relative one does not. Relative names have to be interpreted in some context to uniquely determine their true meaning. In both cases named domain refers to specific node in the tree and all the nodes under it. Domain names are caseinsensitive so edu Edu and EDU mean the same thing. Component names can be up to 63 characters long and full path names must not exceed 255 characters. In principle domains can be inserted into the tree in either generic or country domains. For example cs.washington.edu could equally well be listed under the us country domain as cs.washington.wa.us. In practice however most organiza tions in the United States are under generic domains and most outside the United States are under the domain of their country. There is no rule against registering under multiple toplevel domains. Large companies often do so . Each domain controls how it allocates the domains under it. For example Japan has domains ac.jp and co.jp that mirror edu and com. The Netherlands does not make this distinction and puts all organizations directly under nl. Thus all three of the following are university computer science departments 1. cs.washington.edu . 2. cs.vu.nl . 3. cs.keio.ac.jp . To create new domain permission is required of the domain in which it will be included. For example if VLSI group is started at the University of Wash ington and wants to be known as vlsi.cs.washington.edu it has to get permission from whoever manages cs.washington.edu. Similarly if new university is char tered say the University of Northern South Dakota it must ask the manager of the edu domain to assign it unsd.edu . In this way name conflicts are avoided and each domain can keep track of all its subdomains. Once new domain has been created and registered it can create subdomains such as cs.unsd.edu without getting permission from anybody higher up the tree. Naming follows organizational boundaries not physical networks. For ex ample if the computer science and electrical engineering departments are located in the same building and share the same LAN they can nevertheless have distinct domains. Similarly even if computer science is split over Babbage Hall and Tur ing Hall the hosts in both buildings will normally belong to the same domain. Every domain whether it is single host or toplevel domain can have set of resource records associated with it. These records are the DNS database. For single host the most common resource record is just its IP address but many other kinds of resource records also exist. When resolver gives domain name to DNS what it gets back are the resource records associated with that name. Thus the primary function of DNS is to map domain names onto resource resource record is fivetuple. Although they are encoded in binary for ef ficiency in most expositions resource records are presented as ASCII text one line per resource record. The format we will use is as follows The Domain name tells the domain to which this record applies. Normally many records exist for each domain and each copy of the database holds information about multiple domains. This field is thus the primary search key used to satisfy queries. The order of the records in the database is not significant. The Time to live field gives an indication of how stable the record is. Infor mation that is highly stable is assigned large value such as 86400 . Information that is highly volatile is assigned small value such as 60 . We will come back to this point later when we have dis The third field of every resource record is the Class. For Internet information it is always IN. For nonInternet information other codes can be used but in practice these are rarely seen. The Type field tells what kind of record this is. There are many kinds of DNS An SOA record provides the name of the primary source of information about the name servers zone the email address of its administrator unique serial number and various flags and timeouts. The most important record type is the record. It holds 32bit IPv4 address of an interface for some host. The corresponding AAAA or quad record holds 128bit IPv6 address. Every Internet host must have at least one IP address so that other machines can communicate with it. Some hosts have two or more network interfaces in which case they will have two or more type or AAAA resource records. Consequently DNS can return multiple addresses for common record type is the MX record. It specifies the name of the host prepared to accept email for the specified domain. It is used because not every Descriptive ASCII text machine is prepared to accept email. If someone wants to send email to for ex ample the sending host needs to find some mail server loca ted at microsoft.com that is willing to accept email. The MX record can provide this information. Another important record type is the NS record. It specifies name server for the domain or subdomain. This is host that has copy of the database for do main. It is used as part of the process to look up names which we will describe CNAME records allow aliases to be created. For example person familiar with Internet naming in general and wanting to send message to user paul in the computer science department at . might guess that will work. Actually this address will not work because the domain for . com puter science department is csail.mit.edu. However as service to people who do not know this . could create CNAME entry to point people and programs in the right direction. An entry like this one might do the job Like CNAME PTR points to another name. However unlike CNAME which is really just macro definition PTR is regular DNS data type whose interpretation depends on the context in which it is found. In practice it is nearly always used to associate name with an IP address to allow lookups of the IP address and return the name of the corres ponding machine. These are called reverse lookups. SRV is newer type of record that allows host to be identified for given service in domain. For example the Web server for cs.washington.edu could be identified as cockatoo.cs.washington.edu. This record generalizes the MX record that performs the same task but it is just for mail servers. SPF is also newer type of record. It lets domain encode information about what machines in the domain will send mail to the rest of the Internet. This helps receiving machines check that mail is valid. If mail is being received from ma chine that calls itself dodgy but the domain records say that mail will only be sent out of the domain by machine called smtp chances are that the mail is forged Last on the list TXT records were originally provided to allow domains to identify themselves in arbitrary ways. Nowadays they usually encode machine readable information typically the SPF information. or an ASCII string. The semantics depend on the record type. short description For an example of the kind of information one might find in the DNS database Authoritative data for cs.vu.nl domain which will not concern us further. Then come two entries giving the first and second places to try to deliver email sent to . The zephyr should be tried first. If that fails the top should be tried as the next choice. The next line identifies the name server for the domain as star. After the blank line come lines giving the IP addresses for the star zephyr and top. These are followed by an alias so that this address can be used without designating specific machine. Creating this alias allows cs.vu.nl to change its World Wide Web server without invalidating the address people use to get to it. similar argument holds for ftp.cs.vu.nl. given for handling email sent to flits.cs.vu.nl. First choice is naturally the flits it self but if it is down the zephyr and top are the second and third choices. The next three lines contain typical entry for computer in this case rowboat.cs.vu.nl. The information provided contains the IP address and the pri mary and secondary mail drops. Then comes an entry for computer that is not capable of receiving mail itself followed by an entry that is likely for printer that is connected to the Internet. In theory at least single name server could contain the entire DNS database and respond to all queries about it. In practice this server would be so overloaded as to be useless. Furthermore if it ever went down the entire Internet would be mation the DNS name space is divided into nonoverlapping zones. One possible contains some part of the tree. Where the zone boundaries are placed within zone is up to that zones ad ministrator. This decision is made in large part based on how many name servers has zone for washington.edu that handles eng.washington.edu but does not han dle cs.washington.edu. That is separate zone with its own name servers. Such decision might be made when department such as English does not wish to run its own name server but department such as Computer Science does. Each zone is also associated with one or more name servers. These are hosts that hold the database for the zone. Normally zone will have one primary name server which gets its information from file on its disk and one or more sec ondary name servers which get their information from the primary name server. To improve reliability some of the name servers can be located outside the zone. The process of looking up name and finding an address is called name reso lution. When resolver has query about domain name it passes the query to local name server. If the domain being sought falls under the jurisdiction of the name server such as top.cs.vu.nl falling under cs.vu.nl it returns the authoritative that manages the record and is thus always correct. Authoritative records are in What happens when the domain is remote such as when flits.cs.vu.nl wants to find the IP address of robot.cs.washington.edu at UW In this case and if there is no cached information about the domain avail able locally the name server begins remote query. This query follows the proc ver. The query contains the domain name sought the type and the class. The next step is to start at the top of the name hierarchy by asking one of the root name servers. These name servers have information about each toplevel server must have information about one or more root name servers. This infor mation is normally present in system configuration file that is loaded into the DNS cache when the DNS server is started. It is simply list of NS records for the root and the corresponding records. There are 13 root DNS servers unimaginatively called arootservers.net through .rootservers.net. Each root server could logically be single computer. However since the entire Internet depends on the root servers they are powerful and heavily replicated computers. Most of the servers are present in multiple geo ered to the nearest instance of destination address we described anycast in Chap. 5 The replication improves reliability and performance. The root name server is unlikely to know the address of machine at UW and probably does not know the name server for UW either. But it must know the name server for the edu domain in which cs.washington.edu is located. It returns the name and IP address for that part of the answer in step 3. The local name server then continues its quest. It sends the entire query to the edu name server . That name server returns the name server for UW. This is shown in steps 4 and 5. Closer now the local name server sends the query to the UW name server . If the domain name being sought was in the English department the answer would be found as the UW zone includes the English department. But the Computer Science department has chosen to run its own name server. The query returns the name and IP address of the UW Com puter Science name server . This server is authoritative for the domain cs.washington.edu so it must forwards as response to flits.cs.vu.nl . The name has been resolved. You can explore this process using standard tools such as the dig program that is installed on most UNIX systems. For example typing robot.cs.washington.edu will send query for robot.cs.washington.edu to the .eduservers.net name ser in the example above and you will learn the name and IP address of the UW There are three technical points to discuss about this long scenario. First two sends its query to the local name server that name server handles the resolution on behalf of flits until it has the desired answer to return. It does not return partial mechanism is called recursive query. On the other hand the root name server does not recursively continue the query for the local name server. It just returns partial answer and moves on to the next query. The local name server is responsi ble for continuing the resolution by issuing further queries. This mechanism is called an iterative query. One name resolution can involve both mechanisms as this example showed. recursive query may always seem preferable but many name servers will not handle them. They are too busy. Iterative queries put the bur den on the originator. The rationale for the local name server supporting recur sive query is that it is providing service to hosts in its domain. Those hosts do robot.cs.washington.edu the answer will already be known. Even better if host queries for different host in the same domain say galah.cs.washington.edu the query can be sent directly to the authoritative name server. Similarly queries for other domains in washington.edu can start directly from the washington.edu name performance. The original scenario we sketched is in fact the worst case that oc curs when no useful information is cached. cs.washington.edu will not be propagated to all the caches in the world that may know about it. For this reason cache entries should not live too long. This is the reason that the Time to live field is included in each resource record. It tells re mote name servers how long to cache records. If certain machine has had the same IP address for years it may be safe to cache that information for 1 day. For more volatile information it might be safer to purge the records after few sec onds or minute. sponses. It is UDP. DNS messages are sent in UDP packets with simple format We will not go into the details of this format. If no response arrives within short time the DNS client repeats the query trying another server for the domain after small number of retries. This process is designed to handle the case of the ser ver being down as well as the query or response packet getting lost. 16bit identifier is included in each query and copied to the response so that name ser outstanding at the same time. Even though its purpose is simple it should be clear that DNS is large and complex distributed system that is comprised of millions of name servers that work together. It forms key link between humanreadable domain names and the IP addresses of machines. It includes replication and caching for performance and reliability and is designed to be highly robust. We have not covered security but as you might imagine the ability to change the nametoaddress mapping can have devastating consequences if done mali ciously. For that reason security extensions called DNSSEC have been developed for DNS. We will describe them in Chap. 8. There is also application demand to use names in more flexible ways for ex ample by naming content and resolving to the IP address of nearby host that has the content. This fits the model of searching for and downloading movie. It is the movie that matters not the computer that has copy of it so all that is wanted is the IP address of any nearby computer that has copy of the movie. Content distribution networks are one way to accomplish this mapping. We will describe Electronic mail or more commonly email has been around for over three decades. Faster and cheaper than paper mail email has been popular applica tion since the early days of the Internet. Before 1990 it was mostly used in academia. During the 1990s it became known to the public at large and grew exponentially to the point where the number of emails sent per day now is vastly more than the number of snail mail letters. Other forms of network communication such as instant messaging and voiceoverIP calls have expanded greatly in use over the past decade but email remains the workhorse of Internet communication. It is widely used within industry for intracompany communica tion for example to allow farflung employees all over the world to cooperate on out of 10 messagesis junk mail or spam . Email like most other forms of communication has developed its own con ventions and styles. It is very informal and has low threshold of use. People who would never dream of calling up or even writing letter to Very Important Person do not hesitate for second to send sloppily written email to him or her. By eliminating most cues associated with rank age and gender email debates dent can have more impact than dumb one from an executive vice president. Email is full of jargon such as BTW ROTFL and IMHO . Many people also use lit tle ASCII symbols called smileys starting with the ubiquitous . Rotate the book 90 degrees clockwise if this symbol is unfamiliar. This symbol and other emoticons help to convey the tone of the message. They have spread to other terse forms of communication such as instant messaging. The email protocols have evolved during the period of their use too. The first email systems simply consisted of file transfer protocols with the convention that the first line of each message contained the recipients address. As time went on email diverged from file transfer and many features were added such as the ability to send one message to list of recipients. Multimedia capabilities became important in the 1990s to send messages with images and other nontext material. Programs for reading email became much more sophisticated too shift ing from textbased to graphical user interfaces and adding the ability for users to prevalence of spam mail readers and the mail transfer protocols must now pay attention to finding and removing unwanted email. In our description of email we will focus on the way that mail messages are moved between users rather than the look and feel of mail reader programs. Nevertheless after describing the overall architecture we will begin with the userfacing part of the email system as it is familiar to most readers. ized and what they can do. The architecture of the email system is shown in people to read and send email and the message transfer agents which move the messages from the source to the destination. We will also refer to message trans fer agents informally as mail servers. The user agent is program that provides graphical interface or sometimes text and commandbased interface that lets users interact with the email system. It includes means to compose messages and replies to messages display incom ing messages and organize messages by filing searching and discarding them. The act of sending new messages into the mail system for delivery is called mail Some of the user agent processing may be done automatically anticipating what the user wants. For example incoming mail may be filtered to extract or deprioritize messages that are likely spam. Some user agents include advanced features such as arranging for automatic email responses . user agent runs on the same computer on which user reads her mail. It is just another program and may be run only some of the time. The message transfer agents are typically system processes. They run in the background on mail server machines and are intended to be always available. Their job is to automatically move email through the system from the originator to the recipient with SMTP . This is the message SMTP was originally specified as RFC 821 and revised to become the current and any errors. Numerous applications exist in which confirmation of delivery is important and may even have legal significance email alternative recipients if the primary one is not currently available and the ability for assistants to read and answer their bosses email. Linking user agents and message transfer agents are the concepts of mail boxes and standard format for email messages. Mailboxes store the email that is received for user. They are maintained by mail servers. User agents simply agents send the mail servers commands to manipulate the mailboxes inspecting agents on multiple computers to access one mailbox. Mail is sent between message transfer agents in standard format. The origi nal format RFC 822 has been revised to the current RFC 5322 and extended with support for multimedia content and international text. This scheme is called MIME and will be discussed later. People still refer to Internet email as RFC 822 key idea in the message format is the distinction between the envelope and mation needed for transporting the message such as the destination address prior ity and security level all of which are distinct from the message itself. The mes sage transport agents use the envelope for routing just as the post office does. body is entirely for the human recipient. None of the agents care much about it. Address 180 Main St. We will examine the pieces of this architecture in more detail by looking at the steps that are involved in sending email from one user to another. This journey starts with the user agent. user agent is program that accepts variety of commands for composing receiving and replying to messages as well as for manipulating mailboxes. There are many popular user agents including Google gmail Microsoft Outlook Mozilla Thunderbird and Apple Mail. They can vary greatly in their appearance. Most user agents have menu or icon driven graphical interface that requires mouse or touch interface on smaller mobile devices. Older user agents such as Elm mh and Pine provide textbased interfaces and expect onecharacter commands from the keyboard. Functionally these are the same at least for text messages. mail reader is likely to be much flashier but probably has equivalent functions. some sorted order. It highlights key fields of the message that are extracted from recently completed my undergraduate studies with distinction at an excellent university. will be visiting your From Subject and Received fields in that order to display who sent the message what it is about and when it was received. All the information is formatted in but it is based on the message fields. Thus people who fail to include Subject field often discover that responses to their emails tend not to get the highest prior Many other fields or indications are possible. The icons next to the message tached material and important mail at least as judged by the send er . Many sorting orders are also possible. The most common is to order messages based on the time that they were received most recent first with some indication as to whether the message is new or has already been read by the user. The fields User agents must also be able to display incoming messages as needed so that people can read their email. Often short preview of message is provided as in includes reformatting messages to fit the display and translating or converting After message has been read the user can decide what to do with it. This is called message disposition. Options include deleting the message sending reply forwarding the message to another user and keeping the message for later reference. Most user agents can manage one mailbox for incoming mail with multiple folders for saved mail. The folders allow the user to save message according to sender topic or some other category. Filing can be done automatically by the user agent as well before the user sages are inspected and used along with feedback from the user about previous messages to determine if message is likely to be spam. Many ISPs and com panies run software that labels mail as important or spam so that the user agent can file it in the corresponding mailbox. The ISP and company have the advan tage of seeing mail for many users and may have lists of known spammers. If hun dreds of users have just received similar message it is probably spam. By presorting incoming mail as probably legitimate and probably spam the user agent can save users fair amount of work separating the good stuff from the And the most popular spam It is generated by collections of compromised computers called botnets and its content depends on where you live. Fake diplo mas are topical in Asia and cheap drugs and other dubious product offers are top ical in the . Unclaimed Nigerian bank accounts still abound. Pills for enlarging various body parts are common everywhere. Other filing rules can be constructed by users. Each rule specifies condition and an action. For example rule could say that any message received from the boss goes to one folder for immediate reading and any message from particular mailing list goes to another folder for later reading. Several folders are shown in elsewhere and Junk Mail for messages that are thought to be spam. As well as explicit constructs like folders user agents now provide rich capa bilities let users find messages quickly such as the message about where to buy Vegemite that someone sent in the last month. Email has come long way from the days when it was just file transfer. Pro viders now routinely support mailboxes with up to 1 GB of stored mail that details users interactions over long period of time. The sophisticated mail handling of user agents with search and automatic forms of processing is what makes it possible to manage these large volumes of email. For people who send and re ceive thousands of messages year these tools are invaluable. Another useful feature is the ability to automatically respond to messages in some way. One response is to forward incoming email to different address for responders must run in the mail server because the user agent may not run all the time and may only occasionally retrieve email. Because of these factors the user However the interface for automatic responses is usually presented by the user agent. different example of an automatic response is vacation agent. This is program that examines each incoming message and sends the sender an insipid reply such as Hi. Im on vacation. Ill be back on the 24th of August. Talk to you then. Such replies can also specify how to handle urgent matters in the keep track of whom they have sent canned replies to and refrain from sending the same person second reply. There are pitfalls with these agents however. For example it is not advisable to send canned reply to large mailing list. Let us now turn to the scenario of one user sending message to another user. One of the basic features user agents support that we have not yet discussed is sending these messages into the rest of the mail system for delivery. Although any text editor can be used to create the body of the message editors are usually integrated with the user agent so that it can provide assistance with addressing and ing message the email system can extract the originators address from the in coming email and automatically insert it into the proper place in the reply. Other common features are appending signature block to the bottom of message correcting spelling and computing digital signatures that show the message is Messages that are sent into the mail system have standard format that must be created from the information supplied to the user agent. The most important part of the message for transfer is the envelope and the most important part of the envelope is the destination address. This address must be in format that the message transfer agents can deal with. The expected form of an address is userdnsaddress. Since we studied worth noting that other forms of addressing exist. In particular .400 addresses look radically different from DNS addresses.400 is an ISO standard for messagehandling systems that was at one time competitor to SMTP. SMTP won out handily though .400 systems are still used mostly outside of the .400 addresses are composed of attributevalue This address specifies country state locality personal address and common name . Many other attributes are possible so you can send email to someone whose exact email address you do not know provided you know enough other attributes . Although .400 names are considerably less convenient than DNS names the called nicknames that allow users to enter or select persons name and get the correct email address. Consequently it is usually not necessary to actually type in these strange strings. send the same message to list of people with single command. There are two choices for how the mailing list is maintained. It might be maintained locally by the user agent. In this case the user agent can just send separate message to each intended recipient. Alternatively the list may be maintained remotely at message transfer agent. Messages will then be expanded in the message transfer system which has the effect of allowing multiple users to send to the list. For example if group of bird watchers has mailing list called birders installed on the transfer agent meadowlark.arizona.edu any message sent to will be routed to the University of Arizona and expanded into individual messages to all the mailing list members wherever in the world they may be. Users of this mailing list cannot tell that it is mailing list. It could just as well be the personal mailbox of Prof. Gabriel . Birders. Now we turn from the user interface to the format of the email messages themselves. Messages sent by the user agent must be placed in standard format to be handled by the message transfer agents. First we will look at basic ASCII sage format as described in RFC 822. After that we will look at multimedia ex Messages consist of primitive envelope field is like the Cc field except that this line is deleted from all the copies sent to the primary and secondary recipients. This feature allows people to send copies to third parties without the primary and The next two fields From and Sender tell who wrote and sent the message respectively. These need not be the same. For example business executive may write message but her assistant may be the one who actually transmits it. In this case the executive would be listed in the From field and the assistant in the Sender field. The From field is required but the Sender field may be omit ted if it is the same as the From field. These fields are needed in case the mes sage is undeliverable and must be returned to the sender. line containing Received is added by each message transfer agent along the ceived and other information that can be used for debugging the routing system. intended to tell how to get back to the sender. In theory this information can be box but it is rarely filled in as such and typically just contains the senders ad MessageId of the message to which this is reply The ReplyTo field is sometimes used when neither the person composing the message nor the person sending the message wants to see the reply. For example marketing manager may write an email message telling customers about new product. The message is sent by an assistant but the ReplyTo field lists the head of the sales department who can answer questions and take orders. This field is The MessageId is an automatically generated number that is used to link messages together and to prevent dupli The RFC 5322 document explicitly says that users are allowed to invent op Sometimes wiseguy undergraduates make up fields like XFruitoftheDay or XDiseaseoftheWeek which are legal although not always illuminating. here. Some people terminate their messages with elaborate signatures including quotations from greater and lesser authorities political statements and disclai mers of all kinds nonLatin alphabets or no alphabets as well as sending messages not containing text at all . The solution was the development of MIME . It is widely used for mail messages that are sent across the Internet as well as to describe content for other applications such as Web browsing. MIME is described in RFCs 20452047 4288 4289 and 2049. The basic idea of MIME is to continue to use the RFC 822 format but to add structure to the mes sage body and define encoding rules for the transfer of nonASCII messages. Not deviating from RFC 822 allowed MIME messages to be sent using the existing mail transfer agents and protocols . All that had to be changed were the sending and receiving programs which users could do for themselves. these simply tells the user agent receiving the message that it is dealing with and is processed as such. Humanreadable string telling what is in the message Type and format of the content ing and reading the message. If the string says Photo of Barbaras hamster and the person getting the message is not big hamster fan the message will probably be discarded rather than decoded into highresolution color photograph. The ContentTransferEncoding tells how the body is wrapped for transmis sion through the network. key problem at the time MIME was developed was that the mail transfer protocols expected ASCII messages in which no line exceeded 1000 characters. ASCII characters use 7 bits out of each 8bit byte. Binary data such as executable programs and images use all 8 bits of each byte as do extended character sets. There was no guarantee this data would be transferred safely. Hence some method of carrying binary data that made it look like regu lar ASCII mail message was needed. Extensions to SMTP since the development of MIME do allow 8bit binary data to be transferred though even today binary data may not always go through the mail system correctly if unencoded. MIME provides five transfer encoding schemes plus an escape to new schemesjust in case. The simplest scheme is just ASCII text messages. ASCII characters use 7 bits and can be carried directly by the email protocol provided that no line exceeds 1000 characters. The next simplest scheme is the same thing but using 8bit characters that is all values from 0 up to and including 255 are allowed. Messages using the 8bit encoding must still adhere to the standard maximum line length. Then there are messages that use true binary encoding. These are arbitrary binary files that not only use all 8 bits but also do not adhere to the 1000character line limit. Executable programs fall into this category. Nowadays mail servers can negotiate to send data in binary encoding falling back to ASCII if both ends do not support the extension. The ASCII encoding of binary data is called base64 encoding. scheme groups of 24 bits are broken up into four 6bit units with each unit being sent as legal ASCII character. The coding is for 0 for 1 and so on 63 respectively. The and sequences indicate that the last group contained only 8 or 16 bits respectively. Carriage returns and line feeds are ignored so they can be inserted at will in the encoded character stream to keep the lines short enough. Arbitrary binary text can be sent safely using this scheme albeit ineffi ciently. This encoding was very popular before binarycapable mail servers were widely deployed. It is still commonly seen. For messages that are almost entirely ASCII but with few nonASCII char acters base64 encoding is somewhat inefficient. Instead an encoding known as quotedprintable encoding is used. This is just 7bit ASCII with all the charac ters above 127 encoded as an equals sign followed by the characters value as two hexadecimal digits. Control characters some punctuation marks and math symb ols as well as trailing spaces are also so encoded. possible to specify userdefined encoding in the ContentTransferEncoding fies the nature of the message body and has had an impact well beyond email. For instance content downloaded from the Web is labeled with MIME types so that the browser knows how to present it. So is content sent over streaming media and realtime transports such as voice over IP. Initially seven MIME types were defined in RFC 1521. Each type has one or more available subtypes. The type and subtype are separated by slash as in ContentType videompeg. Since then hundreds of subtypes have been added along with another type. Additional entries are being added all the time as new types of content are developed. The list of assigned types and subtypes is main The types along with examples of commonly used subtypes are given in bination is for ordinary messages that can be displayed as received with no en coding and no further processing. This option allows ordinary messages to be sent in RFC 822 email. subtype for the eXtensible Markup Language textxml is defined in RFC 3023. XML documents have proliferated with the development of the Web. We will study HTML and XML in Sec. 7.3. The next MIME type is image which is used to transmit still pictures. Many formats are widely used for storing and transmitting images nowadays both with and without compression. Several of these including GIF JPEG and TIFF are built into nearly all browsers. Many other formats and corresponding subtypes The audio and video types are for sound and moving pictures respectively. Please note that video may include only the visual information not the sound. If movie with sound is to be transmitted the video and audio portions may have to be transmitted separately depending on the encoding system used. The first video format defined was the one devised by the modestly named Moving Picture Experts Group but others have been added since. audiobasic new audio type audiompeg was added in RFC 3003 to allow peo ple to email MP3 audio files. The videomp4 and audiomp4 types signal video and audio data that are stored in the newer MPEG 4 format. The model type was added after the other content types. It is intended for The application type is catchall for formats that are not covered by one of the other types and that require an application to interpret the data. We have lis ted the subtypes pdf javascript and zip as examples for PDF documents Java Script programs and Zip archives respectively. User agents that receive this con tent use thirdparty library or external program to display the content the dis play may or may not appear to be integrated with the user agent. By using MIME types user agents gain the extensibility to handle new types of application content as it is developed. This is significant benefit. On the other hand many of the new forms of content are executed or interpreted by applica tions which presents some dangers. Obviously running an arbitrary executable program that has arrived via the mail system from friends poses security haz ard. The program may do all sorts of nasty damage to the parts of the computer to which it has access especially if it can read and write files and use the network. Less obviously document formats can pose the same hazards. This is because formats such as PDF are fullblown programming languages in disguise. While they are interpreted and restricted in scope bugs in the interpreter often allow devious documents to escape the restrictions. Besides these examples there are many more application subtypes because there are many more applications. As fallback to be used when no other subtype is known to be more fitting the octetstream subtype denotes sequence of unin terpreted bytes. Upon receiving such stream it is likely that user agent will display it by suggesting to the user that it be copied to file. Subsequent proc essing is then up to the user who presumably knows what kind of content it is. The last two types are useful for composing and manipulating messages them selves. The message type allows one message to be fully encapsulated inside an other. This scheme is useful for forwarding email for example. When com plete RFC 822 message is encapsulated inside an outer message the rfc822 sub type should be used. Similarly it is common for HTML documents to be encap sulated. And the partial subtype makes it possible to break an encapsulated mes sage into pieces and send them separately . Parameters make it possible to reassemble all the parts at the destination in the correct order. with the beginning and end of each part being clearly delimited. The mixed sub type allows each part to be different type with no additional structure imposed. Many email programs allow the user to provide one or more attachments to text message. These attachments are sent using the multipart type. In contrast to mixed the alternative subtype allows the same message to be included multiple times but expressed in two or more different media. For ex ample message could be sent in plain ASCII in HMTL and in PDF. properly designed user agent getting such message would display it according to user choice would be HTML. If neither of these were possible then the flat ASCII text would be displayed. The parts should be ordered from simplest to most com plex to help recipients with preMIME user agents make some sense of the mes sage . The alternative subtype can also be used for multiple languages. In this con text the Rosetta Stone can be thought of as an early multipartalternative mes Of the other two example subtypes the parallel subtype is used when all parts must be viewed simultaneously. For example movies often have an audio channel and video channel. Movies are more effective if these two channels are played back in parallel instead of consecutively. The digest subtype is used when multiple messages are packed together into composite message. For example some discussion groups on the Internet collect messages from subscribers and then send them out to the group periodically as single multipartdigest message. As an example of how MIME types may be used for email messages multi alternative forms as HTML and as an audio file. Assuming the receiver has audio capability the user agent there will play the sound file. In this example the sound is carried by reference as messageexternalbody subtype so first the user agent must fetch the sound file birthday.snd using FTP. If the user agent has no audio capability the lyrics are displayed on the screen in stony silence. The two parts are delimited by two hyphens followed by string specified in the boundary parameter. ample. At the top level it indicates that the message has multiple parts. Within the second part it is required to tell the user agent what kind of external file it is to fetch. To indicate this slight difference in usage we have used lowercase let coding is similarly required for any external body that is not encoded as 7bit Now that we have described user agents and mail messages we are ready to look at how the message transfer agents relay messages from the originator to the recipient. The mail transfer is done with the SMTP protocol. The simplest way to move messages is to establish transport connection from the source machine to the destination machine and then just transfer the mes sage. This is how SMTP originally worked. Over the years however two dif ferent uses of SMTP have been differentiated. The first use is mail submission agents send messages into the mail system for delivery. The second use is to ContentType multipartalternative boundaryqwertyuiopasdfghjklzxcvbnm Subject Earth orbits sun integral number of times This is the preamble. The user agent ignores it. Have nice day. Happy birthday to you Happy birthday to you Happy birthday dear Bob Happy birthday to you qwertyuiopasdfghjklzxcvbnm sequence delivers mail all the way from the sending to the receiving message tension mechanism. Then we will discuss how it is used differently for mail sub mission and message transfer. SMTP and Extensions Within the Internet email is delivered by having the sending computer estab lish TCP connection to port 25 of the receiving computer. Listening to this port is mail server that speaks SMTP . This ser ver accepts incoming connections subject to some security checks and accepts taining the first part of the undeliverable message is returned to the sender. SMTP is simple ASCII protocol. This is not weakness but feature. Using ASCII text makes protocols easy to develop test and debug. They can be tested by sending commands manually and records of the messages are easy to read. Most applicationlevel Internet protocols now work this way . We will walk through simple message transfer between mail servers that de livers message. After establishing the TCP connection to port 25 the sending machine operating as the client waits for the receiving machine operating as the server to talk first. The server starts by sending line of text giving its identity and telling whether it is prepared to receive mail. If it is not the client releases the connection and tries again later. If the server is willing to accept email the client announces whom the email is coming from and whom it is going to. If such recipient exists at the destina tion the server gives the client the goahead to send the message. Then the client sends the message and the server acknowledges it. No checksums are needed be cause TCP provides reliable byte stream. If there is more email that is now sent. When all the email has been exchanged in both directions the connection is ent are marked . Those sent by the server The first command from the client is indeed meant to be HELO. Of the vari ous fourcharacter abbreviations for HELLO this one has numerous advantages over its biggest competitor. Why all the commands had to be four characters has been lost in the mists of time. command is used. Such commands are allowed to send single message to multi ple receivers. Each one is individually acknowledged or rejected. Even if some recipients are rejected the message can be sent to the other ones. rigidly specified the syntax of the replies is less rigid. Only the numerical code really counts. Each implementation can put whatever string it wants after the The basic SMTP works well but it is limited in several respects. It does not include authentication. This means that the FROM command in the example could give any sender address that it pleases. This is quite useful for sending spam. An other limitation is that SMTP transfers ASCII messages not binary data. This is why the base64 MIME content transfer encoding was needed. However with that large messages. third limitation is that SMTP sends messages in the clear. It has no encryption to provide measure of privacy against prying eyes. addressed SMTP was revised to have an extension mechanism. This mechanism is mandatory part of the RFC 5321 standard. The use of SMTP with extensions 250 cs.washington.edu says hello to ee.uwa.edu.au 354 Send mail end with . on line by itself ContentType multipartalternative boundaryqwertyuiopasdfghjklzxcvbnm Subject Earth orbits sun integral number of times This is the preamble. The user agent ignores it. Have nice day. Clients wanting to use an extension send an EHLO message instead of HELO initially. If this is rejected the server is regular SMTP server and the client should proceed in the usual way. If the EHLO is accepted the server replies with the extensions that it supports. The client may then use any of these extensions. as used in the extension mechanism along with description of the new func tionality. We will not go into extensions in further detail. To get better feel for how SMTP and some of the other protocols described the Internet. On UNIX system in shell type substituting the DNS name of your ISPs mail server for mail.isp.com. On Win dows XP system click on Start then Run and type the command in the dialog box. On Vista or Windows 7 machine you may have to first install the telnet program and then start it yourself. This command will establish telnet connection to port 25 on that machine. Port 25 is the SMTP The first three lines are from telnet telling you what it is doing. The last line is from the SMTP server on the remote machine announcing its willingness to talk to you and accept email. To find out what commands it accepts type the server is willing to accept mail from you. Originally user agents ran on the same computer as the sending message transfer agent. In this setting all that is required to send message is for the user agent to talk to the local mail server using the dialog that we have just described. However this setting is no longer the usual case. User agents often run on laptops home PCs and mobile phones. They are not always connected to the Internet. Mail transfer agents run on ISP and company servers. They are always connected to the Internet. This difference means that user agent in Boston may need to contact its regular mail server in Seattle to send mail message because the user is traveling. By itself this remote communication poses no problem. It is exactly what the TCPIP protocols are designed to support. However an ISP or company usually does not want any remote user to be able to submit messages to its mail server to be delivered elsewhere. The ISP or company is not running the server as public service. In addition this kind of open mail relay attracts spammers. This is be cause it provides way to launder the original sender and thus make the message more difficult to identify as spam. Given these considerations SMTP is normally used for mail submission with the AUTH extension. This extension lets the server check the credentials of the client to confirm that the server should be providing There are several other differences in the way SMTP is used for mail submis sion. For example port 587 is used in preference to port 25 and the SMTP server can check and correct the format of the messages sent by the user agent. For more information about the restricted use of SMTP for mail submission please Once the sending mail transfer agent receives message from the user agent it will deliver it to the receiving mail transfer agent using SMTP. To do this the to . To what mail server should the message be delivered To determine the correct mail server to contact DNS is consulted. In the pre ing the MX or mail exchanger record. In this case DNS query is made for the MX records of the domain ee.uwa.edu.au. This query returns an ordered list of the names and IP addresses of one or more mail servers. The sending mail transfer agent then makes TCP connection on port 25 to the IP address of the mail server to reach the receiving mail transfer agent and uses SMTP to relay the message. The receiving mail transfer agent will then place mail for the user bob in the correct mailbox for Bob to read it at later time. This local delivery step may involve moving the message among computers if there is large mail infrastructure. fer agent in single hop. There are no intermediate servers in the message transfer stage. It is possible however for this delivery process to occur multiple times. One example that we have described already is when message transfer agent implements mailing list. In this case message is received for the list. It is then expanded as message to each member of the list that is sent to the individual member addresses. As another example of relaying Bob may have graduated from . and also be reachable via the address . Rather than reading mail on multiple accounts Bob can arrange for mail sent to this address to be forwarded to . In this case mail sent to will undergo two deliveries. First it will be sent to the mail server for alum.mit.edu. Then it will be sent to the mail server for ee.uwa.edu.au. Each of these legs is complete and separate delivery as far as the mail transfer agents are concerned. Another consideration nowadays is spam. Nine out of ten messages sent today are spam . Few people want more spam but it is hard to avoid because it masquerades as regular mail. Before accepting message additional checks may be made to reduce the opportunities for spam. The message for Bob was sent from . The receiving mail transfer agent can look up the sending mail transfer agent in DNS. This lets it check that the IP ad dress of the other end of the TCP connection matches the DNS name. More gen erally the receiving agent may look up the sending domain in DNS to see if it has mail sending policy. This information is often given in the TXT and SPF records. It may indicate that other checks can be made. For example mail sent from cs.washington.edu may always be sent from the host june.cs.washington.edu. If the sending mail transfer agent is not june there is problem. If any of these checks fail the mail is probably being forged with fake send ing address. In this case it is discarded. However passing these checks does not imply that mail is not spam. The checks merely ensure that the mail seems to be coming from the region of the network that it purports to come from. The idea is that spammers should be forced to use the correct sending address when they send mail. This makes spam easier to recognize and delete when it is unwanted. Our mail message is almost delivered. It has arrived at Bobs mailbox. All that remains is to transfer copy of the message to Bobs user agent for display. early Internet when the user agent and mail transfer agent ran on the same ma chine as different processes. The mail transfer agent simply wrote new messages to the end of the mailbox file and the user agent simply checked the mailbox file Nowadays the user agent on PC laptop or mobile is likely to be on dif ferent machine than the ISP or company mail server. Users want to be able to ac cess their mail remotely from wherever they are. They want to access email from work from their home PCs from their laptops when on business trips and from cybercafes when on socalled vacation. They also want to be able to work offline then reconnect to receive incoming mail and send outgoing mail. Moreover each user may run several user agents depending on what computer it is convenient to use at the moment. Several user agents may even be running at the same time. the mailbox and to allow the mailbox to be remotely manipulated. Several dif ferent protocols can be used for this purpose but SMTP is not one of them. SMTP is pushbased protocol. It takes message and connects to remote server to cause the mailbox must continue to be stored on the mail transfer agent and be cause the user agent may not be connected to the Internet at the moment that To use IMAP the mail server runs an IMAP server that listens to port 143. The user agent runs an IMAP client. The client connects to the server and begins to First the client will start secure transport if one is to be used and then log in or otherwise authenticate itself to the server. Once logged in there are many commands to list folders and messages fetch messages or even parts of messages mark messages with flags for later deletion and organize messages into folders. To avoid confu sion please note that we use the term folder here to be consistent with the rest multiple folders. However in the IMAP specification the term mailbox is used instead. One user thus has many IMAP mailboxes each of which is typically pres ented to the user as folder. IMAP has many other features too. It has the ability to address mail not by message number but by using attributes . Searches can be performed on the server to find the messages that satisfy certain criteria so that only those messages are fetched by the client. protocol but supports fewer features and is less secure in typical usage. Mail is usually downloaded to the user agent computer instead of remaining on the mail server. This makes life easier on the server but harder on the user. It is not easy to read mail on multiple computers plus if the user agent computer breaks all email may be lost permanently. Nonetheless you will still find POP3 in use. Proprietary protocols can also be used because the protocol runs between mail server and user agent that can be supplied by the same company. Microsoft Exchange is mail system with proprietary protocol. An increasingly popular alternative to IMAP and SMTP for providing email service is to use the Web as an interface for sending and receiving mail. Widely used Webmail systems include Google Gmail Microsoft Hotmail and Yahoo Mail. Webmail is one example of software that is provided as service using the Web. In this architecture the provider runs mail servers as usual to accept messages for users with SMTP on port 25. However the user agent is different. Instead of This means that users can use any browser they like to access their mail and send We have not yet studied the Web but brief description that you might come form is presented in which the user is asked for login name and password. The ser for display. sages can be read deleted and so on. To make the interface responsive the Web the client in response to local events and can also download and upload messages in the background to prepare the next message for display or new message for submission. In this model mail submission happens using the normal Web protocols by posting data to URL. The Web server takes care of injecting messages into the traditional mail delivery system that we have de scribed. For security the standard Web protocols can be used as well. These pro The Web as the World Wide Web is popularly known is an architectural framework for accessing linked content spread out over millions of machines all over the Internet. In 10 years it went from being way to coordinate the design of highenergy physics experiments in Switzerland to the application that millions of people think of as being The Internet. Its enormous popularity stems from the fact that it is easy for beginners to use and provides access with rich graphical interface to an enormous wealth of information on almost every conceivable sub ject from aardvarks to Zulus. The Web began in 1989 at CERN the European Center for Nuclear Research. The initial idea was to help large teams often with members in half dozen or more countries and time zones collaborate using constantly changing collection ments in particle physics. The proposal for web of linked documents came from CERN physicist Tim BernersLee. The first prototype was opera tional 18 months later. public demonstration given at the Hypertext 91 confer ence caught the attention of other researchers which led Marc Andreessen at the University of Illinois to develop the first graphical browser. It was called Mosaic and released in February 1993. The rest as they say is now history. Mosaic was so popular that year later Andreessen left to form company Netscape Communications Corp. whose goal was to develop Web software. For the next three years Netscape Navigator and Microsofts Internet Explorer engaged in browser war each one trying to capture larger share of the new market by frantically adding more features than the other one. small number of these sites became tremendously popular. Those sites and the companies behind them largely define the Web as people experience it today. Ex amples include bookstore flea market search and social networking . The period through 2000 when many Web companies became worth hundreds of millions of dollars overnight only to go bust practically the next day when they turned out to be hype even has name. It is called the dot com era. New ideas are still striking it rich on the Web. Many of them come from students. For ex ample Mark Zuckerberg was Harvard student when he started Facebook and Perhaps you will come up with the next big thing. In 1994 CERN and . signed an agreement setting up the W3C an organization devoted to further developing the Web standardizing protocols and encouraging interoperability between sites. Berners Lee became the director. Since then several hundred universities and companies have joined the consortium. Although there are now more books about the Web of the consortiums numerous documents and activities. From the users point of view the Web consists of vast worldwide collec called hypertext was invented by visionary . professor of electrical en gineering Vannevar Bush in 1945 . This was long before the Inter net was invented. In fact it was before commercial computers existed although several universities had produced crude prototypes that filled large rooms and had less power than modern pocket calculator. net Explorer and Chrome are examples of popular browsers. The browser fetches matted on the screen. The content itself may be mix of text images and for matting commands in the manner of traditional document or other forms of content such as video or programs that produce graphical interface with which users can interact. for the Computer Science Engineering department at the University of Wash To follow link the user places the mouse cursor on the linked portion of the the Web links were highlighted with underlining and colored text so that they look of linked regions so link might appear as an icon or change its appearance links visually distinct to provide usable interface. googleanalytics.com information especially for them. This link is accessed by clicking in the circled Thus moving between machines while viewing content is seamless. tocol that runs over TCP just as was the case for SMTP. It is called HTTP . The content may simply be document that is contrast if it was generated on demand by program or contains program it is If bookstore customer has bought mystery novels in the past upon visiting the ed whereas more culinaryminded customer might be greeted with new cook books. How the Web site keeps track of who likes what is story to be told short ly. But briefly the answer involves cookies . In the general case the path has hierarchical name that models file directory structure. However the interpretation of the path is up to the server it may or may not reflect the actual directory structure. This URL consists of three parts the protocol the DNS name of the host When user clicks on hyperlink the browser carries out series of steps in ample link is selected 1. The browser determines the URL . wellknown port for the HTTP protocol. fetches the other URLs using the same process. In this case the script from googleanalytics.com. 9. The TCP connections are released if there are no other requests to the same servers for short period. at the bottom of the screen. In this way when the performance is poor the user transmission over slow or congested network. The URL design is openended in the sense that it is straightforward to have browsers use multiple protocols to get at different kinds of resources. In fact URLs for various other protocols have been defined. Slightly simplified forms of Let us briefly go over the list. The http protocol is the Webs native language the one spoken by Web servers. HTTP stands for HyperText Transfer Proto The ftp protocol is used to access files by FTP the Internets file transfer pro The Web makes it easy to obtain files placed on numerous FTP servers throughout the world by providing simple clickable interface instead of com mandline interface. This improved access to information is one reason for the spectacular growth of the Web. more simply by just naming it. This approach does not require having server. Of course it works only for local files not remote ones. is useful anyway. It allows users to send email from Web browser. Most brow sers will respond when mailto link is followed by starting the users mail agent to compose message with the address field already filled in. The rtsp and sip protocols are for establishing streaming media sessions and audio and video calls. browser. For example following the aboutplugins link will cause most browsers called plugins. In short the URLs have been designed not only to allow users to navigate the Web but to run older protocols such as FTP and email as well as newer protocols for audio and video and to provide convenient access to local files and browser information. This approach makes all the specialized user interface programs for those other services unnecessary and integrates nearly all Internet access into single program the Web browser. If it were not for the fact that this idea was thought of by British physicist working research lab in Switzerland it could easily pass for plan dreamed up by some software companys advertising depart Despite all these nice properties the growing use of the Web has turned up an inherent weakness in the URL scheme. URL points to one specific host but tiple copies far apart to reduce the network traffic. There is no way to say To solve this kind of problem URLs have been generalized into URIs . Some URIs tell how to locate resource. These are the URLs. Other URIs tell the name of resource but not where to find it. These URIs are called URNs . The rules for writing URIs are given in RFC 3986 while the different URI schemes in use are tracked by IANA. There are many different kinds of URIs besides the schemes listed in are written in standardized language called HTML. It is the lingua franca of the Although browser is basically an HTML interpreter most browsers have numerous buttons and features to make it easier to navigate the Web. Most have bookmarks making it possible to revisit any of them with only few mouse mat photograph in JPEG format song in MP3 format or any one of hundreds Rather than making the browsers larger and larger by building in interpreters for rapidly growing collection of file types most browsers have chosen more few other builtin types. If the MIME type is not one of the builtin ones the This table associates MIME types with viewers. There are two possibilities plugins and helper applications. plugin is thirdparty code module that is installed as an extension to the browser as illus time to render documents and play audio and video. Because plugins run inside Each browser has set of procedures that all plugins must implement so the browser can call the plugins. For example there is typically procedure the browsers base code calls to supply the plugin with data to display. This set of procedures is the plugins interface and is browser specific. In addition the browser makes set of its own procedures available to the plugin to provide services to plugins. Typical procedures in the browser inter face are for allocating and freeing memory displaying message on the browsers Before plugin can be used it must be installed. The usual installation pro file. Executing the installation file unpacks the plugin and makes the appropriate calls to register the plugins MIME type with the browser and associate the plugin with it. Browsers usually come preloaded with popular plugins. The other way to extend browser is make use of helper application. This 20. Since the helper is separate program the interface is at arms length from the browser. It usually just accepts the name of scratch file where the content are large programs that exist independently of the browser for example Micro soft Word or PowerPoint. Many helper applications use the MIME type application. As consequence considerable number of subtypes have been defined for them to use for exam ple applicationvnd.mspowerpoint for PowerPoint files. vnd denotes vendorspe cific formats. In this way URL can point directly to PowerPoint file and when the user clicks on it PowerPoint is automatically started and handed the content to be displayed. Helper applications are not restricted to using the appli cation MIME type. Adobe Photoshop uses imagexphotoshop for example. number of document types with no changes to themselves. Modern Web servers are often added every time new program is installed. source of conflicts is that multiple plugins and helper applications are available for some subtypes such as videompeg. What happens is that the last one to register overwrites the existing association with the MIME type capturing the type for itself. As consequence installing new program may change the way browser handles existing types. Browsers can also open local files with no network in sight rather than fetch ing them from remote Web servers. However the browser needs some way to de termine the MIME type of the file. The standard method is for the operating sys tem to associate file extension with MIME type. In typical configuration opening foo.pdf will open it in the browser using an applicationpdf plugin and opening bar.doc will open it in Word as the applicationmsword helper. Here too conflicts can arise since many programs are willingno make During installation programs intended for sophisticated users often display checkboxes for the MIME types and extensions they are prepared to handle to allow the user to select the appropriate ones and thus not overwrite existing associations by accident. Programs aimed at the con sumer market assume that the user does not have clue what MIME type is and simply grab everything they can without regard to what previously installed pro grams have done. The ability to extend the browser with large number of new types is con venient but can also lead to trouble. When browser on Windows PC fetches file with the extension exe it realizes that this file is an executable program and therefore has no helper. The obvious action is to run the program. However this could be an enormous security hole. All malicious Web site has to do is pro are linked to virus. single click on picture then causes an unknown and po tentially hostile executable program to be fetched and run on the users machine. to be cautious about running unknown programs automatically but not all users understand what choices are safe rather than convenient. So much for the client side. Now let us take look at the server side. As we saw above when the user types in URL or clicks on line of hypertext the browser parses the URL and interprets the part between http and the next slash as DNS name to look up. Armed with the IP address of the server the browser establishes TCP connection to port 80 on that server. Then it sends over com To first approximation simple Web server is similar to the server of work. In both cases the steps that the server performs in its main loop are 1. Accept TCP connection from client . 5. Release the TCP connection. Modern Web servers have more features but in essence this is what Web server does for the simple case of content that is contained in file. For dynamic con tent the third step may be replaced by the execution of program is to maintain cache in memory of the most recently read files or certain number of gigabytes of con tent. Before going to disk to get file the server checks the cache. If the file is there it can be served directly from memory thus eliminating the disk access. Although effective caching requires large amount of main memory and some time are nearly always worth the overhead and expense. To tackle the problem of serving single request at time one strategy is to make the server multithreaded. In one design the server consists of frontend module that accepts all incoming requests and processing modules as shown in modules all have access to the cache within the process address space. When request comes in the front end accepts it and builds short record describing it. It then hands the record to one of the processing modules. The processing module first checks the cache to see if the file needed is there. there the processing module starts disk operation to read it into the cache to make room for it. When the file comes in from the disk it is put in the cache and also sent back to the client. The advantage of this scheme is that while one or more processing modules are blocked waiting for disk or network operation to complete other modules can be actively working on other requests. With processing modules the throughput can be as much as times higher than with singlethreaded server. Of course when the disk or network is the limiting factor it is necessary to have multiple disks or faster network to get any real im provement over the singlethreaded model. Modern Web servers do more than just accept path names and return files. In fact the actual processing of each request can get quite complicated. For this rea son in many servers each processing module performs series of steps. The front end passes each incoming request to the first available module which then carries it out using some subset of the following steps depending on which ones are needed for that particular request. These steps occur after the TCP connection and any secure transport mechanism . 7. Make an entry in the server log. Step 1 is needed because the incoming request may not contain the actual name of file or program as literal string. It may contain builtin shortcuts that need to be translated. As simple example the URL has an empty It has to be expanded to some default file name that is usually Also modern browsers can specify configuration information such as the browser software and the users default language . This mobile device and in the preferred language if available. In general name expan sion is not quite so trivial as it might at first appear due to variety of conven tions about how to map paths to the file directory and programs. accomplished depends on the design of the server. For the popular Apache server for instance the convention is to place file called .htaccess that lists the access ning programs cannot always be cached because they might produce different result each time they are run. Even files should occasionally be checked to see if the program parameters or input. These data come from the path or other parts of Step 5 is about determining other parts of the response that accompany the extension the first few words of the file or program output configuration file and possibly other sources. fetches. This reuse means that some logic is needed to map request to shared connection and to return each response so that it is associated with the correct re Step 7 makes an entry in the system log for administrative purposes along with keeping any other important statistics. Such logs can later be mined for valu able information about user behavior for example the order in which people ac Navigating the Web as we have described it so far involves series of inde request to server and gets back file. Then the server forgets that it has ever seen that particular client. This model is perfectly adequate for retrieving publicly available documents and it worked well when the Web was first created. However it is not suited for done with the server. This behavior is needed for many ongoing interactions with Web sites. For example some Web sites require clients to reg ister to use them. This raises the question of how ser vers can distinguish between requests from users who have previously registered and everyone else. second example is from ecommerce. If user wanders around an electronic store tossing items into her virtual shopping cart from time ample is customized Web portals such as Yahoo. Users can set up personalized At first glance one might think that servers could track users by observing their IP addresses. However this idea does not work. Many users share com puters especially at home and the IP address merely identifies the computer not the user. Even worse many companies use NAT so that outgoing packets bear the same IP address for all users. That is all of the computers behind the NAT box look the same to the server. And many ISPs assign IP addresses to customers with DHCP. The IP addresses change over time so to server you might sudden ly look like your neighbor. For all of these reasons the server cannot use IP ad dresses to track users. This problem is solved with an oftcritized mechanism called cookies. The name derives from ancient programmer slang in which program calls proce dure and gets something back that it may need to present later to get some work done. In this sense UNIX file descriptor or Windows object handle can be considered to be cookie. Cookies were first implemented in the Netscape brow ser in 1994 and are now specified in RFC 2109. rather small named string that the server can associate with browser. This association is not the same thing as user but it is much closer and more useful than an IP address. Browsers store the offered cookies for an inter val usually in cookie directory on the clients disk so that the cookies persist cross browser invocations unless the user has disabled cookies. Cookies are just strings not executable programs. In principle cookie could contain virus but since cookies are treated as data there is no official way for the virus to actually run and do damage. However it is always possible for some hacker to exploit browser bug to cause activation. tells where the cookie came from. Browsers are supposed to check that servers cookies per client. The Path is path in the servers directory structure that iden tifies which parts of the servers file tree may use the cookie. It is often which means the whole tree. The Content field takes the form name value. Both name and value can be anything the server wants. This field is where the cookies content is stored. The Expires field specifies when the cookie expires. If this field is absent the browser discards the cookie when it exits. Such cookie is called nonper Greenwich Mean Time. To remove cookie from clients hard disk server just sends it again but with an expiration time in the past. UserID4627239101 turn the cookie to server using secure transport namely SSLTLS . This feature is used for ecommerce banking and other secure applications. We have now seen how cookies are acquired but how are they used Just be rectory to see if any cookies there were placed by the domain the request is going to. If so all the cookies placed by that domain and only that domain are in cluded in the request message. When the server gets them it can interpret them any way it wants to. was set by tomscasino.com and is used to identify the customer. When the client returns next week to throw away some more money the browser sends over the cookie so the server knows who it is. Armed with the customer ID the server can look up the customers record in database and use this information to build an The second cookie came from jillsstore.com. The scenario here is that the client is wandering around the store looking for good things to buy. When she finds bargain and clicks on it the server adds it to her shopping cart and also builds cookie containing the product code of the item and sends the cookie back to the client. As the client continues to wander around the taining the full list of purchases is sent along with the request. In this way the server knows exactly what the customer wants to buy. The third cookie is for Web portal. When the customer clicks on link to containing the stock prices for Cisco and Oracle and the New York Jets football results. Since cookie can be up to 4 KB there is plenty of room for more detail more controversial use of cookies is to track the online behavior of users. This lets Web site operators understand how users navigate their sites and advertisers build up profiles of the ads or sites particular user has viewed. The controversy is that users are typically unaware that their activity is being tracked even with detailed profiles and across seemingly unrelated Web sites. Nonethe less Web tracking is big business. DoubleClick which provides and tracks ads is ranked among the 100 busiest Web sites in the world by the Web monitoring company Alexa. Google Analytics which tracks site usage for operators is used by more than half of the busiest 100000 sites on the Web. It is easy for server to track user activity with cookies. Suppose server each visitor looked at before leaving the site. When the first request comes in there will be no accompanying cookie so the server sends back cookie con back to the server. Each time the counter is incremented and sent back to the cli ent. By keeping track of the counters the server can see how many people give Tracking the browsing behavior of users across sites is only slightly more complicated. It works like this. An advertising agency say Sneaky Ads con which it pays the site owners fee. Instead of giving the sites the ad as GIF file hands out contains unique number in the path such as the HTML file. Then the browser inspects the HTML file and sees the link to the image file at so it sends request there for the image. GIF file containing an ad is returned along with cookie containing unique user ID penny from the product manufacturer each time it ships out the ad. the browser first fetches the HTML file from the server. Then it sees the link to Since it already has cookie from the domain sneaky.com the browser includes user has visited. In due course Sneaky can build up detailed profile of the users browsing habits even though the user has never clicked on any of the ads. Of course it does not yet have the users name . However if the user ever supplies his name to any site cooperating with Sneaky complete profile along with name will be available for sale to anyone who wants to buy it. The sale of this information may be profitable enough for Sneaky to place more ads on more Web sites and thus collect more information. And if Sneaky wants to be supersneaky the ad need not be classical banner ad. An ad consisting of single pixel in the background color has exactly the same effect as banner ad it requires the browser to go fetch the 1 1pixel GIF image and send it all cookies originating at the pixels do Cookies have become focal point for the debate over online privacy because of tracking behavior like the above. The most insidious part of the whole business is that many users are completely unaware of this information collection and may even think they are safe because they do not click on any of the ads. For this rea son cookies that track users across sites are considered by many to be spyware. Have look at the cookies that are already stored by your browser. Most brow might be surprised to find names email addresses or passwords as well as opaque identifiers. Hopefully you will not find credit card numbers but the potential for will not work properly without cookies. Alternatively most browsers let users block thirdparty cookies. thirdparty cookie is one from different site than these cookies helps to prevent tracking across Web sites. Browser extensions can also be installed to provide finegrained control over how cookies are used . As the debate continues many companies are developing priva cy policies that limit how they will share information to prevent abuse. Of course the policies are simply how the companies say they will handle information. For example We may use the information collected from you in the conduct of our businesswhich might be selling the information. server that present themselves in the same way each time they are fetched and as foundation for later material. Readers already familiar with HTML can skip HTML was introduced with the Web. It describing how documents are to be formatted. The term markup comes from the old days when copyeditors actually marked up documents to tell the printer in those days human beingwhich fonts to use and so on. Markup languages thus contain explicit commands for formatting. For example in HTML means start boldface mode and means leave boldface mode. LaTeX and TeX are other examples of markup languages that are well known to most academic authors. The key advantage of markup language over one with no explicit markup is that it separates content from how it should be presented. Writing browser is then straightforward the browser simply has to understand the markup commands and apply them to the content. Embedding all the markup commands within each HTML file and standardizing them makes it possible for any Web browser to read duced in 1600 1200 window with 24bit color on highend computer but may have to be displayed in 640 320 window on mobile phone. While it is certainly possible to write documents like this with any plain text editor and many people do it is also possible to use word processors or special HTML editors that do most of the work although most browsers do not bracketed by the and tags and the body is bracketed by the and tags. The strings inside the tags are called directives. Most but not all HTML tags have this format. That is they use to mark the beginning of something and to mark its end. Tags can be in either lowercase or uppercase. Thus and mean the same thing but lower case is best for compatibility. Actual layout of the HTML document is irrelevant. HTML parsers ignore extra spaces and carriage returns since they have to reformat the text to make it fit the current display area. Consequently white space can be added at will to make HTML documents more readable something most of them are badly in need of. As another consequence blank lines cannot be used to separate paragraphs as they are simply ignored. An explicit tag is required. Some tags have parameters called attributes. For example the two attributes src and alt. The first attribute gives the URL for the image. The HTML standard does not specify which image formats are permitted. In practice all browsers support GIF and JPEG files. Browsers are free to support other for mats but this extension is twoedged sword. If user is accustomed to brow be surprised when other browsers just ignore all of his wonderful art. The second attribute gives alternate text to use if the image cannot be dis played. For each tag the HTML standard gives list of what the permitted pa rameters if any are and what they mean. Because each parameter is named the order in which the parameters are given is not significant. Technically HTML documents are written in the ISO 88591 Latin1 charac ter set but for users whose keyboards support only ASCII escape sequences are present for the special characters such as . The list of special characters is given in the standard. All of them begin with an ampersand and end with semicolon. For example produces space è produces and é pro duces . Since and have special meanings they can be expressed only with their escape sequences and respectively. The main item in the head is the title delimited by and . Certain kinds of metainformation may also be present though none are present in our ex tag where is digit in the range 1 to 6. Thus is the most important head ingis the least important one. It is up to the browser to render these ap propriately on the screen. Typically the lowernumbered headings will be dis played in larger and heavier font. The browser may also choose to use different colors for each level of heading. Usuallyheadings are large and boldface with at least one blank line above and below. In contrastheadings are in smaller font with less space above and below. The tags and are used to enter boldface and italics mode respectively. The tag forces break and draws horizontal line across the display. The tag starts paragraph. The browser might display this by inserting blank line and some indentation for example. Interestingly the tag that exists to mark the end of paragraph is often omitted by lazy HTML pro HTML provides various mechanisms for making lists including nested lists.

Service provider bridges allow devices in service provider network to transparently carry the Layer 2 control frames of customer. Spanning Tree Protocol bridge protocol data units or Cisco Discovery Protocol frames are carried separately from the service provider traffic and from other customer traffic in the network of service provider. User network interface ports of provider bridge interface with customer devices have specific set of requirements defined by the IEEE 802.1ad standard. These requirements enable provider bridges to have the same functionality as Layer 2 protocol tunneling and QinQ bridges. This document describes the IEEE 802.1ad implementation on Cisco devices using Layer 2 switch ports. Your software release may not support all the features documented in this module. For the latest caveats and feature information see Bug Search Tool and the release notes for your platform and software release. To find information about the features documented in this module and to see list of the releases in which each feature is supported see the feature information table. Use Cisco Feature Navigator to find information about platform support and Cisco software image support. Carrier Ethernet Configuration Guide Cisco IOS XE Release 3E Restrictions for IEEE 802.1ad Support on Provider Bridges The IEEE 802.1ad Support on Provider Bridges feature is not supported on the Cisco ME3400 series In Cisco IOS Release 12.2SE the Cisco ME 3400E and Catalyst 3750 Metro switch platforms support this feature. The Cisco ME3400 switch platform does not support this feature. Information About IEEE 802.1ad Support on Provider Bridges Provider bridges pass the network traffic of multiple customers. The traffic flow of each customer must be isolated from one another. For Layer 2 protocols within customer domains to function properly geographically separated customer sites must appear to be connected via LAN and the provider network must be transparent. The IEEE has reserved 33 Layer 2 MAC addresses for customer devices that operate Layer 2 protocols. If provider bridge uses these standard MAC addresses for its Layer 2 protocols the Layer 2 traffic of the customer Layer 2 protocol data unit tunneling when provider bridge component and provider edge The Sbridge component is capable of inserting or removing service provider VLAN for all traffic on particular port. IEEE 802.1ad adds new tag called Service tag to all ingress frames traveling from the customer to the service provider. Carrier Ethernet Configuration Guide Cisco IOS XE Release 3E Restrictions for IEEE 802.1ad Support on Provider Bridges The VLAN in the Stag is used for forwarding the traffic in the service provider network. Different customers use different SVLANs which results in isolation of traffic of each customer. In the Stag provider bridges do not understand the standard Ethertype. Hence they use an Ethertype value that is different from the standard 802.1Q Ethertype value. This difference makes customer traffic that is tagged with the standard Ethertype appear as untagged in the provider network. The customer traffic is tunneled in the port VLAN of the provider port. 802.1ad service provider user network interfaces and networknetwork interfaces implement the Sbridge component. For example VLAN tag has VLAN ID of 1 the Ctag Ethertype has value of 8100 0001 the Stag Ethertype has value of 88A8 0001 and the class of service has value of zero. 0x8100 Priority bits CFI CVLANID 0x88A8 Priority bits 0 SVLANID All customer VLANs that enter user network interface port in an Sbridge component receive the same service . CVLAN components are not supported but customer may want to tag particular CVLAN packet separately to differentiate between services. Provider bridges allow CVLAN packet tagging with provider edge bridge called the Cbridge component of the provider bridge. Cbridge components are CVLAN aware and can insert or remove CVLAN 802.1Q tag. The Cbridge UNI port is capable of identifying the customer 802.1Q tag and inserting or removing an Stag on the packet on perservice instance or CVLAN basis. CVLAN tagged service instance allows service instance selection and identification by CVLAN. The 801.1ad customer user network interfaces Layer 2 protocol data units of customers that are received by provider bridge are not forwarded. Hence Layer 2 protocols running at customer sites do not know the complete network topology. By using different set of addresses for the Layer 2 protocols running on provider bridges IEEE 802.1ad causes Layer forwards it on customer ports . Layer 2 protocols of customer device can then run transparently. The table below shows Layer 2 MAC addresses that are reserved for the CVLAN component. Carrier Ethernet Configuration Guide Cisco IOS XE Release 3E The table below shows Layer 2 MAC addresses that are reserved for the SVLAN component. These addresses are subset of the CVLAN component addresses and the Cbridge does not forward the bridge protocol data units of provider to customer network. Carrier Ethernet Configuration Guide Cisco IOS XE Release 3E The IEEE 802.1ad Support on Provider Bridges feature is implemented on switch ports and supports the Operation of individual provider bridges Configuration and management of individual provider bridges Management of spanning tree and VLAN topologies within provider network Layer 2 PDU Destination MAC Addresses for CustomerFacing CBridge UNI Ports The table below shows the Layer 2 protocol data unit destination MAC addresses for customerfacing Cbridge user network interface ports and how the frames are processed. Carrier Ethernet Configuration Guide Cisco IOS XE Release 3E Layer 2 PDU Destination MAC Addresses for CustomerFacing SBridge UNI Ports If port is operating as customerfacing Sbridge user network interface the destination MAC addresses shown in the below table are used for defining the Layer 2 protocol protocol data unit processing at the Sbridge UNI. Carrier Ethernet Configuration Guide Cisco IOS XE Release 3E Carrier Ethernet Configuration Guide Cisco IOS XE Release 3E Configuring Switch Port to Process 802.1ad BPDUs In an 802.1ad network the default behavior for Layer 2 protocol data units on an interface depends PDUs are tunneled. If the interface type is Cbridge UNI all Layer 2 PDUs are processed . PDU processing on the Sbridge UNI is the same as on an 802.1ad networknetwork interface . Both Carrier Ethernet Configuration Guide Cisco IOS XE Release 3E Processes or forwards Layer 2 bridge protocol data units BPDUs are processed. Carrier Ethernet Configuration Guide Cisco IOS XE Release 3E Configuring Switch Port to Process 802.1ad BPDUs Configuration Examples for IEEE 802.1ad Support on Provider Example Configuring an 802.1ad SBridge UNI 802.1ad Sbridge user network interface . In this example only Cisco Discovery Protocol protocol data units will be forwarded . Cisco Discovery Protocol PDUs are forwarded between the PE Example Configuring an 802.1ad CBridge UNI user network interface . In this example only Cisco Discovery Protocol protocol data units are Carrier Ethernet Configuration Guide Cisco IOS XE Release 3E Configuration Examples for IEEE 802.1ad Support on Provider Bridges The Cisco Support and Documentation website provides online resources to download documentation software and tools. Use these resources to install and Documentation website requires Cisco.com user ID Feature Information for IEEE 802.1ad Support on Provider Bridges The following table provides release information about the feature or features described in this module. This table lists only the software release that introduced support for given feature in given software release train. Unless noted otherwise subsequent releases of that software release train also support that feature. Use Cisco Feature Navigator to find information about platform support and Cisco software image support. Carrier Ethernet Configuration Guide Cisco IOS XE Release 3E VTPVLAN Trunk Protocol. Carrier Ethernet Configuration Guide Cisco IOS XE Release 3E

3 and 4 Protocols hile many of the concepts well known to traditional Layer 2 and Layer 3 net working still hold true in content switching applications the area introduces new and more complex themes that need to be well understood for any success ful implementation. Within the discussion of content networking we will replace terms such as our attention further up the OSI Seven Layer Model. Before we move into these new terms however lets look at some standard Layer 2 3 and 4 network Established in 1947 the International Organization for Standardization was formed to bring together the standards bodies from countries around the world. Their deﬁnition of the model for Open Systems Interconnection or OSI is used to deﬁne modes of interconnection between different components in networking system. This means that the physical method of transport can be designed independently of the protocols and applications running over it. For example TCPIP can be run over both Ethernet and FDDI networks and Novells IPX and Apples AppleTalk protocols can both be run over Token Ring networks. These are examples of having independence between the physical net work type and the upper layer protocols running across them. Consider also two TCPIPenabled end systems communicating across multitude of different OSI Seven Layer Model. When we talk about Layer 2 and Layer 3 networking it is these layers that were referring to and logically the further up the OSI model we move the greater intelligence we can use in networking decisions. Each layer plays its part in moving data from one device to another across net work infrastructure by providing standard interface to the surrounding layers. The top layer in the stack the Application layer is where the enduser applica tion resides. Think of the Application layer as the browser application or email client for user surﬁng the Web or sending email. Many protocols are deﬁned for use at the Application layer such as HTTP FTP SMTP and Telnet. In content switching terms Layer 7 refers to the ability to parse information directly generated by the user or application in decision making such as the URL typed by the user in the Web browser. For example The Presentation layer is used to provide common way for applications to translate between data formats or perform encryption and decryption. Mechanisms to convert between text formats such as ASCII and Unicode may be considered part of the Presentation layer along with compression techniques for image ﬁles such as GIF and JPEG. The Session layer coordinates multiple Presentation layer processes communi cating between end devices. The Session layer is used by applications at either end of the communication between end devices to tie together multiple Trans port layer sessions and provide synchronization between them. The HTTP protocol can use multiple TCP connections to retrieve objects The Transport layer is responsible for providing an identiﬁable and sometimes reliable transport mechanism between two communicating devices. User or application data having passed through the Presentation and Session layers will typically be sequenced and checked before being passed down to the Network layer for addressing. The Transport layer is the ﬁrst at which we see the concept of packets or data grams of information that will be transported across the network. TCP UDP and ICMP are examples of Layer 4 protocols used to provide delivery mechanism between end stations. It is also at this layer in the model that applications will be switching operates most commonly at this layer by using this information to distin guish between different applications and different users using the same application. of the packets within communi cation channel the Network layer is concerned with the This layer deﬁnes the addressing structure of the internetwork and how packets should be routed between end systems. The Network layer typically provides information about which Transport layer protocol is being used as well as local checksums to ensure data integrity. Internet Protocol and Internet Packet Exchange are examples of Network layer protocols. Traditional Internet routers operate at the Network layer by examining Layer 3 addressing information before making decision on where packet should be forwarded. Hardwarebased Layer 3 switches also use Layer 3 information in forwarding decisions. Layer 3 routers and switches are not concerned whether the packets contain HTTP FTP or SMTP data but simply where the packet is The Data Link layer also deﬁnes lower level addressing structure to be used between end systems as well as the lower level framing and checksums being used to transmit onto the physical medium. Ethernet Token Ring and Frame Relay are all examples of Data Link layer or Layer 2 protocols. Traditional Ethernet switches operate at the Data Link layer and are con cerned with forwarding packets based on the Layer 2 addressing scheme. Layer IPX or AppleTalk but only with where the MAC address of the recipient end As with all computer systems networking is ultimately about making moving and storing 1s and 0s. In networking terms the Physical layer deﬁnes how the users browser application data is turned into 1s and 0s to be transmitted onto the physical medium. The Physical layer deﬁnes the physical medium such as cabling and interface speciﬁcations. AUI 10BaseT and RJ45 are all examples Lets take an example of Web user visiting the Web site of Foocorp Inc. Within the browser application at the Application layer the user will type in . While this is the will provide the application there is much more information generated by the browser application itself including The type of browser being used is used in HTTP applications to ensure this successful packet delivery. Other applications will make use of different Transport layer protocols. TFTP for example uses the User Datagram Protocol as its Layer 4 transport because it does not require the guaranteed deliv OSPF RIP or BGP as their Layer 4 transport. At the Transport layer information about the port numbers sequence num Once the Transport layer information has been added to the head of the The Network layer will include information on the IP addresses of both the cli ent and the end system and reference to which Transport layer protocol has been used. The Network layer information is used to ensure the correct delivery path from the client to the end system and the ability for the receiver to identify which Transport layer process the frames should be forwarded to once they arrive. For the Web user example the Network layer information would look as For transmission across the local physical network the frame is then passed to the Data Link layer for the addition of the local physical addresses. In terms of Ethernet this would be the Ethernet Media Access Control address of the user machine and the MAC address of the default gateway router on the Ethernet network. The Layer 2 protocol such as Ethernet will also include reference to which Layer 3 protocol has been Now that weve seen examples of different information available within different layers of the OSI model lets look at how this information can be used to make intelligent trafﬁc forwarding decisions. Before the development of switching Ethernet relied on broadcast or ﬂooding of packets to all end stations within network to forward trafﬁc. Ethernet is effectively shared medium with only one Ethernet end station able to transmit at any time. Combine this with early implementation techniques relying on every end station in an Ethernet network The ﬁrst implementation of Ethernet or Layer 2 switching uses information in learn which ports have which end stations attached by recording the Ethernet MAC addresses of packets ingressing the switch. Using this information along switch need only forward frames out of ports where it knows the end station to be. For end station addresses that have not yet been learned frames with unknown destination MAC addresses are ﬂooded out of every port in the switch to force the recipient to reply. This will allow the switch to learn the relevant address on the reply frame. Layer 2 switching is implemented along side Layer 3 routing for local area networks to facilitate communication between devices in common IP subnet. As the information at this layer is relatively limited the opportunity to conﬁgure Layer 2 switches to interpret address information and act upon it in any way other than described previously is generally not required. Many Layer 2 switches will offer the ability to conﬁgure intelligent services such as Quality of Service bandwidth shaping or VLAN membership based on the Layer 2 infor tion that might be used to make switching decisions. Ethernet frames. While routing platforms exist for many different protocols in TCPIP terms router or routing make forwarding decision. The main advantage of Layer 3 routing in its earli est guises was that it gave the network designer the ability to segregate the net work into distinct IP networks and carefully control the trafﬁc and reachability Many of the early implementers and pioneers of Layer 3 routing devices used softwarebased devices as platforms that while offering ﬂexible platform for development of the technology often provided limitations in terms of perfor mance. As Layer 2 switching became more commonplace and the price per port of Ethernet switching systems dropped manufacturers looked to combine the performance of ASICbased Layer 2 switching with the functionality and ﬂexi bility of Layer 3 routing. Step forward the Layer 3 switch. Layer 3 switches work by examining the destination IP address and making forwarding decision based on the routing conﬁguration implemented. The destination subnet might be learned via connected interface static route or dynamic routing protocol such as RIP OSPF or BGP. In all instances once the Layer 3 switch has exam ined the frame and compared the destination IP address against the information in its routing database the destination MAC address is changed and the frame decremented to indicate to end stations and intermediaries that routing hop It is once we reach the Layer 3 switching environment that conﬁguration for devices become inherently more complex. The administrator must conﬁgure the correct routing information to enable basic trafﬁc ﬂow along with the interface IP addresses in each of the subnets to which the Layer 3 switch is attached. To appreciate the part that content switch plays in the lifecycle of user ses sion it is important to understand the component parts that make up such ses sion. Many protocols can be considered as Layer 4. Routing protocols such as OSPF proprietary ones such as EIGRP redundancy protocols such as the Vir tual Router Redundancy Protocol and host of others such as ICMP IGMP and IP itself can all be identiﬁed by unique protocol number in the IP The list of IP protocol numbers is administered and controlled by the Inter Some Layer 4 protocols effectively operate at this layer alone. VRRP for participating routers in an IP subnet and consequently has no need for upper layer protocol information. Its payload is simply the information contained at Layer 4. Other routing protocols such as the Border Gateway Protocol will use the reliable Layer 4 Transport layer protocol with the BGP routing protocols are TCP and UDP. The majority of the standard Application layer protocols are carried either within TCP or UDP depending on whether there is requirement for reliable endtoend connection. Taking Web user example the browser application needs to ensure that all packets are successfully delivered Different IP protocol numbers identify which Layer 4 protocol is being used. Transport Control Protocol therefore rely on TCP as its Transport layer protocol to guarantee delivery which in turn will use IP as its delivery mechanism. As the Layer 3 IP protocol is principally connectionless and bestefforts delivery mechanism there is requirement for many applications to ensure the correctly sequenced delivery of packets within conversation. Consequently many appli cations will use Transport Control Protocol at Layer 4 to guarantee success ful delivery. TCP has several characteristics built in to ensure this delivery end station uses this checksum to ensure that the packet arrived without Each octet of data sent and received by end stations has an associated sequence number associated with it. These sequence numbers are cumulative whereby certain packet delivery over IP. The TCP windowing technique allows two communicat by removing the need for each sequence of data to be individually acknowledged. In LANs where packet loss is usually minimal it is far more efﬁcient to allow the sender to transmit several frames of data Along with these mechanisms TCP must also be able to uniquely identify each conversation within an internetwork. Weve already seen the idea of TCP port number that is used among other things to identify the application process to the high OSI layers during the conversation. Within TCP conversation there are in fact two port numbers used one to identify the senders listening port and the other to identify the receivers listening port. Depending on the direction of each individual frame in the conversation these ports become either This combination of source and destination ports along with the Layer 3 IP addressing gives TCP the ability to uniquely identify each conversation or ses sion within an internetwork even in the case of the Internet itself. Lets put these concepts of addresses ports and sequencing numbers together and look at how conversation between two end stations is initiated sustained and terminated. Throughout the following example we will assume that the cli ent is PC initiating connection to Web server . Before initiating the session there are two pieces of information upon which the client must decide. First in order to identify the session uniquely between itself and the server it selects TCP port number to represent the session. This port will be the source port for packets from the client to the server and the destina tion port for packets from the server to the client. The client will select the source port sequentially on connectionbyconnection basis starting from value greater than 1024. Port numbers below 1024 are typically referred to as shows some wellknown reserved ports as deﬁned by IANA. The second element that needs to be decided by the client is the starting sequence number. This will be selected based on an internal 32bit clock that ensures both randomness and that sequence numbers will not overlap should lost packet reappear some time after its original transmission. Just as with the TCP ports used by both the client and server each side also uses its own sequence numbering to identify where within the session each frame ﬁts. Once the client has determined these two variables it is ready to send the ﬁrst packet of the session and initiate the connection to the server. Using TCP ﬂags the client will indicate to the server that it wants to initiate connection by set ting the SYN or synchronize ﬂag showing that this is the ﬁrst pack in the ses sion. In TCP terms this element is the ﬁrst packet in what is commonly referred to as the threeway handshake. This is simply because three packets are exchanged between the client and server to bring the TCP state into that which can transport data. Consequently no Application layer data is transmit ted until at least the fourth packet in the session concept which we will see has simpliﬁed representation of the threeway handshake to illustrate which side sends which of the packets when new connection is initiated. Taking this sequence packet by packet we can see the importance of the port and sequence numbers in ensuring the reliable transport between the client and server. The ﬁrst frame from the client to the server initiates the connection by setting the the client chooses random source port that will be used by the client to identify this session uniquely in cases where it has concurrent sessions to the same server. When the server replies both the SYN and ACK ﬂags are set in the TCP To ensure that each packet can be accounted for the server will set an acknowl edgment number that is equal to the last byte received from the client relative to the starting sequence number plus one. In our example the client started with sequence number of 713245119 and transmitted no user data meaning that the It is also important to notice the change in source and destination ports depending on which way particular packet is directed. In our example the cli ent sends on port 80 and listens on port 3086 whereas the server sends on port The ﬁnal packet exchanged during this handshake period is an acknowledg ment from the client to the server. This allows the client to correctly acknowl edge the sequence numbering used by the server in the previous packet and remove the SYN ﬂag being used to show the start of the session. Once this ﬁnal Transport Control Protocol packet of the handshake has been received both sides of the connection can move into the established state indicating that the transfer of user or application ing to match that initiated by the server and has also removed the SYN ﬂag in Once the connection has moved into the established state data transmission can begin between the two end points. During this state the ACK ﬂag is always set track the successful delivery of each segment of data. TCP also employs num ber of windowing and buffering techniques to ensure the optimal delivery retransmission and buffering of data during this state. The discussions of such Unlike the session initiation the termination of TCP connection can be initi ated from either side. Once one side of the connection decides that it has no the other side that it is ready to terminate the connection. In simple terms the receiving station will then acknowledge the FIN by setting the ACK ﬂag and set its own FIN ﬂag to show that it too is ready to terminate the connection. This series of exchanges results in both sides moving through the TIME WAIT state to the CLOSED state and the connection is closed. In some instances when the client receives the FIN it might still have data to FIN to show that the termination of the session can commence. During this period the initiator and recipient of the initial FIN are referred to as being in the FIN WAIT 2 and CLOSE WAIT states respectively. Some applications such User Datagram Protocol as Web browsers will often use this type of exchange to leave the connection in type of halfclosed state thereby allowing the connection to be brought back into use when needed without having to reinitiate the entire connection requesting name resolution from DNS server It is important to note that UDP trafﬁc can be both bidirectional such as the requestresponse nature of DNS queries and unidirectional such as alerts raised through the Simple Network Management Protocol . In both instances the nature of the application determines whether response is required UDP simply provides datagram format for the data between the two length and the checksum. It is clear from this that many of the techniques used by TCP are simply not present in UDP such as sequencing handshaking and As DNS is bidirectional requestresponse application the frame shown in ports are reversed as with TCP as the client sending the request will be listen Virtual Router Redundancy Protocol description is available in RFC 768 available on the IETF Web site. The Virtual Router Redundancy Protocol or VRRP is inextricably linked with the implementation of content switching not because it is used by user applica tions but because it provides mechanism to eliminate single points of failure within content switching topologies. VRRP provides mechanism to group two or more IP addresses typically representing routed interface and make them appear to all surrounding devices as single logical IP address. Many of the topologies described later in this book will show how multiple content switches and other routers can be deployed to ensure resilient and faulttolerant implementation. For this reason we need to examine the concepts Layer 2 and 3 Redundancy failure for clients on the network accessing the Internet the network adminis trator might consider deploying two Internet facing routers R1 and R2. The client PC on the network will have been conﬁgured with default route for example 10.10.10.2 pointing to router R1. This hardcoding of the default gateway IP address into the clients TCPIP settings presents the network administrator with two challenges when consider Router R1 might fail leaving the client with default gateway of an unreachable IP address. The client PC will resolve the IP address of the default gateway to the Ethernet address of router R1. This means that even if we replace the hardware of router R1 the client will still not have access to the Internet until its ARP cache has timed out or has been cleared. It is for these reasons that we need VRRP to provide resilience at both Layer 2 by providing virtual MAC address and at Layer 3 by providing virtual IP address. This virtualization of addresses amongst two or more physical units means that the client or client router will always have default gateway both in RFC 2338 deﬁnes the following component parts in network running VRRP VRRP router router running VRRP. It can participate in one or more virtual routers. Virtual router An abstract object managed by VRRP that acts as default router for hosts on shared LAN. It consists of virtual router identiﬁer and set of associated IP address across common LAN. VRRP router can back up one or more virtual routers. IP address owner The VRRP router that has the virtual routers IP address as real interface address. This is the router that when up will respond to packets addressed to one of these IP addresses for ICMP pings TCP connections and so forth. Other routers that do not have an IP interface equal to the virtual IP address are commonly referred to as an IP address renter. Primary IP address An IP address selected from the set of real interface addresses. One possible selection algorithm is to always select the ﬁrst Where did the MAC address. VRRP advertisements are always sent using the primary IP address as the source of the IP packet. Virtual router master The VRRP router that is assuming the responsi bility of forwarding packets sent to the IP address associated with the virtual router and answering ARP requests for these IP addresses. Note that if the IP address owner is available it will always become the master. Virtual router backup The set of VRRP routers available to assume for warding responsibility for virtual router should the current master fail. VRID Conﬁgured item in the range 1255 . There is no default. Priority Priority value to be used by this VRRP router in master elec tion for this virtual router. The value of 255 is reserved for the router that owns the IP addresses associated with the virtual router. The value of 0 is reserved for the master router to indicate that it is releasing responsibility for the virtual router. The range 1254 is available for VRRP routers backing up the virtual router. The default value is 100 . Lets take our previous example and expand it now to include VRRP on the two routers R1 and R2. Assuming that router R1 is conﬁgured with the IP address that matches the proposed VRRP address it will become the VRRP master and VRRP owner. Router R2 will become the VRRP backup. The IP address of router 1 10.10.10.1 is also conﬁgured to be the VRRP router address and this will be used by all clients on the network as default route to the Internet. If router R1 was to fail router R2 would take over while MAC address the following standard is used where the virtual router ID is used to make the last byte of the MAC address. In our example lets assume that VRID of 10 has been used giving us the VRRP with the new VRRP addressing. Now that we have all of the component parts in place lets look at how the rout ers operate together to provide resilient pair. VRRP uses advertisement mes sages between all participating routers to indicate the health and availability of the current virtual router master. These messages are exchanged using com mon multicast destination address of 224.0.0.18 and it is to this address that the current master router will continually advertise to indicate that it is still opera tional on the network. In our example topology during normal operation router R1 will continually advertise the virtual router ID the virtual router address and its priority inside the multicast frame. The source IP address on these advertisements will be the interface on router R1 along with source MAC address of the virtual MAC address we calculated earlier. The use of this virtual MAC address in these advertisements allows any Layer 2 infrastructure surrounding the VRRP rout erstypically Layer 2 switchesto source learn where the common MAC address is currently located. Now for the interesting part router failure. Lets imagine that router R1 experiences power failure and effectively disappears from the network. In this instance the following series of events would occur 1. The master router R1 would cease sending multicast packets advertis ing the virtual router. 2. After several missed packets the standby router R2 will acknowledge this occurrence by commencing with its own multicast advertisements. When it does it will use source MAC address of the VRRP virtual MAC address thus informing the attached Layer 2 switch that the MAC address has moved ports. 3. Since the virtual IP address and associated virtual MAC address have now survived the failure of router R1 the client will notice only minimal disruption during the reelection. This period is dependent on the con ﬁgurable parameters associated with the advertisement intervals but should typically be no more than 2 to 3 seconds. VRRP or variations on it is commonly implemented in many content switching platforms and as such it forms an important part of any implementa tion. More information about VRRP can be found in RFC 2338. Many books have been written on the TCPIP protocol stack and the higher layer applications such as HTTP and FTP that it supports. While it is outside standing the concept of user sessionbeing the total user experience of inter acting over period of time with resourceand how that maps down the OSI sevenlayer model and into the frames packets and TCP sessions below is key Understanding Application Layer Protocols well look at some of the Application

Usernetwork interface data link layer specification Postal address F06921 Sophia Antipolis CEDEX FRANCE Office address 650 Route des Lucioles Sophia Antipolis Valbonne FRANCE .400 cfr aatlas petsi ssecretariat Internet foregoing restriction extend to reproduction in all media. European Telecommunications Standards Institute 1991. All rights reserved. typographical or otherwise may occur. If you have comments concerning its accuracy please write to ISDN usernetwork interface data link layer General aspects.11 Data link layer representation as seen by layer 3.23 Pointtopoint data link connection endpoint services .23 Sequences of primitives at one pointtopoint data link Enhanced functional block diagram and block interaction diagram .27 ISDN usernetwork interface Data link layer specification .32 Frame structure for peertopeer communication .32 Definition of the peertopeer procedures of the data link layer .49 Acknowledged multiple frame information transfer.50 Procedures for unacknowledged information transfer.50 Terminal Endpoint Identifier management procedures.51 Action taken by the data link layer entity receiving the Operation of the TEI identity verify procedure .57 Automatic negotiation of data link layer parameters.60 Procedures for establishment and release of multiple frame operation .60 Maximum number of transmission of the TEI identity request message Annex An SDL representation of the broadcast procedures of the data Annex State transition table of the pointtopoint procedures of the data Definition of cell of the state transition table .111 Introduction.143 states and actions to be taken by the management entity.144 IV.4 Automatic notification of data link layer parameter values.150 This European Telecommunication Standard has been produced by the Signalling Protocols and Switching Technical Committee of the European Telecommunications Standard Institute and was adopted having passed through the ETSI standards approval procedure. according to the BLUE BOOK Vol. VI Fascicle VI.10. It is intended to reduce the number of options which exist within CCITT Recommendations .920I.440 and .921I.441 and to provide the additional specification text necessary for clarification as well as to ensure harmonisation of the ISDN usernetwork interface within Europe. All procedures at the ISDN usernetwork interface apply to both the and reference points. CCITT Recommendations .920I.440 and .921I.441 apply with the modifications specified below. The modifications are presented based on the CCITT Recommendations according to the BLUE BOOK Vol. VI Fascicle VI.10. The following editorial conventions have been applied the layout of this standard is aligned with CCITT Recommendations .920I.440 and .921I.441 according to the BLUE BOOK Vol. VI Fascicle VI.10 exept modifications which require additional In addition this standard is based on the following considerations data link layer there are number of options and points requiring further specification in Recommendations the harmonisation of the ISDN usernetwork interface is an important requirement for European European network operators who wish to provide ISDN services should apply the CCITT Recommendations .920I.440 and .921I.441 in accordance with the specification defined below. CEPT working groups. For practical reasons it has not always been possible to reedit the text appropriately at this stage. This standard specifies the usernetwork interface data link layer of the pan European Integrated Services Digital Network as provided by European public telecommunication operators at the reference point or coincident and reference point . PART 1 Application of CCITT Recomendation .920 ISDN usernetwork interface data link layer General aspects This Recommendation describes in general terms the Link Access Procedure on the Dchannel LAPD. The application of this protocol to other channel types is for further study. Details are provided in Recommendation .921 . The purpose of LAPD is to convey information between layer 3 entities across the ISDN usernetwork interface using the Dchannel. The definition of LAPD takes into consideration the principles and terminology of Recommendations .200 and .210 the reference model and layer service conventions for Recommendation .25 LAPB usernetwork interface for packet mode terminals and ISO 3309 and ISO 4335 Highlevel Data Link Control standards for frame structure and elements of procedures. LAPD is protocol that operates at the data link layer of the OSI architecture. The relationship between the data link layer and other protocol layers is defined in Recommendation .320 . The physical layer is currently defined in Recommendations .430 and .431 and prETS 300 012 and prETS 300 011 respectively and layer 3 is defined in Recommendations .930 .931 and ETS 300 1021 ETS for the complete definition of the protocols and procedures across the ISDN user network interface. The term data link layer is used in the main text of this Recommendation. However LAPD is independent of transmission bit rate. It requires duplex bit transparent The characteristics of the Dchannel are defined in Recommendation .412 . Recommendation .921. 4 summarises the services that the data link layer provides to layer 3 and the services that the data link layer requires from the physical layer. The basic structuring technique in the OSI reference model is layering. According to this technique communication among application processes is viewed as being logically partitioned into an ordered set of data link layer Service Access Point is the point at which the data link layer provides services to layer 3. Associatied with each data link layer SAP is one or more data link connection endpoint. See seen from layer 3 and by Data Link Connection Identifier as seen from data link layer. Entities exist in each layer. Entities in the same layer but in different systems which must exchange information to achieve common objective are called peer entities. Entities in adjacent layers interact through their common boundary. The services provided by the data link layer are the combination of the services and functions provided by both the data link layer and the physical layer. Cooperation between data link layer entities is governed by peertopeer protocol specific to the layer. In order for information to be exchanged between two or more layer 3 entities an association must be established between the layer 3 entities in the data link layer using data link layer protocol. This association is called data link connection. Data link connections are provided by the data link layer Data link layer message units are conveyed between data link layer entities by means of physical Layer 3 requests services from the data link layer via service primitives. The same applies for the interaction between the data link layer and the physical layer. The primitives represent in an abstract way the logical exchange of information and control between the data link layer and adjacent layers. They do not specify or constrain implementation. The primitives that are exchanged between the data link layer and adjacent layers are of the following four The same principle applies for data link layer physical layer interactions. The REQUEST primitive type is used when higher layer is requesting service from the next lower The INDICATION primitive type is used by layer providing service to notify the next higher layer of any specific activity which is service related. The INDICATION primitive may be the result of an activity of the lower layer related to the primitive type REQUEST at the peer entity. The RESPONSE primitive type is used by layer to acknowledge receipt from lower layer of the primitive type INDICATION. The CONFIRM primitive type is used by the layer providing the requested service to confirm that the activity has been completed. Layertolayer interactions are specified in Recommendation .921. Information is transferred in various types of message units between peer entities and between entities in adjacent layers that are attached to specific SAP. The message units are of two types The message units of the layer 3 peertopeer protocol are carried by the data link connection. The are never conveyed over data link connection or physical connection. the peertopeer protocol for the transfer of information and control between any pair of data link layer service access points and the interactions between the data link layer and layer 3 and between the data link layer and the The purpose of LAPD is to convey information between layer 3 entities across the ISDN usernetwork interface using the Dchannel. Specifically LAPD will support multiple terminal installations at the usernetwork interface multiple layer 3 entities. All data link layer messages are transmitted in frames which are delimited by flags. The frame structure is defined in Recommendation .921. LAPD includes functions for the provision of one or more data link connections on Dchannel. Discrimination between the data link connnections is by means of data link connection identifier contained in each frame frame delimiting alignment and transparency allowing recognition of sequence of bits transmitted over Dchannel as frame sequence control to maintain the sequential order of frames across data link connection detection of transmission format and operational errors on data link connection recovery from detected transmission format and operational errors notification to the management entitiy of unrecoverable errors and Data link layer functions provide the means for information transfer between multiple combinations of data link connection endpoints. The information transfer may be via pointtopoint data link connections or via broadcast data link connections. In the case of pointtopoint information transfer frame is directed to single endpoint while in the case of broadcast information transfer frame is directed to one or more Two types of operation of the data link layer are defined for layer 3 information transfer unacknowledged and acknowledged. They may coexist on single Dchannel. With this type of operation layer 3 information is transmitted in Unnumbered Information frames. At the data link layer the UI frames are not acknowledged. Even if transmission and format errors are detected no error recovery mechanism is defined. Flow control mechanisms are not defined. Unacknowledged operation is applicable for pointtopoint and broadcast information transfer that is UI frame may be sent to specific endpoint or broadcast to multiple endpoints associated with specific With this type of operation layer 3 information is transmitted in frames that are acknowledged at the data Error recovery procedures based on retransmission of unacknowledged frames are specified. In the case Flow control procedures are also defined. Acknowledged operation is applicable for pointtopoint information transfer. One form of acknowledged information transfer is defined multiple frame operation. Layer 3 information is sent in numbered Information frames. number of frames may be outstanding at the same time. Multiple frame operation is initiated by multiple frame establishment procedure using Set Asynchronous Balanced Mode Extended command. data link connection is identified by Data Link Connection Identifier carried in the address field The data link connection identifier is associated with connection endpoint identifier at the two ends of the The connection endpoint identifier is used to identify message units passed between the data link layer and layer 3. It consists of the SAPI and the Connection Endpoint Suffix . The DLCI consists of two elements the SAPI and the Terminal Endpoint Identifier . The SAPI is used to identify the service access point on the network side or the user side of the user network interface. The TEI is used to identify specific connection endpoint within service access point. The TEI is assigned by the network if the user equipment is of the automatic TEI assignment category or it is entered into the user equipment for example by the user or the manufacturer if the user equipment is of the nonautomatic TEI assignment category . The DLCI is pure data link layer concept. It will be internally used by the data link layer entitiy and is not known by the layer 3 entity or management entity. In these latter entities the concept of Connection Endpoint Identifier will be used instead. The CEI is composed of the SAPI information and reference value named CES. The CES is value selected by the layer 3 or management entity to address the data link layer entity. When the relevant TEI is known by this entity it will internally associate the DLCI to the CEI. The layer 3 and management entities will use this CEI to address its peer entity. pointtopoint data link entity may be in one of three basic states TEIunassigned state. In this state TEI has not been assigned. No layer 3 information transfer is TEIassigned state. In this state TEI has been assigned by means of the TEI assignment procedure. Unacknowledged information transfer is possible or multipleframeestablished state. This state is established by means of multiple frame establishment procedure. Acknowledged and unacknowledged information transfer are possible. For the detailed description of procedures in Recommendation .921 an expansion of the basic set of states listed above is required. broadcast data link entity is always in an information transfer state capable of only unacknowledged information transfer . ote 2 The selection of the SAPI and TEI values is based on The purpose of the TEI assignment procedure is to allow user equipment to obtain TEI value that the data link layer entities within the user equipment will use in subsequent communications over the data link The TEI value is typically common to all SAPs in user equipment. The procedure is conceptually located in the management entity. When TEI has been assigned the user equipment establishes an association between the TEI and CES in each SAP . In the network the corresponding association is made upon reception of the first frame containing the assigned TEI or at the time of TEI At that point in time data link layer peertopeer association has been formed. The association between the DLCI and CEI will be removed by the TEI removal procedures on request from the management entity when recognising that the TEI value is no longer valid. When in the TEIassigned state or the multipleframeestablished state the TEI check procedure may be disconnected from an installation. Optionally the user equipment may request the network to initiate the TEI check procedure. Examples of criteria for initiation of TEI assignment procedure the TEI check procedure and the TEI removal procedures are described in Recommendation .921. establishing and removing an association between the DLCI and CEI. Before pointtopoint acknowledged information transfer can start an exchange of SABME frame and an Unnumbered Acknowledgement frame must take place. The multiple frame establishment procedure is specified in detail in Recommendation .921. The data link layer provides services to layer 3 and management and utilises the services provided by the physical layer and layer management. formal description of the data link layer service provided to layer 3 and layer management is given in 4.2 and 4.3 respectively. The layer management service provided to the data link layer is given in 4.4. Communication between different layers in the OSI reference model makes use of primitives which are passed across the layer boundaries. The data link layer primitives defined in this Recommendation represent in an abstract way the logical exchange of information and control between the data link layer and adjacent layers. They do not specify nor constrain implementations. The specification of the interactions with layer 3 provides description of the services that the data link layer plus the physical layer offer to layer 3 as viewed from layer 3. Two forms of information transfer service are associated with layer 3. The first is based on unacknowledged information transfer at the data link layer while the second service is based on acknowledged information transfer at the data link layer. Layer 3 message units are handled according to their respective layer 2 priority . In this case the information transfer is not acknowledged at the data link layer. Acknowledgement procedures may be provided at higher layers. The information transfer is via broadcast or pointtopoint data link connections. The characteristics of the unacknowledged information transfer service are summarised in the following provision of data link connection between layer 3 entities for unacknowledged information transfer of layer 3 message units identification of data link connection endpoints and no verification of message arrival within the peer data link layer entity. The primitives asociated with the unacknowledged information transfer service are The DLUNIT DATAREQUEST primitive is used to request that message unit be sent using the procedures for unacknowledged information transfer service. The DLUNIT DATAINDICATION primitive indicates the arrival of message unit received by means of unacknowledged information transfer One mode of operation is defined multiple frame. The characteristics of the acknowledged information transfer service are summarised in the following provision of data link connection between layer 3 entities for acknowledged information transfer of identification of data link connection endpoints sequence integrity of data link layer message units in the absence of malfunctions notification to the peer entity in the case of errors for example loss of sequence notification to the management entity of unrecoverable errors detected by the data link layer and The primitives asociated with the acknowledged information transfer services are The DLDATAREQUEST primitive is used to request that message unit be sent using the procedures for the acknowledged information transfer service. The DLDATAINDICATION primitive indicates the arrival of message unit received by means of the acknowledged information These primitives are used respectively to request indicate and confirm the establishment of multiple frame operation between two service access points. These primitives are used respectively to request indicate and confirm an attempt to terminate multiple frame operation between two service access points. Only the unacknowledged information transfer service is provided to layer management in order that the data link layer management can commmunicate with its peer layer management. In this case the information transfer is not acknowledged at the data link layer. Acknowledgement procedures may be provided by layer management. The information transfer is via broadcast connections but in principle information transfer can also be via pointtopoint connections . The characteristics of the unacknowledged information transfer service are summarised in the following provision of data link connection between layer management entities for unacknowledged information transfer of data units identification of data link connection endpoints and no verification of message arrival within the peer data link layer entity. The primitives associated with the unacknowledged information transfer service provided for layer The MDLUNIT DATAREQUEST primitive is used to request that message unit be sent using the procedure for unacknowledged information transfer service for layer management. The MDLUNIT DATA INDICATION primitive indicates the arrival of message unit received by means of the unacknowledged The characteristics of the administrative services currently recognised are summarised in the following assignment checking and removal of TEI values and data link connection parameter passing . This service is currently not supported by European networks. These services are considered to be conceptually provided by layer management either on the user side or the network side. The method or describing these administrative functions uses service primitives. The primitives associated with these services are The MDLASSIGNINDICATION primitive is used to indicate to the layer management the need for TEI value. The MDLASSIGNREQUEST primitive is used to pass the TEI value from the layer management to the data link layer in order that the user data link layer entities can begin to communicate with the network data link layer entities. This primitive is used to convey layer management function request for removal of TEI value that has been previously asigned via the MDLASSIGN primitives. The ability of the data link layer to execute service request by layer 3 depends on the internal state of the data link layer. For each layer 3 entity the internal state of the data link layer is represanted by the state of entity to invoke service. Consequently the data link service may be defined by means of data link connection endpoint states whereby the capabilities provided by the data link layer and the service primitives may be related to these In order to allow data link service user to invoke service making use of primitives the DLprimitives defined in Recommendation .921 have to be related to pointtopoint data link connections andor broadcast data link connections An unconfirmed service is defined as service which does not result in an explicit confirmation. confirmed service is defined as service which results in an explicit confirmation from the service provider. There is not necessarily any relationship to response from the peer serviceuser. The states of data link connection endpoint may be derived from the internal states of the data link layer entity supporting this type of data link connection. broadcast data link connection provides an unacknowledged information transfer service. Within each data link service access point there is only one broadcast data link connection endpoint. The broadcast data link connection endpoint is always in the information transfer state. Pointtopoint data link connection endpoint services pointtopoint data link connection provides both an unacknowledged and acknowledged information transfer service. Within each data link service access point one or more than one data link connection endpoint may be present each identified by CES. The acknowledged information transfer service in addition implies the presence or the services link establishment link reestablishment and link release. The pointtopoint data link connection endpoint states are link connection established state. Sequences of primitives at one pointtopoint data link connection endpoint The primitives provide the procedural means to specify conceptually how data link service user can invoke service. are related to the states at one pointtopoint data link connection endpoint. The possible overall sequences of primitives at pointtopoint data link connection endpoint are defined established states are stable states whilst the awaiting establish and awaiting release states are transition The services provided by the physical layer are described in detail in Recommendation .430 or .431 and prETS 300 012 or prETS 300 011 respectively. They are summarised in the following physical layer connection for the transparent transmission of bits in the same order in which they are submitted to the physical layer transmission of data link layer message units according to their respective data link layer priority. Some of the above services may be implemented in the management entity on the user side or network side. The method of describing these services is by means of service primitives. The primitives between the data link layer and the physical layer are These primitives are used to request that message unit be sent and to indicate the arrival of These primitives are used to request activation of the physical layer connection and to indicate that the physical layer connection has been activated. This primitive is used to indicate that the physical layer connection has been deactivated. The primitives between management entities and the physical layer are This primitive is used to indicate that the physical layer connection has been activated. These primitives are used to request deactivation of the physical layer connection and to indicate that the physical layer connection has been deactivated. The REQUEST is used at the network side This primitive is used to indicate to user management entities information regarding the physical layer condition. Two parameters are defined connected and disconnected. Data link layer Management structure illustrative purposes only and does not constrain implementations. The Layer Management Entity provides for the management of resources that have layerwide impact. Access to the LME is provided by means of specific SAPI. Functions provided by the LME are The Connection Management Entity provides for the management of resources that have an impact on individual connections. Selection of the CME is based on specific data link layer frame type not used in the acknowledged or unacknowledged information transfer services. Functions provided by State transition diagram for sequences of primitives at pointtopoint data link connection as Note The broadcast links for SAPs other than SAP63 are not shown This procedure analyses the control field of the received frame and provides appropriate peertopeer responses and layertolayer indications. In addition it analyses the data link layer service primitives and transmits the appropriate peertopeer commands and responses. This procedure analyses the flag Frame Check Sequence and address octets of received frame. If the frame is correct it distributes the frame to the appropriate data link procedures block based on the DLCI . On frame transmission this procedure may provide data link layer contention resolution between the various data link procedure blocks. The contention resolution is based on the SAPI giving priority to SAPI Structure of the data link procedure several functional blocks for pointtopoint and broadcast connections. Additions for European networks identifies the layer management and the two procedural types data link procedures and multiplex procedure. The data link procedures comprise broadcast link procedures and pointtopoint link procedures the layer management the layer management entity and the connection management The data link layer structure represents the framework for the specification of various protocols related to the data link layer and to define the relationship between the data link layer and adjacent layers such as layer 3 the physical layer and management entities. The interactions between the data link layer and adjacent layers are modelled by service primitives. The functional partitioning of the data link layer into data link procedures and multiplex procedure implies internal signals which support the communication between these functional blocks. Enhanced functional block diagram and block interaction diagram CCITT Recommendation .920 . The data link layer entity is structured into two main functional blocks multiplexing and peertopeer The multiplex procedure maps all the data link connections to one physical Dchannel connection. The multiplex procedure represents the user of the physical Dchannel connection on behalf of the various data link connections thus has to invoke the physical layer to provide its services if there is at least one data link connection to be supported. The function activation is conceptually included in the multiplex The peertopeer procedures take place as result of interactions between adjacent entities. The layer management entity provides administrative services globally to the data link layer entities such as TEI management. The connection management entity provides administrative services to each of the data link layer entities. block interaction diagram relates the service primitives to these functional blocks which have to interact communication between pointtopoint link procedures or broadcast link procedures respectively and the multiplex procedure. Recommendation. Other additional primitives may be defined in other Recommendations eg. dealing with CCITT Recommendation .921 ISDN usernetwork interface Data link layer specification . CCITT Recommendation .200 Reference model of open systems interconnection for CCITT applications . CCITT Recommendation .210 Open systems interconnection layer service definition conventions . CCITT Recommendation .25 Interface between data terminal equipment and data circuitterminating equipment for terminals operating in the packet mode and connected to public data networks by dedicated circuit . ISO 3309 Data communication Highlevel data link control procedures Frame structure. ISO 4335 Data communication Highlevel data link control procedures Consolidation of elements of procedures. CCITT Recommendation .320 ISDN protocol reference model . specification . specification . CCITT Recommendation .930 ISDN usernetwork interface layer 3 General aspects . specification for basic call control . CCITT Recommendation .412 ISDN usernetwork interfaces Interface structures and access capabilities . CCITT Recommendation .411 ISDN usernetwork interfaces Reference configurations . prETS 300 012 March 1990 Integrated Services Digital network Basic usernetwork interface layer 1 specification and test principles. prETS 300 011 March 1990 Integrated Services Digital network Primary rate usernetwork interface layer 1 specification and test ETS 300 1021 Integrated Services Didital Network User network interface layer 3 Specifications for basic call control Application of Description Language diagrams Application of CCITT Recommendation ISDN usernetwork interface Data link layer specification This Recommendation specifies the frame structure elements of procedure format of fields and procedures for the proper operation of the Link Access Procedure on the Dchannel LAPD. with other Recommendations are described in general terms in Recommendation .920 . As stated in Recommendation .920 the term data link layer is used in the layer 2 and L2 are used as abbreviations. Furthermore in accordance with ETS 300 1022 the term layer 3 is used to indicate the layer above the data link All reference within this document to layer management entity andor connection management entity refer to those entities at the data link layer. Frame structure for peertopeer communication and format for frames containing an information field. bits and one 0 bit. The flag preceding the address field is defined as the opening flag. The flag following the Frame Check Sequence field is defined as the closing flag. The closing flag may also serve as the opening flag of the next frame in some applications. However all receivers must be able to Recommendations .430 and .431 and prETS 300 012 and prETS 300 011 respectively for intended receiver of command frame and the transmitter of response frame. The format of the address field is defined in 3.2. single octet address field is reserved for LAPB operation in order to allow single LAPB data link connection to be multiplexed along with LAPD data link connections. The support of LAPB data link connection within the Dchannel is optional at both the Multple Frame operation Modulo 128 two octets for frames with seqiuence numbers each with control field of one or two octets depending upon the type of operation being used. The format of the control field is defined in 3.4. The information field of frame when present follows the control field and precedes integer number of octets. The maximum number of octets in the information field is defined in 5.9.3. transmitting data link layer entity shall examine the frame content between the opening and closing frag sequences and shall insert 0 bit after all sequences of five contiguous 1 bits to ensure that flag or an abort sequence is The FCS field shall be sixteenbit sequence. It shall be the ones complement of the sum of the remainder of divided by the generator polynomial x16 x12 x5 1 where is the the first bit of the FCS excluding bits inserted for transparency and the remainder of the division by the generator polynomial x16 x12 x5 1 of the product and the first bit of the FCS excluding bits inserted for transparency. As typical implementation at the transmitter the initial content of the register of the device computing the remainder of the division is preset to all 1s and is then modified by division by the generator polynomial on the address control and information fields the ones complement of the resulting remainder is transmitted as the sixteenbit FCS sequence. As typical implementation at the receiver the initial content of the register of the device computing the by the generator polynomial x16 x12 x5 1 of the serial incoming protected bits and the FCS will be 0001 1101 0000 1111 in the absence of transmission errors. Numbering Convention into octets. The bits of an octet are shown horizontally and are numbered from 1 to 8. Multiple octets are The octets are transmitted in ascending numerical order inside an octet bit 1 is the first bit to be When field is contained within single octet the lowest bit number of the field represents the lowest When field spans more than one octet the order of bit values within each octet progressively decreases as the octet number increases. The lowest bit number associated with the field represents the lowest For example bit number can be identified as couple where is the octet number and is the The high order bit of the field is mapped on bit and the low order bit is mapped on bit . An exception to the preceding field mapping convention is the data link layer FCS field which spans two octets. In this case bit 1 of the first octet is the high order bit and bit 8 of the second octet is the low order is not properly bounded by two flags or octets between flags of frames that do not contain sequence numbers or does not consist of an integral number of octets prior to zero bit insertion or following zero bit contains single octet address field or contains service access point identifier which is not supported by the receiver. Invalid frames shall be discarded without notification to the sender. No action is taken as the result of that Receipt or seven or more contiguous 1 bits shall be interpreted as an abort and the data link layer shall ignore the frame currently being received. Elements of procedures and formats of fields for data link layer peerto The elements of procedures define the commands and responses that are used on the data link connections carried on the Dchannel. Procedures are derived from these elements of procedures and are described in 5. Address field format commandresponse indication bit data link layer Service Access Point Identifier subfield and The address field range is extended by reserving the first transmitted bit of the address field octets to have bit 1 of the first octet set to 0 and bit 1 of the second octet set to 1. The CR bit identifies frame as either command or response. The user side shall send commands with the CR bit set to 0 and responses with the CR bit set to 1. The network side shall do the opposite that is commands are sent with CR set to 1 and responses are sent with CR set to 0. The combinations In conformance with HDLC rules commands use the address of the peer data link layer entity while responses use the address of the own data link layer entity. According to these rules both peer entities on pointtopoint data link connection use the same Data Link Connection Identifier composed of The SAPI identifies point at which data link layer services are provided by data link layer entity to layer 3 or management entity. Consequently the SAPI specifies data link layer entity that should process data link layer frame and also layer 3 or management entity which is to receive information of the address field octet containing the SAPI is the least significant binary digit and bit 8 is the most Reserved for packet mode communications using .931 call control procedures Packet communication conforming to .25 level 3 procedures The reservation of SAPI values for experimental purposes is for further study. The TEI for pointtopoint data link connection may be associated with single Terminal Equipment . TE may contain one or more pointtopoint TEIs. The TEI for broadcast data link connection is is the most significant binary digit. The following conventions shall apply in the assignment of these The TEI subfield bit pattern 111 1111 is defined as the group TEI. The group TEI is assigned to the broadcast data link connection associated with the addressed Service Access Point . The remaining TEI values are used for the pointtopoint data link connections associated with the For further information regarding pointtopoint situations see Annex . The control field identifies the type of frame which will be either command or response. The control field will contain sequence numbers where applicable. Three types of control field formats are specified numbered information transfer supervisory functions and unnumbered information transfers and control functions . The control The format shall be used to perform an information transfer between layer 3 entities. The functions of and are independent that is each frame has an sequence number an sequence number which may or may not acknowledge additional frames received by the data link layer entity and bit that may be set to 0 or 1. The format shall be used to perform data link supervisory control functions such as acknowledge frames request retransmission of frames and request temporary suspension of transmission of frames. The functions of and PF are independent that is each supervisory frame has an sequence number which may or may not acknowledge additional frames received by the data link layer The format shall be used to provide additional data link control functions and unnumbered information transfers for unacknowledged information transfer. This format does not contain sequence numbers. It does include PF bit that may be set to 0 or 1. Control field parameters and associated state variables of the bits within these parameters is such that the lowest numbered bit within the parameter field is the least significant bit. response frames. In command frames the PF bit is referred to as the bit. In response frames it is referred to as the bit. The bit set to 1 is used by data link layer entity to solicit response frame from the peer data link layer entity. The bit set to 1 is used by data link layer entity to indicate the response frame transmitted as result of soliciting command. The use of the PF bit is described in 5. Multiple frame operation variables and sequence numbers Each frame is sequentially numbered and may have the value 0 through minus 1 . The modulus equals 128 and the sequence numbers cycle through the entire range 0 through 127. All arithmetic operations on state variables and sequence numbers contained in this Recommendation are affected by the modulus operation. Each pointtopoint data link connection endpoint shall have an associated when using frame commands. denotes the sequence number of the next frame to be transmitted. can take on the value 0 through minus 1. The value of shall be incremented by 1 with each successive frame transmission and shall not exceed by more that the maximum number of outstanding frames . Each pointtopoint data link connection endpoint shall have an associated when using frame commands and supervisory frame commandsresponses. identifies the last frame that has been acknowledged by its peer 1 equals the of the last acknowledged frame. can take on the values received from its peer . valid value is one that is in the range Only frames contain the send sequence number of transmitted frames. At the time that an in sequence frame is designated for transmission the value of is set equal to . Each pointtopoint data link connection endpoint shall have an associated when using frame commands and supervisory frame commandresponses. denotes the sequence number of the next insequence frame expected to be received. can take on the value 0 through minus 1. The value of shall be incremented by one with the receipt of an error free insequence frame whose All frames and supervisory frames contain the expected send sequence number of the next received frame. At the time that frame of the above types is designated for transmission the value of is set equal to . indicates that the data link layer entity transmitting the has correctly received all frames numbered up to and including 1. Unacknowledged operation variables and parameters No variables are defined. One parameter is defined N201 . The following commands and responses are used by either the user or the network data link layer entities and responses for each application implemented. The frame types associated with each of the two Frame types associated with an application not implemented shall be discarded and no action shall be taken as result of that frame. For purposes of the LAPD procedures in each application those frame types not identified in table 5Q.921 are identified as undefined command andor response control fields. The actions to be taken are specified in 5.8.5. Use of the XID frame other than for parameter negotiation procedures is for The function of the information command is to transfer across data link connection sequentially numbered frames containing information fields provided by layer 3. This command is used in the multiple frame operation on pointtopoint data link connections. Set asynchronous balanced mode extended command The SABME unnumbered command is used to place the addressed user side or network side into modulo 128 multiple frame acknowledged operation. No information field is permitted with the SABME command. data link layer entity confirms acceptance of an SABME command by the transmission at the first opportunity of UA response. Upon acceptance of this command the data link layer entitys and are set to 0. The transmission of an SABME command indicates the clearance of all exception conditions. Previously transmitted frames that are unacknowledged when this command is processed remain unacknowledged and are discarded. It is the responsibility of higher level or the The DISC unnumbered command is used to terminate the multiple frame operation. No information field is permitted with the DISC command. The data link layer entity receiving the DISC command confirms the acceptance of DISC command by the transmission of UA response. The data link layer entity sending the DISC command terminates the multiple frame operation when it receives the acknowledging UA or DM response. Previously transmitted frames that are unacknowledged when this command is processed remain unacknowledged and are discarded. It is the responsibility of higher level or the When layer 3 or management entity requests unacknowledged information transfer the UI unnumbered command is used to send information to its peer without affecting data link layer variables. UI command frames do not carry sequence number and therefore the UI frame may be lost without notification. The RR supervisory frame is used by data link layer entity to indicate it is ready to receive an frame acknowledge previously received frames numbered up to and including N1 clear busy condition that was indicated by the earlier transmission of an RNR frame by that same The REJ supervisory frame is used by data link layer entity to request retransmission of frames starting with the frame numbered . The value of in the REJ frame acknowledges frames numbered up to and including N1. New frames pending initial transmission shall be transmitted following the retransmitted frame. Only one REJ exception condition for given direction of information transfer is established at time. The REJ exception condition is cleared upon the receipt of an frame with an equal to the of is not used by European networks. The transmission of an REJ frame shall also indicate the clearance of any busy condition within the The RNR supervisory frame is used by data link layer entity to indicate busy condition that is temporary inability to accept additional incoming frames. The value of in the RNR frame acknowledges frames numbered up to and including N1. The UA unnumbered response is used by data link layer entity to acknowledge the receipt and acceptance of the modesetting commands . Received modesetting commands are not processed until the UA response is transmitted. No information field is permitted with the UA response. the earlier transmission of an RNR frame by that same data link layer entity. Disconnected Mode response layer is in state such that multiple frame operation cannot be performed. No information field is Frame reject response condition not recoverable by retransmission of the identical frame that is at least one of the following error conditions resulting from the receipt of valid frame the receipt of command or response control field that is undefined or not implemented the receipt of supervisory or unnumbered frame with the incorrect length the receipt of an invalid or the receipt of frame with an information field which exceeds the maximum established length. valid value is one that is in the range VNV. operation is returned with this response and provides the reason for the FRMR response. This Rejected frame control field is the control field of the received frame which caused the frame reject. When the rejected frame is an unnumbered frame the control field of the rejected frame is positioned in octet 5 with octet 6 set to 0000 0000. rejection condition. CR is set to 1 if the frame rejected was response and is set to 0 if the frame rejected was rejection condition. set to 1 indicates that the control field received and returned in octets 5 and 6 was undefined or not implemented. set to 1 indicates that the control field received and returned in octets 5 and 6 was considered invalid because the frame contained an information field which is not permitted with this frame or is supervisory or unnumbered frame with incorrect length. Bit must be set to 1 in conjunction with this bit. set to 1 indicates that the information field received exceeded the maximum established set to 1 indicates that the control field received and returned in octets 5 and 6 contained an The XID frame may contain an information field in which the identification information is conveyed. The exchange of XID frames is compelled arrangement used in connection management . No sequence numbers are contained within the control field. The information field is not mandatory. However if valid XID command contains an information field and containing an information field. If the information field cannot be interpreted by the receiving entity or length information field. The maximum length of the information field must conform to the value N201. Sending or receiving an XID frame shall have no effect on the operational mode or state variables Communications between layer and for this Recommendation between the data link layer and the layer management are accomplished by means of primitives. Primitives represent in an abstract way the logical exchange of information and control between the data link and adjacent layers. They do not specify or constrain implementations. Primitives consist of commands and their respective responses associated with the services requested of lower layer. The general syntax of primitive is Where XX designates the interface across which the primitive flows. For this Recommendation XX is DL for communication between layer 3 and the data link layer PH for communication between the data link layer and the physical layer MDL for communication between the layer management and the data link layer or MPH for communication between the management entity and the physical layer. defined in this Recommendation. Note that not all primitives have associated parameters. The primitive generic names that are defined in this Recommendation are The DLESTABLISH primitives are used to request indicate and confirm the outcome of the procedures for establishing multiple frame operation. The DLRELEASE primitives are used to request indicate and confirm the outcome of the procedures for establishment attempt. The DLDATA primitives are used to request and indicate layer 3 messages which are to be transmitted or have been received by the data link layer using the acknowledged information transfer service. The DLUNIT DATA primitives are used to request and indicate layer 3 messages which are to be transmitted or have been received by the data link layer using the unacknowledged information transfer The MDLASSIGN primitives are used by the layer management entity to request that the data link layer associate the TEI value contained within the message portion of the primitive with the specified Connection Endpoint Suffix across all SAPIs. The MDLASSIGN primitive is used by the data link layer to indicate to the layer management entity the need for TEI value to be associated with the CES specified in the primitive message unit. The MDLREMOVE primitives are used by the layer management entity to request that the data link layer remove the association of the specified TEI value with the specified CES across all SAPIs. The TEI and CES are specified by the MDLREMOVE primitive message unit. The MDLERROR primitives are used to indicate to the connection management entity that an error has occurred associated with previous management function request or detected as result of communication with the data link layer peer entity. The layer management entity may respond with an MDLERROR primitive if the layer management entity cannot obtain TEI value. The MDLUNIT DATA primitives are used to request and indicate layer management entity messages which are to be transmitted or have been received by the data link layer using the unacknowledged information transfer service. The MDLXID primitives are used by the connection management entity to request indicate respond and confirm the outcome of the actions used in the XID procedures. The PHDATA primitives are used to request and indicate message units containing frames used for data link layer peertopeer communications passed to and from the physical layer. The PHACTIVATE primitives are used to request activation of the physical layer connection or to indicate that the physical layer connection has been activated. The PHDEACTIVATE primitive is used to indicate that the physical layer connection has been The MPHACTIVATE primitive is used to indicate that the physical layer connection has been activated. The MPHDEACTIVATE primitives are used to request deactivation of the physical layer connection or to indicate that the physical layer connection has been deactivated. The REQUEST is for use by the network side system management entity. The MPHINFORMATION primitive is for use by the user side management entity and provides an indication as to whether the terminal is disconnected or unable to provide sufficient power to support the TEI management procedures. The primitive types defined in this Recommendation are The REQUEST primitive type is used when higher layer or layer management is requesting service from the lower layer. The INDICATION primitive type is used by layer providing service to inform the higher layer or layer The RESPONSE primitive type is used by layer management as consequence of the INDICATION The CONFIRM primitive type is used by the layer providing the requested service to confirm that the Since several SAPs may exist on the network side or user side protocol messages units sent by one SAP may contend with those of other service access points for the physical resources available for message transfer. The priority indicator is used to determine which message unit will have greater priority when contention exists. The priority indicator is only needed at the user side for distinguishing message units sent by the SAP with SAPI value of 0 from all other message units. The message unit contains additional layertolayer information concerning actions and results associated with requests. In the case of the DATA primitves the message unit contains the requesting layer peerto peer messages. For example the DLDATA message unit contains layer 3 information. The PHDATA message unit contains the data link layer frame. The operations across the data link layerlayer 3 boundary shall be such that the layer sending the DLDATA or DLUNIT DATA primitve can assume temporal order of the bits within the message unit and that the layer receiving the primitive can reconstruct Primitive procedures specify the interactions between adjacent layers to invoke and provide service. The service primitives represent the elements of the procedures. In the scope of this Recommendation the interactions between layer 3 and the data link layer are The states of data link connection endpoint may be derived from the internal states of the data link layer entity supporting this type of data link connection. Data link connection endpoint states are defined as follows link connection established state. The primitives provide the procedural means to specify conceptually how data link service user can invoke service. are related to the states at one pointtopoint data link connection endpoint. The possible overall sequences of primitives at pointtopoint data link connection endpoint are defined established states are stable states whilst the awaiting establish and awaiting release states are transition The model illustrates the behaviour of layer 2 as seen by layer 3. This model assumes that the primitives passed between layers are implemented by first in first out queue. In this model collisions of REQUEST and INDICATION primitives can occur thereby illustrating actions that seems to be in conflict with the actual layer 2 protocol description. In some implementations these collisions could occur. State transition diagram for sequences of primitives at pointtopoint data link connection case of data link layer initiated or peer system initiated reestablishment DL RELEASECONFIRM or DLRELEASEINDICATION this indicates the discard of all the data link service data units representing DLDATAREQUESTs. This primitive notifies layer 3 of link reestablishment. This primitive will occur if DLRELEASEREQUEST collides with DLRELEASE This primitive will occur if DLESTABLISHREQUEST collides with DL This primitive will occur if DLRELEASEREQUEST collides with DLESTABLISH This primitive will occur if DLESTABLISHREQUEST collides with DLRELEASEINDICATION. Since this DLRELEASEINDICATION is not related to the DLESTABLISHREQUEST the also occur if establishment was initiated upon receipt of an unsolicited DM response with the bit set to 0. This primitive will occur as result of multiple collisions of primitives. If first DL ESTABLISHREQUEST collides with DLRELEASEINDICATION the data link layer ESTABLISHCONFIRM would 3 is not aware that the DLRELEASEINDICATION was not related to the first DL ESTABLISHREQUEST. Since layer 3 relates this DLESTABLISHCONFIRM to the subsequent DLESTABLISHREQUEST it assumes the data link layer in the link again DLESTABLISHCONFIRM. This primitive will occur if DLESTABLISHREQUEST collides with DLRELEASEINDICATION. Since this DLRELEASEINDICATION is not related to the DLESTABLISHREQUEST the This primitive will occur as result of multiple collisions of primitives. If first DL ESTABLISHREQUEST collides with DLRELEASEINDICATION the data link layer ESTABLISHCONFIRM may collide with subsequent DLESTABLISHREQUEST CONFIRM . This second DLESTABLISHCONFIRM may collide with subsequent DLRELEASE relates this first DLESTABLISHCONFIRM to the subsequent DLESTABLISH REQUEST it assumes the data link layer in the link connection established state but CONFIRM . Definition of the peertopeer procedures of the data link layer The elements of procedure which apply are An FRMRresponse shall not be generated by data link layer entity. However on receipt of this frame actions according to this specification have to be taken. for connection management entity information transfer For unacknowledged information transfer the PF bit is not used and shall be set to 0. data link layer entity receiving an SABME DISC RR RNR REJ or frame with the bit set to 1 shall LAPB data link layer entity may transmit an FRMR or DM response with the bit set to 1 in response to an frame or supervisory command with the bit set to 1. Procedures for unacknowledged information transfer The procedure which apply to the transmission of information in unacknowledged operation are defined No data link layer error recovery procedures are defined for unacknowledged operation. The term transmission of UI frame refers to the delivery of UI frame by the data link layer to the physical layer. Unacknowledged information is passed to the data link layer by layer 3 or management entities using the primitives DLUNIT DATAREQUEST or MDLUNIT DATAREQUEST respectively. The layer 3 or management message unit shall be transmitted in UI command frame. 1111 the group value. For pointtopoint operation the appropriate TEI value shall be used. The bit shall be set to 0. In the case of persistent layer 1 deactivation the data link layer will be informed by an appropriate indication. Upon receipt of this indication all UI transmission queues shall be discarded. At the network side the system management entity provides that the PHDEACTIVATEINDICATION primitive will be PHDEACTIVATEINDICATION primitive depend on the implementation of the physical layer. The network side system management deactivation procedures should ensure that layer 1 is not deactivated before all UI data transfer is completed. Receipt of unacknowledged information of the information field shall be passed to the layer 3 or management entity using the data link layer to layer 3 primitive DLUNIT DATAINDICATION or the data link layer to management primitive MDLUNIT DATAINDICATION respectively. Otherwise the UI command frame shall be discarded. Terminal Endpoint Identifier management procedures TEI management is based on the following procedural means user equipment in the TEIunassigned state shall use the TEI assignment procedures to enter the TEI assigned state. Conceptually these procedures exist in the layer management entity. The layer management entity on the network side is referred to as the Assignment Source Point in this The purpose of these procedures is to allow automatic TEI equipment to request the network to assign TEI value that the data link layer entities within the requesting user equipment will use in their subsequent communications allow network to remove previously assigned TEI value from specific or all user equipments whether multipleTEI assignment has occurred. allow user equipment the option to request that the network invoke TEI check procedures. The user side layer management entity shall instruct the user data link layer entities to remove all TEI values when it is notified that the terminal is disconnected at the interface . The layer management entity shall use the MDLREMOVEREQUEST primitive for these purposes. 5.3.4.1 includes the actions taken by data link layer entity receiving an MDLREMOVEREQUEST Typically one TEI value would be used by the user equipment . If required number of TEI values may be requested by multiple use of the procedures defined in 5.3.2. It shall be the responsibility of the user to maintain the association between TEI and SAPI values. The initiation of TEI assignment procedures occurs on the receipt of request for establishment or unacknowledged information transfer while in the TEIunassigned state. The data link layer entity shall inform the layer management entity using the MDLASSIGNINDICATION primitive. Alternatively the user side layer management entity may initiate the TEI assignment procedures for its own reasons. In the case of initialisation from no power condition the user equipment should postpone the start of the TEI assignment procedure until layer 2 service that needs TEI is to be provided. All layer management entity messages used for these TEI management procedures are transmitted to or received from the data link layer entity using the MDLUNIT DATAREQUEST primitive or the MDLUNIT DATAINDICATION primitive respectively. The data link layer entity shall transmit management entity messages in UI command frames. The SAPI value shall be 63. The TEI value shall be 127. If the user equipment is of the nonautomatic TEI assignment category the user side layer management entity shall deliver the TEI value to be used to the data link layer entity via the MDLASSIGNREQUEST If the user equipment is of the automatic TEI assignment category upon initiation of the TEI assignment procedure the user side layer management entity shall transmit to its peer message containing the Action indicator . The reference number Ri shall be used to differentiate between number of user equipments which may simultaneously request assignment of TEI value. The Ri shall be 2 octets in length and shall be randomly generated for each request message by the user equipments. All values in the range 0 to 65535 shall be available from the random number generator. The design of the random number generator should minimise the probability of identical reference numbers being generated by terminals which initiate their TEI assignment procedures simultaneously. However there exists small probability that double assignment will occur. Possible procedures to resolve this problem are listed in The singleoctet action indicator Ai shall be used to indicate request to the ASP for the assignment of any TEI value available. The coding of the Ai shall be Ai Group address TEI 127. This Ai value requests the ASP to assign any timer T202 shall be started. The ASP on receipt of the identity request message shall either select TEI value deny identity requests with Ai values in the range 64 126 and ignore identity requests with the Ai value in the range 0 63 or ignore the identity request message if previous identity request message that contains an identical TEI value to either request. Selection of TEI value shall be on the basis of information stored at the ASP. This may consist of map of the full range of automatic TEI values or The ASP after having selected the TEI value shall inform the network data link layer entities by means of the MDLASSIGNREQUEST primitive and transmit to its peer message containing the following the assigned TEI value in the Ai field. If the available TEI informationresources are exhausted TEI check procedure should be initiated. user side layer management entity receiving this identity assigned message shall compare the TEI value in the Ai field to its own TEI value to see if it is already allocated if an identity request message is outstanding. Additionally the TEI value in the Ai field may be compared to its TEI on the receipt of all identity assigned messages. If there is match the management entity shall either initiate the TEI identity verify procedures. If there is no match the user side layer management entity shall compare the Ri value with any outstanding identity request message and if it matches consider the TEI value assigned to the user equipment discard the value of Ri inform the user side data link layer entities by means of the MDLASSIGNREQUEST primitive and stop timer T202 compare the Ri value with any outstanding identity request message and if there is no match do if there is no outstanding identity request message do nothing. When the data link layer receives the MDLASSIGNREQUEST primitive from the layer management enter the TEIassigned state and proceed with data link establishment procedures if DLESTABLISHREQUEST primitive is outstanding or the transmission of UI command frame if DLUNIT DATAREQUEST primitive is To deny an identity request message the ASP shall transmit to its peer message containing the Reference number and the value of TEI which is denied in the Ai field . The randomlygenerated Ri is present in the identity check response to ensure that in the case where more than one user equipment happens to commece transmission of the identity check response at precisely the same time due to different Ri values collision at layer 1 occurs. The resolution of this collision results in multiple identity check responses. When the TEI check procedure is used to verify multipleTEI assignment if more than one identity check response is received within T201 then multipleTEI assingment shall be considered present otherwise the request shall be repeated once and timer T201 if more than one identity check response is received within the second T201 period multipleTEI assignment shall be considered present if no identity check response is received after both T201 periods the TEI value shall be assumed to be free and available for assignment if one identity check response is received in one or both T201 periods the TEI value shall be assumed to be in use. When the TEI check procedure is used to test whether TEI value is in use it is completed upon the receipt or the first TEI identity check response message and the TEI value is assumed to be in use. if no identity check response is received within T201 the identity check request shall be repeated once and timer T201 restarted if no identity check response is received after the second identity check request the TEI value shall be assumed to be free and available for reassignment. If the Ai value in the identity check request is equal to 127 it is preferred that the receiving user side layer management entity respond with single identity check response message that contains all of the TEI values in use within that user equipment . If an identity check request with Ai equal to 127 is transmitted and an identity check response is received making use of the extension facility each Ai variable in the Ai field shall be processed as if received in separate identity check responses for parallel When the network side layer management entity determines that the removal of TEI value . The identity remove message shall be sent twice in succession to overcome possible message loss. When the user side layer management entity determines that the removal of TEI value is necessary it shall instruct the data link layer entity to enter the TEIunassigned state using the MDL REMOVEREQUEST primitive. This action would also be taken for all TEI values when the Ai field contains the value of 127. Further action to be taken shall be either initiation of automatic TEI assignment for new TEI value or notification to the equipment user for the need for corrective action . Action taken by the data link layer entity receiving the MDLREMOVEREQUEST data link layer entity receiving an MDLREMOVEREQUEST primitive shall if no DLRELEASEREQUEST primitive is outstanding and the user equipment is not in the TEI At the user equipment automatic TEI values shall be removed and in the case of nonautomatic TEI values an appropriate indication shall be made to the user under the following conditions on request from the ASP by an identity remove message on receipt of an MPHINFORMATIONINDICATION primitive on receipt of an MDLERRORINDICATION primitive indicating that the data link layer entity has assumed possible multipleassignment of TEI value rather than requesting TEI check procedure by the transmission of an identity verify request message or optionally on receipt of an identity assigned message containing TEI value in the Ai field which is already in use within the user equipment . At the network side TEI values should be removed following TEI audit procedure showing that TEI value is no longer in use or that multiple TEI assignment has occurred or on receipt of an MDLERRORINDICATION primitive indicating possible multipleTEI assignment which may be confirmed by the invocation of the TEI check procedures. The TEI identity verify procedure allows the user side layer management entity to have the capability to request that the network invoke the identity check procedure for verification of multipleTEI assignment. The TEI identity verify procedure is optional for both the network and user equipment. The Ai in the ID verify will be in the range 0 to 126. Ai 127 is not allowed. The user equipment shall transmit an identity verify message containing the following elements the TEI value to be checked in the Ai field and the Ri field is not processed by the network and is coded 0. Timer T202 is started. The ASP on receipt of the identity verify message shall if implemented invoke the TEI check procedure as defined in 5.3.3. This will result in the ASP sending an identity check request message to the user The user side layer management entity receiving any identity check message shall compare the content of the Ai field to its TEI value requested being verified and to the value 127 . It shall stop timer T202 if there is match. In any case it has to respond on an identity check message according to the TEI check procedure as defined in 5.3.3. If the user equipment receives no identity check request message with an Ai equal to its TEI or an Ai equal to 127 before the expiry of timer T202 the user side layer management entity shall restart the timer and the TEI identity verify message shall be retransmitted. If no identity check request message is received from the ASP after the second TEI identity verify request message the TEI shall be removed. All messages used for TEI management procedures are carried in the information field of UI command frames with SAPI value set to 63 and TEI value set to 127 . Fields that are not used in specific message are coded all zeroes and are not to be processed by either For TEI administration procedures the layer mangement entity identifier octet is 0000 1111. Other values Octets 2 and 3 contain Ri. When used it can assume any value between 0 and 65535. Octet 4 contains the message type. The purpose of the message type is to identify the function of the of the Ai field. Ai variables in the Ai field are coded as follows bit 1 is the extension bit and is coded as follows bits 2 to 8 contain the Action indicator. The purpose of the Action indicator is to identify the concerned TEI value. The use of the extension mechanism is confined to the identity check response when identity check response upon receipt of an identity check request with an Ai equal to Automatic negotiation of data link layer parameters Procedures for establishment and release of multiple frame operation The provision of extended multiple frame operation is recommended. These procedures shall be used to establish multiple frame operation between the network and designated user entity. Layer 3 will request establishment of the multiple frame operation by the use of the DLESTABLISH REQUEST primitive. Reestablishment may be initiated as result of the data link layer procedures defined in 5.7. All frames other than unnumbered frame formats received during the establishment data link layer entity shall initiate request for the multiple frame operation to be set by transmitting the SABME command. All existing exception conditions shall be cleared the retransmission counter shall be reset and timer T200 shall then be started All mode setting commands shall be transmitted with the bit set to 1. Layer 3 initiated establishment procedures imply the discard of all outstanding DLDATAREQUEST primitives and all frames in queue. data link layer entity receiving an SABME command if it is able to enter the multipleframeestablished respond with an UA response with the bit set to the same binary value as the bit in the received set and to 0 enter the multipleframeestablished state and inform layer 3 using the DLESTABLISH clear all existing exception conditions clear any existing peer receiver busy condition and start timer T203 if implemented. If the data link layer entity is unable to enter the multipleframeestablished state it shall respond to the SABME command with DM response with the bit set to the same binary value as the bit in the received SABME command. Upon reception of the UA response with the bit set to 1 the originator of the SABME command shall set and to 0 and enter the multipleframeestablished state and inform layer 3 using the DLESTABLISHECONFIRM Upon reception of DM response with the bit set to 1 the originator of the SABME command shall indicate this to layer 3 by means of the DLRELEASEINDICATION primitive and reset timer T200. It shall then enter the TEIassigned state. DM responses with the bit set to 0 shall be ignored in this case. DLRELEASEREQUEST primitive received during data link layer initiated reestablishment shall be serviced on completion of the establishment modesetting operation. If timer T200 expires before the UA or DM response with the bit set to 1 is received the data link layer increment the retransmission counter. After retransmission of the SABME command N200 times the data link layer entity shall indicate this to layer 3 and the connection management entity by means of the DLRELEASEINDICATION and MDL ERRORINDICATION primitives respectively and enter the TEIassigned state after discarding all outstanding DLDATAREQUEST primitives and all frames in queue. Having either transmitted the UA response to received SABME command or received the UA response to transmitted SABME command frames and supervisory frames shall be transmitted and received according to the procedures described in 5.6. If an SABME command is received while in the multipleframeestablished state the data link layer entity shall conform to the reestablishment procedure described in 5.7. On receipt of UI command the procedures defined in 5.2 shall be followed. These procedures shall be used to terminate the multiple frame operation between the network and designated user entity. Layer 3 will request termination of the multiple frame operation by use of the DLRELEASEREQUEST All frames other than unnumbered frames received during the release procedures shall be ignored. All outstanding DLDATAREQUEST primitives and all frames in queue shall be discarded. In the case of persistent layer 1 deactivation the data link layer entity shall discard all queues and deliver to layer 3 DLRELEASECONFIRM primitive if DLRELEASEREQUEST primitive is outstanding or otherwise DLRELEASEINDICATION primitive. At the network side the system management entity depend on the implementation of the physical layer. data link layer entity shall initiate request for release of the multiple frame operation by transmitting the Disconnect command with the bit set to 1. Timer T200 shall then be started and the retransmission counter reset. data link layer entity receiving DISC command while in the multipleframeestablished or timer recovery state shall transmit UA response with the bit set to the same binary value as the bit in the received DISC command. DLRELEASEINDICATION primitive shall be passed to layer 3 and the TEI assigned state shall be entered. If the originator of the DISC command receives either UA response with the bit set to 1 or DM response with the bit set to 1 indicating that the peer data link layer entity is already in the TEIassigned state it shall enter the TEIassigned state and reset timer T200. layer 3 by means of the DLRELEASECONFIRM primitive. The conditions relating to this state are If timer T200 expires before UA or DM response with the bit set to 1 is received the originator of the increment the retransmission counter. If the data link layer entity has not received the correct response as difined in 5.5.3.2 after N200 attempts to recover the data link layer entity shall indicate this to the connection management entity by means or the MDLERRORINDICATION primitive enter the TEIassigned state and notify layer 3 by While in the TEIassigned state the receipt of DISC command shall result in the transmission of DM response with the bit set to the value of the received bit on receipt of an SABME command the procedures defined in 5.5.1 shall be followed on receipt of an unsolicited DM response with the bit set to 0 the data link layer entity shall if it is able to initiate the establishment procedures by the transmission of an SABME . Otherwise the DM shall be ignored on receipt of UI commands the procedures defined in 5.2 shall be followed on receipt of any unsolicited UA response an MDLERRORINDICATION primitive indicating If the transmitted and received unnumbered commands are the same the data link layer entities shall send the UA response at the earliest possible opportunity. The indicated state shall be entered after receiving the UA response. The data link layer entity shall notify layer 3 by means of the If the transmitted and received unnumbered commands are different the data link layer with the bit set to 1 the data link layer shall enter the TEIassigned state and notify layer 3 by means of When DM response with the bit set to 0 is received by data link layer entity collision between transmitted SABME or DISC command and the unsolicited DM response may have occurred. This is typically caused by user equipment applying protocol procedure according to .25 LAPB to ask for modesetting command. In order to avoid misinterpretation of the DM response received data link layer entity shall always send its SABME or DISC command with the bit set to 1. DM response with the bit set to 0 colliding with an SABME or DISC command shall be ignored. Procedures for information transfer in multiple frame operation The procedures which apply to the transmission of frames are defined below. The term transmission of an frame refers to the delivery of an frame by the data Information received by the data link layer entity from layer 3 by means of DLDATAREQUEST primitive shall be transmitted in an frame. The control field parameters and shall be assigned the values and respectively. shall be incremented by 1 at the end of the transmission of the If timer T200 is not running at the time of transmission of an frame it shall be started. If timer T200 expires the procedures defined in 5.6.7 shall be followed. If is equal to plus the data link layer entity shall not transmit any new frames but may retransmit an frame as result of the error recovery procedures as described in 5.6.4 and 5.6.7. When the network side or user side is in the own receiver busy condition it may still transmit frames provided that peer receiver busy condition does not exist. Any DLDATAREQUEST primitives received whilst in the timer recovery condition Independent of timer recovery condition when data link layer entity is not in an own receiver busy condition and receives valid frame whose is equal to the current the data link layer entity pass the information field of this frame to layer 3 using the DLDATAINDICATION primitive increment by 1 its and act as indicated below. If the bit of the received frame was set to 1 the data link layer entity shall respond to its peer in one of the following ways if the data link layer entity receiving the frame is still not in an own receiver busy condition it shall send an RR response with the bit set to 1 if the data link layer entity receiving the frame enters the own receiver busy condition upon receipt of the frame it shall send an RNR response with the bit set to 1. If the bit of the received frame was set to 0 and if the data link layer entity is still not in an own receiver busy condition if no frame is available for transmission or if an frame is available for transmission but peer receiver busy condition exists the data link layer entity shall transmit an RR response with the bit set to 0 or if an frame is available for transmission and no peer receiver busy condition exists the data link layer entity shall transmit the frame with the value of set to the current value of as defined in 5.6.1. if on receipt of this frame the data link layer entity is now in an own receiver busy condition it shall transmit an RNR response with the bit set to 0. When the data link layer entity is in an own receiver busy condition it shall process any received frame Whenever data link layer entity transmits an frame or supervisory frame shall be set equal to On receipt of valid frame or supervisory frame even in the own receiver busy or timer recovery conditions the data link layer entity shall treat the contained in this frame as an acknowledgement for all the frames it has transmitted with an up to and including the received N1. shall be set to . The data link layer entity shall reset the timer T200 on receipt of valid frame or supervisory frame with the higher than or an REJ frame with an equal to . If supervisory frame with the bit set to 1 has been transmitted and not acknowledged timer T200 shall not be reset. Upon receipt of valid frame timer T200 shall not be reset if the data link layer entity is in the peer receiver busy condition. If timer T200 has been reset by the receipt of an RR or RNR frame and if there are outstanding frames still unacknowledged the data link layer entity shall restart timer T200. If timer T200 then expires the data link layer entity shall follow the recovery procedure as defined in 5.6.7 with respect to the unacknowledged frames. If timer T200 has been reset by the receipt of an REJ frame the data link layer entitiy shall follow the On receipt of valid REJ frame the data link layer entity shall act as follows clear an existing peer receiver busy condition set its and its to the value of the contained in the REJ frame control field start timer T203 if implemented if it was an REJ command frame with the bit set to 1 transmit an appropriate supervisory response frame with the bit set to 1 transmit the corresponding frame as soon as possible as difined in 5.6.1 taking into account the items 1 to 3 below and the paragraph following items 1 to 3 and notify protocol violation to the connection management entity by means of the MDL ERRORINDICATION primitive if it was an REJ response frame with the bit set to 1. if it is in the timer recovery condition and it was an REJ response frame with the bit set to 1 clear an existing peer receiver busy condition set its and its to the value contained in the REJ frame control field enter the multipleframeestablished state and transmit the corresponding frame as soon as possible as difined in 5.6.1 taking into account the items 1 to 3 below and the paragraph following items 1 to 3. if it is in the timer recovery condition and it was an REJ frame other than an REJ response frame clear an existing peer receiver busy condition set its to the value of the contained in the REJ frame control field and if it was an REJ command frame with the bit set to 1 transmit an appropriate supervisory response frame with the bit set to 1 . Transmission of frames shall take account of the following if the data link layer entity is transmitting supervisory frame when it receives the REJ frame it shall complete that transmission before commencing transmission of the requested frame if the data link layer entity is transmitting an SABME command DISC command UA response or DM response when it receives the REJ frame it shall ignore the request for retransmission if the data link layer entity is not transmitting frame when the REJ is receiveed it shall immediately commence transmission of the requested frame. All outstanding unacknowledged frames commencing with the frame identified in the received REJ frame shall be transmitted. Other frames not yet transmitted may be transmitted following the After receiving valid RNR command or response if the data link layer entity is not engaged in mode setting operation it shall set peer receiver busy condition and then if it was an RNR command with the bit set to 1 it shall respond with an RR response with the bit set to 1 if the data link layer entity is not in an own receiver busy condition and shall respond with an RNR response with the bit set to 1 if the data link layer entity is in an own receiver busy if it was an RNR response with the bit set to 1 an existing timer recovery condition shall be The data link layer entity shall take note of the peer receiver busy condition and not transmit any frames to the peer which has indicated the busy condition. The in any received RR or RNR command frame contained in the received RNR frame as an acknowledgement for all the frames that have been transmitted with an up to and including minus 1 and set its to the value of the contained in the RNR frame and restart timer T200 unless supervisory response frame with the bit set to 1 is still expected. If timer T200 expires the data link layer entity shall if it is not yet in timer recovery condition enter the timer recovery condition and reset the retransmission count variable or if it is already in timer recovery condition add one to its retransmission count variable. The data link layer entity shall then if the value of the retransmission count variable is less than N200 transmit an appropriate supervisory command with bit set to 1 restart timer T200 and if the value of the retransmission count variable is equal to N200 initiate reestablishment procedure as defined in 5.7 and indicate this by means of the MDLERRORINDICATION primitive to the connection management entity. The data link layer entity receiving the supervisory frame with the bit set to 1 shall respond at the earliest opportunity with an appropriate supervisory response frame with the bit set to 1 to indicate whether or not its own receiver busy condition still exists. Upon receipt or the supervisory response with the bit set to 1 the data link layer entity shall reset timer if the response is an RR or REJ response the peer receiver busy condition is cleared and the data link layer entity may transmit new frames or retransmit frames as defined in 5.6.1 or 5.6.4 respectively or if the response is an RNR response the data link layer entity receiving the response shall proceed according to this 5.6.5 first paragraph. If supervisory command with the bit set to 0 or 1 or supervisory response frame with the bit set to 0 is received during the enquiry process the data link layer entity if the supervisory frame is an RR or REJ command frame or an RR or REJ response frame with the bit set to 0 clear the peer receiver busy condition and if the supervisory frame received was command with the bit set to 1 transmit the appropriate supervisory response frame with the bit set to 1. However the transmission or retransmission of frames shall not be undertaken until the appropriate supervisory response frame with the bit set to 1 is received or until the expiry of timer T200 or if the supervisory frame is an RNR command frame or an RNR response frame with the bit set to 0 retain the peer receiver busy condition and if the supervisory frame received was an RNR command with the bit set to 1 transmit the appropriate supervisory response frame with the bit set to 1. Upon receipt of an SABME command the data link layer entity shall clear the peer receiver busy If the data link layer entity is not in an own receiver busy condition and is in reject exception condition sequence error has been received and an REJ frame has been transmitted but the requested frame has not been received the appropriate supervisory frame is the RR frame. If the data link layer entity is not in an own receiver busy condition but is in an sequence error exception condition sequence error has been received but an REJ frame has not been transmitted the appropriate supervisory frame is the If the data link layer entity is in its own receiver busy condition the appropriate supervisory frame is the RNR frame. Otherwise the appropriate supervisory frame is the RR frame. When the data link layer entity enters an own receiver busy condition it shall transmit an RNR frame at an RNR response with the bit set to 0 or if this condition is entered on receiving command frame with the bit set to 1 an RNR response with the bit set to 1 or if this condition is entered on expiry of timer T200 an RNR command with the bit set to1. All received frames with the bit set to 0 shall be discarded after updating . All received supervisory frames with the PF bit set to 0 shall be processed including updating . All received frames with the bit set to 1 shall be discarded after updating . However an RNR response frame with the bit set to 1 shall be transmitted. All received supervisory frames with the bit set to 1 shall be processed including updating . An RNR response with the bit set to 1 shall be transmitted. To indicate to the peer data link layer entity the clearance of the own receiver busy condition the data link layer entity shall transmit an RR frame or if previously detected sequence error has not yet been The transmission of an SABME command or UA response also indicates to the peer data link layer entity the clearance of the own receiver busy condition. The data link layer entity shall maintain an internal retransmission count variable. If timer T200 expires the data link layer entity shall if it is not yet in the timer recovery condition enter the timer recovery condition and reset the retransmission count variable or if it is already in the timer recovery condition add one to its retransmission count variable. The data link layer entity shall then if the value of the retransmission count variable is less than N200 restart timer T200 and either transmit an appropriate supervisory command with the bit set to 1 retransmit the last transmitted frame 1 with the bit set to 1 or if the value of the retransmission count variable is equal to N200 initiate reestablishment procedure as defined in 5.7 and indicate this by means of the MDLERRORINDICATION primitive to the connection management entity. The timer recovery condition is cleared when the data link layer entity receives valid supervisory frame response with the bit set to 1. If the received supervisory frame is within the range from its current to its current inclusive it shall set its to the value of the received . Timer T200 shall be reset if the received supervisory frame response is an RR or REJ response and then the data link layer entity shall resume with frame transmission or retransmission as appropriate. Timer T200 shall be reset and restarted if the received supervisory response is an RNR response to proceed with the enquiry following conditions the receipt while in the multipleframe mode of operation of an SABME the receipt of DLESTABLISHREQUEST primitive from layer 3 the occurrence of N200 retransmission failures while in the timer recovery condition the ocurence of frame rejection condition as identified in 5.8.5 on the receipt while in the multipleframe mode of operation of an FRMR response frame the receipt while in the timerrecovery condition of DM response with the bit set to 1. In all reestablishment situations the data link layer entity shall follow the procedures defined in 5.5.1. All locally generated conditions for reestablishment will cause the transmission of the SABME. In the case of data link layer and peer initiated reestablishment the data link layer entity shall also discard all queues. In case of layer 3 initiated reestablishment or if DLESTABLISHREQUEST primitive occurs pending reestablishment the DLESTABLISHCONFIRM primitive shall be used. Exception conditions may occur as the result of physical layer errors or data link layer procedural errors. The error recovery procedures which are available to effect recovery following the detection of an The actions to be taken by the connection mangement entity on receipt of an MDLERRORINDICATION An sequence error exception condition occurs in the receiver when valid frame is received which contains an value which is not equal to the at the receiver. The information field of all frames whose does not equal the shall be discarded. The receiver shall not acknowledge the frame causing the sequence error nor any frames which may follow until an frame with the correct is received. data link layer entity which receives one or more frames having sequence errors but otherwise error free or subsequent supervisory frames shall use the control field information contained in the field and the or bit to perform data link control functions for example to receive acknowledgement of previously transmitted frames and to cause the data link layer entity to respond if the bit is set to 1. Therefore the retransmitted frame may contain an field value and bit that are The REJ frame is used by receiving data link layer entity to initiate an exception condition recovery following the detection of an sequence error. Only one REJ exception condition for given direction of information transfer shall be established at data link layer entity receiving an REJ command or response shall initiate sequential transmission of frames starting with the frame indicated by the contained in the REJ frame. An REJ exception condition is cleared when the requested frame is received or when an SABME or An sequence error exception condition occurs in the transmitter when valid supervisory frame or frame is received which contains an invalid value. valid is one that is in the range VNV. The information field contained in an frame which is correct in sequence and format may be delivered to layer 3 by means of the DLDATAINDICATION primitive. The data link layer entity shall inform the connection management entity of this exception condition by means of the MDLERRORINDICATION primitive and initiate reestablishment according to 5.7.2. If data link layer entity due to transmission error does not receive single frame or the last frame in sequence of frames it will not detect an outofsequence exception condition and therefore will not transmit an REJ frame. The data link layer entity which transmitted the unacknowledged frame shall on the expiry of timer T200 take appropriate recovery action as defined in 5.6.7 to determine at which frame retransmission Any frame received which is invalid shall be discarded and no action shall be taken frame rejection condition results from one of the following conditions the receipt of an undefined frame the receipt of supervisory or unnumbered frame with incorrect length the receipt of an invalid or the receipt of frame with an information field which exceeds the maximum established length. Upon occurence of frame rejection condition whilst in the multiple frame operation the data link layer initiate reestablishment . Upon occurence of frame rejection condition during establishment or release from multiple frame operation or whilst data link is not established the data link layer entity shall discard the frame. For satisfactory operation it is essential that receiver is able to discriminate between invalid frames as defined in 2.9 and frames with an information field which exceeds the maximum established length . An unbounded frame may be assumed and thus discarded if two times the longest permissable frame plus two Upon receipt of an FRMR response frame in the multipleframe mode of operation the data link layer Unsolicited response frames data link layer entity shall assume possible multipleTEI assignment on the receipt of an unsolicited UA data link layer entity shall assume multipleassignment of TEI value and initiate recovery as specified the receipt of UA response frame whilst in the multipleframeestablished state the receipt of UA response frame whilst in the timer recovery state the receipt of UA response frame whilst in the TEIassigned state. data link layer entity after assuming multipleassignment of TEI value shall inform the connection management entity by means of the MDLERRORINDICATION primitive. The system parameters listed below are associated with each individual SAP. method of assigning these parameters is defined in 5.4. The term default implies that the value defined should be used in the absence of any assignment or negotiation of alternative values. The default value for timer T200 at the end of which transmission of frame may be initiated according to the procedures described in 5.6 shall be one second. The proper operation of the procedure requires that timer T200 be greater than the maximum time between transmission of command frames and the reception of their corresponding response or acknowledgement frames. When an implementation includes multiple terminals on the user side together with satellite connection in the transmission path value of T200 greater than 1 second may be necessary. value of 2.5 seconds is suggested. The maximum number of retransmissions of frame is system parameter. The default value of N200 shall be 3. Maximum number of octets in an information field For an SAP supporting signalling the default value shall be 260 octets. For SAPs supporting packet information the default value shall be 260 octets. Maximum number of transmission of the TEI identity request message The maximum number of transmission of TEI identity request message is system parameter. The default value of N202 shall be 3. The maximum number of sequentially numbered frames that may be outstanding at any given time is system parameter which shall not exceed 127 for extended operation. for an SAP supporting basic access signalling the default value shall be 1 for an SAP supporting primary rate signalling the default value shall be 7 for an SAP supporting basic access packet information the default value shall be 3 for an SAP supporting primary rate packet information the default value shall be 7. The minimum time between retransmission of the TEI identity check messages is system parameter which shall be set to T200 seconds. The minimum time between the transmission of TEI identity request messages is system parameter which shall be set to 2 seconds. Timer T203 represents the maximum time allowed without frames being exchanged. The default value of timer T203 shall be 10 seconds. types and user or network side data link layer entities and summarises the recommended default or fixed values respectively to be used in European networks. The procedural elements defined in 5 allow for the supervision of the data link layer resource. This function is mandatory for network side data link layer entities in European networks but optional for user side data link layer entity. Data link layer supervision in the multipleframeestablished state The procedures specified herein propose solution which is already identified in the HDLC classes of procedures. The connection verification is service provided by data link layer to layer 3. This implies that layer 3 is informed in case of failure only. Furthermore the procedure may be incorporated in the normal exchange of information and may become more efficient than procedure based on the involvement of layer 3. The procedure is based on supervisory command frames and timer T203 and operates in the multipleframeestablished state as follows. If there are no frames being exchanged on the data link connection there is no means to detect faulty data link connection condition or user equipment having been unplugged. Timer T203 represents the maximum time allowed without frames being exchanged. If timer T203 expires supervisory command with bit set to 1 is transmitted. Such procedure is protected against transmission errors by making use of the normal timer T200 procedure including The timer T203 is started when the multipleframeestablished state is entered and in the multipleframeestablished state whenever timer T200 is stopped . Upon receiving an or supervisory frame timer T203 will be restarted if timer T200 is not to be started. The timer T203 is stopped when in the multipleframeestablished state the timer T200 is started and upon leaving the multipleframeestablished state. These two conditions mean that timer T203 is only started whenever timer T200 is If timer T203 expires the data link layer entity will act as follows transmit an RR command or if there is receiver busy condition transmit an RNR command start timer T200 and send MDLERRORINDICATION primitive to connection management after N200 retransmission. Provision of pointtopoint signalling In certain applications it may be advantageous to have single pointtopoint signalling connection at layer in such applications does not preclude using that value in other applications or networks. In European networks the TEI value 0 is reserved for NT2 in pointtopoint configurations using single data link connection in addition to this the broadcast data link connection between the NT2 and the network may be supported within each SAP. Annex SDL for pointtopoint procedures An SDL representation of the pointtopoint procedures of the data link layer The purpose of this annex is to provide one example of an SDL representation of the pointtopoint procedures of the data link layer to assist in the understanding of this Recommendation. This representation does not describe all of the possible actions of the data link layer entity as non partitioned representation was selected in order to minimise its complexity. The SDL representation does not therefore constrain implementations from exploiting the full scope of the procedures as presented within the text of this Recommendation. The text description of the procedures is definitive. The representation is peertopeer model of the pointtopoint procedures of the data link layer and is applicable to the data link layer entities at both the user and network sides for all ranges of TEI values. The SDL representation of the pointtopoint procedures are based on an expansion of the three basic states identified in 3.4.2Q.920 to the following 8 states Multiple frame established incomplete and serves only as an introduction to the SDL representation. All data link layer entities are conceptually initiated in the TEI unassigned state and will interact with the layer management in order to request TEI value. TEI assignment initiated by Unit data request will cause the data link layer entity to move to the TEI assigned state via the assign awaiting TEI state . Initiation by an establish request will cause transition to the awaiting establishment state via the establish awaiting TEI state . Direct TEI assignment will cause an immediate transition to the TEI assigned state . In states 48 Unit data requests can be directly serviced by the data link layer entity. The receipt of an establish request in the TEI assigned state will cause the initiation of the establishment procedures and the transition to the awaiting establishment state . Completion of the LAP establishment procedures takes the data link layer entity into the multiple frame established state . Peer initiated establishment causes direct transition from the TEI assigned state to the multiple frame established state . In the multiple frame established state Acknowledged data transfer requests can be serviced directly subject to the restrictions of the procedures. Expiry of timer T200 which is used in both the flow control and data transfer aspects of the data link layer enititys procedures initiates the transition to the timer recovery state . Completion of the timer recovery procedures will return the data link layer entity to the multiple frame established state . In states 7 and 8 of the SDL representation the following conditions which are identified within the own receiver busy. In addition other conditions are used in order to avoid identification of additional states. The complete combination of both of these categories of conditions with the 8 states of the SDL representation is the basis for the state transition table description of the data link layer entity. peer initiated LAP release will take the data link layer entity directly into the TEI assigned state whilst release request will be via the awaiting release state . TEI removal will cause transition to the TEI unassigned state The following symbols and abbreviations are used within this description. full description of the symbols and their meaning and application can be found in the Series Recommendation To mark an event or signal required as result of the representation approach adopted which is local to the data link layer entity. The codes used in the MDLERRORINDICATION signals are defined in To enable satisfactory representation of the data link layer entity conceptual queues for the UI frame and frame transmission have been explicitly brought out. These conceptual queues are finite but unbounded and should in no way restrict the implementation of the pointtopoint procedures. Two additional signals have been provided in order to cause the servicing of these queues to be initiated UI The use of these events on the network side is for further study. This function may be implemented over geographically distributed architecture. This primitive may occur on initialisation for fixed TEIs at the network side or as appropriate in order to correctly process frame carrying fixed TEI. Only possible in cases of Layer 2 initiated reestablishment. Only possible in cases of layer 2 initiated reestablishment. The regeneration of this signal does not affect the sequence integrity of the queue. These signals are generated outside of this SDL representation and may be generated by the connection management entity. These signals are generated outside of this SDL representation and may be The data link layer returns to the state it was in prior to the events shown. The data link layer returns to the state it was in prior to the events shown. The generation of the correct number of signals in order to cause the required retransmission of frames does not alter their sequence integrity. An SDL representation of the broadcast procedures of the data link layer State transition table of the pointto The state transition table presented in Tables D1.1Q.921 to D3.10Q.921 is based on the eight basic states recognised in the SDL representation and the related transmitter and receiver The state transition table relinquishes to any partitioning of the procedures. It is conceptual and does not prevent designer from partitioning in his implementation. Moreover all the processes related to primitive procedures the management of queues and the exchange of information between adjacent layers are conceptual not visible from outside of the system and would not impose any constraints on the The eight basic states apply to both the transmitter and the receiver within one data link layer entity. However some of the conditions are confined to the transmitter whilst some are confined to the receiver . This implies if the concept of nonpartitioning is adopted that each transmitter condition has to be combined with each receiver condition resulting in composite states. This state transition table comprises 24 composite states representing the 8 basic states and the related combinations of transmitter and receiver conditions. repertoire of frames to be received unnumbered frames information frame internal events . The actions to be taken when an event occurs whilst in specific state comprise Definition of cell of the state transition table Impossible by the definition of the data link layer service Impossible by the definition of the peertopeer data link procedures Collective term for the two actions VN and VN The acknowledgement of the received frame may be conveyed by an frame associated with the information flow in the opposite direction or supervisory Indicates the discarding of the information contained in the information field of The codes used in MDLERRORINDICATION signals are defined in table II In general this state transition table does not prevent an implementation from using to acknowledge more than one frame. TABLE D1.1Q.921 STATE TRANSITION TABLE Receiving primitive TABLE D1.2Q921 STATE TRANSITION TABLE Receiving unnumbered frame with correct format TABLE D1.3Q.921 STATE TRANSITION TABLE Receiving FRMR unnumbered frame with correct format TABLE D1.4 .921 STATE TRANSITION TABLE Receiving RR supervisory frame with correct format TABLE D1.5Q.921 STATE TRANSITION TABLE Receiving REJ supervisory frame with correct format TABLE D1.6Q.921 STATE TRANSITION TABLE Receiving RNR supervisory frame with correct format TABLE D1.7Q.921 STATE TRANSITION TABLE Receiving command frame with correct format acknowledging all outstanding frames or containing an TABLE D1.8Q.921 STATE TRANSITION TABLE Receiving command frame with correct format an which is VNV or an error TABLE D1.9Q.921 STATE TRANSITION TABLE Internal events These signals are generated outside the procedures specified in this state transition table and may be generated by the connection management entity. TABLE D1.10Q.921 STATE TRANSITION TABLE Receiving frame with incorrect format or frame with undefined control field TABLE D2.2aQ.921 STATE TRANSITION TABLE Receiving unnumbered frame with correct format TABLE D2.2bQ.921 STATE TRANSITION TABLE Receiving unnumbered frame with correct format TABLE D2.3Q.921 STATE TRANSITION TABLE Receiving FRMR unnumbered frame with correct format TABLE D2.4aQ.921 STATE TRANSITION TABLE Receiving RR supervisory frame with correct format TABLE D2.4bQ.921 STATE TRANSITION TABLE Receiving RR supervisory frame with correct format TABLE D2.5aQ.921 STATE TRANSITION TABLE Receiving REJ supervisory frame with correct format This event is impossibe by the definition of the peertopeer data link procedures. However it would not harm the information transfer if the actions TABLE D2.5bQ.921 STATE TRANSITION TABLE Receiving REJ supervisory frame with correct format TABLE D2.6aQ.921 STATE TRANSITION TABLE Receiving RNR supervisory frame with correct format TABLE D2.6bQ.921 STATE TRANSITION TABLE Receiving RNR supervisory frame with correct format TABLE D2.7Q.921 STATE TRANSITION TABLE Receiving command frame with correct format acknowledging all outstanding frames or containing an TABLE D2.8Q.921 STATE TRANSITION TABLE Receiving command frame with correct format containing an which is or an TABLE D2.9Q.921 STATE TRANSITION TABLE Internal events These signals are generated outside the procedures specified in this state transition table and may be generated by the connection management TABLE D2.10Q.921 STATE TRANSITION TABLE Receiving frame with incorrect format or frame with undefined control field TABLE D3.2aQ.921 STATE TRANSITION TABLE Receiving unnumbered frame with correct format TABLE D3.2bQ.921 STATE TRANSITION TABLE Receiving unnumbered frame with correct format TABLE D3.3Q.921 STATE TRANSITION TABLE Receiving FRMR unnumbered frame with correct format TABLE D3.4Q.921 STATE TRANSITION TABLE Receiving RR supervisory frame with correct format clearance of timer recovery if there is F1 only TABLE D3.5Q.921 STATE TRANSITION TABLE Receiving REJ supervisory frame with correct format clearance of timer recovery if there is F1 only TABLE D3.6Q.921 STATE TRANSITION TABLE Receiving RNR supervisory frame with correct format clearance of timer recovery if there is F1 only TABLE D3.7Q.921 STATE TRANSITION TABLE Receiving command frame with correct format acknowledging all outstanding frames or containing an which is no clearance of timer recovery TABLE D3.8Q.921 STATE TRANSITION TABLE Receiving command frame with correct format containing an which is or an TABLE D3.9Q.921 STATE TRANSITION TABLE Internal events Initiation of reestablishment procedure if the value of the retransmission count variable is equal to N200 These signals are generated outside the procedures specified in the state transition table and may be generated by the connection management entity TABLE D3.10Q.921 STATE TRANSITION TABLE Receiving frame with incorrect format or frame with undefined control field Retransmission of REJ response frames This optional reject retransmission procedure can supplement the .921 LAPD protocol by defining new variable for multiple frame operation and by modifying the sequence error NOT FOR APPLICATION IN EUROPEAN NETWORKS within the basic states and actions to be taken by the management Table II1Q.921 gives the error situations in which the MDLERRORINDICATION primitive will be generated. This primitive notifies the data link layers connection management entity of the occurred error situation. The associated error parameter contains the error code that describes the unique error conditions. Table II1Q.921 also identifies the associated connection management actions to be taken The error code column gives the identification value of each error situation to be included as parameter with the MDLERRORINDICATION primitive. The column entitled error condition together with the Affected states describes unique protocol error events and the basic state of the data link layer entity at the point that the MDLERRORINDICATION primitive is generated. For given error condition the column entitled Network management action describes the preferred action to be taken by the network management entity. The column entitled User management action describes the preferred action to be taken by the user side The various preferred layer management actions on an error situation may be described as one of the This suggests that the network side connection management entity has the preferred action of logging the event into an error counter. The length and the operation of the counter mechanisms for the error situations is implementation dependent. This means that the network side layer management entity invokes the TEI check procedure. This means that the user side layer management entity may optionally invoke TEI verify request This means that the user side layer management entity may directly remove its TEI value from In most of the described error situations there is either no action to be taken by the user side layer management or the action to be taken is implementation dependent as table II1Q.921 shows. Implementation dependent means that it is optional whether the user side layer management has management has to take into account that the data link layer will have initiated recovery procedure. Table II1Q.921 Management entity actions for MDL error indications For the description of the affected states see Annex . According to .921 5.8.5 this error code will never be generated. III1Q.921 provides conceptual model of the interactions which are required for this deactivation The monitor function uses layer 2 activity as the basis for establishing whether deactivation of the access INFORMATION indicates that there is no data link connection in the multipleframe mode of INFORMATION indicates that there is at least one data link connection in the mode INFORMATION indicates that UI frame is about to be transmitted or has just been Within the data link layer entity the DLESTABLISHREQUESTINDICATION primitive and DLRELEASE INDICATIONCONFIRM primitives mark the duration of the multipleframe mode of operation and the MDLDLUNIT DATAREQUESTINDICATION primitives mark the transmission and reception of UI INDICATION primitives are used as described in 4. The definition and usage of these primitives are also described in Recommendation .430 and prETS 300 012 which specify layer 1. Since in the Recommendation .430 the usage of MPHDEACTIVATEINDICATION primitive is an implementation option two cases of deactivation procedures are described below. primitive is delivered to the system management entity. primitive is not delivered to the system management entity. These procedures require that all layer 3 entities making use of the acknowledged information transfer service must release the data link connection at an approporiate point after the completion of the information transfer. Deactivation procedure with MPHDEACTIVATEINDICATION This deactivation procedure makes use of the MPHDEACTIVATEINDICATION primitive to provide an DEACTIVATEINDICATION primitive. This deactivation procedure can be represented by six states Information transfer not available and free Information transfer available and free Information transfer available and in use Information transfer not available and in use Information transfer interrupted and free Information transfer interrupted and in use These six states are described as follows state 1 represents the state where the access is assumed to be deactivated and no data link connections are in mode setting or multiple frame mode of operation state 2 represents the state where the access is activated and no data link connection is in mode setting or multipleframe mode of operation. Timer TM01 is running and upon its expiry if The access is then assumed to be deactivated state 3 represents the state where the access is activatied and at least one data link connection is in modesetting or multipleframe mode of operation state 4 represents the state where the access is regarded to be in transient state and at least one data link connection is in modesetting or multiple frame mode of operation. signal before an MPHACTIVATEINDICATION primitive state 5 represents the state where the access is regarded to be in transient state and no data link connection is in modesetting or multipleframe mode of operation. Timer TM01 is running and upon its expiry if deactivation is enabled then an MPH state 6 represents the state where the access is regarded to be in transient state and at least one data link connection is in modesetting or multiple Timer TM01 is started whenever state 2 is entered on receipt of an MPHACTIVATEINDICATION primitive in state 1 and on receipt of an INFORMATION signal in state 3. Timer TM01 is started when state 5 is entered on receipt of an INFORMATION signal in state 6. Timer TM01 is restarted in states 2 and 5 when an INFORMATION signal is received in order to allow sufficient time for current and further unacknowledged information transfer. Timer TM01 has value of ten seconds at the network side. State transition diagram of deactivation procedure with MPHDEACTIVATEINDICATION Deactivation procedure without MPHDEACTIVATEINDICATION This deactivation procedure does not make use of the MPHDEACTIVATEINDICATION primitive to provide an option of layer 1 implementation so that this procedure can be represented by only four states . state 1 state 2 state 3 and state 4. States 5 and 6 have disappeared. Each data link layer entity has an associated data link connection management entity. The data link connection management entity has the responsability for initialising the link parameters necessary for correct peertopeer information transport. The method of initialisation of the parameters follows one of the two methods below initialisation based on the values supplied by its peer entity. the assignment of TEI value to the management entity the data link connection management entity is notified by its layer management entity that parameter initialisation is required. The data link connection management entity will invoke the peer to peer notification procedure. After parameter initialisation the data link connection management entity will notify the layer management entity The parameter initialisation procedure may invoke either the internal initialisation procedure or the automatic notification of data link parameter procedure. When the layer management entity notifies the connection management entity of TEI assignment the connection management entity shall initialise the link parameters to the default values and notify the layer management of task completion. Automatic notification of data link layer parameter values Abbreviations and acronyms used in Recommendation .921 Communication between layer Management and Data Link layer Communication between Management and PHysical layer Communication between data link layer and PHysical layer Receive sequence variable 1A different acronym has to be found for Supervisory function bit. CCITT Recommendation .920 ISDN usernetwork interface data link layer General aspects . CCITT Recommendation .930 ISDN usernetwork interface layer 3 General aspects . specification for basic call control . specification . specification . CCITT Recommendation .25 Interface between data terminal equipment and data circuitterminating equipment for terminals operating in the packet mode and connected to public data networks by dedicated circuit . prETS 300 012 March 1990 Integrated Services Digital network Basic usernetwork interface layer 1 specification and test principles. prETS 300 011 March 1990 Integrated Services Digital network Primary rate usernetwork interface layer 1 specification and test ETS 300 1021 Integrated Services Digital Network User network interface layer 3 Specifications for basic call control Application of Description Language diagrams Application of CCITT Recommendation Converted into Adobe Acrobat Portable Document Format

Layer 2 is the Data Link Layer of the Open Systems Interconnection model of computer Data is not getting forwarded instead it is being flooded to all ports of VLAN. MAC address learning is disabled because of loop detection. severity 2 syslog message STM_LOOP_DETECT should have been received. After waiting for 180 seconds learning is enabled automatically. severity 2 syslog message STM_LEARNING_RE_ENABLE should be received. The MAC address table is full. severity 2 syslog message STM_LIMIT_REACHED should have been After waiting for 180 seconds the MAC table is flushed and learning is enabled automatically. Alternatively wait until some MAC entries are aged out so that the total learned entries fall below 1500 or enter the clear mac addresstable dynamic command to clear out entries. This creates free space for new MAC entries to be learned. severity 2 syslog message STM_LEARNING_RE_ENABLE should be received. MAC address learning is disabled due to learning overload . severity 4 syslog message STM_LEARNING_OVERLOAD should have been received. After waiting for 120 seconds learning is enabled automatically. MAC address is not learned by the switch. This causes the MAC address not to be listed in the MAC MAC address learning is disabled because of loop detection. severity 2 syslog message STM_LOOP_DETECT should have been received. After waiting for 180 seconds learning is enabled automatically. severity 2 syslog message STM_LEARNING_RE_ENABLE should be received. The MAC address table is full. severity 2 syslog message STM_LIMIT_REACHED should have been After waiting for 180 seconds the MAC table is flushed and learning is enabled automatically. Alternatively wait until some MAC entries are aged out so that the total learned entries fall below 1500 or enter the clear mac addresstable dynamic command to clear out entries. This creates free space for new MAC entries to be learned. severity 2 syslog message STM_LEARNING_RE_ENABLE should be received. MAC address learning is disabled due to learning overload . severity 4 syslog message STM_LEARNING_OVERLOAD should have been received. After waiting for 120 seconds learning is enabled automatically. No egress path was set for the incoming data traffic. The MAC address from data stream is not learned if there is no path for that data going out of the switch. For example the VLAN may not have been enabled on any of the interfaces other than the one on which data is coming in on. Alternatively the outgoing interfaces may be down. If this is the case you need to bring up those interfaces. CO CO Data is not getting forwarded instead it is being flooded in the presence of VPC scenario. The MAC address is learned on one switch only. Typically this situation would be bug regarding the synchronization of the MAC address with the VPC peer. Clear the MAC address from the switch where it was learned. This triggers new learning and synchronization of the MAC addresses across the VPC switches. HIFs go down with the BPDUGuard errDisable message HIFs go down accompanied with the message BPDUGuard errDisable. By default the HIFs are in STP edge mode with the BPDU guard enabled. This means that the HIFs are supposed to be connected to hosts or nonswitching devices. If they are connected to nonhost deviceswitch that is sending BPDUs the HIFs become errordisabled upon receiving BPDU. Enable the BPDU filter on the HIF and on the peer connecting device. With the filter enabled the HIFs do not send or receive any BPDUs. Use the following commands to confirm the details of the STP port FWM2STM_LOOP_DETECT detected on switch dynamic learning disabled When FWM2STM_LOOP_DETECT is detected on the switch dynamic learning is disabled. MAC addresses are moving because of incorrect STPport state convergence. MAC addresses are moving because the source of the data being physically moved across all switches while STP states are converged and in correct states. Use the following commands to verify the STP port state across VLANS on the switches Check for correct STP convergence and for STP port states across all switches in the topography. Also confirm that there are no disputes or incorrect port states. If the source of the data frames which are physically moving is identified then control the source to halt rapid and continuous moves. By default dynamic learning is opened after 180 seconds. At that point any STP disputes or inconsistencies should be resolved. Port stuck in STP blocking state with BLK port is stuck in STP blocking state with BLK. type inconsistency might exist on an access port when it is connected to trunk port on the other end. The port becomes BLK to indicate that there is an incorrect configuration on the link. Use the following commands to confirm the details of the STP port state for the port show spanningtree interface mode. Both should be in access or trunk mode. Once the modes are synchronized the port moves out of the inconsistency state. Port stuck in STP blocking state with BLK port is stuck in STP blocking state with BLK. PVID inconsistency may exist when there is native VLAN mismatch across trunk link. When this occurs the port state becomes BLK . Use the following commands to confirm the details VLAN. Once the native VLANs are synchronized the port moves out of the inconsistency state. Port stuck in STP blocking state with BLK port is stuck in STP blocking state with BLK. This is supposed to prevent loops when unidirectional link failures occur. However the port is put into BLK state. Use the following commands to confirm the details of the STP port state for VLAN. Once the native VLANs are synchronized the port moves out of the inconsistency state. In this situation the source MAC addresses of IGMP joins are learned. However source MAC addresses of IGMP joins are usually not learned by the switch in order to conserve MAC address space. Receiving joins and performing an ISSU simultaneously might cause the situation. The MAC addresses age out if the join stops. Alternatively you can clear the MAC addresses specifically by using the clear mac addresstable dynamic mac command. The join is not registered. Ensure that the host application is sending the joins. show vlan id command. Check if the relevant VLAN is active by using the show vlan id command. Check if the switch port is in STP forwarding state by using the show spanningtree vlan Multicast data traffic not received when host is registered for group Multicast data traffic is not received when the host is registered for the group. bug may exist in exist in the communication between the IGMP and FWM processes. Review the output from the following commands show mac addresstable multicast vlan 1001 igmpsnooping show platform fwm info vlan 1001 all_macgs verbose Perform shutnoshut operation on the host interface and send the join again. In VPC setup multicast traffic is being flooded. Enable IGMP snooping on both switches. Groups for link local IP addresses are not created. Nexus 5000 does not have the same VLANs as switch running VTP server VLANs for the Nexus 5000 are not the same as for the switch running the VTP server. The Nexus 5000 currently supports VTP only in transparent mode . both communicate through Nexus 5000 by using the following commands An internal VLAN range is used. Use VLAN number that is not being reserved for internal use. The VLAN range of 3968 to 4047 is reserved for internal use. The VLAN was not created. Although VLAN is not yet created the NXOS allows the configuration of the interface vlan . As result the interface vlan does not come up. Use the show vlan command to determine if VLAN exists. If it does not exist use the vlan command to create the VLAN. After the VLAN is created you must bounce the interface VLAN to have it come up. VLAN was suspended by the vPC configuration on the Nexus 5000 pair. Show that the vPC consistency parameters are global and make sure that the VLAN was not suspended. Otherwise fix the configuration mismatch on the Nexus 5000 pair Type 1 vPC will be suspended in case of mismatch Network Qos 1 Input Queuing Output Queuing STP Port Type Edge 1 Normal Disabled Normal Disabled Configuring interface to access port does not allow VLAN to go through After configuring an interface to access port for allowing VLAN the VLAN does not go The VLAN was not created. In NXOS configuring with the switchport access vlan command on an interface does not automatically create VLAN . You must specifically create VLAN using the vlan command. Use the show vlan command to determine if VLAN exists. If it does not exist then All VLAN resources are exhausted. For the Nexus 5000 the maximum number of active VLANs and VSANs per switch is 512 . Use the show resource vlan command to determine the The interfacevlan feature is not enabled. The interfacevlan feature must be enabled before configuring the SVI. Use the show feature command The privatevlan feature is not enabled. The privatevlan feature must be enabled prior to PVLAN configuration which makes the PVLAN command available. Use the show feature command to determine which features are enabled. There are logical and physical causes for the Nexus 5000 to drop frame. There are also situations when frame cannot be dropped because of the cutthrough nature of the switch architecture. If drop is necessary but the frame is being switched in cutthrough path then the only option is to stomp the Ethernet frame check sequence . Stomping frame involves setting the FCS to known value that does not pass CRC check. This causes subsequent CRC checks to fail later in the path for this frame. downstream storeandforward device or host will be able to drop this frame. When frame is received on 10 Gbs interface it is considered to be in the cutthrough path. The following example output shows all discards and drops seen on given interface except for queuing drops. The queuing drops may be expected or resulting from errors. The show platform fwm info gatoserrors command increments 3 times for given drop. When queue is full you need to increment discards in the respective queue on the ingress interface. Increments for an access list matching the frame. If an ACL is not applied this will increment if large amount of CPUbound traffic is being received and the rate limit in the hardware is enabled to protect NXOS from Denial of Service. CO CO The Nexus 5000 is cutthrough switch at 10 Gbs. This means that an MTU can be checked but the frame will already be transmitting before the length is known. Therefore the frame cannot be dropped. The frame is truncated after the MTU is reached and the CRC value is stomped. The ingress interface increments an Rx Jumbo and the egress interface will increment Tx CRC and Tx Jumbo. If jumbo frames are seen with the show interface or the show hardware internal gatos port ethernet 11 counters rx commands this is not an indication that the frames are being dropped. jumbo frame is just an Ethernet frame greater than 1500 bytes that was received or transmitted. drop due to an MTU violation can be seen with the show hardware internal gatos counters interrupt match mtu command. counter that matches the Gatos number and fw_instance from the show hardware internal gatos port ethernet 11 include instancemac command is the indicator that an MTU violation has taken When CRC error is seen in the FCS on cutthrough port the Rx CRC counter of the show interface command is incremented. However the frame cannot be dropped because the FCS is at the end of the Ethernet frame on the wire. The egress interface increments Tx CRC error and it propagates through to the next device in the path. You can use the show hardware internal gatos counters interrupt match stomp command to determine if the Nexus 5000 is propagating CRCs or generating them. If stomp values exist they should have matching CRC values on that interface. If Rx CRC values exist then you know it entered the switchport with the error already. You can move on to the connected device to trace it back. During normal operation Nexus 5000 encounters frames that cannot be forwarded. Frames are characterized as good frames or bad frames. good frame is frame that does not have CRC error or other kind of error bad frame is frame that has CRC error or other kind of error All counters include MAC Control frames where applicable. CO CO Number of frames with transmit packet size less than 64 bytes. Number of frames with transmit packet size equal to 64 bytes. Number of frames with transmit packet size between 65 and 127 bytes. Number of frames with transmit packet size between 128 and 255 bytes. Number of frames with transmit packet size between 256 and 511 bytes. Number of frames with transmit packet size between 512 and 1023 bytes. Number of frames with transmit packet size between 1024 and 1518 bytes. Number of frames with transmit packet size between 1519 and 2047 bytes. Number of frames with transmit packet size between 2048 and 4095 bytes. Number of frames with transmit packet size between 4096 and 8191 bytes. Number of frames with transmit packet size between 8192 and 9216 bytes. Number of frames with transmit regardless of frame length. Total byte count of packet octets Number of 802.3x PAUSE frames transmitted. Number of transmitted priority flow control frames. Number of frames with cl_im_tx_err set to 1 at EOP. Total byte count of good frames. CO CO Number of frames with receive packet size less than 64 bytes. Number of frames with receive packet size equal to 64 bytes. Number of frames with receive packet size between 65 and 127 bytes. Number of frames with receive packet size between 128 and 255 bytes. Number of frames with receive packet size between 256 and 511 bytes. Number of frames with receive packet size between 512 and 1023 bytes. Number of frames with receive packet size between 1024 and 1518 bytes. Number of frames with receive packet size between 1519 and 2047 bytes. Number of frames with receive packet size between 2048 and 4095 bytes. Number of frames with receive packet size between 4096 and 8191 bytes. Number of frames with receive packet size between 8192 and 9216 bytes. Number of frames with receive packet size greater than 9216 bytes. Total number of frames received. This number is the sum of all received packets regardless of frame length. Total byte count of packet octets Number of good broadcast frames received. Number of good 802.1Q tagged VLAN frames received. Number of good frames received that are greater than CFG_xg_rx_stats_max_frame_len. Number of frames received that are The value of this counter is always 0. CO CO CFG_xg_rx_stats_min_frame_len. Number of bad frames received that are not stomped. Number of bad frames received that are stomped. Number of frames with length field check error but no ethertypelength field. When the ethertypelength treated as the length of the frame. This length is compared with the actual frame length. An error is raised when the lengths do not match. Number of bad frames received with frame length greater than CFG_xg_rx_stats_max_frame_len. Number of good 802.3x MAC control frames received. Number of good priority flow control frames received.

Training delivered effectively globally Training in classroom at your cubicle or home ofﬁ ce Concurrently delivered multiplesite training realworld tech training put into practice worldwide Are your companys technical training needs being addressed in the most effective manner MindShare has over 25 years experience in conducting technical training on cuttingedge technologies. We understand the challenges companies have when searching for quality effective training which reduces the to meet those needs. Our courses are taught by highlyskilled enthusiastic knowledgeable and experienced instructors. We bring life to knowledge through wide variety of learning methods and delivery options. MindShare training courses expand your technical skillset PCI Express is registered trademark of the PCISIG Have knowledge that you want to bring to life MindShare will work with you to Bring Your Knowledge to Life. Engage us to transform your knowledge and design courses that can be delivered in classroom or virtual class room settings create online eLearning modules or publish book that you author. We are proud to be the preferred training provider at an extensive list of clients that include Montreal London Munich Paris Madrid Sydney Many of the designations used by manufacturers and sellers to distinguish their prod ucts are claimed as trademarks. Where those designators appear in this book and Add isonWesley was aware of the trademark claim the designations have been printed in initial capital letters or all capital letters. The authors and publishers have taken care in preparation of this book but make no expressed or implied warranty of any kind and assume no responsibility for errors or omissions. No liability is assumed for incidental or consequential damages in connec tion with or arising out of the use of the information or programs contained herein. system or transmitted in any form or by any means electronic mechanical photocopy ing recording or otherwise without the prior written permission of the publisher. Printed in the United States of America. Published simultaneously in Canada. Set in 10 point Palatino by MindShare Inc. AddisonWesley books available for bulk purchases by corporations institutions and other organizations. For more information please contact the Corporate Government and Special Sales Department at 2389682. Find AW Developers Press on the WorldWide Web at How Does Software Tell Device to Generate Hot PCI Express Bus Driver Accesses PCI Express Configuration and PM Registers. Scenario 2 Upstream Component Transmits TLP Just Prior to Receiving L1 Re Split Configuration Transaction Requires Single Bridge Routes ID Addressed Packets Using Bus Number performance characteristics and features of predecessor buses such as PCI and PCIX buses with the goal of discussing the evolution of PCI Express from these predecessor buses. The reader will be able to compare and contrast features and performance points of PCI PCIX and PCI Express buses. The key features of examples of PCI Express system topologies. It describes the layered architecture of device design while providing brief packet formation at transmitter device the transmission and reception of the packet over the PCI Express Link and packet decode at receiver device. PCI Express is the third generation high performance IO bus used to intercon nect peripheral devices in applications such as computing and communication platforms. The first generation buses include the ISA EISA VESA and Micro Channel buses while the second generation buses include PCI AGP and PCIX. PCI Express is an all encompassing IO device interconnect bus that has appli cations in the mobile desktop workstation server embedded computing and The PCI Express architects have carried forward the most beneficial features from previous generation bus architectures and have also taken advantages of new developments in computer architecture. For example PCI Express employs the same usage model and loadstore com munication model as PCI and PCIX. PCI Express supports familiar transactions such as memory readwrite IO readwrite and configuration readwrite trans actions. The memory IO and configuration address space model is the same as PCI and PCIX address spaces. By maintaining the address space model exist ing OSs and driver software will run in PCI Express system without any mod ifications. In other words PCI Express is software backwards compatible with PCI and PCIX systems. In fact PCI Express system will boot an existing OS with no changes to current drivers and application programs. Even PCIACPI power management software will still run. Like predecessor buses PCI Express supports chiptochip interconnect and boardtoboard interconnect via cards and connectors. The connector and card structure are similar to PCI and PCIX connectors and cards. PCI Express motherboard will have similar form factor to existing FR4 ATX motherboards To improve bus performance reduce overall system cost and take advantage of new developments in computer design the PCI Express architecture had to be significantly redesigned from its predecessor buses. PCI and PCIX buses are multidrop parallel interconnect buses in which many devices share one bus. PCI Express on the other hand implements serial pointtopoint type inter connect for communication between two devices. Multiple PCI Express devices are interconnected via the use of switches which means one can practically con nect large number of devices together in system. pointtopoint intercon nect implies limited electrical load on the link allowing transmission and reception frequencies to scale to much higher numbers. Currently PCI Express transmission and reception data rate is 2.5 Gbitssec. serial interconnect between two devices results in fewer pins per device package which reduces PCI Express chip and board design cost and reduces board design complexity. PCI Express performance is also highly scalable. This is achieved by implement ing scalable numbers for pins and signal Lanes per interconnect based on com munication performance requirements for that interconnect. PCI Express implements switchbased technology to interconnect large num ber of devices. Communication over the serial interconnect is accomplished using packetbased communication protocol. Quality Of Service fea tures provide differentiated transmission performance for different applica tions. Hot PlugHot Swap support enables alwayson systems. Advanced power management features allow one to design for low power mobile applica tions. RAS error handling features make PCI Express suitable for robust highend server applications. Hot plug power man agement error handling and interrupt signaling are accomplished inband using packet based messaging rather than sideband signals. This keeps the device pin count low and reduces system cost. The configuration address space available per function is extended to 4KB allowing designers to define additional registers. However new software is required to access this extended configuration register space. In the future PCI Express communication frequencies are expected to double and quadruple to 5 Gbitssec and 10 Gbitssec. Taking advantage of these fre quencies will require Physical Layer redesign of device with no changes nec essary to the higher layers of the device design. Additional mechanical form factors are expected. Support for Server IO Mod ule Newcard and Cable form factors are expected. In an effort to compare and contrast features of predecessor buses the next sec defined by the PCI Special Interest Group . These buses shown in In comparing these buses it is not the authors intention to suggest that any one bus is better than any other bus. Each bus architecture has its advantages and disadvantages. After evaluating the features of each bus architecture particu lar bus architecture may turn out to be more suitable for specific application than another bus architecture. For example it is the system designers responsi bility to determine whether to implement PCIX bus or PCI Express for the ment the features of each bus architecture so that the designer can evaluate the The table shows the evolution of bus frequencies and bandwidths. As is obvi ous increasing bus frequency results in increased bandwidth. However increasing bus frequency compromises the number of electrical loads or num ber of connectors allowable on bus at that frequency. At some point for given bus architecture there is an upper limit beyond which one cannot further increase the bus frequency hence requiring the definition of new bus architec PCI Express interconnect that connects two devices together is referred to as Link. Link consists of either x1 x2 x4 x8 x12 x16 or x32 signal pairs in each direction. These signals are referred to as Lanes. designer determines how many Lanes to implement based on the targeted performance benchmark required on given Link. mentations. As is apparent from this table the peak bandwidth achievable with PCI Express is significantly higher than any existing bus today. Let us consider how these bandwidth numbers are calculated. The transmis sionreception rate is 2.5 Gbitssec per Lane per direction. To support greater degree of robustness during data transmission and reception each byte of data transmitted is converted into 10bit code . In other words for every Byte of data to be transmitted 10bits of encoded data are actually transmitted. The result is 25 additional overhead Peak Bandwidth Double all these bandwidth numbers for 64bit bus implementations PCI Express implements dualsimplex Link which implies that data is trans mitted and received simultaneously on transmit and receive Lane. The aggre gate bandwidth assumes simultaneous traffic in both directions. by 10bits per Byte . Performance Per Pin Compared pin. This results in device package with fewer pins and motherboard imple mentation with few wires and hence overall reduced system cost per unit band assume 84 pins per device. This includes 46 signal pins interrupt and power management pins error pins and the remainder are power and ground pins. The last bar associated with x8 PCI Express Link assumes 40 pins per device which include 32 signal lines and the rest are IO Bus Architecture Perspective sists of Host bustoPCI bus bridge also referred to as the North bridge. Associated with the North bridge is the system memory bus graphics bus and 33 MHz PCI bus. IO devices share the PCI bus and are connected to it in multidrop fashion. These devices are either connected directly to the PCI bus on the motherboard or by way of peripheral card plugged into connec tor on the bus. Devices connected directly to the motherboard consume one electrical load while connectors are accounted for as 2 loads. South bridge bridges the PCI bus to the ISA bus where slower lower performance peripher als exist. Associated with the south bridge is USB and IDE bus. CD or hard disk is associated with the IDE bus. The South bridge contains an interrupt con troller to which interrupt signals from PCI devices are connected. The interrupt controller is connected to the CPU via an INTR signal or an APIC bus. The South bridge is the central resource that provides the source of reset along with Super IO chip which includes keyboard mouse floppy disk con troller and serialparallel bus controllers. The PCI bus arbiter logic is included in the North bridge. MHz. The address bus width is 32bits although PCI optionally supports 64bit address bus. The data bus width is implemented as either 32bits or 64bits depending on bus performance requirement. The address and data bus signals are multiplexed on the same pins to reduce pin count. Command signals encode the transaction type of the bus cycle that master devices initiate. PCI supports 12 transaction types that include memory IO and configuration readwrite bus cycles. Control signals such as FRAME DEVSEL TRDY IRDY STOP are handshake signals related signals interrupt signals and power management signals. PCI master device implements minimum of 49 signals. Any PCI master device that wishes to initiate bus cycle first arbitrates for use of the PCI bus by asserting request to the arbiter in the North bridge. After receiving grant from the arbiter and checking that the bus is The PCI specification theoretically supports 32 devices per PCI bus. This means that PCI enumeration software will detect and recognize up to 32 devices per bus. However as rule of thumb PCI bus can support maximum of 1012 electrical loads at 33 MHz. PCI implements static clocking protocol with clock period of 30 ns at 33 MHz. PCI implements reflectedwave switching signal drivers. The driver drives half signal swing signal on the rising edge of PCI clock. The signal propagates down the PCI bus transmission line and is reflected at the end of the transmis sion line where there is no termination. The reflection causes the half swing sig nal to double. The doubled signal must settle to steady state PCI Express System Architecture value with sufficient setup time prior to the next rising edge of PCI clock where receiving devices sample the signal. The total time from when driver drives signal until the receiver detects valid signal must be less than the clock period of 30 ns. The more electrical loads on bus the longer it takes for the signal to propagate and double with sufficient setup to the next rising edge of clock. As mentioned earlier 33 MHz PCI bus meets signal timing with no more than 1012 loads. Connectors on the PCI bus are counted as 2 loads because the connector is accounted for as one load and the peripheral card with PCI device is the sec designed with maximum of 45 connectors. To connect any more than 1012 loads in system requires the implementation loads to be connected on the secondary PCI bus 1. The PCI specification theoret ically supports up to 256 buses in system. This means that PCI enumeration software will detect and recognize up to 256 PCI bridges per system. Consider an example in which the CPU communicates with PCI peripheral ure which is initiated by the CPU and targets peripheral device is referred to as programmed IO transaction. Software commands the CPU to initiate memory or IO readwrite bus cycle on the host bus targeting an address mapped in PCI devices address space. The North bridge arbitrates for use of the PCI bus and when it wins ownership of the bus generates PCI memory or clock of this bus cycle all target devices decode PCI Express System Architecture the address. One target decodes the address and claims the transaction. The master com municates with the claiming target . Data is transferred between master and target in subsequent clocks after the address phase of the bus cycle. Either 4 bytes or 8 bytes of data are transferred per clock tick depend ing on the PCI bus width. The bus cycle is referred to as burst bus cycle if data is transferred backtoback between master and target during multiple data phases of that bus cycle. Burst bus cycles result in the most efficient use of PCI At 33 MHz and the bus width of 32bits peak bandwidth achievable is 1 Programmed IO described performance characteristics and features of predecessor buses such as PCI and PCIX buses with the goal of discussing the evolution of PCI Express from these predecessor buses. It compared and contrasted features and perfor mance points of PCI PCIX and PCI Express buses. The key features of PCI ples of PCI Express system topologies. describes the layered approach to PCI Express device design while describing the function of each device layer. Packet types employed in accomplishing data memory read to read data from completer across Link. consisting of switches. Packets are routed based on memory address IO PCI Express employs packets to accomplish data transfers between devices. root complex can communicate with an endpoint. An endpoint can communi cate with root complex. An endpoint can communicate with another end point. Communication involves the transmission and reception of packets PCI Express transactions can be grouped into four categories 1 memory 2 IO 3 configuration and 4 message transactions. Memory IO and configuration transactions are supported in PCI and PCIX architectures but the message transaction is new to PCI Express. Transactions are defined as series of one or more packet transmissions required to complete an informa list of transactions. These transactions can be categorized into nonposted trans actions and posted transactions. For Nonposted transactions requester transmits TLP request packet to completer. At later time the completer returns TLP completion packet back to the requester. Nonposted transactions are handled as split transactions simi purpose of the completion TLP is to confirm to the requester that the completer has received the request TLP. In addition nonposted read transactions contain data in the completion TLP. NonPosted write transactions contain data in the write request TLP. For Posted transactions requester transmits TLP request packet to compl eter. The completer however does NOT return completion TLP back to the requester. Posted transactions are optimized for best performance in completing the transaction at the expense of the requester not having knowledge of success ful reception of the request by the completer. Posted transactions may or may PCI Express Transaction Protocol describe how these packets are used to complete transactions at system level and not to describe the packet routing through the PCI Express fabric nor to Completion without Data associated with Locked Memory Read Completion with Data associated with Locked Memory Read NonPosted Read Transactions plete nonposted read transaction. To complete this transfer requester trans mits nonposted read request TLP to completer it intends to read data from. Nonposted read request TLPs include memory read request IO read request and configuration read request type 0 or type 1 TLPs. Requesters may be root complex or endpoint devices . The request TLP is routed through the fabric of switches using information in The completer can be root complex switches bridges or endpoints. amount of data specified in the request from the targeted address. The compl eter creates single completion TLP or multiple completion TLPs with data KBytes of data per CplD packet. The completion packet contains routing information necessary to route the packet back to the requester. This completion packet travels through the same path and hierarchy of switches as the request packet. Requesters uses tag field in the completion to associate it with request TLP of the same tag value it transmitted earlier. Use of tag in the request and com pletion TLPs allows requester to manage multiple outstanding transactions. If completer is unable to obtain requested data as result of an error it returns requester determines how to handle the error at the software layer. NonPosted Read Transaction for Locked Requests to complete nonposted locked read transaction. To complete this transfer requester transmits memory read locked request TLP. The requester can only be root complex which initiates locked request on the behalf of the CPU. Endpoints are not allowed to initiate locked requests. The locked memory read request TLP is routed downstream through the fabric makes its way to targeted completer. The completer can only be legacy end point. The entire path from root complex to the endpoint is locked including the ingress and egress port of switches in the pathway. CplD Completion with data for normal completion of MRd IORd CfgRd0 CfgRd1 Cpl Completion without data for error completion of MRd IORd CfgRd0 CfgRd1 PCI Express System Architecture amount of data specified in the request from the targeted address. The compl eter creates one or more locked completion TLP with data along with via the path and hierarchy of switches as the original request. The CplDLk packet contains routing information necessary to route the packet back to the requester. Requesters uses tag field in the completion to associate it with request TLP of the same tag value it transmitted earlier. Use of tag in the request and completion TLPs allows requester to manage multiple out standing transactions. If the completer is unable to obtain the requested data as result of an error it within the packet. The requester who receives the error notification via the CplLk TLP must assume that atomicity of the lock is no longer guaranteed and thus determine how to handle the error at the software layer. The path from requester to completer remains locked until the requester at later time transmits an unlock message to the completer. The path and ingress egress ports of switch that the unlock message passes through are unlocked. CplDLk Locked normal Completion with data for normal completion of MRdLk CplLk Locked error Completion without data for error completion of MRdLk NonPosted Write Transactions eter to complete nonposted write transaction. To complete this transfer requester transmits nonposted write request TLP to completer it intends to write data to. Nonposted write request TLPs include IO write request configuration write request type 0 or type 1 TLPs. Memory write request and message requests are posted requests. Requesters may be root complex or endpoint device . request packet with data is routed through the fabric of switches using infor data. The completer creates single completion packet without data to confirm reception of the write request. This is the purpose of the completion. Cpl Completion without data for normal or error completion of IOWr CfgWr0 CfgWr1 The completion packet contains routing information necessary to route the packet back to the requester. This completion packet will propagate through the same hierarchy of switches that the request packet went through before making its way back to the requester. The requester gets confirmation notification that the write request did make its way successfully to the completer. If the completer is unable to successfully write the data in the request to the indication. The requester who receives the error notification via the Cpl TLP determines how to handle the error at the software layer. Posted Memory Write Transactions implies that the completer returns no completion notification to inform the requester that the memory write request packet has reached its destination suc cessfully. No time is wasted in returning completion thus backtoback posted writes complete with higher performance relative to nonposted transactions. The write request packet which contains data is routed through the fabric of makes its way to completer. The completer accepts the specified amount of data within the packet. Transaction over. If the write request is received by the completer in error or is unable to write the is not informed via the hardware protocol. The completer could log an error and generate an error message notification to the root complex. Error handling soft Posted Message Transactions 64. There are two categories of message request TLPs Msg and MsgD. Some message requests propagate from requester to completer some are broadcast requests from the root complex to all endpoints some are transmitted by an endpoint to the root complex. Message packets may be routed to completer based on the messages address device ID or routed implicitly. Message request The completer accepts any data that may be contained in the packet andor performs the task specified by the message. Message request support eliminates the need for sideband signals in PCI Express system. They are used for PCI style legacy interrupt signaling power management protocol error signaling unlocking path in the PCI Express fab ric slot power support hot plug protocol and vender defined purposes. MWr Memory Write Request. No completions for this transaction Some Examples of Transactions between requester and completer to accomplish transaction. The examples consist of memory read IO write and Memory write. Memory Read Originated by CPU Targeting an Endpoint memory read transaction. The root complex on the behalf of the CPU initiates nonposted memory read from the completer endpoint shown. The root com plex transmits an MRd packet which contains amongst other fields an address TLP type requester ID and length of transfer field. Switch which is 3 port switch receives the packet on its MsgD Message Request with data upstream port. The switch logically appears like 3 virtual bridge device con nected by an internal bus. The logical bridges within the switch contain mem ory and IO base and limit address registers within their configuration space similar to PCI bridges. The MRd packet address is decoded by the switch and compared with the baselimit address range registers of the two downstream logical bridges. The switch internally forwards the MRd packet from the upstream ingress port to the correct downstream port . The MRd packet is forwarded to switch . Switch decodes the address in similar manner. Assume the MRd packets is forwarded to the righthand port so that the completer endpoint receives the MRd packet. ers the requested data and returns completion packet with data . The original request TLP. The requester ID is used to route the completion packet Transaction Routing described the layered approach to PCI Express device design while describing the function of each device layer. Packet types employed in accomplishing data memory read to read data from completer across Link. and the mechanisms used by device in deciding whether to accept forward or reject packet arriving at an ingress port. Because Data Link Layer Packets and Physical Layer ordered set link traffic are never forwarded the emphasis here is on Transaction Layer Packet types and the three routing methods associated with them address routing ID routing and implicit rout up PCIcompatible plugandplay addressing within system IO and memory maps as well as key elements in the PCI Express packet protocol used in mak ing routing decisions. Packets and Data Link Layer Packets . The use and format of each TLP and DLLP packet type is covered along with definitions of the field within Unlike sharedbus architectures such as PCI and PCIX where traffic is visible to each device and routing is mainly concern of bridges PCI Express devices are dependent on each other to accept traffic or forward it in the direction of the ultimate recipient. independent pointtopoint links connecting each device with one or more neighbors. As traffic arrives at the inbound side of link interface the device checks for errors then makes one of three decisions Accept the traffic and use it internally. Forward the traffic to the appropriate outbound port. Reject the traffic because it is neither the intended target nor an interface to it Assuming link is fully operational the physical layer receiver interface of each device is prepared to monitor the logical idle condition and detect the arrival of the three types of link traffic Ordered Sets DLLPs and TLPs. Using control symbols which accompany the traffic to determine framing boundaries and traffic type PCI Express devices then make distinction between traffic which is local to the link vs. traffic which may require routing to other links . Local link traffic which includes Ordered Sets and Data Link Layer Packets isnt forwarded and carries no routing information. Transaction Layer Packets can and do move from link to link using routing information Express ports are responsible for handling their own traffic as well as forward ing other traffic between ingress ports and any enabled egress ports. Also note that while peerpeer transaction support is required of switches it is optional for multiport Root Complex. It is up to the system designer to account for peertopeer traffic when selecting devices and laying out motherboard. Endpoints Have Limited Routing Responsibilities single link interface and lack the ability to route inbound traffic to other links. For this reason and because they dont reside on shared busses endpoints never expect to see ingress port traffic which is not intended for them where devices commonly decode addresses PCI Express System Architecture and commands not targeting them. Endpoint routing is limited to accepting or Before transactions can be generated by requester accepted by the completer and forwarded by any devices in the path between the two all devices must be on traffic type system memory and IO address assignments etc. In keeping with PCI plugandplay configuration methods each PCI express device is dis covered memory and IO address resources are assigned to them and switch bridge devices are programmed to forward transactions on their behalf. Once routing is programmed bus mastering and target address decoding are enabled. Thereafter devices are prepared to generate accept forward or reject Local traffic occurs between the transmit interface of one device and the receive interface of its neighbor for the purpose of managing the link itself. This traffic is never forwarded or flow controlled when sent it must be accepted. Local traffic is further classified as Ordered Sets exchanged between the Physical Lay ers of two devices on link or Data Link Layer packets exchanged between the Data Link Layers of the two devices. These are sent by each physical layer transmitter to the physical layer of the cor responding receiver to initiate link training compensate for clock tolerance or Each ordered set is constructed of 10bit control symbols that are created within the physical layer. These symbols have common name as well as alphnumeric code that defines the 10 bits pattern of 1s and 0s of which they are comprised. For example the SKP symbol has 10bit value represented each ordered set is fixed in size consisting of 4 or 16 characters. Again the receiver is required to consume them as they are sent. Note that the COM con trol symbol is used to indicate the start of any ordered set. PCI Express System Architecture routing and the mechanisms used by device in deciding whether to accept forward or reject packet arriving at an ingress port. Because Data Link Layer Packets and Physical Layer ordered set link traffic are never forwarded the emphasis here is on Transaction Layer Packet types and the three routing methods associated with them address routing ID routing and Express to set up PCIcompatible plugandplay addressing within system IO and memory maps as well as key elements in the PCI Express packet protocol used in making routing decisions. Information moves between PCI Express devices in packets and the two major classes of packets are Transaction Layer Packets and Data Link Layer Pack ets . The use format and definition of all TLP and DLLP packet types TLPs between each port as they travel between the requester and completer cally triggered when TLP transmission error is detected on given link. The PCI Express protocol improves upon methods used by earlier busses to exchange data and to signal system events. In addition to supporting basic memory IO and configuration readwrite transactions the links elimi nate many sideband signals and replaces them with inband messages. With the exception of the logical idle indication and physical layer Ordered Sets all information moves across an active PCI Express link in fundamental chunks called packets which are comprised of 10 bit control and data symbols. The two major classes of packets exchanged between two PCI Express devices are high level Transaction Layer Packets and lowlevel link maintenance packets called Data Link Layer Packets . Collectively the various TLPs and DLLPs allow two devices to perform memory IO and Configuration Space transactions reliably and use messages to initiate power management events There are some distinct advantages in using packetbased protocol especially when it comes to data integrity. Three important aspects of PCI Express packet protocol help promote data integrity during link transmission Some early bus protocols allow transfers of indeterminate size making identification of payload boundaries impossible until the end of the transfer. In addition an early transaction end might be signaled by either agent resulting in partial transfer. In these cases it is difficult for the sender of data to calculate and send checksum or CRC covering an entire payload when it may terminate unexpectedly. Instead PCI uses simple parity scheme which is applied and checked for each bus phase completed. In contrast each PCI Express packet has known size and format and the cates the packet type and presence of any optional fields. The size of each packet field is either fixed or defined by the packet type. The size of any data there are no early transaction terminations by the recipient. This structured packet format makes it possible to insert additional information into the packet Each TLP and DLLP packet sent is framed with Start and End control symbol clearly defining the packet boundaries to the receiver. Note that the Start and bits each. This is big improvement over PCI and PCIX which use the assertion and deassertion of single FRAME signal to indicate the beginning and end of transaction. glitch on the FRAME signal could cause target to misconstrue bus events. In contrast PCI Express receiver must properly decode complete 10 bit symbol before concluding link activity is beginning or ending. Unexpected or unrecognized Unlike the sideband parity signals used by PCI devices during the address and each data phase of transaction the inband 16bit or 32bit PCI Express CRC value protects the entire packet . In addition to CRC TLP packets also have packet sequence number appended to them by the transmitter so that if an error is detected at the receiver the specific packet which were received in error may be resent. The transmitter maintains copy of each TLP sent in Retry Buffer until it is checked and acknowledged by the receiver. This TLP acknowledgement mechanism forms the basis of linklevel TLP error correction and is very important in deep topologies where devices may be many links away from the host in the event an error occurs and CPU intervention would otherwise be In PCI Express terminology highlevel transactions originate at the device core of the transmitting device and terminate at the core of the receiving device. The Transaction Layer is the starting point in the assembly of outbound Transaction Layer Packets and the end point for disassembly of inbound TLPs at the receiver. Along the way the Data Link Layer and Physical Layer of each device contribute to the packet assembly and disassembly as described below. TLPs Are Assembled And Disassembled side of link and disassembly at the receiver. The key stages in Transaction Layer Packet protocol are listed below. The numbers correspond to those in Fig Device Bs core passes request for service to the PCI Express hardware interface. How this done is not covered by the PCI Express Specification and is devicespecific. General information contained in the request would The PCI Express command to be performed Start address or ID of target Transaction type Data payload size Virtual ChannelTraffic class information Attributes of the transfer No Snoop bit set Relaxed Ordering set etc. based on the request from the core. Before sending TLP to the Data Link Layer flow control credits and ordering rules must be applied. When the TLP is received at the Data Link Layer Sequence Number is assigned and Link CRC is calculated for the TLP . The TLP is then passed on to the Physical Layer. At the Physical Layer byte striping scrambling encoding and serialization are performed. STP and END control characters are appended to the packet. The packet is sent out on the transmit side of the link. At the Physical Layer receiver of Device deserialization framing symbol check decoding and byte unstriping are performed. Note that at the Phys ical Layer the first level or error checking is performed the transmitter Information moves between PCI Express devices in packets. The two major classes of packets are Transaction Layer Packets and Data Link Layer Packets . The use format and definition of all TLP and DLLP packet of TLPs from one device to another device across the Link. The use of ACK DLLPs to confirm reception of TLPs and the use of NAK DLLPs to indicate ing TLPs in the event that NAK DLLP is received. that support Quality of Service concepts in PCI Express implementations. The concept of Quality of Service in the context of PCI Express is an attempt to pre dict the bandwidth and latency associated with the flow of different transaction streams traversing the PCI Express fabric. The use of QoS is based on applica tionspecific software assigning Traffic Class values to transactions which define the priority of each transaction as it travels between the Requester and Completer devices. Each TC is mapped to Virtual Channel that is used to manage transaction priority via two arbitration schemes called port and VC Reliable Transport of TLPs Across Each Link Reliable transport of TLPs from one device to another device across the The receivers Transaction Layer should receive TLPs in the same order that the transmitter sent them. The Data Link Layer must preserve this order despite any occurrence of errors that require TLPs to be replayed . Memory IO Configuration RW Requests or Message Requests or Completions The ACKNAK protocol associated with the Data Link Layer is described with For every TLP that is sent from one device to another across one Link the receiver checks for errors in the TLP . The receiver Device notifies transmitter Device on good or bad recep tion of TLPs by returning an ACK or NAK DLLP. Reception of an ACK DLLP by the transmitter indicates that the receiver has received one or more TLP successfully. Reception of NAK DLLP by the transmitter indicates that the receiver has received one or more TLP in error. Device which receives NAK DLLP then resends associated TLP which will hopefully arrive at the receiver successfully without error. The error checking capability in the receiver and the transmitters ability to re send TLPs if TLP is not received correctly is the core of the ACKNAK proto Elements of the ACKNAK Protocol via Link. The diagram shows all of the major Data Link Layer elements associ ated with reliable TLP transfer from the transmitters Transaction Layer to the receivers Transaction Layer. Packet order is maintained by the transmitters Transmitter Elements of the ACKNAK Protocol associated with processing of outbound TLPs and inbound ACKNAK DLLPs. The replay buffer stores TLPs with all fields including the Data Link Layer related Sequence Number and LCRC fields. The TLPs are saved in the order of arrival from the Transaction Layer before transmission. Each TLP in the Replay Buffer contains Sequence Number which is incrementally greater than the sequence number of the previous TLP in the buffer. When the transmitter receives acknowledgement via an ACK DLLP that TLPs have reached the receiver successfully it purges the associated TLPs from the Replay Buffer. If on the other hand the transmitter receives NAK DLLP it This counter generates the Sequence Number assigned to each new transmitted TLP. The counter is 12bit counter that is initialized to 0 at reset or when the Data Link Layer is in the inactive state. It increments until it reaches 4095 and then rolls over to 0 . The LCRC Generator provides 32bit LCRC for the TLP. The LCRC is calcu Sequence Number. The receiver uses the TLPs LCRC field to check for CRC This 2bit counter stores the number of replay attempts following either recep tion of NAK DLLP or REPLAY_TIMER timeout. When the REPLAY_NUM count rolls over from 11b to 00b the Data Link Layer triggers Physical Layer waits for completion of retraining before attempting to transmit TLPs once again. The REPLAY_NUM counter is initialized to 00b at reset or when the Data Link Layer is inactive. It is also reset whenever an ACK is received indi cating that forward progress is being made in transmitting TLPs. The REPLAY_TIMER is used to measure the time from when TLP is transmit ted until an associated ACK or NAK DLLP is received. The REPLAY_TIMER is started when the last Symbol of any TLP is sent. It restarts from 0 each time that there are outstanding TLPs in the Replay Buffer. It resets to 0 and holds when there are no outstanding TLPs in the Replay Buffer or until restart conditions are met for each NAK received or when the REPLAY_TIMER expires. It is not advanced during Link retraining. This 12bit register tracks or stores the Sequence Number of the most recently received ACK or NAK DLLP. It is initialized to all 1s at reset or when the Data field of received ACK or NAK DLLP. The ACKD_SEQ count is com pared with the NEXT_TRANSMIT_SEQ count. IF mod 4096 2048 THEN New TLPs from Transaction Layer are not accepted by Data Link Layer until this equation is no longer true. In addition Data Link Layer protocol error there is separation greater than 2047 between NEXT_TRANSMIT_SEQ and ACKD_SEQ. separation greater than 2047 between the sequence number of TLP being transmitted and that of TLP in the replay buffer that receives an ACK or NAK DLLP. Also the ACKD_SEQ count is used to check for forward progress made in transmitting TLPs. If no forward progress is made after 3 additional replay attempts the Link in retrained. This block checks for CRC errors in DLLPs returned from the receiver. Good DLLPs are further processed. If DLLP CRC error is detected the DLLP is dis Definition The Data Link Layer is in the inactive state when the Physical Layer State Machine is in the Detect Polling Configuration Disabled Reset of TLPs between each port as they travel between the requester and completer cally triggered when TLP transmission error is detected on given link. support Quality of Service concepts in PCI Express implementations. The con cept of Quality of Service in the context of PCI Express is an attempt to predict the bandwidth and latency associated with the flow of different transaction streams traversing the PCI Express fabric. The use of QoS is based on applica tionspecific software assigning Traffic Class values to transactions which define the priority of each transaction as it travels between the Requester and Completer devices. Each TC is mapped to Virtual Channel that is used to manage transaction priority via two arbitration schemes called port and VC Control Protocol. This protocol requires each device to implement creditbased link flow control for each virtual channel on each port. Flow control guarantees that transmitters will never send Transaction Layer Packets that the receiver cant accept. This prevents receive buffer overruns and eliminates the need for inefficient disconnects retries and waitstates on the link. Flow Con trol also helps enable compliance with PCI Express ordering rules by maintain ing separate virtual channel Flow Control buffers for three types of transactions Posted NonPosted and Completions . Quality of Service is generic term that normally refers to the ability of network or other entity to provide predictable latency and bandwidth. QoS is of particular interest when applications require guaran teed bus bandwidth at regular intervals such as audio data. To help deal with this type of requirement PCI Express defines isochronous transactions that require high degree of QoS. However QoS can apply to any transaction or series of transactions that must traverse the PCI Express fabric. Note that QoS can only be supported when the system and devicespecific software is PCI QoS can involve many elements of performance including Several features of PCI Express architecture provide the mechanisms that make QoS achievable. The PCI Express features that support QoS include PCI Express uses these features to support two general classes of transactions that can benefit from the PCI Express implementation of QoS. Isochronous Transactions from Iso chronous these transac tions require constant bus bandwidth at regular intervals along with guaran teed latency. Isochronous transactions are most often used when synchronous connection is required between two devices. For example CDROM drive containing music CD may be sourcing data to speakers. synchronous con nection exists when headset is plugged directly into the drive. However when the audio card is used to deliver the audio information to set of external speakers isochronous transactions may be used to simplify the delivery of the Asynchronous Transactions This class of transactions involves wide vari ety of applications that have widely varying requirements for bandwidth and latency. QoS can provide the more demanding applications with higher priority than the less demanding applications. In this way software can establish hierarchy of traf fic classes for transactions that permits differentiation of transaction priority based on their requirements. The specification refers to this capability as differ PCI Express supports QoS and the associated TC VC and arbitration mecha nisms so that isochronous transactions can be performed. classic example of device that benefits from isochronous transaction support is video camera attached to tape deck. This realtime application requires that image and audio data be transferred at constant rate . This type of application is typically supported via direct synchronous attachment Two devices connected directly perform synchronous transfers. synchronous source delivers data directly to the synchronous sink through use of common reference clock. In our example the video camera sends audio and video data to the tape deck which immediately stores the data in real time with little or no data buffering and with only slight delay due to signal propagation. When these devices are connected via PCI Express synchronous connection is not possible. Instead PCI Express emulates synchronous connections through the use of isochronous transactions and data buffering. In this scenario isochro nous transactions can be used to ensure that constant amount of data is deliv ered at specified intervals thus achieving the required The synchronous source accumu lates data in Buffer during service interval 1 . The camera delivers the accumulated data to the synchronous sink sometime during the next service interval . The camera also The tape deck buffers the incoming data which can then be delivered synchronously for recording on tape during service interval 3. During SI 3 the camera once again accumulates data into Buffer and the Management of an isochronous communications channel is based on Traffic Class value and an associated Virtual Channel number that software assigns during initialization. Hardware components including the Requester of transaction and all devices in the path between the requester and completer hipriority virtual channel. The requester initiates isochronous transactions that include TC value repre senting the desired QoS. The Requester injects isochronous packets into the fab ric at the required rate and all devices in the path between the isochronous transactions at the specified interval. Any intermediate device along the path must convert the TC to the associated VC used to control transac tion arbitration. This arbitration results in the desired bandwidth and latency for transactions with the assigned TC. Note that the TC value remains constant for given transaction while the VC number may change from link to link. Various types of asynchronous traffic have different priority from the system perspective. For example ethernet traffic requires higher priority than mass storage transactions. PCI Express software can establish different TC values and associated virtual chan nels and can set up the communications paths to ensure different delivery poli cies are established as required. Note that the specification does not define specific methods for identifying delivery requirements or the policies to be used PCI does not include any QoSrelated features similar to those defined by PCI Express. Many questions arise regarding the need for such an elaborate scheme for managing traffic flow based on QoS and differentiated services. Without implementing these new features the bandwidth available with PCI Express system is far greater and latencies much shorter than PCIbased implementa tions due primarily to the topology and higher delivery rates. Consequently aside from the possible advantage of isochronous transactions there appears to be little advantage to implementing systems that support multiple Traffic Classes and Virtual Channels. While this may be true for most desktop PCs other highend applications may benefit significantly from these new features. The PCI Express specification also opens the door to applications that demand the ability to differentiate and man age system traffic based on Traffic Class prioritization. During initialization PCI Express devicedriver communicates the levels of QoS that it desires for its transactions and the operating system returns TC val ues that correspond to the QoS requested. The TC value ultimately determines the relative priority of given transaction as it traverses the PCI Express fabric. Two hardware mechanisms provide guaranteed isochronous bandwidth and These arbitration mechanisms use VC numbers to manage transaction priority. System configuration software must assign VC IDs and set up the association between the traffic class assigned to transaction and the virtual channel to be used when traversing each link. This is done via VC configuration registers mapped within the extended configuration address space. The list of these reg tion that supports Quality of Service concepts in PCI Express implementations. The concept of Quality of Service in the context of PCI Express is an attempt to predict the bandwidth and latency associated with the flow of different transac tion streams traversing the PCI Express fabric. The use of QoS is based on appli cationspecific software assigning Traffic Class values to transactions which define the priority of each transaction as it travels between the Requester and Completer devices. Each TC is mapped to Virtual Channel that is used to manage transaction priority via two arbitration schemes called port and Protocol. This protocol requires each device to implement creditbased link flow control for each virtual channel on each port. Flow control guarantees that transmitters will never send Transaction Layer Packets that the receiver cant accept. This prevents receive buffer overruns and eliminates the need for inefficient disconnects retries and waitstates on the link. Flow Control also helps enable compliance with PCI Express ordering rules by maintaining sepa rate virtual channel Flow Control buffers for three types of transactions Posted NonPosted and Completions . as well as PCI and PCIX devices that may be attached to PCI Express fabric. The discussion describes the ProducerConsumer programming model upon which the fundamental ordering rules are based. It also describes the potential describes the weak ordering solution and specifies the rules defined for dead The ports at each end of every PCI Express link must implement Flow Control. Before transaction packet can be sent across link to the receiving port the transmitting port must verify that the receiving port has sufficient buffer space to accept the transaction to be sent. In many other architectures including PCI and PCIX transactions are delivered to target device without knowing if it can accept the transaction. If the transaction is rejected due to insufficient buffer space the transaction is resent until the transaction completes. This procedure can severely reduce the efficiency of bus by wasting bus band width when other transactions are ready to be sent. Because PCI Express is pointtopoint implementation the Flow Control mechanism would be ineffective if only one transaction stream was pending transmission across link. That is if the receive buffer was temporarily full the transmitter would be prevented from sending subsequent transaction due to transaction ordering requirements thereby blocking any further transfers. PCI Express improves link efficiency by implementing multiple flowcontrol buffers for separate transaction streams . Because Flow Control is managed separately for each virtual channel implemented for given link if the Flow Control buffer for one VC is full the transmitter can advance to another VC buffer and send transactions associated with it. The link Flow Control mechanism uses creditbased mechanism that allows the transmitting port to check buffer space availability at the receiving port. Control credits to the port at the opposite end of the link. The receiving port of credits that have been freed up. This is accomplished via Flow Control Flow control logic is located in the transaction layer of the transmitting and receiving devices. Both transmitter and receiver sides of each device are control buffer space they have available to the device on the opposite end of . The number of Flow Control Credits within each buffer is for warded from the transaction layer to the transmit side of the link layer as this credit information to the receiver at the opposite end of the link. This is done for each Flow Control Buffer. Flow Control DLLPs from the device at the opposite end of the link. This trol Counters that track the amount of Flow Control Buffer space in the Credit Checks Made Each transmitter check consults the Flow Control Counters to check available credits. If sufficient credits are available to receive the transaction pending delivery then the transaction is forwarded to the link layer and is ultimately sent to the opposite device. If enough credits are not available the transaction is temporarily blocked until addi Flow control buffers are implemented for each VC resource supported by PCI Express port. Recall that devices at each end of the link may not support the same number of VC resources therefore the maximum number of VCs config ured and enabled by software is the greatest number of VCs in common Each VC Flow Control buffer at the receiver is managed for each category of transaction flowing through the virtual channel. These categories are NonPosted Transactions Memory Reads Configuration Reads and Completions Read Completions and Write Completions of each transaction. Flow control operates independently for each of the six before the transaction can be sent. Note that when transaction is received into VC Flow Control buffer that ordering must be maintained when the transac tions are forwarded to software or to an egress port in the case of switch. The Packets. Note that DLLPs do not require Flow Control credits because they orig The specification defines the requirements of the Flow Control mechanism by describing conceptual registers and counters along with procedures and mecha These elements define the functional requirements however the actual imple specified model that serves to explain the concept and define the requirements. The approach taken focuses on single flow control example for nonposted The Flow Control Elements receiver when managing flow control. This diagram illustrates transactions flowing in single direction across link but of course another set of these ele ments is used to support transfers in the opposite direction. The primary func tion of each element within the transmitting and receiving devices is listed below. Note that for single direction these Flow Control elements are dupli cated for each Flow Control receive buffer yielding six sets of elements. This Flow Control Protocol. This protocol requires each device to implement credit based link flow control for each virtual channel on each port. Flow control guar antees that transmitters will never send Transaction Layer Packets that the receiver cant accept. This prevents receive buffer overruns and eliminates the need for inefficient disconnects retries and waitstates on the link. Flow Control also helps enable compliance with PCI Express ordering rules by main taining separate Virtual Channel Flow Control buffers for three types of transac tions Posted NonPosted and Completions . well as PCI and PCIX devices that may be attached to PCI Express fabric. The discussion describes the ProducerConsumer programming model upon which the fundamental ordering rules are based. It also describes the potential perfor fies the rules defined for deadlock avoidance. Native PCI Express devices that require interrupt support must use the Mes sage Signaled Interrupt mechanism defined originally in the PCI 2.2 spec legacy support that permits virtualization of the PCI INTx signals required by As with other protocols PCI Express imposes ordering rules on transactions moving through the fabric at the same time. The reasons for the ordering rules Ensuring that the completion of transactions is deterministic and in the Avoiding deadlocks conditions. Maintaining compatibility with ordering already used on legacy buses . Maximize performance and throughput by minimizing read latencies and managing readwrite ordering. PCI Express ordering is based on the same ProducerConsumer model as PCI. The split transaction protocol and related ordering rules are fairly straight for ward when restricting the discussion to transactions involving only native PCI Express devices. However ordering becomes more complex when including support for the legacy buses mentioned in bullet three above. Rather than presenting the ordering rules defined by the specification and block approach. Each major ordering concern is introduced one at time. The discussion begins with the most conservative approach to ordering progresses to more aggressive approach and culmi nates with the ordering rules presented in the specification. The discussion is The ProducerConsumer programming model upon which the fundamen tal ordering rules are based. The fundamental PCI Express device ordering requirements that ensure the ProducerConsumer model functions correctly. The Relaxed Ordering feature that permits violation of the ProducerCon sumer ordering when the device issuing request knows that the transac tion is not part of ProducerConsumer programming sequence. Modification of the strong ordering rules to improve performance. Avoiding deadlock conditions and support for PCI legacy implementations. Readers familiar with the ProducerConsumer programming model may The ProducerConsumer model is common methodology that two requester capable devices might use to communicate with each other. Consider the fol network adapter begins to receive stream of compressed video data over the network and performs series of memory write transactions to deliver the stream of compressed video data into Data buffer in memory . After the Producer moves the data to memory it performs memory write to indicate that the data is ready for processing. Another requester periodically performs cessed. In this example this requester is video decompressor that will decompress and display the data. When it sees that the Flag has been set by the Producer it performs mem ory write to clear the Flag followed by burst memory read transaction to read the compressed data from the Data buffer in memory. When it is done consuming the Data the Consumer writes the completion In the meantime the Producer has been reading periodically from the Sta other requester completes the data processing and writes the completion sumer has completed processing the Data the Producer then performs The process then repeats whenever the Producer has more data to be pro Ordering rules are required to ensure that the ProducerConsumer model works correctly no matter where the Producer the Consumer the Data buffer words no matter how they are distributed on various links in the system. PCI Express transaction ordering for native devices can be summarized with PCI Express requires strong ordering of transactions . Because all transactions that have the same TC value assigned to them are mapped to given VC the same rules apply to transactions within each VC. No ordering relationship exists between transactions with different TC The ordering rules apply in the same way to all types of transactions mem ory IO configuration and messages. Under limited circumstances transactions with the Relaxed Ordering attribute bit set can be ordered ahead of other transactions with the same These fundamental rules ensure that transactions always complete in the order intended by software. However these rules are extremely conservative and do not necessarily result in optimum performance. For example when transactions from many devices merge within switches there may be no ordering relation ship between transactions from these different devices. In such cases more aggressive rules can be applied to improve performance as discussed in Modi Because the ProducerConsumer model depends on strong ordering when the following conditions are met native PCI Express devices support this model All elements associated with the ProducerConsumer model reside within native PCI Express devices. All transactions associated with the operation of the ProducerConsumer model transverse only PCI Express links within the same fabric. All associated transactions have the same TC values. If different TC values are used then the strong ordering relationship between the transactions is no longer guaranteed. The Relaxed Ordering attribute bit of the transactions must be cleared to avoid reordering the transactions that are part of the ProducerCon sumer transaction series. When PCI legacy devices reside within PCI Express system the ordering rules become more involved. Consequently additional ordering rules apply because of PCIs delayed transaction protocol. Without ordering rules this protocol could permit ProducerConsumer transactions to complete out of order and PCI Express supports the Relaxed Ordering mechanism introduced by PCIX The concept of Relaxed Ordering in the PCI Express environment allows switches in the path between the Requester and Completer to reorder some transactions just received before others that were previously enqueued. The ordering rules that exist to support the ProducerConsumer model may result in transactions being blocked when in fact the blocked transactions are completely unrelated to any ProducerConsumer transaction sequence. Conse quently in certain circumstances transaction with its Relaxed Ordering attribute bit set can be reordered ahead of other transactions. The Relaxed Ordering bit may be set by the device if its device driver has enabled it to do so the Root Complex is required memory but is permitted to write each byte to memory in any address All read transactions in PCI Express are handled as split transactions. When traverse one or more switches on its journey to the Completer. The Completer returns the requested read data in series of one or more split completion trans actions and uses the same RO setting as in the request. Switch behavior for the switch that receives memory read request with the RO bit set must for ward the request in the order received and must not reorder it ahead of memory write transactions that were previously posted. This action guar antees that all write transactions moving in the direction of the read request are pushed ahead of the read. Such actions are not necessarily part of the ProducerConsumer programming sequence but software may depend on this flushing action taking place. Also the RO bit must not be modified by When the Completer receives the memory read request it fetches the requested read data and delivers series of one or more memory read Com pletion transactions with the RO bit set . switch receiving the memory read Completion detects the RO bit set and knows that it is allowed to order the read Completion ahead of previ ously posted memory writes moving in the direction of the Completion. If the memory write transaction were blocked then the memory read Completion would also be blocked if the RO was not set. Relaxed ordering in this case improves read performance. well as PCI and PCIX devices that may be attached to PCI Express fabric. The discussion describes the ProducerConsumer programming model upon which the fundamental ordering rules are based. It also describes the potential perfor fies the rules defined for deadlock avoidance. Native PCI Express devices that require interrupt support must use the Mes sage Signaled Interrupt mechanism defined originally in the PCI 2.2 ver describes the legacy support that permits virtualization of the PCI INTx signals required by devices such as PCI ExpresstoPCI Bridges. To this point it has been presumed that transactions traversing the fabric have not encountered any errors that cannot be corrected by hardware. The next compatible mechanisms brief review of the PCI error handling is included as Interrupt delivery is conditionally optional for PCI Express devices. When native PCI Express function does depend upon delivering interrupts to call its device driver Message Signaled Interrupts must be used. However in the event that device connecting to PCI Express link cannot use MSIs an alternate mechanism is defined. Both mechanisms are summarized Native PCI Express Interrupt Delivery PCI Express eliminates the need for sideband signals by using the Message Signaled Interrupt first defined required by PCIX devices. The term Message Signaled Interrupt can be mis leading in the context of PCI Express because of possible confusion with PCI Expresss Message transactions. Message Signaled Interrupt is not PCI Express Message instead it is simply Memory Write transaction. memory write associated with an MSI can only be distinguished from other memory Interrupt delivery. Legacy PCI Interrupt Delivery This mechanism supports devices that must use PCICompatible interrupt signaling defined for the PCI bus. Legacy functions use one of the interrupt lines to signal an interrupt. An INTx signal is asserted to request interrupt service and deasserted when the interrupt service accesses devicespecific register thereby indicating the interrupt is being serviced. PCI Express defines inband messages that act as virtual INTx wires which target the interrupt controller Legacy endpoint device must support MSI and optionally support INTx messages. Such devices may be boot devices that must use legacy interrupts during boot but once its driver loads MSIs are used. PCI ExpresstoPCI Bridge must support INTx messages Message Signaled Interrupts are delivered to the Root Complex via memory write transactions. The MSI Capability register provides all the infor mation that the device requires to signal MSIs. This register is set up by configu ration software and includes the following information The number of messages that can be encoded into the data PCI Express function indicates its support for MSI via the MSI Capability reg isters. Each native PCI Express function must implement single MSI register set within its own configuration space. Note that the PCI Express specification defines two register formats all native PCI Express devices and optionally implemented by Legacy end supported by Legacy endpoints. The Capability ID that identifies the MSI register set is 05h. This is hardwired The second byte of the register set either points to the next New Capabilitys register set or contains 00h if this is the end of the New Capabilities list. This is hardwired readonly value. If nonzero it must be dwordaligned value. pable of generating 64bit memory address. 1 Function implements the upper 32bits of the table to determine how many messages are requested by the device it programs 3bit value into this field indicating the actual number of mes sages allocated to the device. The number allocated can be equal to or less than the number actually requested. The state of this field after reset is 000b. determine how many messages the device would like allocated to it. The requested number of mes would like three messages must request that four messages be allocated to it. The field is encoded as Native PCI Express devices that require interrupt support must use the Mes sage Signaled Interrupt mechanism defined originally in the PCI 2.2 ver also described the legacy support that permits virtualization of the PCI INTx signals required by devices such as PCI ExpresstoPCI Bridges. To this point it has been presumed that transactions traversing the fabric have discusses both correctable and noncorrectable errors and discusses the mecha classifies errors into three classes correctable nonfatal and fatal. PCI Express review of the PCI error handling is included as background information. how an outbound packet is processed before clocking the packet out differen is processed and sent to the Data Link Layer. Subblock functions of the Physical Layer such as Byte Striping and UnStriping logic Scrambler and DeScrambler 8b10b Encoder and Decoder Elastic Buffers are discussed and more. The original PCI bus implementation provides for basic parity checks on each transaction as it passes between two devices residing on the same bus. When transaction crosses bridge the bridge is involved in the parity checks at both the originating and destination busses. Any error detected is registered by the via the SERR signal may involve only hardware devicespecific software or system software. software. handling requirements depending on whether devicespecific error handling software is present. If devicespecific error handler is not present then all par PCIX 2.0 adds limited support for Error Correction Codes designed to automatically detect and correct singlebit errors within the address or data. Introduction to PCI Express Error Management those errors and identifying the appropriate hardware and software elements PCI Express error checking focuses on errors associated with the PCI Express interface and the delivery of transactions between the requester and completer defined by the PCI Express specification and it is recommended that such cific interrupts. Each layer of the PCI Express interface includes error checking The transaction layer checks are performed only by the Requestor and Compl eter. Packets traversing switches do not perform any transaction layer checks. Checks performed at the transaction layer include ECRC check failure Unexpected Completion vides backward compatibility with existing PCI compatible software and is enabled via the PCI configuration Command Register. This approach requires that PCI Express errors be mapped to PCI compatible error registers. PCI Express Capability Registers this mechanism is available only to software that has knowledge of PCI Express. This required error within PCIcompatible configuration space. nism involves registers mapped into the extended configuration address ual errors via the Error Mask Register. mechanisms require access to the PCIcompatible registers and PCI Express Errors are categorized into three classes that specify the severity of an error as listed below. Note also the specification defines the entity that should handle Uncorrectable errorsnonfatal handled by devicespecific software Uncorrectable errorsfatal handled by system software included as background information. logic. It describes how an outbound packet is processed before clocking the describes subblock functions of the Physical Layer such as Byte Striping and UnStriping logic Scrambler and DeScrambler 8b10b Encoder and Decoder Elastic Buffers and more. describes the analog characteristics of the differential drivers and receivers that connect PCI Express device to the Link. one side and interfaces to the Data Link Layer on the other side. The Physical Layer processes outbound packets before transmission to the Link and pro Layer associated with transmission and reception of packets are referred to as The transmit logic of the Physical Layer essentially processes packets arriving from the Data Link Layer then converts them into serial bit stream. The bit stream is clocked out at 2.5 GbitssLane onto the Link. The receive logic clocks in serial bit stream arriving on the Lanes of the Link with clock that is recovered from the incoming bit stream. The receive logic converts the serial bit steam into parallel symbol stream processes the incom ing symbols assembles packets and sends them to the Data Link Layer. beyond. When this happens an existing design can be adapted to the higher data rates by redesigning the Physical Layer while maximizing reuse of the Data Link Layer Transaction Layer and Device CoreSoftware Layer. The Phys Memory IO Configuration RW Requests or Message Requests or Completions Transaction Layer Packet ical Layer may be designed as standalone entity separate from the Data Link Layer and Transaction Layer. This allows design to be migrated to higher data rates or even to an optical implementation if such Physical Layer is supported Two subblocks make up the Physical Layer. These are the logical Physical block. Both subblocks are split into transmit logic and receive logic which allow dual simplex communication. To facilitate description of the Physical Layer functionality an example imple mentation is described that is not necessarily the implementation assumed by the specification nor is designer compelled to implement Physical Layer in such manner. designer may implement the Physical Layer in any manner that is compliant with the functionality expected by the PCI Express specifica multiplexer byte striping logic Buffer. With the aid of multiplexer the Physical Layer frames the TLPs or DLLPs with Start and End characters. These characters are framing symbols which the receiver device uses to detect start and end of packet. The framed packet is sent to the Byte Striping logic which multiplexes the bytes of the packet onto the Lanes. One byte of the packet is transferred on one Lane the next byte on the next Lane and so on for the available Lanes. The Scrambler uses an algorithm to pseudorandomly scramble each byte of the packet. The Start and End framing bytes are not scrambled. Scrambling eliminates repetitive patterns in the bit stream. Repetitive patterns result in large amounts of energy concentrated in discrete frequencies which leads to sig nificant EMI noise generation. Scrambling spreads energy over frequency range hence minimizing average EMI noise generated. The scrambled 8bit characters are encoded into 10bit symbols by the 8b10b Encoder logic. And yes there is 25 loss in trans mission performance due to the expansion of each byte into 10bit character. Character is defined as the 8bit unencoded byte of packet. Symbol is defined as the 10bit encoded equivalent of the 8bit character. The purpose of 8b10b Encoding the packet characters is primarily to create sufficient 1to0 and 0to1 transition density in the bit stream so that the receiver can recreate receive clock with the aid of receiver Phase Lock Loop . Note that the clock used to clock the serial data bit stream out of the transmitter is not itself transmitted onto the wire. Rather the receive clock is used to clock in an The 10b symbols are converted to serial bit stream by the ParalleltoSerial converter. This logic uses 2.5 GHz clock to serially clock the packets out on each Lane. The serial bit stream is sent to the electrical subblock which differ entially transmits the packet onto each Lane of the Link. serialtoparallel converter byte unstriping logic and differential receivers . The transmitter seri alizes outbound symbols on each Lane and converts the bit stream to electrical PCI Express System Architecture signals that have an embedded clock. The receiver detects electrical signaling on each Lane and generates serial bit stream that it deserializes into symbols and supplies the symbol stream to the logical Physical Layer along with the clock recovered from the inbound serial bit stream. In the future this subblock could be redesigned to support cable interface or an optical interface. In addition the electrical Physical Layer contains Phase Lock Loop that drives the Serializer in the transmitter and receiver PLL that is syncd to the transitions in the incoming serial symbol stream. When the Link is in the L0 fullon state the differential drivers drive the differ ential voltage associated with logical 1 and logical 0 while driving the correct DC common mode voltage. The receivers sense differential voltages that indi cate logical 1 or 0 and inaddition can sense the electrical idle state of the Link. An eye diagram clearly illustrates the electrical characteristics of driver and The electrical Physical Layer is responsible for placing the differential drivers differential receivers and the Link in the correct state when the Link is placed in low power state such as L0s L1 or L2. While in the L2 low power state device can signal wakeup event upstream via Beacon signaling mechanism. The differential drivers support signal deemphasis especially on lossy Link. The drivers and receivers are shortcircuit tolerant making them ideally suited for hot insertion and removal events. The Link connecting two devices is AC coupled. capacitor at the transmitter side of the Link DC decouples it from the receiver. As result two devices at opposite ends of Link can have their High Speed Electrical Signaling electrical signaling is used in driver and receiver implementations. Drivers and receivers from different manufacturers must be interoperable and may be designed to be hotpluggable. standard FR4 board can be used to route the The transmitter clocks data out at 2.5Gbitss. The clock used to do so must be accurate to 300 ppm of the center frequency. It is allowed to skew maxi mum of 1 clock every 1666 clocks. The two devices at the opposite ends of Link could have their transmit clocks out of phase by as much as 600 ppm. device may derive its clock from an external clock source. The system board supplies 100 MHz clock that is made available to devices on the system board as well as to addin cards via the connector. With the aid of PLLs device may generate its required clocks from this 100 MHz clock. Spread spectrum clocking is technique used to modulate the clock frequency slowly so as to reduce EMI radiated noise at the center frequency of the clock. With SSC the radiated energy does not produce noise spike at 2.5GHz because the radiated energy is spread over small frequency range around SCC is not required by the specification. However if supported the following The clock can be modulated by 0 to 0.5 from nominal frequency of The modulation rate must be between 30KHz and 33KHz. The 300 ppm requirement for clock frequency accuracy still holds. Fur ther the maximum of 600 ppm frequency variation between the two devices at opposite ends of Link also remains true. This almost certainly imposes requirement that the two devices at opposite ends of the Link be driven from the same clock source when the clock is modulated with SSC. The characteristic impedance of the Link is 100 Ohms differential while singleended DC common mode impedance is 50 Ohms. This impedance is matched to the transmitter and receiver impedances. Transmitter Impedance Requirements anytime differential signals are transmitted during the fullon L0 power state. When differential signal is not driven the state. Placing driver in the high impedance state may be helpful while in L0s or L1 low power states to help reduce power drain in these states. Receiver Impedance Requirements parameter of 100 Ohms anytime differential signals are transmitted during the fullon L0 power state as well as in all other lower power states wherein ade quate power is provided to the device. receiver is excluded from this imped ance requirement when the device is powered down . When receiver is powered down to the L2 or L3 state or during Fundamental Reset its receiver goes to the high impedance state and must meet the ZRX Once driven after poweron and during the Detect state of Link training the must remain at the same voltage. The common mode voltage is turned off only when the transmitter is placed in the L2 or L3 low power state during which main power to the device is removed. designer can choose any common The receiver is DC decoupled from the transmitter by capacitor. This allows the receiver to have its own DC common mode voltage. This voltage is specified at 0V. The specification is unclear about the meaning of this 0V receiver DC common mode voltage requirement and does not require the common mode voltage to be 0V at the input to the receiver differential amplifier. Rather sim ple bias voltage network allows the receiver to operate at optimal common All signals and power pins must withstand 2000V Electro Static Discharge using the human body model and 500V using the charged device model. For more details on this topic see the JEDEC JESE22 A114A specification. The ESD requirement not only protects against electrostatic damage but facili tates support of surprise hot insertion and removal events. Transmitters and receivers are also required to be shortcircuit tolerant. They must be able to withstand sustained shortcircuit currents of ITXSHORT Ratio of resistors analog characteristics of the differential drivers and receivers that connect PCI Express device to the Link. Timing and driverreceiver parameters are docu reset signal called PERST. It describes the usage of the TS1 OrderedSet to gen erate an inband Hot Reset. The effect of reset on device and system is process of the Link from PowerOn or Reset until the fullon L0 state where management states L0s L1 L2 L3 and briefly describes entry and exit proce The PCI Express specification describes two reset generation mechanisms. The first mechanism is system generated reset referred to as Fundamental Reset. The second mechanism is an Inband Reset referred to as the Hot Reset. Fundamental Reset causes devices state machines hardware logic port states and configuration registers to initialize to their default conditions. Cold Reset. This is reset generated as result of application of main power to the system. Warm Reset. Triggered by hardware without the removal and reapplica tion of main power. Warm Reset could be triggered due to toggling of the system POWERGOOD signal with the system power stable. The mecha nism for generating Warm Reset is not defined by specification. It is up to the system designer to optionally provide mechanism to generate Warm The receiver terminations are required to meet the ZRXHIGHIMPDC param The transmitter terminations are required to meet the output impedance at the driver in high impedance state. The transmitter holds constant DC common mode voltage between 0 The receiver must reenable its receiver terminations ZRXDIFFDC . When PERST is not provided to an addin card or component Fundamental Reset is generated autonomously by the component or addin card. Below is description of the two mechanisms of Fundamental Reset generation. PERST Type Fundamental Reset Generation. device . chipset in the PCI Express system provides this source of reset. POWERGOOD signal once main power is turned on and stable. The ICH Reset logic inturn uses this signal to assert PERST when POWERGOOD is deasserted. If power is cycled POWERGOOD toggles and causes PERST to assert and deassert. This is the Cold Reset. If the system provides method of toggling POWERGOOD without cycling through power then also PERST asserts and deasserts. This is the Warm The PERST signal feeds all PCI Express devices on the motherboard including the connectors and graphics controller. Devices may choose to use PERST but are not required to use it as the source of reset. The PERST signal also feeds the PCI ExpresstoPCIX bridge shown in the fig ure. The bridge forwards this reset to the PCIX bus as PCIX bus RST. ICH also generates PRST for the PCI bus. Autonomous Method of Fundamental Reset Generation. device can be designed to generate its own Fundamental Reset upon detection of application of main power. The specification does not describe the mechanism for doing so. The self reset generation mechanism can be built into the device or may be designed as external logic for example on addin card that detects PowerOn and generates local reset to the device. The device must also generate an autonomous Fundamental Reset if it detects its power go outside of the limits specified. device should support the autonomous method of triggering Fundamental Reset given that the specification is not clear about requirement of system Hot Reset is propagated inband via the transmission of TS1 OrderedSets The TS1 OrderedSet is transmitted on all Lanes with the correct Link and Lane symbols. These TS1 OrderedSets are continuously transmitted for 2 ms. Both transmitter and receiver of Hot Reset end up in the detect state of the Link Training state machine and starts the Link training process fol lowed by initialization of VC0. Its state machines hardware logic port states and configuration registers initialize to their default conditions. Switches Generate Hot Reset on Their Downstream Ports The following are list of bullets that indicate when switch generates Hot This state occurs when the upstream port has been disconnected or when the upstream port has lost connection with an upstream device due to an error that is not recoverable by the Physical Layer and Data Link Layer. Software sets the Secondary Bus Reset bit of the Bridge Control configura If bridge such as PCI ExpresstoPCI bridge detects Hot Reset on its upstream port it must assert the PRST signal on its secondary PCI bus. How Does Software Tell Device then goes through the Recovery state of the LTSSM form factor which targets the workstation and server market and the NEW CARD form factor which targets both mobile and desktop markets. Link initialization and training is Physical Layer control process that config ures and initializes devices Physical Layer port and associated Link so that normal packet traffic can proceed on the Link. This process is automatically ini tiated after reset without any software involvement. subset of the Link train ing and initialization process referred to as Link retraining is initiated automatically as result of wakeup event from low power mode or due to an error condition that renders the Link inoperable. The Link Training and Sta tus State Machine is the Physical Layer subblock responsible for the receiver may optionally check for violations of the Link training and initial Link Width is established and set. Two devices with different number of port Lanes may be connected. For example one device with x2 port may be connected to device with x4 port. During Link training and initializa tion the Physical Layer of both devices determines and sets the Link width to the minimum Lane width of the two . Other Link negotiated behaviors include Lane reversal splitting of ports into multiple Links and the configuration of crossLink. Lane Reversal on multiLane devices port . The Lanes on devices port are numbered by design. When wiring up Link to connect two devices board designer should match up the lane numbers of each devices port so that Lane 0 of one devices port connects to Lane 0 of the remote devices port Lane to Lane of the remote devices port and Due to the way the Lanes are organized on the pins of the devices package it may not be possible to match up the Lanes of the two devices without will introduce interference into the Link. If however one or both of the devices support Lane Reversal the designer could wire the Lanes in paral lel fashion. During the Link training and initialization process one device reverses the Lane numbering so the Lane numbers of the two ports would not require devices to support the Lane Reversal feature. Hence the Memory IO Configuration RW Requests or Message Requests or Completions PCI Express System Architecture designer must verify that at least one of the two devices connected via Link supports this feature before wiring the Lanes of the two ports in reverse order. If the device supports this feature the Lane Reversal process may permit multiLane Link to be split into multiple Links that connect to multiple devices. More on this feature later. nals for two devices may not be connected correctly or may be intentionally reversed so that the signals do not crisscross when wiring the Link. If Lanes are wired with and of one device wired to and of the remote Link Data Rate. Link initialization and training is completed at the default 2.5Gbits Generation 1 data rate. In the future Generation 2 PCI Express will support higher data rates of 5Gbits and 10Gbits. During training each node advertises its highest data rate capability. The Link is then initial ized with the highest common frequency that both neighbors can support. Bit Lock. Before Link training begins the receiver PLL is not yet syncd with the remote transmitters transmit clock and the receiver is unable to differentiate between one received bit and another. During Link training the receiver PLL is syncd to the transmit clock and the receiver is then able to shift in the received serial bit stream. See Achieving Bit Lock on Symbol Lock. Before training the receiver has no way of discerning the boundary between two 10bit sympols. During training when TS1 and TS2 OrderedSets are exchanged the receiver is able to locate the COM symbol and uses it to initialize the deserializer. See LanetoLane Deskew. Due to Link wire length variations and the different driverreceiver characteristics on multiLane Link each of the parallel bit streams that represent packet are transmitted simultaneously but they do not arrive at the receivers on each lane at the same time. The receiver circuit must compensate for this skew by adding or removing delays on each Lane so that the receiver can receive and align the serial bit streams of the packet PCI Express System Architecture the designers task of wiring up the high speed Link. OrderedSets Used During Link Training and Initialization Physical Layer Packets referred to as OrderedSets are exchanged between neighboring devices during the Link training and initialization pro State Machine of the Physical Layer. It also described the initialization process of the Link from PowerOn or Reset until the fullon L0 state where management states L0s L1 L2 L3 and briefly discusses entry and exit proce dure tofrom these states. whether the system can support an addin card based on the amount of power and cooling capacity it requires. addition PCI Express defines extensions that are orthogonal to the PCIPM specification. These extensions focus primarily on Link Power and PM event power management by including description of the OnNow Initiative ACPI and the involvement of the Windows OS is also provided. The primary goal of the PCI Express power budgeting capability is to allocate power for PCI Express hot plug devices which can be added to the system dur ing runtime. This capability ensures that the system can allocate the proper The specification states that power budgeting capability is optional for PCI Express devices implemented in form factor which does not require hot plug or that are integrated on the system board. None of the form factor specifica tions released at the time of this writing required support for hot plug and did not require the power budgeting capability. However form factor specifications under development will require hot plug support and may also require power budgeting capability. System power budgeting is always required to support all system board devices and addin cards. The new power budgeting capability provides mechanisms for managing the budgeting process. Each form factor specification defines the minimum and maximum power for given expansion slot. For example the Electromechanical specification limits the amount of power an expansion card and enabled it can consume the maximum amount of power specified for the quently in the absence of the power budgeting capability registers the system designer is responsible for guaranteeing that power has been budgeted cor rectly and that sufficient cooling is available to support any compliant card installed into the connector. The specification defines the configuration registers that are designed to sup port the power budgeting process but does not define the power budgeting elements that would be involved in power budgeting including the specified System Firmware Power Management Expansion Ports System Firmware System firmware having knowledge of the system design budget manager which allocates and verifies power consumption and dissipa Number and type of slots in the system. Firmware may also allocate power to PCI Express devices that support the power budgeting capability configuration register set contains System Allocated bit that is intended to be set by firmware to notify the power budget manager that power for this device has been included save power information for hotplug devices that are allocated by the system in case they are removed during runtime. The Power Manager The power manager initializes when the OS installs at which time it receives powerbudget information from system firmware. The specification does not define the method for communicating this information. The power budget manager is responsible for allocating power for all PCI PCI Express devices that have not already been allocated by the system . PCI Express System Architecture have the Slot Power Limit and Slot Power Scale fields within the Slot Capabili ties register implemented. The firmware or power budget manager must load these fields with value that represents the maximum amount of power sup ported by this port. When software writes to these fields the port delivers the Set_Slot_Power_Limit message to the device. These fields are also written when Any downstream port of Switch or Root Complex that has slot attached must implement the Slot Capabilities register. Software must initialize the Slot Power Limit Value and Scale fields of the Slot Capabilities register of the Switch or Root Complex Downstream Port that is connected to an addin slot. The Upstream Port of an Endpoint Switch or PCI ExpressPCI Bridge must implement the Device Capabilities register. scale values in the Downstream port of the Switch or Root Complex that port will automatically transmit the Set_Slot_Power_Limit message to the Upstream Port of the Endpoint Switch or PCI ExpressPCI Bridge on the The recipient of the Message must use the value in the Message data pay load to limit usage of the power for the entire cardmodule unless the cardmodule will never exceed the lowest value specified in the corre sponding electromechanical specification. Addin DevicesExpansion cards that support the power budgeting capability Slot Power Limit Value and Slot Limit Scale fields within the Device Capa bilities register. These devices must not consume more power than the lowest power specified by the form factor specification. Once power budgeting software allocates addi tional power via the Set_Slot_Power_Limit message the device can consume Device DriverThe devices software driver is responsible for verifying that sufficient power is available for proper device operation prior to enabling it. If the power is lower than that required by the device the device driver is respon Software is responsible for determining the maximum amount of power that an power partitioning within the system thermal capabilities etc. Knowledge of the systems power and thermal limits comes from system firmware. The firm ware or power manager is Software writes to the Slot Power Limit Value and Slot Power Limit Scale fields of the Slot Capability register to specify the maximum power that can be con sumed by the device. Software is required to specify power value that reflects one of the maximum values defined by the specification. For example the elec When these registers are written by power budget software the expansion port sends Set_Slot_Power_Limit message to the expansion device. This procedure Low Profile Card mine whether the system can support an addin card based on the amount of power and cooling capacity it requires. addition PCI Express defines extensions that are orthogonal to the PCIPM specification. These extensions focus primarily on Link Power and PM event power management by including description of the OnNow Initiative ACPI and the involvement of the Windows OS is also provided. PCI Express includes native support for hot plug implementations. The next cation defines standard usage model for all device and platform form factors that support hot plug capability. The usage model defines as an example how push buttons and indicators behave if implemented on the chassis addin card or module. The definitions assigned to the indicators and push but PCI Express power management defines two major areas of support PCICompatible Power Management. PCI Express power management is based upon hardware and software compatible with the PCI Bus Power . This support requires that all PCI Express functions include the PCI Power Management Capability registers which permits transitions between function PM states. Native PCI Express Extensions. These extensions define autonomous hard warebased Link Power Management mechanisms for waking the system agement by reviewing the role of system software in controlling power powermanagement software from the Windows Operating System per PCIPM required by PCI Express for placing functions into their low power PCI Express. Note that some of the register definitions are modified or not used by PCI Express functions. autonomous Link power management that occurs when device is in its active state . Active State Power Management is hard warebased link power conservation mechanism. Software enables ASPM and reads latency values to determine the level of ASPM appropriate but does not initiate transitions into ASPM. PCIPM software when it changes the power state of device. PCI Express devices are required to automatically conserve link power when software places device into low power state including D3cold . Power Management Events and wakeup signaling. Devices may request that software return them to the active state so they can handle an event that has occurred. This is done by sending PME messages. When power has been removed from device auxiliary power is required to monitor events and to signal Wakeup for reactivating the link. Once device has been repowered and the link has been retrained the PME mes The PCI Bus PM Interface Specification describes how to implement the PCI PM registers that are required in PCI Express. These registers permit the OS to man age the power environment of both PCI and PCI Express functions. Rather than immediately diving into detailed nutsandbolts description of the PCI Bus PM Interface Specification its good idea to begin by describing where it fits within the overall context of the OS and the system. Otherwise this would just be disconnected discussion of registers bits signals etc. with no The most popular OSs currently in use on PCcompatible machines are Win acts with other major software and hardware elements to manage the power introduces the major elements involved in this process and provides very basic description of how they relate to each other. It should be noted that neither the PCI Power Management spec nor the ACPI spec dictate the policies that the OS uses to manage power. It does however define the registers that are used to control the power usage of PCI and PCI Express functions. Directs the overall system power management. To accomplish this goal Model device drivers and to the PCI Express Bus Driver. Application programs that are power conservationaware interact with the OS to Manages configuration power management and thermal control of devices embedded on the system board that do not adhere to any industry standard interface specification. Examples could be chipsetspecific registers system boardspecific registers that control power planes etc. The PM registers within PCI Express functions are defined by the PCI PM spec and are there fore not managed by the ACPI driver but rather by the PCI Express Bus Driver . The WDM driver is Class driver that can work with any device that falls within the Class of devices that it was written to control. The fact that its not written for specific device from specific vendor means that it doesnt have register and bitlevel knowledge of the devices vendor of the specific device. The WDM also doesnt understand device characteristics that are pecu liar to specific bus implementation of that device type. As an example the WDM doesnt understand PCI Express devices configuration reg ister set. It depends on the PCI Express Bus Driver to communicate with PCI Express configuration registers. When it receives requests from the OS to control the power state of its PCI Express device it passes the request to the PCI Express Bus Driver When request to power down its device is received from the OS the devicespecific registers and then passes the request to the PCI Express Bus Driver to change the power state of the device. Conversely when request to repower the device is received from the OS the WDM passes the request to the PCI Express Bus Driver to change the power state of the device. After the PCI Express Bus Driver has repowered the device the WDM then restores the context to the PCI Express functions devicespecific registers. Supplied by the vendor of device it receives requests from the WDM Class driver and converts them into the proper series of accesses to the whitepaper on Microsofts website clearly defines the goals of the OnNow This driver is generic to all PCI Expresscompliant devices. It manages their power states and configuration registers but does not have knowledge of PCI Express functions devicespecific register set . It receives requests from the devices WDM to change the state of the devices power man When request is received to power down the device the PCI Express Bus Driver is responsible for saving the context of the functions PCI ters that the device implements. Using the devices PCI Express config uration Command register it then disables the ability of the device to writes to the PCI Express functions PM registers to change its state. Conversely when the device must be repowered the PCI Express Bus Driver writes to the PCI Express functions PM registers to change its Express PM spec. The PCI Express Bus Driver understands this spec and therefore is the entity responsible for accessing functions PM reg isters when requested to do so by the functions device driver . In addition PCI Express defines extensions that are orthogonal to the PCIPM specification. These extensions focus primarily on Link Power and PM sion of power management by including description of the OnNow Initiative ACPI and the involvement of the Windows OS is also provided. discusses hot plug and hot removal of PCI Express devices. The specification defines standard usage model for all device and platform form factors that support hot plug capability. The usage model defines as an example how push buttons and indicators behave if implemented on the chassis addin card or module. The definitions assigned to the indicators and push buttons tromechanical specifications. It describes the card form factor the connector details and the auxiliary signals with description of their function. Other card Some systems that employ the use of PCI and PCIX require high availability or nonstop operation. For example many customers require computer systems that experience downtimes of just few minutes year or less. Clearly manu facturers must focus on equipment reliability and also provide method of identifying and repairing equipment failures quickly. An important feature in important capabilities method of replacing failed expansion cards without turning the system off keeping the OS and other services running during the repair shutting down and restarting software associated with the failed device were available to support this type of removal and replacement of expansion cards. However the original PCI implementation was not designed to support hot removal and insertion of cards but standardized solution for supporting this capability in PCI was needed. Consequently two major approaches to hot replacement of PCI expansion devices have been developed. These approaches Hot Plug PCI Card used in PC Server motherboard and expansion chas Hot Swap used in CompactPCI systems based on passive PCI back plane implementation. PCI bus via electronic switches. In conjunction with isolation logic power reset and clock are controlled to ensure an orderly power down and power up of vide indications to the user that it is safe to remove or install the card. The need to extend hot plug support to PCI Express cards is clear. Designers of PCI Express have incorporated Hot removal and replacement of cards as native feature. The specification defines configuration registers Hot Plug Controller specification for PCI. The goals of PCI Express Hot Plug support the same Standardized Usage Model as defined by the Standard Hot Plug Controller specification. This ensures that the PCI Express hot plug is identical from the user perspective to existing implementations based on the SHPC 1.0 specification support the same software model implemented by existing operating sys tems. However if the OS includes SHPC 1.0 compliant driver it will not work with PCI Express Hot Plug controllers which have different pro gramming interface. PCI Express defines the registers necessary to support the integration of Hot Plug Controller within individual root and switch ports. Under Hot Plug soft ware control these Hot Plug controllers and the associated port interface within the root or switch port must control the card interface signals to ensure orderly power down and power up as cards are removed and replaced. Hot Plug con assert and deassert the PERST signal to the PCI Express card connector remove or apply power to the card connector. Selectively turn on or turn off the Power and Attention Indicators associ ated with specific card connector to draw the users attention to the con nector and advertise whether power is applied to the slot. PCI Express HotPlug is designed as no surprises HotPlug meth odology. In other words the user is not permitted to install or remove PCI Express card without first notifying software. System software then prepares notification that installation or removal may be performed. PCI Express cards must implement the edge contacts with card presence detect pins that break contact first . This gives advanced notice to software of sur prise removal and enough time to remove power prior to the signals breaking The elements needed to support hot plug are essentially the same between PCI implement single standardized hot plug controller on the system board that permits all hot plug slots on the bus to be controlled by single controller. Also isolation logic is needed in the PCI environment to electrically disconnect sin gle card slot from the bus prior to card removal. PCI Express Hot Plug differs from the PCI implementation due to pointto eliminate the need for isolation logic and permit the hot plug controller to be distributed to each port interface to which connector is attached. standard ized software interface defined for each root and switch port permits stan dardized software interface to control hot plug operations. Note that the programming interface for the PCI Express and PCI Hot Plug Controllers vary PCI Express includes native support for hot plug implementations. The previ specification defines standard usage model for all device and platform form factors that support hot plug capability. The usage model defines as an exam ple how push buttons and indicators behave if implemented on the chassis addin card or module. The definitions assigned to the indicators and chanical specifications. It describes the card form factor the connector details and the auxiliary signals with description of their function. Other card form factors are also briefly described but it should be stressed that some of them envionment. It introduces the configuration space in which functions config uration registers are implemented how function is discovered how configu ration transactions are routed PCIcompatible space PCI Express extended configuration space and how to differentiate between normal function and One goal of the PCI Express addin card electromechanical spec was to encour age migration from the PCI architecture found in many desktop and mobile devices today by making the migration path straightforward and minimizing the required hardware changes. Towards this end PCI Express addin cards are defined to be very similar to the current PCI addin card form factor allowing them to readily coexist with PCI slots in system boards designed to the ATX or be designed using the fourlayer FR4 board construction commonly used today. As result much of an existing system board design can remain the same when it is modified to use the new architecture and no changes are required for exist Addin Connector different pinout and does not supply 12V or 5V power. The physical dimen sions of card are the same as the PCI addin cards and the same IO bracket is Express cards up to x16 . Several signals are referred to as auxil iary signals in the spec and these are highlighted and described in more detail Note that cards with fewer lanes can be plugged into larger connectors that will installing larger card into smaller slot is called Downplugging and unlike PCI is physically prevented in PCI Express by the connector keying. Conse x4 x8 or x16. This flexibility in the connector is highlighted by notes in the table that indicate each group of signals. For example x4 card plugged into this slot would only make use of pins 1 through 32 and so the note indicating the end of the x4 group of signals appears after pin 32. These segment indicators do not represent physical spaces or keys however because there is only one mechani cal key on the connector located between pins 11 and 12. Transmitter differential electromechanical specifications. It described the card form factor the connector details and the auxiliary signals with description of their function. Other card form factors were also briefly described but it should be stressed that some of onment. It introduces the configuration space in which functions configura tion registers are implemented how function is discovered how configuration transactions are routed PCIcompatible space PCI Express extended configuration space how function is discovered and how to differ entiate between normal function and bridge. anisms used in PCI Express platform the PCIcompatible configuration mech anism and the PCI Express enhanced configuration mechanism. It provides detailed description of the initialization period immediately following power Just as in the PCI environment device resides on bus and contains one or more functions . Each of the functions within multifunction device provides standalone functionality. As an example one function could be graphics con troller while another might be network interface. Just as in PCI device may contain up to maximum of eight functions num The oneandonly function implemented in singlefunction device must In multifunction device the first function must be function 0 while the remaining functions do not have to be implemented in sequential manner. In other words device could implement functions 0 2 and 7. ing two functions each of which implements its own set of configuration regis The bus connected to the upstream side of bridge is referred to as its primary bus while the bus connected to its downstream side is referred to as its second Topology Is Unknown At Startup figuration software has not yet scanned the PCI Express fabric to discover the machine topology and how the fabric is populated. The configuration software is only aware of the existence of the HostPCI bridge within the Root Complex and that bus number 0 is directly connected to the downstream side of the bridge. It has not yet scanned bus 0 and therefore does not yet know how many PCI Express ports are implemented on the Root Complex. The process of scanning the PCI Express fabric to discover its topology is referred to as the enumeration At the behest of software executing on the processor the Root Complex initiates configuration transactions to read from or write to functions configuration registers. These registers are accessed to discover the existence of function as message space PCI Express also defines dedicated block of configuration space allocated to each function within which its configuration registers are The 256 byte PCIcompatible space occupies the first 256 bytes of this 4KB space. It contains the functions PCIcompatible configuration regis ters. This area can be accessed using either of two mechanisms within this area. full description of the PCIcompati ble registers may be found in PCI Compatible Configuration Registers on The remaining 3840 byte area is referred to as the PCI Express Extended Configuration Space. It is utilized to implement the optional PCI Power Budgeting Capability register set. full description of the these optional register sets may be found in Express The HostPCI bridges configuration register set does not have to be accessed using either of the specdefined configuration mechanisms mentioned in the address space that is known to the platform specific BIOS firmware. However its configuration register layout and usage must adhere to the standard Type 0 template defined by the PCI 2.3 spec is the PCIcompatible configuration space while the upper 960 dwords is the PCI Express extended configuration There are two mechanisms available that allow configuration software running on the processor to stimulate the Root Complex to generate configuration trans The PCI express enhanced configuration mechanism. Intel x86 and PowerPC processors do not possess the ability to perform configuration read and write transactions. They use memory and IO read and write transactions to communicate with external devices. This means that the Root Complex must be designed to recognize certain IO or memory accesses initiated by the processor For x86based PCAT compatible systems the 2.3 PCI spec defines method that utilizes processorinitiated IO accesses to instruct the hostPCI bridge to perform PCI configuration accesses. The PCI Express System Architecture spec does not define configuration mechanism to be used in systems other than PCAT compatible systems. The x86 processor family is capable of addressing up to but no more than 64KB of IO address space. In the EISA spec the usage of this IO space was defined in such manner that the only IO address ranges available for the implementation of the PCI Configuration Mechanism were 0400h 04FFh 0800h 08FFh and 0C00h 0CFFh. Many EISA sys tem board controllers already resided within the 0400h 04FFh address range dwords of configuration registers. Each PCI function on each PCI bus requires 64 dwords of dedicated config Due to the lack of available IO real estate within the 64KB of IO space it wasnt feasible to map each configuration register directly into the processors IO address space. Alternatively the system designer could implement the configu ration registers within the processors memory space. The amount of memory space consumed aside the address range utilized would be unavailable for allo cation to regular memory. This would limit the systems flexibility regarding the The PCICompatible Configuration Mechanism utilizes two 32bit IO ports implemented in the HostPCI bridge within the Root Complex located at IO The 32bit Configuration Address Port occupying IO addresses 0CF8h The 32bit Configuration Data Port occupying IO addresses 0CFCh Accessing one of functions PCIcompatible configuration registers is two Write the target bus number device number function number and dword number to the Configuration Address Port and set the Enable bit in it to Perform onebyte twobyte or fourbyte IO read from or write to the Configuration Data Port. In response the hostPCI bridge within the Root Complex compares the speci fied target bus to the range of buses that exist on the other side of the bridge and if the target bus resides beyond the bridge it initiates configuration read or write transaction within the target functions PCI compatible configuration space. When the Root Complex subsequently generates the resultant configuration request packet this bit field supplies the content of the packets Register Number field and the packets Extended Register Number field is set to all zeros. This configuration access mecha nism is therefore limited to addressing the first 64 dwords of the targeted functions configuration space within the target bits identify the target device number . bits are reserved and must be zero. bit 31 must be set to one enabling the translation of subsequent proces sor IO access to the Configuration Data Port into configuration access. If bit 31 is zero and the processor initiates an IO read from or IO write to the Configuration Data Port the transaction is treated as an IO transaction Bus Compare and Data Port Usage plex implements Bus Number register and Subordinate Bus Number regis ter. In chipset that only supports one Root Complex the bridge may have bus number register that is hardwired to 0 readwrite register that reset forces to 0 or it just implicitly knows that it is the bridge to bus 0. If bit 31 in the one the bridge compares the target bus number to the range of buses that exists beyond the bridge. Target Bus 0. If the target bus is the same as the value in the Bus Num ber register this is request to perform configuration transaction on bus 0. subsequent IO read from or write to the bridges Configuration Data Port at 0CFCh causes the bridge to generate Type 0 configuration read or write transaction. When devices that reside on PCI bus detect Type 0 configu ration transaction in progress this informs them that one of them is the tar get device . Should always be zeros mechanisms used in PCI Express platform the PCIcompatible configuration mechanism and the PCI Express enhanced configuration mechanism. It pro vided detailed description of the initialization period immediately following powerup as well as error handling during this period. multifunction device within Root Complex or Switch Root Complex Register Blocks residing functions PCIcompatible configuration space. This includes the reg In reality at power up time the configuration software only knows of the exist ence of bus 0 the enumeration software attempts to read the Vendor ID from function 0 in each of the 32 possible devices on bus If valid Vendor ID is returned from bus 0 device 0 func tion 0 this indicates that the device is implemented and contains at least one function. Proceed to the next step. If value of FFFFh were returned as the Vendor ID this would indicate that function 0 is not implemented in device 0. Since it is rule that the first function implemented in any device must be function 0 this would mean that device was not implemented and the enumeration software would proceed to probe bus 0 device 1 function 0. PCItoPCI bridge with the PCIcompatible register layout shown in Fig function in this bridge. It should be noted that the spec does not preclude imple menting multiple functions within this bridge and each of these functions in turn could represent virtual PCItoPCI bridges. Software now performs series of configuration writes to set the bridges Subordinate Bus Number Register 1. The bridge is now aware that the number of the bus directly attached to its downstream side is 1 and the number of the bus farthest downstream of it is 1 . value 0100b in the registers DevicePort Type field indicates that this Root Port on the Root Complex. The specification states that the enumeration software must perform depthfirst search so before proceeding to discover additional functions devices on bus 0 it must proceed to search bus 1. Software reads the Vendor ID of bus 1 device 0 function 0. valid Vendor ID is returned indicating that bus 1 device 0 function 0 exists. indicating that this is PCItoPCI bridge. In addition bit 7 is 0 indicating that bridge is singlefunction device. Bridge Cs Capability Register contains the value 0101b in the DevicePort Type field indicating that this is the upstream Port on switch. 10. Software now performs series of configuration writes to set bridge Cs bus Subordinate Bus Number Register 2. Bridge is now aware that the number of the bus directly attached to its downstream side is 2 and the number of the bus farthest downstream of it is 2 . bridge and in bridge to 2. 12. Continuing with its depthfirst search read is performed from bus 2 device 0 function 0s Vendor ID register. The example assumes that bridge is device 0 function 0 on bus 2. indicating that this is PCItoPCI bridge. In addition bit 7 is 0 indicating that bridge is singlefunction device. 15. Bridge Ds Capability Register contains the value 0110b in the DevicePort Type field indicating that this is the downstream Port on switch. 16. Software now performs series of configuration writes to set bridge Ds bus Subordinate Bus Number Register 3. Bridge is now aware that the number of the bus directly attached to its downstream side is 3 and the number of the bus farthest downstream of it is 3 . bridge bridge and bridge to 3. 18. Continuing with its depthfirst search read is performed from bus 3 device 0 function 0s Vendor ID register. indicating that this is an Endpoint device. In addition bit 7 is 1 indicating that this is multifunction device. 21. The devices Capability Register contains the value 0000b in the Device Port Type field indicating that this is an Endpoint device. 22. The enumeration software performs accesses to the Vendor ID of functions 1through7 in bus 3 device 0 and determines that only function 1 exists in addition to function 0. 23. Having exhausted the current leg of the depth first search the enumeration software backs up one level and moves on to read the Vendor ID of the next device . The example assumes that bridge is device 1 function 0 on bus 2. indicating that this is PCItoPCI bridge. In addition bit 7 is 0 indicating that bridge is singlefunction device. 26. Bridge Es Capability Register contains the value 0110b in the DevicePort Type field indicating that this is the downstream Port on switch. 27. Software now performs series of configuration writes to set bridge Es bus Subordinate Bus Number Register 4. Bridge is now aware that the number of the bus directly attached to its downstream side is 4 and the number of the bus farthest downstream of it is 4 . bridge bridge and bridge to 4. 29. Continuing with its depthfirst search read is performed from bus 4 device 0 function 0s Vendor ID register. indicating that this is an Endpoint device. In addition bit 7 is 0 indicating that this is singlefunction device. 32. The devices Capability Register contains the value 0000b in the Device Port Type field indicating that this is an Endpoint device. 33. Having exhausted the current leg of the depth first search the enumeration software backs up one level and moves on to read the Vendor ID of the next device . The example assumes that devices 2through 31 are not implemented on bus 2 so no additional devices are discovered on 34. The enumeration software backs up to the bus within the Root Complex and moves on to read the Vendor ID of the next device . The example assumes that bridge is device 1 function 0 on bus 0. 35. In the same manner as previously described the enumeration software dis covers bridge and performs series of configuration writes to set bridge Subordinate Bus Number Register 5. Bridge is now aware that the number of the bus directly attached to its downstream side is 5 and the number of the bus farthest downstream of it is 5 . 37. Bridge is then discovered and series of configuration writes are per formed to set its bus number registers as follows Subordinate Bus Number Register 6. Bridge is now aware that the number of the bus directly attached to its downstream side is 6 and the number of the bus farthest downstream of it is 6 . 38. The HostPCI bridges and bridge Subordinate Bus Number registers are 39. Bridge is then discovered and series of configuration writes are per formed to set its bus number registers as follows Subordinate Bus Number Register 7. Bridge is now aware that the number of the bus directly attached to its downstream side is 7 and the number of the bus farthest downstream of it is 7 . 41. singlefunction Endpoint device is discovered at bus 7 device 0 function 0. 42. Bridge is then discovered and series of configuration writes are per formed to set its bus number registers as follows Subordinate Bus Number Register 8. Bridge is now aware that the number of the bus directly attached to its downstream side is 8 and the number of the bus farthest downstream of it is 8 . 44. Bridge is discovered and its Capability registers DevicePort Type fields identifies it as PCI ExpresstoPCI bridge. 45. series of configuration writes are performed to set bridge Js bus number multifunction device within Root Complex or Switch Root Complex Register Blocks ing functions PCIcompatible configuration space. This includes the registers for both nonbridge and bridge functions. with PCI PCI Express and PCIX functions. This includes the following topics interrupt hooking. functions other than PCItoPCI bridges and CardBus bridges. The registers marked in black are always mandatory. Note that although many of the config register and any circumstances wherein it may be mandatory. dwords of PCIcompatible configuration space is intended for devicespecific registers but with the advent of the 2.2 PCI spec is also used as an overflow area for some new registers defined in the PCI spec . For full description of their implementation in PCIX function refer to the MindShare book entitled PCIX System Architecture The OS uses some combination of the following mandatory registers to deter PCICompatible register. Always mandatory. This 16bit register identifies the man ufacturer of the function. The value hardwired in this readonly register is bers. The value FFFFh is reserved and must be returned by the HostPCI bridge when an attempt is made to perform configuration read from configuration register in nonexistent function. In PCI or PCIX the read attempt results in Master Abort while in PCI Express it results in the return of UR more specific function subclass and in some cases registerspecific programming interface . The upper byte defines the base Class of the function the middle byte defines subclass within the base Class and the lower byte defines the Programming Interface. within each base Class. For many ClassSubClass categories the Program ming Interface byte is hardwired to return zeros . For some such as VGAcompatible functions and IDE control lers it does have meaning. This register is useful when the OS is attempting to locate function that Class driver can work with. As an example assume that particular device driver has been written to work with any display adapter that is 100 XGA register setcompatible. If the OS can locate function with Class of 03h flexible than driver that has been written to work only with specific function from specific vendor. The Programming Interface Byte. For some functions device and is typically an IDE or SCSI hard display adapter to enable progress messages to be displayed during the boot process. In this context this is typically referred to as the output keyboard to allow the user to interact with the machine during the boot process. In this context this is typically referred to as the input device. The OS must locate three devices that fall into these categories and must also locate device driver associated with each of the devices. Remember that the OS hasnt been booted into memory yet and therefore hasnt loaded any load able device drivers into memory from disk This is the main reason that device ROMs exist. It contains device driver that permits the device to be used dur ing the boot process. When the configuration software is configuring PCI PCIX or PCIExpress function it determines if functionspecific ROM exists by checking to see if the designer has implemented an Expansion ROM Base Address Register Address Register. Assume that the register returns value of FFFE0000h when read back after writing all ones to it. Bit 17 is the leastsignificant bit that was successfully changed to one and has binaryweighted value of 128K. This indicates that it is 128KB ROM decoder and bits within the Base Address field are writable. The programmer now writes 32bit start address into the register and sets bit zero to one to enable its ROM address decoder. The functions ROM address decoder is then enabled and the ROM can be accessed. The maximum ROM decoder size permitted by the PCI spec is 16MB dictating that bits must be readwrite. and checks for return value of AA55h. If this pattern is not received the ROM is not present. The programmer disables the ROM address decoder . If AA55h is received the ROM exists and device driver code image must be copied into main memory and its initialization code must be executed. This topic is covered The PCI spec requires that device ROM code is never executed in place . It must be copied to main memory. This is referred to as shad owing the ROM code. This requirement exists for two reasons ROM access time is typically quite slow resulting in poor performance whenever the ROM code is fetched for execution. Once the initialization portion of the device driver in the ROM has been executed it can be discarded and the code image in main memory can be shortened to include only the code necessary for runtime operation. The portion of main memory allocated to hold the initialization portion of the code can be freed up allowing more efficient use of main memory. Once the presence of the device ROM has been established . In nonPC environment the area of memory the code image is copied to could be anywhere in memory space. The specification for that environment may define particular area. In PC environment the ROM code image must be copied into main memory into the range of addresses historically associated with device ROMs 000C0000h through 000DFFFFh. If the Class Code indicates that this is the VGAs device ROM its code image must be copied into memory starting at configuration software determines which code image . The configuration soft ware can then scan through the images in the ROM and select the one best PCI Express System Architecture suited to the system processor type. The ROM might contain drivers for various types of devices made by this devices vendor. The code image copied into main memory should match up with the functions ID. To this end each code image the Vendor ID and Device ID. This is useful for matching up the driver with function that has vendordevice match. the Class Code. This is useful if the driver is Class driver that can work with any compatible device within ClassSubClass. For more informa ded within device ROM. Each image must start on an address evenlydivisible by 512. Each image consists of two data structures as well as runtime code image and an initialization code image. The configuration software interrogates the data structures in order to determine if this is the image it will copy to main memory and use. If it is the configuration software If the initialization code shortens the length indicator in the data structure the configuration software deallocates the area of main memory that held that the initialization portion of the driver is always at the end of the The area of main memory containing the image is then writeprotected. and the initialization process. ated with PCI PCI Express and PCIX functions. This included the following interrupt hooking. The PCI Express Capability register set in functions PCIcompatible con figuration space. The optional PCI Express Extended Capabilities register sets in functions PCI Express System Architecture has dedicated 4KB memory address range within which its configuration reg isters are implemented. Each Express function must implement the PCI Express Capability register set somewhere in the lower 48 dwords of the PCIcompatible register space . In addition the function may optionally implement any of vide detailed description of each of these Expressspecific register sets. PCI Express Capability Register Set bility Structure implementation of the PCI Express Capability register set is mandatory for each function. It is implemented as part of the linked list of Capability register sets that reside in the lower 48 dwords of functions PCI compatible register area. It should be noted however that some portions of this Every Express function must implement the registers that reside in dwords The bridge associated with each Root Port must implement the registers that reside in dwords seven and eight. Each bridge associated with Root Port or downstream Switch Port that is connected to slot must implement the registers Express Capability register set. The following registers must be implemented by This readonly field must contain the value 10h indicating this is the start of the This readonly field contains one of the following The dwordaligned nonzero offset to the next capability register set in the lower 48 dwords of the functions PCIcompatible configuration space. linked list of capability register sets in the functions PCIcompatible config PCI Express Capabilities Register vides description of each bit field in this register. The need for greater IO bandwidth in the computer industry has caused designers to shift from using parallel buses like ISA PCITM and PCIXTM to using multilane serial interconnects running at Gigabit speed. The industry has settled on PCI ExpressTM technology as the key IO technology of the future as it delivers on the higher bandwidth requirements helps to reduce cost for sili con vendors and leverages the software environment from the pervasive PCI PCIX technology. While the change from parallel buses to multilane serial buses sounds like small step it presented whole set of new debug and vali dation challenges to designers. Serial technology requires different approach to testing starting from the physical layer and moving up through the transaction layer. In many cases the parallel bus had several slots connected to the same physical lines which allowed you to connect test equipment to the same bus and monitor other devices. With the pointtopoint nature of serial technologies this is no longer possible and with the speed moving from the megahertz range to the gigahertz range probing of the signal becomes real challenge. The second generation of PCI Express known as PCI Express 2.0 is based on PCI Express 1.0 principles but it supports speeds of up to 5 GTs. Pre serving backwards compatibility with PCI Express 1.0 presents its own set of challenges. Also new and extended capabilities related to energy savings including active state power management and dynamic link width negotiation makes achieving interoperability between devices more challeng ing especially if these features are implemented incorrectly. Careful design and validation processes can help you avoid costly chip respins to fix interoperabil PCI Express specification requires devices to have builtin mechanism for test ing the electrical characteristics of the devices such as exists on motherboards and systems. When the transmit lanes of device are terminated with 50ohm load the transmit lanes are forced into special mode known as compliance When device is in compliance mode it automatically generates specific pat tern known as the compliance pattern. Two different deemphasis modes are introduced with the 5.0 Gbs transfer rate. All addin cards should be tested at ure A2 with both 3.5 dB deemphasis and 6 dB deemphasis. The equipment required to carry out electrical testing on PCIe 2.0 devices includes highperformance oscilloscope such as the Agilent Technologies DSO81304B 13GHz Infiniium scope and board into which you can plug an addin card to provide load on its transmitters. Alternatively you can use load board that plugs into system and forces its transmitters into compliance mode ensuring that the device is generating measurable signal. PCI Express specifications requires you to capture and process one million unit intervals of data to be able to make valid measurement. The Agilent 81304B scope has QuickMeas function that provides user defined macros and data capture functionality intended to meet needs that may be very specific to given application or measurement. The PCISIG provides compliance base board and compliance load board to help accomplish these tasks. These boards provide consistent platform to PCI Express System Architecture scope. Postprocessing is used to measure jitter on the reference clock and to measure the random and deterministic jitter on the data lines. In electrical test ing you need to test each individual lane independently as each lane is likely to have different electrical characteristics. The data is captured and then postpro Using the eye diagram you can measure the tolerances of voltage and jitter against the specification to determine if the device is compliant electrically. If you find the device is not compliant you have an early indicator that interoper By Larry Chisvin Akber Kazmi and Danny Chi Since its definition in the early 1990s PCI has become one of the most success ful interconnect technologies ever used in computers. Originally intended for personal computer systems the PCI architecture has penetrated into virtually every computing platform category including servers storage communica tions and wide range of embedded control applications. From its early incar nation as 32bit 33MHz interconnect it has been expanded to offer higher the way. Most importantly each advancement in PCI bus speed and width pro vided backward software compatibility allowing designers to leverage the broad code base. As successful as the PCI architecture has become there is limit to what can be accomplished with multidrop parallel shared bus interconnect technology. circuit boards bandwidth and latency requirements physical scalability and the need to support Quality of Service within system for wide variety of applications lead to the definition of the PCI Express architecture. PCI Express is the natural successor to PCI and was developed to provide the advantages of stateoftheart highspeed serial interconnect technology and packet based layered architecture but maintain backward compatibility with the large PCI software infrastructure. The key goal was to provide an opti PCI Express System Architecture mized and universal interconnect solution for great variety of future plat forms including desktop server workstation storage communications and embedded systems. Express is expected to serve with an explanation of how the technology will be integrated into each application and some exploration of the advantages that PCI Express brings to each usage. Lets review the key benefits of the PCI Express architecture before we discuss its application in different markets. Some of the key features of the architecture Linktolink and endtoend error detection Differential low voltage signals for noise immunity Software compatibility with legacy PCI systems PCI Express is expected to be deployed initially in desktop and server systems. These computers typically utilize chipset solution that includes one or more microprocessors and two types of special interconnect devices called north bridges and southbridges. Northbridges connect the CPU with memory graph ics and IO. Southbridges connect to standardized IO devices such as hard disk drives networking modules or devices and often PCI expansion slots. bridged to PCI slots that are used for legacy plugin cards. In some implemen tations the PCI Express interconnections will be completely hidden from the user behind PCI bridges and in other implementations there will be PCI Express slots in new PCI Express connector form factor. The major benefit for using PCI Express in this application is the low pin count associated with serial interface technology which will translate into lower cost. This low pin count provides the ability to create northbridges and IO bridges with smaller footprints and significantly fewer number of board traces between the components. This provides major reduction in the area and com plexity of the signaltrace routing in PCBs. tem. This system has similarities to the desktop system since there is north bridge and southbridge providing functions that parallel their roles in the desktop system and the form factor of the system is often similar. Servers however place greater emphasis on performance than desktop systems do. To achieve their performance and time to market objectives server designers have adopted PCIX. The primary attraction to PCIX has been increased throughput but with PCI code compatibility. PCIX offers clear benefits com pared to PCI and will remain in server systems for long while but it suffers from the same shared bus limitations that have already been discussed. The high throughput of PCI Express serial interconnection provides measurable benefit versus legacy interconnect technologies especially as the speed of the IO interconnect and the number of high speed IO ports on each card Some systems will only provide PCIX slots but many newer systems will also offer several PCI Express slots. The number of PCI Express slots will grow over time compared to the PCIX slots and eventually will become dominant in the same way that PCI did with previous interconnect technologies. Since band width is primary motivator for server typical PCI Express slots will be either In most low to midrange server systems the PCIX bridging and PCI Express slots will be provided by using the ports right off of the northbridge. However highend systems will require more IO slots of both kinds. Since PCI Express is pointtopoint technology the only way to provide additional connection links is through device called fan out switch. Specifically the purpose of fan out switch is to multiply the number of PCI Express lanes from an upstream below shows PCI Express switch used in the system for this purpose. One of the many areas that PCI has penetrated is embeddedcontrol systems. This describes wide range of applications that measure test monitor or dis play data and includes applications such as industrial control office automa tion test equipment and imaging. In these applications system designers typically utilize embedded processors. In many instances leadingedge companies will differentiate their products by utilizing some custom logic in the form of an ASIC or FPGA. bridge is often used to translate the simple custom interface and connect it to the bus. It is expected that the embeddedcontrol market will quickly migrate to PCI such as imaging and video streaming are always hungry for bandwidth and the additional throughput of x4 or x8 PCI Express links will translate into PCI Express System Architecture higher video resolution or the handling of more video streams by the system. Others will implement PCI Express because of the noise resistance its LVDS traces provide or because of its efficient routing and its ability to hook together subsystems through standard cable. Still others will choose PCI Express sim ply because of its ubiquity. PCI has become common backplane technology for mainstream storage sys tems. Although it provides good mix of features low cost and throughput the use of PCI Express in storage system. Systems similar to the one shown in We have highlighted increased bandwidth as one of the advantages of moving to PCI Express and nowhere is it more beneficial and obvious than in storage. The bandwidth demanded by IO connections such as Ethernet Fibre Channel SCSI and InfiniBand is increasing rapidly. And the ability to move data between IO modules and the host processor is critical to overall system perfor By Jack Regula Danny Chi and Tim Canepa Intelligent adapters host failover mechanisms and multiprocessor systems are three usage models that are common today and expected to become more prev alent as market requirements for next generation systems. Despite the fact that each of these was developed in response to completely different market demands all share the common requirement that systems that utilize them lines how PCI Express can address these needs through nontransparent bridg Because of the widespread popularity of systems using intelligent adapters host failover and multihost technologies PCI Express silicon vendors must pro vide means to support them. This is actually relatively low risk endeavor given that PCI Express is software compatible with PCI and PCI systems have long implemented distributed processing. The most obvious approach and the one that PLX espouses is to emulate the most popular implementation used in the PCI space for PCI Express. This strategy allows system designers to use not only familiar implementation but one that is proven methodology and one that can provide significant software reuse as they migrate from PCI to PCI This paper outlines how multiprocessor PCI Express systems will be imple mented using industry standard practices established in the PCI paradigm. We first however will define the different usage models and review the successful transparent bridging to provide the functionality needed for these types of sys Intelligent adapters are typically peripheral devices that use local processor to offload tasks from the host. Examples of intelligent adapters include RAID con trollers modem cards and content processing blades that perform tasks such as security and flow processing. Generally these tasks are either computationally onerous or require significant IO bandwidth if performed by the host. By add ing local processor to the endpoint system designers can enjoy significant incremental performance. In the RAID market significant number of products utilize local intelligence for their IO processing. Another example of intelligent adapters is an ecommerce blade. Because gen eral purpose host processors are not optimized for the exponential mathematics necessary for SSL utilizing host processor to perform an SSL handshake typi cally reduces system performance by over 90. Furthermore one of the require ments for the SSL handshake operation is true random number generator. Many general purpose processors do not have this feature so it is actually diffi cult to perform SSL handshakes without dedicated hardware. Similar examples abound throughout the intelligent adapter marketplace in fact this usage model is so prevalent that for many applications it has become the de facto stan dard implementation. Host failover capabilities are designed into systems that require high availabil ity. High availability has become an increasingly important requirement espe cially in storage and communication platforms. The only practical way to ensure that the overall system remains operational is to provide redundancy for all components. Host failover systems typically include host based system attached to several endpoints. In addition backup host is attached to the sys fails the backup host processor must not only recognize the failure but then take steps to assume primary control remove the failed host to prevent addi tional disruptions reconstitute the system state and continue the operation of Multiprocessor systems provide greater processing bandwidth by allowing plex problem. Unlike systems utilizing host failover where the backup proces sor is essentially idle multiprocessor systems utilize all the engines to boost computational throughput. This enables system to reach performance levels not possible by using only single host processor. Multiprocessor systems typi cally consist of two or more complete subsystems that can pass data between themselves via special interconnect. good example of multihost system is blade server chassis. Each blade is complete subsystem often replete with its own CPU Direct Attached Storage and IO. The History MultiProcessor Implementations Using PCI To better understand the implementation proposed for PCI Express one needs to first understand the PCI implementation. PCI was originally defined in 1992 for personal computers. Because of the nature of PCs at that time the protocol architects did not anticipate the need for multiprocessors. Therefore they designed the system assuming that the host processor would enumerate the entire memory space. Obviously if another pro cessor is added the system operation would fail as both processors would attempt to service the system requests. requirement for multiprocessor capabilities using PCI. The most popular imple mentation and the one discussed in this paper for PCI Express is the use of nontransparent bridging between the processing subsystems to isolate their Because the host does not know the system topology when it is first powered up or reset it must perform discovery to learn what devices are present and then map them into the memory space. To support standard discovery and configu ration software the PCI specification defines standard format for Control and 1. Unless explicitly noted the architecture for multiprocessor systems using PCI and PCI Express are similar and may be used interchangeably. PCI Express System Architecture nate bus number registers that when written by the host define the CSR addresses of devices on the other side of the bridge. Bridges that employ Type address registers used to request memory or IO apertures from the what kind of bridge or endpoint is represented with further information avail able in subclass field and in device ID and vendor ID registers. The CSR branches of PCI hierarchy from the host bridge down to each of its leaves reading the class code registers of each device it finds as it proceeds and assign ing bus numbers as appropriate as it discovers PCItoPCI bridges along the way. At the completion of discovery the host knows which devices are present and the memory and IO space each device requires to function. These concepts Implementing MultihostIntelligent Adapters in PCI Up to this point our discussions have been limited to one processor with one memory space. As technology progressed system designers began developing end points with their own native processors built in. The problem that this caused was that both the host processor and the intelligent adapter would upon power up or reset attempt to enumerate the entire system causing sys tem conflict and ultimately nonfunctional system.2 To get around this architects designed nontransparent bridges. nontrans parent PCItoPCI Bridge or PCI ExpresstoPCI Express Bridge is bridge that side to the other with address translation through apertures created by the appears to be an endpoint to discovery and configuration software eliminating potential discovery software conflicts. Each BAR on each side of the bridge cre ates tunnel or window into the memory space on the other side of the bridge. To facilitate communication between the processing domains on each side the nontransparent bridge also typically includes doorbell registers to send inter rupts from each side of the bridge to the other and scratchpad registers accessi ble from both sides. nontransparent bridge is functionally similar to transparent bridge in that both provide path between two independent PCI buses . The key difference is that when nontransparent bridge is used devices on the downstream side of the bridge are not visible from the upstream side. This allows an intelligent controller on the downstream side to manage the devices in its local domain while at the same time making them appear as single device to the upstream controller. The path between the two buses allows the devices on the downstream side to transfer data directly to the upstream side of the bus without directly involving the intelligent controller in the data movement. Thus transactions are forwarded across the bus unfettered just as in PCItoPCI Bridge but the resources responsible are hidden from the host which sees single device. 2. While we are using an intelligent endpoint as the examples we should note that similar problem exists for multihost systems. Because we now have two memory spaces the PCI Express system needs to translate addresses of transactions that cross from one memory space to the other. This is accomplished via Translation and Limit Registers associated with Translation. Address translation can be done by Direct Address Translation table lookup or by adding tion used to create multiple windows spread across system memory space for packet originated in local IO processors domain as well as Direct Address Translation used to create single window in the opposite direction. byte definitions currently provided in the 2.3 PCI specification. Function built before class codes were defined Table D9 Class Code 7 Simple Communications Controllers Multiport serial controller. Native PCI Express implementations do not support lock. Support for Locked transaction sequences exist solely for supporting legacy device software execut ing on the host processor that performs locked RMW operation on memory semaphore that may reside within the memory of leg supporting locked access sequences that target legacy devices. Failure to sup port lock may result in deadlocks. PCI Express continues the PCI 2.3 tradition of supporting locked transaction sequences to support legacy device software. PCI Express devices and their software drivers are never allowed to use instructions that cause the CPU to generate locked operations that target memory that resides beneath the Roor Complex level. Locked operations consist of the basic RMW sequence that is The modification of the data within processor register. One or more writes to write the modified semaphore value back to the tar This transaction sequence must be performed such that no other accesses are requires blocking other transactions during the operation. The result potentially can result in deadlocks and poor performance. The devices required to support locked sequences are The Root Complex. Any Switches in the path leading to legacy devices that may be the target ory residing within the legacy device. No other devices must support locked transactions and must ignore any locked transactions that they receive. Lock in the PCI environment is achieved in part via the use of the PCI LOCK signal. The equivalent functionality in PCI Express is accomplished via trans action that emulates the LOCK signal functionality. The only source of lock supported by PCI Express is the system processor and as consequence the source of all locked operations in PCI Express is the Root Complex . locked operation is performed between Root Complex downstream port and the PCI Express downstream port to which the targeted legacy device is attached. In most systems the legacy device is typically PCI ExpresstoPCI or PCI ExpresstoPCIX bridge. Only one locked sequence at time is supported for given hierarchical path. PCI Express limits locked transactions to Traffic Class 0 and Virtual Channel 0. All transactions with TC values other than zero that are mapped to VC other than zero are permitted to traverse the fabric without regard to the locked oper ation. All transactions that are mapped to VC0 are subject to the lock rules dix presumes that all transactions have been assigned to TC0 Originates locked sequence. The first MRdLk transaction blocks other requests from reaching the target Memory Read Lock Completion with Data Returns data and confirms that the path to the target is locked. successful read Completion that returns data for the first Memory Read Lock request results in the path between the Root Complex and the target device being locked. That is transactions traversing the same path from other ports are blocked from reaching either the root port or the target port. Transactions being routed in buffers for VC1VC7 are unaffected by the lock. Memory Read Lock Completion without Data Completion without data payload indicates that the lock sequence cannot complete currently and the path remains unlocked. from the locked root port. This message unlocks the path between the root The Root Complex that initiates the Locked transaction series on behalf of the host processor. Switch in the path between the root port and targeted legacy endpoint. PCI ExpresstoPCI Bridge in the path to the target. The target PCI device whos Device Driver initiated the locked RMW. PCI Express endpoint is included to describe Switch behavior during In this example the locked operation completes normally. The steps that occur The Memory Read Lock Operation The CPU initiates the locked sequence as result of driver executing locked RMW instruction that targets PCI target. Complex is always the source of locked sequence. The Switch receives the lock request on its upstream port and forwards the request to the target egress port . The switch upon forwarding the request to the egress port must block all requests from ports other than the ingress port from being sent from the egress port. subsequent peertopeer transfer from the illustrated PCI Express end point to the PCI bus would be blocked until the lock is cleared. Note that the lock is not yet established in the other direction. Transactions from the PCI Express endpoint could be sent to the The Memory Read Lock Request is sent from the Switchs egress port to the PCI ExpresstoPCI Bridge. This bridge will implement PCI lock semantics . The bridge performs the Memory Read transaction on the PCI bus with the PCI LOCK signal asserted. The target memory device returns the requested semaphore data to the bridge. Read data is returned to the Bridge and is delivered back to the Switch via Memory Read Lock Completion with Data . The switch uses ID routing to return the packet upstream towards the host processor. When the CplDLk packet is forwarded to the upstream port of the Switch it establishes lock in the upstream direction to prevent traffic from other ports from being routed upstream. The PCI Express endpoint is completely blocked from sending any transaction to the Switch ports via the path of the locked operation. Note that transfers between Switch ports not involved in the locked operation would be permitted . 15. The Switch receives the Unlock message unlocks its ports and forwards the message to the egress port that was locked to notify any other Switches andor bridges in the locked path that the lock must be cleared. 16. Upon detecting the Unlock message the bridge must also release the lock

Note to . Government Users Restricted Rights Use duplication or disclosure restricted by GSA ADP Note Before using this information and the product it supports read the information in This IBM Redbook is designed for onesemester course intended to provide information systems students with the background knowledge and skills necessary to begin using the basic communications facilities of mainframebased system. It provides broad understanding of networking principles and the hardware and software components necessary to allow the mainframe to participate in high volume data communications network. It is part of planned series of textbooks designed to introduce students to mainframe concepts and help prepare them for career in large systems computing. While many of the networking concepts covered are operating systemindependent the main emphasis is on the zOS operating system. You are assumed to have successfully completed introductory courses in computer system concepts including courses in computer organization and architecture operating systems data management and data communications and systems design and analysis. basic understanding of zOS job control library structure and system libraries is assumed. It is strongly recommended that you have already completed an introductory course on zOS such as one that uses Introduction to the New Mainframe zOS Basics or comparable text. In some instances the zOS Basics course and the course associated with this textbook could be taken This book can also be used as prerequisite for courses in advanced topics or for internships and special studies. However it does not comprehensively cover all aspects of data communications nor is it reference book that discusses every feature and option of the zSeries communications facilities. Others who will benefit from this text include data processing professionals who have experience on nonmainframebased platforms or who are familiar with some aspects of the mainframe environment or applications but want to learn about the security and integrity facilities and benefits of the mainframe As we go through this book we suggest that the instructor alternate between cumulative and are designed to show the student how to design and implement an integral part of the course material and can include topics not covered in this textbook. We use simplified examples and focus mainly on basic system Introduction to the New Mainframe Networking students explore the mainframe style of computing. Methods for implementing TCPIP on the zOS operating system Methods for implementing SNA on the zOS operating system Basic skills in network operations security and problem determination This text is organized in four parts as follows functions and roles of networking professionals. It also introduces you to several typical applications and environments that are used throughout the text as examples when discussing specific networking scenarios. networking the latest networking technologies and the role of the networking network layers the protocols at each layer and the hardware facilities that support data transfer. the various ways mainframe connects to network. configuration that is used to demonstrate various course concepts. of IPbased communications in networkattached zOS system and among multiple systems in zOS cluster or Parallel Sysplex. TCPIP in Parallel Sysplex. examines IBMs System Network Architecture including its evolution and its current integration into IP networks. describes the second SNA implementation which is Advanced PeertoPeer Networking . display management protocol is integrated into IP operations security and problem determination. includes network commands and displays commonly used to monitor and Topics that teach central theme related to mainframe networking Questions for review to help students verify their understanding of the This redbook was produced by team of specialists from around the world working at the International Technical Support Organization Poughkeepsie Mike Ebbers has worked with mainframe systems at IBM for 32 years. For part of that time he taught handson mainframe classes to new IT professionals who were just out of college. Mike currently creates IBM Redbooks popular set of Christopher Hastings is Senior Software Engineer at IBM Poughkeepsie. He has 25 years of experience developing user assistance manuals and online help for large operating systems such as zVM and zOS. Chris holds Master of Arts degree in Philosophy from Duquesne University Pittsburgh Pennsylvania. Matt Nuttall is an information technology specialist with IBM Global Services in Canada. He has over 16 years of experience in information technology most of it in networking. Matt holds degree in Computer Science from the University of British Columbia. His areas of expertise include networking and networking security. He has written several IBM Redbooks relating to zOS. Micky Reichenberg is an independent consultant with more than 30 years of experience in mainframe networking. He specializes in mainframe TCPIP SNA and open systems connectivity to the mainframe. Prior to becoming consultant Micky was systems programmer with IBM in Israel for 17 years. During his assignment with the International Technical Support Center in Raleigh North Carolina he published five networkingrelated Redbooks. He holds Bachelors degree in Aeronautical Engineering from the Techion Israel Institute Grant Cuming is Senior Systems Programmer with IBM Global Services in New Zealand. He has more than 20 years of experience working with government private companies and IBM outsourced accounts. Grant specializes in implementing and maintaining zOS TCPIP VTAM and years. He works for Network and Channel Connectivity Services in the EMEA Product Support Group. His areas of expertise include zOS TCPIP VTAM OSAExpress and Parallel Sysplex for zSeries. Joel has taught OSAExpress and FICON problem determination classes and provided on site assistance in these areas in numerous countries. He also coauthored the IBM Redbooks Using the IBM S390 Application StarterPak SG242095 OSAExpress Gigabit Ethernet Implementation Guide SG245443 and OSAExpress Implementation Guide SG245948. Special thanks to the editors at the ITSO center in Poughkeepsie New York We want our Redbooks to be as helpful as possible. Send us your comments about this or other Redbooks in one of the following ways Use the online Contact us review redbook form found at IBM Corporation International Technical Support Organization Objective As data communications expert in the world of mainframe computing you will need to understand the role of the network in your companys business objectives and corporate infrastructure. You also need to understand how the latest networking technologies work with your companys mainframe computer. Explain the use of networks in high volume industry transaction List at least three typical advantages of mainframe in network Describe the role of network administrator in large network. List the major software components of the zOS Communications Server. In the broadest sense of the word network is an interconnected system of people or things. In the fastpaced lively field of information technology network is defined as the hardware and software that enables computers to share files and resources and exchange data. Depending on the size of business network can be as simple as two personal computers on locally connected network or as complex as the Internet worldwide network of millions of computers of various types. To send or receive data through network companys employees interact through variety of communication devices such as telephones workstations and computers. Network data can flow through an even greater variety of mechanisms communication software and hardware telephone wires broadband cable wireless and microwave transmission units satellite fiber optics and so on. Regardless of which elements comprise particular network youthe end userare the ultimate source and destination of the information that flows To some extent the definition of network depends upon who is using the network. For example even though voice and data share the same network an IT professional hired to support the voice traffic will likely view the network differently than the person assigned to maintain data traffic. The telephony expert or electrical engineer might describe the network as group or system of electronic components and connecting circuitry designed to function in specific manner while network designer or architect might explain the network as system of lines or channels that cross or interconnect forming complex In this text well try not to be so tedious or evasive. Our definition of network group of interconnected computers capable of exchanging information collection of computers and associated devices connected by communications facilities that share information The entity that allows users applications and computers in corporation to exchange data and files for the purpose of transacting business And our primary focus will be on how network technology relates to mainframe computers the workhorses of corporate IT. To be effective corporate communications rely on secure links between thousands of end users applications and computers of various sizes including mainframes. Wherever speed and security are essential mainframes are used as the most critical servers in the network infrastructure. If you have never pondered the incredible interconnectedness of the modern world its computers and its end users consider your own experience you use Withdraw money from bank account through an automated teller machine Submit payment at the supermarket with debit or credit card Computer networks touch nearly every aspect of everyday life. And when large organization needs transaction processing the odds are that the network is connected to mainframe. Networks are categorized as internets intranets and extranets Internet is collection of individually managed networks connected by intermediate networking devices that function as single large network. Internetworking refers to the industry products and procedures that help to create and administer internets. Intranet is privately maintained computer network that can be accessed only by authorized persons and is limited to one institution. Extranet is an extension of an institutions intranet used to connect business partners. In todays IT environment the World Wide Web is the enabler for communication between the institution business partners and people it deals with often by providing limited access to its intranet. What is mainframe Its computer that supports dozens of applications and inputoutput devices to serve tens of thousands of users simultaneously. What separates the mainframe from other computers is not just its processing capabilities. mainframe has redundant features and system health awareness capabilities that enable it to deliver 99.999 availability. Throughout this text the general term mainframe refers to large computers like those in the IBM System z9 and eServer zSeries processor families. Before the advent of the Internet employees in corporation perceived the network as the terminals that served the companys business transactions. This workload was rather predictable both in transaction rate and mix of transactions and much of the work could be done after hours through batch processing on the The paradigm used today is online transaction processing . OLTP is class of program that facilitates and manages transactionoriented applications typically for data entry order entry and retrieval transactions in number of industries including banking airlines mail order supermarkets and manufacturers. Probably the most widely installed OLTP product excluding the Web servers that frontend most OLTP is the IBM Customer Information Control System or CICS . Todays online transaction processing increasingly requires support for transactions that span network and may include more than one company. For this reason new OLTP software uses clientserver processing and brokering software that allows transactions to run on different computer platforms in Todays networks and transactional systems must be able to support an unpredictable number of concurrent users and transaction types. Most transaction programs need to respond in short time periodsfractions of second in some cases. For example inside bank branch office or through the Internet customers are using online services when checking an account balance or transferring fund In fact an online transaction system has many of the characteristics of an Managing and controlling simultaneous access to data files Most of the traffic in network involves transaction processing where one side initiates the transaction and the other side processes authorizes and approves or declines the transaction. Examples of activities that result in network traffic include Ordering and receiving parts to assemble automobiles Cash withdrawal from an automated teller machine Purchasing merchandise at retail pointofsale Paying bills over the Web using home banking application Such ebusiness as flight and car rental reservations In fact even receiving traffic citation can generate network traffic. How else can the patrol officer check for outstanding warrants In todays competitive market responsiveness to customer or supplier demand is often decisive factor in the success of an organization. The network is considered one of the most critical resources in an organization both in the private and public sectors. Networks are created to provide means to satisfy an objective or need. These objectives and needs are frequently critical therefore the network itself is critical. Consider the metaphor of transportation network . If any of these conduits were to become suddenly unavailable our ability to distribute food clothes and products would be seriously compromised. The residents of town or country who need the food clothes and products are the end users of this particular type of network. Similarly computer network is created to provide means of transmitting data sometimes essential data from one computer to another. The accuracy and speed of daily business transactions for large organizations are vital to their success. Unscheduled disruption resulting in the failure to process these daily business transactions are costly and potentially disastrous. The widespread use of networks extends the reach of organizations. These remote interactions with customers suppliers and business partners have significantly benefited countless businesses. It has correspondingly positively impacted the overall productivity of many countries. Such productivity gains however are only as good as the network. Mainframes are used by large organizations as their central transaction processing system. Transaction processing in this context requires high availability security performance and responsiveness. For example Introduction to the New Mainframe Networking consumers expect to be able to use their credit card 24 hours day 365 days year. They expect those transactions to be safe and they dont expect to be left standing at the checkout waiting for it to all happen. The mainframe is specifically designed to be the best of breed for performing massive concurrent transaction processing in the range of hundreds of transaction per second. In the examples that follow we look at typical cases of networks as they are commonly used in high volume business transactions. Each of these examples shows an industry that relies on messages being sent electronically over communication network. In most cases mainframe is used to send the message one or more mainframes may be needed to route it to the appropriate place and third mainframe is used to receive it. Although simplified to some extent these examples provide some insight into the extent and complexity of electronic communication networks In practice the number of transactions the interfaces among the business partners and the number of data elements is several orders of magnitude more The simple act of withdrawing cash from an automated teller machine is much more complicated than it appears. You begin by inserting your identification card and entering personal identification number . Your identity is verified online when computer in the network compares the information you entered to database of customers belonging to that financial institution. Internal electronic messages are created to access the specific checking or savings account where the money is held. Then the account disperse the funds or refuse the transaction. appropriate checking or savings accounts this is usually done in realtime. By the time the money is dispensed from the machine the account balance will reflect the withdrawal. It becomes more complex if you make an outofterritory withdrawal. For example you use Bank 1s ATM to withdraw money from your account at Bank 2. The peer banks database must be accessed and the account All of this occurs as the customer waits at the machine. The network and mainframe computers involved must be very fast to keep the response time reasonable from the customers point of view. The successful completion of the transaction depends on among other things both banks using compatible network technology to exchange information. When you use credit card to purchase goods from retailer then network and most likely mainframe computer is involved. When your credit card is electronically scanned the identification is initially handled by the company that provides the point of sale credit card reader. From there the transaction is sent through the network to the credit card companys mainframe. The advantage of sending the transaction immediately is to detect whether you are exceeding your credit limit and to prevent such violations. Furthermore if the card is stolen or if you have exceeded the credit limits the merchant must be notified in time to void the purchase. Often an intermediate host is used to handle and approve or disapprove the transaction. All of this can only be effective when robust responsive communication network is in place among the merchant credit card company and the issuing bank. The transactions that were described take advantage of the following functionality that the mainframe can provide to an OLTP system Availability Customers do not expect an ATM to be unavailable. Ever. Security The PIN number entered is encrypted at the ATM and decrypted at the host that executed the ATM transaction. Responsiveness How long is customer willing to wait until the transaction It is unlikely that each and every business supplier in these examples uses the same network components. Many may use IBMs Systems Network Architecture protocols while most will use the TCPIP protocols. An even smaller number may use proprietary protocols . When two or more partners do not use identical network components there must be some process to enable them to coexist and to interpret each others messages. In this course the technology options available to design and implement network to handle business transactions will be explored and applied to the above examples of business transactions. Some of these products exist primarily to allow different protocols to function together. In particular SNA is rapidly adapting to the IPcentric networks favored by todays organizations. Network communications has both software and hardware aspect and separation of software and hardware administrative duties is common in large enterprises. The network administrator skilled software data communication expert however needs to understand both aspects. The network administrator must bring thorough understanding of the operating systems communications interfaces to any project that involves working with the companys network. While network hardware technicians have specific skills and tools for supporting the physical network their expertise often does not extend to the operating systems communications interfaces. When nationwide retail chain opens new store the network administrators and network hardware technicians must coordinate their efforts to open the new store. Most of this textbook focusses on zOS network concepts implementations and hardware. You may also find it useful to have working knowledge of other operating systems available for mainframes. One reason for this is that given mainframe computer might run multiple operating systems. For example the use of zOS zVM and Linux on the same mainframe is common. The responsibilities of zOS network administrator may include Defining maintaining and modifying an existing mainframe network Providing technical guidance to application development and business unit Maintaining an awareness of emerging network technologies Recommending and implementing new network technologies This course is intended to assist in preparing you to fulfill these responsibilities as member of the zOS networking team in large enterprise. Basic elements of computer network include hardware software and protocols. The interrelationship of these basic elements constitutes the infrastructure of the network. To revisit the transportation metaphor we used earlier if we think of network as roads highways rails and other means of transport the network protocols are the traffic rules. network infrastructure is the topology in which the nodes of local area network or wide area network are connected to each other. These connections involve equipment like routers switches bridges and hubs using cables or wireless technologies . The network protocols define how two devices in the network communicate. The specification of the network protocols starts with the electrical specifications of how networking device is connected to the infrastructure. For example line voltage levels carrier signals and the designation of which line might be used for what types of signals must all be specified. Building up from there network protocols include such specifications as the methods that can be used to control congestion in the network and how application programs will communicate and popular method of documenting network protocols is to use layered network architecture model. Network architecture models separate specific functions into layers which collectively form network stack. While protocol consists of rules that define characteristics for transporting data between network nodes the layered model separates the endtoend communication into specific functions performed within each layer. Ideally the layers are isolated from each othereach layer does not need to know how the layer below it functions. All layer needs to know is how to interact with the layers adjacent to it. You can learn more about network layers in Today TCPIP is by far the most dominant suite of networking protocols. Prior to TCPIP SNA was arguably the dominant protocol suite. There is some irony here because TCPIP is the older of the two protocols. Many networks in larger organizations are using both of these protocol suites. As with most networking protocols both SNA and TCPIP are layered protocol stacks. IBMs current mainframe technology provides significantly large servers with distinctive strength of handling high volume of transactions and inputoutput operations in parallel. The mainframe is capable of serving large number of network nodes geographically dispersed across the world while simultaneously handling high volume of input and output operations to disk storage printers and other attached computers. Mainframe architecture includes variety of network capabilities. Some of these IP communication among large numbers of Linux and zOS operating systems running as zVM guest machines IP communication among independent operating systems running in logical IP communications among tightly coupled cluster of mainframe LPARs Communications via the TCPIP suite of protocols applications and equipment System Network Architecture suite of protocols and equipment including subarea and Advanced PeertoPeer Networking with high Integration of SNA into IP networks using Enterprise Extender If you are unfamiliar with some of these terms this is to be expected. The mainframe is usually connected to the outside world using an integrated LAN adapter called the Open Systems AdapterExpress . The Note What is zVM guest machine zVM is another mainframe operating system that on its own does nothing more than reproduce the instruction set of mainframe machine. It provides guest operating system with selfcontained environment that appears to the guest as though it were real physical machine. zVM requires very low overhead to produce guest machines and can consequently support very large numbers of them . OSAExpress is the equivalent of the network interface card used in Windows and UNIX systems. It supports various operational modes and protocols. Most commonly the OSAExpress card will use the Ethernet protocol running over copper wire or fiber optic cabling. The latest OSAExpress card called OSAExpress2 supports Ethernet at speed of 10 Gbs. Because the IO subsystem of the mainframe is different from Intel or UNIX systems the OSA card implements advanced technologies required for The OSAExpress card is connected to redundant backbone switchrouter that implements the Note redundant backbone switch or router is used to connect critical business servers to the primary network for given organization. The switch or router provides redundancy by providing more than one physical path to the backbone network. The switch or router also is aware of the network through routing protocol which ensures that changes The backbone network itself is an organizations hightraffic density network. backup site takes over the data processing for planned and unplanned outages of the production site. The backup site is selfcontained and can provide data processing services for long time. Duplicating the production site can be very costly. The level and the type of services the backup site will provide is determined by the cost of backup compared to the cost of failure. The larger the organization the higher the cost of failure and hence the greater value placed upon fully functional backup site. The backup and the production site are connected using high speed connections normally using fiber optics. In addition to networking related data Backbone Network the connections are used to mirror data stored on disks from the production site at the backup site. Mirroring may be done in realtime. Offices used for the computer personnel administration and back office services may be in the same building the same campus or few blocks away. These sites also would be connected using high speed connections. Remote sites such as branch offices and remote offices are connected to the backbone network. The backbone network might use carriersupplied communication lines. The speed the protocol and the topology are designed and implemented by the networking department and the network users. The zOS operating system includes software component called zOS Communications Server. zOS Communications Server implements the SNA and TCPIP protocols. SNA applications and transaction servers can use SNA or TCPIP to send and receive data. Industry standard internet applications can use TCPIP to send and receive data. For example zOS server may run FTP telnet Web servers and mail programs . zOS Communications Server provides set of communications protocols that support connectivity functions for both local and widearea networks including the Internet. zOS Communications Server also provides performance enhancements that can benefit variety of wellknown TCPIP applications. These performance enhancements which may be softwarebased or hardwarebased are discussed in their appropriate contexts later in this book. Note carriersupplied network is network that is provided on behalf of another organization. It is form of outsourcing an organization simply needs the network so it enlists another The TCPIP protocol stack. The SNA protocol stack contained in Virtual Telecommunications Access The Communications Storage Manager which provides shared IO buffer area for both TCPIP and VTAM data flow. The CSM function allows authorized host applications to share data without having to physically move Similar to TCPIP functions SNA functions are implemented on number of platforms besides zOS for example AIX AS400 Microsoft Windows and Linux. As result zOS application programmers can exploit technological advancements in communications across distinctly different operating systems. In the past mainframe backbone network used SNA. With the prevalence of TCPIP and the introduction of SNAIP integration technology and additional tools current mainframe networks are migrating to IPbased networks. SNA was developed by IBM for the business community. SNA provided industry with technology that permitted unparalleled business opportunities. What TCPIP and the Internet were to the public in the 1990s SNA was to large enterprises in the 1980s. TCPIP was widely embraced when the Internet came of age because it permitted access to remote data and processing for relatively small cost. TCPIP and the Internet resulted in proliferation of small computers and communications equipment for chat email conducting business and downloading and uploading data. Large SNA enterprises have recognized the increased business potential of expanding the reach of SNAhosted data and applications to this proliferation of small computers and communications equipment in customers homes and small Today more than ever businesses depend on the critical data that flows over networks. large amount of sensitive and confidential data is stored and retrieved from zOS systems so the data that moves through zOSattached networks must be secured have high integrity and be available at all times. The mainframe environment includes both hardware and software tools that meet Note So why isnt the Internet running SNA protocols What happened The answer is simple complexity. SNA is deterministic architecture. It uses hierarchical method of definitions and leaves very little to chance. Bandwidth connections and users all need to be predefined completely or at least to some degree. Contrast this to IP in which nothing is predetermined and large amount of unpredictability exists within bandwidth connectivity and New mainframe hardware and software are ideal for network transactions because they are designed to allow huge numbers of users and applications to access rapidly and simultaneously the same data without interfering with each other. In networks that support thousands of end users the mainframe concepts of data integrity security and reliability are extended to include the network. The designer of large network must balance the need for data and transaction security with the requirement to provide rapid response time and reliability and availability of the network. Some of the aspects of security that need to be taken into account are discussed Data protection not only includes privacy but also integrity. For example financial transaction should be kept confidential no matter where it exists on network. But just as importantly there must be controls in place to ensure that the data has not been altered. place to ensure that sender cannot deny having sent packet. Conversely nonrepudiation requires mechanism such that receiver cannot deny having received packet . Again it is paramount for financial institution to be able to confirm that transaction has genuinely been sent by who we believe sent it and that it has been received by who we expect to receive it. The networking protocols such as TCP have builtin services which guarantee that data sent from an application arrives at its destination in the same sequence as it was transmitted and is errorfree. By errorfree we mean that the same bit sequence that was transmitted is delivered to the destination node. The lower two layers in the networking architecture have the responsibility for the bit sequence and the transport layer has the responsibility for the correct sequence. To implement these network design goals zOS and affiliated products provide zOS system and resource security is provided by both the IBM Security Server and the zOS Communications Server components. IBM Security Server includes Resource Access Control Facility for authentication authorization and restriction. The zOS Communications Server components each include parameters to encrypt network traffic. For example TCPIP includes firewall filtering Virtual Private Network and Transport Layer Security capabilities as part of the protocol stack itself. Each of the major IBM subsystems used for deploying business applications such as Customer Information Control System Database 2 Information Management System WebSphere Application Server HTTP Server Message Queuing Series and so forth in conjunction with RACF and other mainframe components have security mechanisms available that provide additional levels of security. Each of the available tools for securing resources and data can be used independently or together to accomplish security objectives. Availability which is the degree to which system is ready when needed to process data is key in providing around the clock services. The networkand particularly network attached to mainframeis considered critical and availability is mandatory for the continuity of business processes. Designers of large networks enhance availability by introducing redundant communication lines routers and switches and implementing server clusters. Maximizing redundancy has high price tag and the network designer together with management must decide on the risks and impact of an outage. This will determine the availability level that suits the application and the organization. The level of reliability and redundancy introduced in mainframes is in the range of 99.999 which still leaves unplanned outage of about 5.3 minutes year. To achieve even higher availability IBM introduced clustering technology called Parallel Sysplex. network is the hardware and software that enables computers to share files and resources and exchange data. Networks play significant role in much of the worlds transaction processing. large corporation conducts daily operations over one or more networks that connect the businesslocally or remotelyto partners suppliers and customers around the world. To support the changing requirements of online transactions enterprise networks can be designed customized operated and supported using the combined features and functions of network protocols such as SNA and TCPIP. Introduction to the New Mainframe Networking zOS network capability includes fullyfeatured communications server with integration of SNA and TCPIP protocols making the mainframe large server capable of serving large number of worldwide clients simultaneously. Many technology options exist to transport secure protect and encrypt zOS hosted business sensitive and customer confidential data between the mainframe and authorized clients. The requirements and specifications of the business transactions should determine the technologies chosen to handle the transactions. 1. What might be some typical networkbased activities that you perform in 2. What is network 3. List three typical advantages of mainframe in network communications. zOS Communications 4. Describe the role of network administrator in network that supports thousands of end users. 5. List the major software components of the zOS Communications Server. 1. What is the most widely used computer in the world 2. What are consequences of failing to secure businesssensitive and customer confidential data that passes through public domain network Objective As network administrator you must have general knowledge of network layers the protocols at each layer and the hardware that facilitates Describe the network layers and their protocols. Describe the characteristics of local area networks and wide area Define hubs switches routers gateways and secure gateways. List upper layer protocols. If you are already comfortable with the terms associated with these topics It goes without saying that between two endpoints on network there must be an agreement on the protocol or language that is in use. There is some irony in the fact that the same requirement is sometimes ignored when the communications endpoints are the network administrator and the zOS system programmers. It is not unheard of to have the network administrator maintain that the problem is with the data link control only to have the system programmer reply that there have been no linkage errors with any programs. administrators and systems programmers must know in order to do their jobs is enormous. While each specialist has clearly defined domain some overlap is understand in order to communicate with network administrator. No networking book would be complete without discussing the fact that IP networks are implemented as layers as illustrated in Note It is assumed that you are already somewhat familiar with IPrelated Although it is beyond the scope of this text to present IP as new topic there are many other sources of that information such as the IBM Redbook TCPIP One way to look at layering in network is as an isolation of concerns. Each layer has certain capabilities that it is required to uphold. For example the IP layer does not include reliability of delivery while the TCP layer does not concern itself with routing details. The physical network begins at the network interface card . The NIC is effectively method of connecting the internal data bus of computer to the external media of the network. In the case of zOS there is essentially only one NIC the Open Systems Adapter Note Isolation of concerns of protocols is not as foreign as it first For example when you write check you conform to various requirements of and signature on each check. When it comes to delivery of that check you put it in an envelope and follow the postal mail protocol to ensure you have destination address zip code return address and of course correct postage. The postal service doesnt care what protocol is being encapsulated within the envelope. The content of There are other ways of attaching network to zOS host but they are very seldom used. These other interfaces media. When connected to the latter the ubiquitous RJ45 is the connection type used. What exactly is twisted pair Exactly what it says wires running from an RJ45 adapter are twisted as pairs and housed as unit within larger cable casing. This cable is referred to as UTP Cat 5 which stands for Unshielded Twisted Pair The UTP Cat 5 standard tops out at 100 Mbps . To get faster speeds the OSA cards switch to higher quality cabling such as 100BaseTX. And rather than staying with copper media higher speed networks can use fiber optic cables 1 Gbps and 10 Gbps speeds are supported at the time of writing. OSA card fiber optic connections can be accomplished using one of two interface types the SC or LC. In addition each of these interface types can be attached to one of several cable types. Thus in order to explain how that RJ45 adapter attached to UTP CAT 5 cable is going to be used we must begin talking about layers. The layer that is concerned with how data signalling and movement is effected over the physical layer is called the data link layer. Note RJ45 is the older sibling of RJ11 . RJ stands for Register Jack and the 11 standard is the North American standard for phone equipmentpresumably everyone has seen an RJ11 connection. RJ45 is the worldwide standard for copper mediabased Ethernet cabling. So whether connecting simple personal computer to twisted pair local area network or enterprise scale mainframe RJ45 is used. simply the link layer. The actual protocols encompassed in the link layer are numerous and the implementation details can be found in various documents throughout the Internet and in trade texts. For the purpose of this discussion well limit the scope to aspects of the link layer that network administrator would need to know. The foremost data link layer protocol is the Ethernet protocol. Ethernet technology is everywhere. It is believed that more than 90 of network installations use Ethernet. The remaining network connections are combination of Token Ring Fiber Distributed Data Interface Asynchronous Transfer Mode and other protocols. Ethernet gained acceptance because of its simplicity of installation and management. Like the check being placed into the envelope the Ethernet protocol encapsulates data passed to it from higher layers. It also does the reverse it decapsulates data that is presented to it from the physical layer. Thus it stuffs envelopes when data is moving down through the layers and it opens envelopes The Ethernet standard was defined in 1985 by the Institute of Electrical and Electronic Engineers in specification known as IEEE 802.3. The standard specifies the physical medium carrier sense multiple access with collision detection access method and frame format. In the CSMACD access method each station contends for access to the shared medium. If two stations try sending the packets at the same time collision will result. The CSMACD access method is designed to restore the network to normal activity after collision occurs and collisions are normal in an Ethernet The original 10 Mbps shared Ethernet network was based on coaxial cable physical medium and later the standard was extended to shielded and unshielded twisted pair and fiber optic cable media. The most common physical media is unshielded twisted pair because it is inexpensive easy to install The 10 Mbps twisted pair standard is referred to as 10BaseT. Fast Ethernet is an extension of the popular 10BaseT Ethernet standard supporting both 10 Mbps and 100 Mbps media speed. Fast Ethernet retains the data format and protocols of 10 Mbps Ethernet so no changes are required in higher level protocols and applications. Fast Ethernet standards provide for autonegotiation of media speed allowing vendors to provide dualspeed Ethernet interfaces that can be installed and run at either 10 or 100 Mbps. With dual speed products users who are planning future 100 Mbps implementations can purchase 10100 Mbps product today and use the 10 Mbps speed in their existing networks and then later upgrade to Gigabit Ethernet is an extension to 10 Mbps and Fast Ethernet. It provides seamless interoperability with the existing 10 Mb and Fast Ethernet and is compatible with existing networking protocols networking operating systems network applications and networking management tools. It uses combination of proven protocol technologies adopted by the original IEEE Gigabit Ethernet retains the standard 10100BaseT frame size and format and the same CSMACD scheme. However it can use fiber channels physical layer as the underlying transport mechanism. The full duplex implementation of Gigabit Ethernet as in Fast Ethernet does not require the CSMACD scheme but retains support for the Ethernet frame format. The initial Gigabit Ethernet offering supported one fiber physical interface. Two common fiber types in use today are single mode fiber for longer distances up to 60 kilometers and multimode fiber for shorter distances in the range of 300 to 500 meters. They are covered by the 1000BaseLX and the 1000BaseSX specification respectively. standard has been defined by the IEEE 802.3ab task force for Gigabit Ethernet over copper physical medium. Note Star topology is so named because it allows all hosts in network to be logically connected at central point. The central point of connectivity means that the loss of any individual host on the network will not affect the remaining connected hosts. Compare this to chain topology where the loss of host in the chain would cause disruption in connectivity. The evolution of Ethernet speeds continues. An OSAExpress2 card is also Gbps. As with 1Gigabit Ethernet there is copper medium option . Ethernet designates the frame format and the speed of the data travelling over the physical network. However there is still need for controlling how individual hosts attached to the physical network locate each other. The answer is the media access control address. Every host connected to the network has unique MAC address associated with its NIC. This MAC address via the NIC uniquely identifies the host. MAC addresses are generally built into the NIC itself but TCPIP on zOS does allow MAC addresses of OSA cards to be manually altered. The most significant protocol at layer 3 is the Internet Protocol or IP. IP is the standard for routing packets across interconnected networkshence the name internet. It is an encapsulating protocol similar to the way Ethernet is an encapsulating protocol. If we view the original check as unit of data needed to be sent we now have two envelopes required to do the transmissionthe check first goes into an IP envelope and then the entire IP envelope is placed into an Ethernet frame. The format of an IP packet is documented in RFC 791. The most significant aspect of the IP protocol is the addressing every IP packet includes the IP source address and the IP destination address . Note The address assigned to NIC might also be referred to as universally administered address because all NICs sold worldwide must be uniquely addressed. If the address of NIC has been manually overridden it is considered to be locally The fields that are of most interest in the example are the source and destination some control over the way this packet is treated as it is moved around the packets can contain variable amount of data. Up to this point we havent dealt with anything other than single network. Technically two hosts could communicate with each other just fine using only MAC addresses and the Ethernet protocol. However this never happens. Instead the actual locating and delivery of data is facilitated by IP at layer 3 is an international community that keeps the world of the Internet Protocol running smoothly. The IETF governs standards for IP applications IPrelated protocols and related areas. The standards are defined using documents called Request for Comments or RFCs. The IETF is here to stay and the RFCs they write are your friends. Get The Address Resolution Protocol is layer 2 protocol used to map MAC addresses to IP addresses. All hosts on network are located by their IP address but NICs do not have IP addresses they have MAC addresses. ARP is the protocol used to associate the IP address to MAC address. When host wants to send packet to another host say IP address 10.5.5.1 on its local area network it first sends out an ARP packet. The ARP packet contains simple question What is the MAC address corresponding responds with an ARP packet containing its MAC address. Up until now all discussion has centered around Ethernet which is broadcast network type. Hence Ethernet has the ability to do ARP broadcasts to find out what hosts are on the network. broadcast network has other capabilities. For example host can send packet to all other hosts within the LAN segment or the host can target subset of all other hosts on the LAN . Other network types exist such as pointtopoint. pointtopoint network as the name suggests consists of only two hosts one at each end of the network. Broadcasting is not possible or required because there is only one other host within the network. The zOS host supports pointtopoint interfaces in various contexts. In addition pointtomultipoint type network is also possible particularly in sysplex . Historically LAN segment is connected using an Ethernet hub. The hub is layer 1 device only and thus it will repeat any ARP packets to all hosts connected to the hub. Any network devices at the higher layers will not forward To summarize at layer 3 an IP address is used to locate every host on the network. But hosts are located at layer 2 by MAC address not an IP address. Consequently layer 3 uses ARP broadcasts to solicit mapping of IP addresses to MAC addresses. However we have distinctly stated that the scope of an ARP address is within LAN segment itselfunless of course network switch or bridge is available to extend the scope of the segment. For the sake of discussion the term wide area network will be used to denote group of two or more local area networks connected at layer 3. WAN would include the link that is used for the interconnection of local area networks. Other definitions of area networks exist that do not have bearing on this text. Although LAN segment represents physically contiguous network with ARP broadcast capabilities it might also be desirable to divide such LAN into one or more logical LANs. Such LAN is called virtual LAN . VLAN is implemented as an extension to the 802.3 protocol and is defined as 802.1Q. Note There are intelligent hubs that operate at layer data relating to each LAN segment connected to the switch. There are also devices called bridges that operate at layer 1 and can seamlessly extend LAN segments. When using 802.1Q protocol frames leaving host are tagged with VLAN ID. The VLAN ID causes the packet to be recognized only by other hosts that have adapters activated to recognize that same VLAN ID. The result is that more than one VLAN can exist completely independent of each other on single physical segment. The advantage to this is that congestion on LAN segment can be reduced and security can be improved by isolation of the traffic. In addition the VLAN ID can span multiple switches in corporation. Thus VLAN ID can differentiate traffic across network. representation of some of the topics discussed earlier. This drawing is not intended to be representative of good network design. Its purpose is to continue the discussion of how layer 3 functions in an IP network. in yellow represents simple network comprised of several workstations. The yellow area represents the scope of an ARP broadcast the three workstations Introduction to the New Mainframe Networking and one of the NICs on the router are all considered to be on single LAN The green shading on the lower half of the drawing represents separate LAN segment. This LAN has been extended using network switch to allow separate LAN segments to function as one large contiguous segment. In order for the router to be part of the LAN segment it will be using second NIC. So how does computer send an IP packet to the zOS host on different LAN segment at the lower corner of the drawing The answer is that it must use an IP route to get there. The IP route is the direction sign of internets and hence of the Internet itself. An IP route consists of simply mapping of destination IP address or network to next hop and interface. The routes are collected into routing table. Each time an IP packet needs to be sent from host the routing table is consulted for information about where next to send the packet. To illustrate this Example 22 In Example 22 the first line tells us that to reach hosts on the 10.1.1.0 network there is no need to use router because the hosts for that network are directly attached to the same network as this host. The second line says that to reach any other host send the packet to the router at 10.1.1.1. This is referred to as default route and the assumption here is that once the packet reaches 10.1.1.1 that host will know which hop is next. Network masks How did we arrive at 10.1.1.024 denoting network The 24 refers to the number of bits in the network ID portion which corresponds to the first three octets. However to illustrate network masking properly more subtle example should be used. How about network ID such as 201.2.10.192 with 26bit mask value An retains all network or subnetwork bits . But what about that last octet where First we should confirm what our network ID really is Now weve confirmed that the network ID in our last octet is 11000000. This means our first IP address is 11000001 or 193 in in the last octet. We cannot have host address of all 1s since this is reserved for broadcast so 11111111 or 255 is excluded. The eligible IP address range is from 201.2.10.193 to 201.2.10.254. Introduction to the New Mainframe Networking green areas are no longer labelled as ARP broadcast areas. Instead they are referred to as IP networks. The yellow area has been given network ID of bits or 255.255.255.0. Workstation has an IP address of 10.1.1.4 which is within the yellow areas network ID. The zOS host is within network ID 10.1.2.0 with IP address 10.1.2.5. But how can we simply change an ARP broadcast area into an IP network The answer is in how the transition from layer 3 addressing to layer 2 addressing is performed. Remember the 10.1.1.024 route from workstation Ws routing table This was referred to as direct route. It is route that informs the IP layer that IP addresses for this network ID are to be found directly attached to the LAN segment. It is here that transition occurs when the routing table indicates packet should be sent to the directlyattached network this is an indication that an ARP broadcast should be sent out to determine the MAC address of the destination host. The result is that given network ID must not span more than one ARP area. Once host is reached that has route indicating the destination is directly attached an ARP broadcast will been sent out to request the MAC address of the destination host. ARP responses are cached locally so that an ARP request does not need to flow every time packet is sent to destination host. There are two different methods for populating routing table with routes using static routing or dynamic routing. Static routes is the term applied to any routes in routing table that have been requirements between networks are very simple. Routing tables such as connectivity requirements. Static routing has limitations when networks become larger. The number of routes can become difficult to manage. Also networks can change routers can become unavailable causing certain routes to be unusable. At the same time new routes can become available and these must manually be added to the routing table before they can be utilized. To overcome such limitations dynamic routing can be used. Dynamic routing involves the usage of routing protocols to communicate dynamic routing protocols Routing Information Protocol and Open Routing Information Protocol server running on the host. This daemon communicates with other hosts running an RIP daemon on the network. Information about the routing tables of each daemon host is exchanged periodically. Routing tables are built based upon information about the network supplied from other routers. The advantage here is that if network changes for whatever reason the exchange of information among routers allows this change to be communicated. The drawback of RIP is that the routing tables become large very quickly. large network can require huge routing tables. And RIP can be slow in recognizing changes in the network. The recognition of changes in network by dynamic routing protocol is referred to as convergence. RIPv1 is seldom used today. RIPv2 has all the functions of RIPv1 plus some improvements. RIPv2 provides mechanism for authentication of other routers. It provides the ability to multicast which can reduce network traffic when compared to network broadcasting. RIPv2 also supports variable length network Most zOS networks are moving away from using RIPv2 and are instead utilizing OSPF effectively accomplishes the same thing as RIP does it populates the routing table of host with routes. It essentially has all the capabilities of RIPv2. However OSPF is more scalable and configurable than RIP. In addition OSPF supports the organization of networks into areas. These areas can be used to limit the amount of information that is required to be moved around an entire internet yet there is no compromise of connectivity. From network route management perspective OSPF differs significantly from RIP. OSPF exchanges information on the state of links instead of State Advertisement. Consequently network convergence is fast and consistent. In addition hosts participating in OSPF routing are assigned specific roles . The protocol itself is stateoriented protocol. Interfaces and neighboring routers are always classified as being in particular state. Events on the network will cause these states to change in predetermined way providing predictability and control to the OSPF routers on the network. The routing daemon on zOS is capable of handling both OSPF and RIP interfaces concurrently. OSPF is one of the most widely implemented routing protocols. It is defined in ICMP is actually user of the IP protocolin other words ICMP messages must be encapsulated within IP packets. However ICMP is implemented as part of the IP layer. So ICMP processing can be viewed as occurring parallel to or as part ICMP is probably most well known as the message protocol used for the ping command. ping command sends an ICMP echo request to the target host. The target host responds with an echo reply. The ping command is losing some of its usefulness in todays more securityconscious networks many routers disable responses to echo requests. with the delivery of packets. For example ICMP can inform hosts about When packet that is too large for network to handle arrives at router the router will break it into smaller packets . If the packet has flag stipulating the packet cannot be fragmented then the router will discard packet back to the original sender. The time exceeded after packet has traversed too many For example when an ARP broadcast fails to elicit When host believes better route exists. Note that this is not desirable feature of ICMP and should be disabled under almost all circumstances. Routing protocols do better job of determining the best ICMP is defined in RFC 792. There are numerous other protocols present at the network layer. All of them are related to routing or addressing of data in some fashion or another and usually they are more specialized with respect to their function or purpose. Unlike layer 3 there are really only two protocols of note found in layer 4 Transmission Control Protocol and User Datagram Protocol . Returning to our postal mail protocol analogy layer 3 is preoccupied with ensuring that the address on the envelope could be located and that the envelope could ultimately be delivered. Layer 4 shifts the focus to the process of the actual delivery of the envelope. The standard way of ensuring the delivery of postal mail is to register the mail with the mail carrier. When the mail is received at the other end an acknowledgement in the form of signature is required. This signature is the senders assurance that the mail has been received successfully at the remote TCP is the registered mail protocol of internets. It is used when host requires assurance that the remote end has actually received the data it sends. But instead of requesting signature at the remote end TCP requires an acknowledgement be returned. To get into details on how this is done well begin by having look at Example 23 on Options Padding is counter used to keep track of every byte sent outward by host. If TCP packet contains 1400 bytes of data then the sequence number will be increased by 1400 after the packet is transmitted. At offset 64 is the acknowledgement number. This number is counter to keep track of every byte that has been received. If 1000 bytes are received by host it increases the acknowledgement number by 1000 when it sends out packet in As mentioned receiving data from remote host causes the acknowledgement number at the local host to be increased by the number of bytes received. When acknowledgement number and it will also turn on the ACK flag to Note The local hosts sequence number usually matches the remote hosts acknowledgement number and the local hosts acknowledgement number usually matches the remote hosts sequence number. Because transmitted packet might not reach its destination for whatever reason and transmitted packet might take some time to cross the network to its destination the difference between senders sequence number and the remote hosts acknowledgement number represents any outstanding unacknowledged data. indicate to the other end that it is acknowledging the receipt of data. This is the nearest thing to signature that TCP can do. The result is that TCP is capable of ensuring reliable delivery of data. and the destination port number. TCPcapable host and particularly zOS is capable of running more than one TCP application. For example Web server and FTP server might both be running on the same host using the same IP address. After packet has been delivered how does the host know which application should receive the packet The answer is by using the port number. Port numbers are TCPs method of knowing which application should receive packet. Returning to our postal mail envelope we did not mention that the address placed onto that envelope included an apartment number. Sure the IP address gets us to the correct host but the port number tells us which application In order to facilitate communication many applications are assigned specific ports. Such ports are called wellknown ports. For example Web server normally listens on port 80. An FTP server normally listens on port 21. TCP is always referred to as connectionoriented protocol. What this entails is that prior to any communication occurring between two endpoints connection must be established. During the communications the state of the connection is continually tracked. And when the connection is no longer needed the connection must be ended. Because TCP forms the backbone of so much activity over an internet An application is awaiting an connection request and acknowledgement have been sent in response. Awaiting an acknowledgement of the Introduction to the New Mainframe Networking connection request sent out as response to the original All connection requests and acknowledgements have been sent and received. Data can move freely over the acknowledgement has been received. An end connection request has been acknowledged by the remote host but no corresponding end connection has been received from the remote host. An end connection request has been sent out and an end An end connection request was received and acknowledged but no corresponding end connection has been sent out yet. Waiting reasonable amount of time to ensure that the been received at the remote end. connection in response to having received an end of connection request. When postal mail is not registered there is small chance that the letter we send might never be seen again. The letter is addressed and sent and no more effort is spent on it. It is matter of the mail carrier doing its job correctly. number is wrapped around whatever data needs to be sent and the packet is handed over to the IP layer. As long as the lower layers do their jobs correctly the remote end should receive the datagram as expected. There are no acknowledgement counters and no connection states. The term socket in TCP or UDP context fully describes the endpoint of connection. The socket is consequently combination of an IP address port number and the protocol being used. Sitting above layer 4 are the applications. Applications can use either TCP or UDP to communicate. Because of its inherent reliability TCP tends to be used more often. Examples of applications running on zOS using TCP include sendmail Web servers FTP and telnet. Applications using UDP on zOS are Traceroute Enterprise Extender of an organizations security policies. Any large organization has formal document explaining the classification of company data as well as the classification of company networks. firewall controls and limits access between networks of different security classifications and sometimes even within network that is already protected by firewall. Firewalls can filter based upon port numbers and IP addresses . Data travelling into this firewall would likewise be secured. Tip Enterprise Extender is really SNA encapsulated in UDP datagram. At first it appears odd that UDP and not TCP would be chosen to carry traffic as important as SNA traffic. However the flow control and reliability capabilities of TCP are already built into SNA so TCP is not required. Introduction to the New Mainframe Networking through RFC 2409 in the form of virtual private network . We discuss firewall that acts as VPN endpoint and allows data to continue on through the secure network to destination hosts is often called security gateway. The term gateway is traditionally used to describe host that connects networks using different protocols. network layer. An alternative form of security for data on the network is the Secure Sockets Layer . SSL is implemented at the transport layer. The new standard for SSL is called Transport Layer Security and is also discussed further host such as zOS includes intrusion detection services that allow the host to detect and react to malicious activities coming from the network. Some IDS is built into TCPIP on zOS itself while other aspects of IDS are configurable. IDS can be an integral part of host availability. Classic SNA based on subarea nodes is the original networking architecture used by mainframe computers. However with the popularity and growth of TCPIP SNA is changing from being true network architecture to being what could be termed an application and application access architecture. In other words there are many applications that still need to communicate in SNA but the required SNA protocols are carried over the network by IP. New SNA is SNAAPPN . This addition to SNA is based on APPN Peer nodes and is somewhat more dynamic and less deterministic than SNASubarea and usually requires considerably less definition. It is possible to have SNAAPPN traffic travel over an Ethernet LAN network using the IEEE 802.2 frame format. The 802.2 frame is referred to as logical link control . Similar to an 802.3 frame the 802.2 frame contains something called service access point or SAP. The SAP is used to identify the SNA resource at the The first step in discussing network technology is to ensure that you understand the terms and acronyms. Starting from the physical layer progressing to the data link layer and moving up through the network layer on to the transport layer there are large number of terms to be understood. These terms need to be clearly understood when zOS systems programmers communicate with network administrators in an organization. 1. How many different types of connectors are possible on fiber optic cable using an OSA card 2. Using the online zOS Internet Library locate the zOS IP Configuration does this statement do 3. Which protocol has faster convergence RIP or OSPF 4. If an FTP daemon is in listen state on host 10.1.1.4 what are the other two values that would normally describe the socket 5. Find an RFC repository on the Web and look up RFC 2401. What does the operates in host or security gateway environment 1. Is there 10 Gbps OSA card using an RJ45 type connector If not why 2. What would happen if second computer using the same IP address as an existing host on the same LAN segment were to be connected TCP RFC to determine what purpose these flags serve. 5. It is unlikely that the zOS host would be used as secure gateway. Also zOS host should normally be protected by networkbased IDS and firewalls. 1. Try issuing TSO NETSTAT ARP ALL command from TSO session. What do you learn from the results 2. Try issuing an ARP query from your workstation . Can you locate the cached mappings for the 3. Try issuing TSO NETSTAT ROUTE command to determine the default route on the zOS host. 4. What sort of options are available for the TSO PING command Try issuing running what connections are established and what state the connections Understand the different types of hardware connections to network. Understand the protocols running over these different connections. Understand the configuration options and advantages of OSAExpress. Explain how to associate protocols with the OSAExpress. The design intention of the mainframe and most of its evolution is for the mainframe to be highly available transaction processing server. Obviously central processing capabilities are evolving to handle more and more transactions. However in order to be an effective transaction processing server there must be proportional capability of moving data in and out of the central processor complex rapidly . The result is that the IO options capabilities and configuration choices of an IBM mainframe are varied complex and very performance Mainframe computers are probably unique in that they require Hardware Management Console or HMC. The HMC is separate interface to the central processor complex that is used for hardware configuration operations. It also provides an interface to the zOS system console. The heart of moving data into and out of mainframe host is the channel subsystem or CSS. The CSS is from central processor standpoint independent of the processors of the mainframe host itself. This means that inputoutput within mainframe host can be done asynchronously. When an IO operation is required the CSS is passed the request from the main processor. While awaiting completion of an IO request the main processor is able to continue processing other work. This is critical requirement in system designed to handle massive numbers of concurrent transactions. All LPARs within the central processor complex can make use of the channel Note The processors that run the channel subsystem are called the system assist processors . There can be more than one SAP running the channel subsystem. Reminder logical partition or LPAR is an independent subset of CPC. Operating systems and applications running within an LPAR cannot distinguish the LPAR resources from those of dedicated CPC. Effectively an LPAR is seamless division of CPC into multiple simulated CPCs. simplified example of how the channel subsystem functionally resides within the large box represents an entire mainframe processor . The asynchronous IO is handled within the channel subsystem by channel program. Each LPAR ultimately communicates using subchannel. In addition the channel subsystem can be used to communicate between LPARs. Each CPC has channel subsystem. Its role is to control communication of internal and external channels to control units and devices. The channels permit transfer of data between main storage and IO devices or other servers under the control of channel program. Some of the other Within the central processor complex are logical partitions that divide the CPC into independent machines that can run any mainframe architecture system control program . Partitions have access to CPC memory and subchannels. The subchannel represents an IO device. This is the mechanism by which an IO request is passed to the channel subsystem itself. The channel represented by channel path ID or CHPID represents the actual communication path. CHPID is the handle by which communication between the CPC and an external device is facilitated. CHPID must be unique since it denotes unique path of communication for the CPC. The maximum number of allowable CHPIDs within channel subsystem is 256. Channels can be shared between LPARs. Historically CHPID had correspondence with real physical channel connected to the CPC. However for performance and enhanced capabilities CHPID now maps to physical CHPID using simple mapping table and CHPID mapping tool or CMT. One of the main tasks of the channel subsystem is to communicate with storage devices such as tape and direct access storage devices . This is Although this is significant aspect of the channel subsystem this will not be discussed within this text since it is not network device. To facilitate the usage of more CHPIDs the mainframe architecture supports logical channel subsystem or LCSS. The LCSS is functionally identical to the channel subsystem but up to four LCSSs can be defined within central processor complex. CHPIDs are unique within the LCSS only consequently the As mentioned CHPID no longer directly corresponds to hardware channel and CHPID numbers may be arbitrarily assigned. hardware channel is now identified by physical channel identifier or PCHID. gives us an opportunity to go little deeper into how the IO subsystem functions. Two logical channel subsystems are defined . Each LCSS has three logical partitions with their associated MIF identifiers. An explanation of Note The IO configuration of the central processor complex is defined in data set called the IO Configuration Data Set or IOCDS. The IO configuration is normally done using tool called the Hardware Configuration Dialog or HCD. HCD also creates data set called an IO definition file or IODF. The IODF is read by the zOS operating system. statement syntax called IOCP statements. IOCP stands for IO Configuration Program . The IOCP creates an IO configuration data set . IOCP statements can be migrated to IODF statements using HCD. This name is userdefined through HCD or IOCP and is the partition name in the RESOURCE statement in the configuration definitions. The names must be unique across all logical channel subsystems defined for the The logical partition identifier is number in the range from 00 to 3F. It is assigned by the user on the image profile through the support element or the Hardware Management Console . Note The logical partition identifier is unique within the central processor The MIF number is used to facilitate channel sharing among LPARs. The MIF ID is number that is defined through Hardware Configuration Dialog . It is number that is specified in the RESOURCE statement in the configuration definitions. It is in the range 1 to and is unique within logical channel subsystem but it is not unique within the z990. Multiple logical channel subsystems may specify CHPID logical channel subsystem. The CHPID number range is still from 00 to FF and must be unique within logical channel subsystem. The CU provides the logical capability necessary to operate and control an IO device and it adapts the characteristics of each IO device to the standard form of control provided by the channel. control unit may be housed separately or it may be physically and logically been logically divided into logical control units or LCUs. IO devices are used to provide external storage to communicate between data processing systems and to communicate between data processing system and the external world. director is an IO interface providing multiple connectivity capabilities between the channels on the mainframe and the control units of the devices. This topic describes the different channels on mainframe and how they attach to the network. How the mainframe connects to the network depends on the There are effectively three ways that network traffic can travel between an external network and zOS host through channelcommand word channel coupling channel or QDIO channel. The CCW is the original IO operation used for communications with the channel subsystem. The CCW contains channel command such as read write or control along with the data address of the data area involved. The data is of the IO back to the issuing application. When channel communicates with an application in an asynchronous fashion it is referred to as channel interrupt. CCWbased channels include parallel ESCON and FICON channels. CCW can also be used to talk to an OSA card uses two cables called bus and tag. Of course it is copper media only. bus cable carries information and tag cable indicates the meaning of the information on the bus cable. Devices are daisychained off of each other to form string of devices. The parallel IO interface is the communication channel path between physical channel on mainframe and an IO control unit . The interface was designed for past mainframes and works compatibly with older and current mainframe architectures. The maximum data rate of parallel channel is 4.5 MBps to control bit transmission and reception over the physical medium. the top layer an application makes an IO request using macro or supervisor call . This in turn causes START SUBCHANNEL which moves the IO request to the CSS. The SSCH includes subsystem identifier and operationrequest block as its operand for execution of the channel Reminder Halfduplex for ESCON is effectively requestresponse format. Bidirectional communications are possible but the synchronization of the ESCON IO protocol limits communications to halfduplex. ESCON has somewhat different topology for control unit and channel attachment compared to parallel channel. ESCON control units can be Directly to an ESCON channel which is called pointtopoint or Dynamically switched through device called the ESCON Director which is The maximum channel data rate of an ESCON channel is 17 MBps and the maximum unrepeated distance is 3 kilometers. Even though ESCON channels are fiberbased the next generation of ESCON was simply called FICON for fiber connection. The advent of FICON allowed concurrent sharing of the fiber channel . The maximum channel data rate of 2 Gbps At this stage we have discussed in detail channel connection types but what is being attached at the other end of the channel As mentioned earlier it is often control unit with DASDs attached. In networking context however the device attached at the other end can be router. In order to have zOS talk to channelattached router protocol above the channel protocol must be agreed upon. The protocol used for this is called CLAW. CLAW stands for Common Link Access to Workstation. CLAW can be used to talk to either CISCO CIP host or an AIX pSeries host. CLAWconnected hosts are steadily becoming less common in The parallel channel can of course be used to connect two LPARs directly or even two separate central processor complexes. The other LPAR could be running zOS or it could be running zVM with multiple Linux images within Communication among LPARs can be facilitated by Coupling Facility links. Coupling Facility links are used to support the crosssystem Coupling Facility or XCF. The XCF component in turn can be used to support the IP protocol. There are two ways to define IP connectivity over Coupling Facility link Static XCF links can be defined to TCPIP on zOS using hardcoded statements. Dynamic XCF links can be automatically generated any time TCPIP becomes active within sysplex. XCF communications can flow over copper or fiber media. Over fiber media the maximum data rate is 200 MBps. Additionally coupling channels are not CCWbased channels. The Open Systems Adapter is actually network controller that you can install in mainframe IO cage. The adapter integrates several hardware features and supports many networking transport protocols. The OSA card is the strategic communications device for the mainframe architecture. It has several key features that distinguish it from CCWbased communications. Effectively the OSA integrates the control unit and device into the same hardware. It does so by placing it on single card that directly connects to the The OSA2 card is no longer available but significant number of installations still utilize them. The OSA2 card is of interest here because it could only run using CCWbased operations. OSAExpress and OSAExpress2 cards utilized much faster method of direct access called Queued Direct IO . In addition OSAExpress provides significant enhancements over the OSA2 in function connectivity bandwidth data throughput network availability reliability and recovery. Meanwhile the OSAExpress2 card represents the latest and available on mainframe servers. Note zOS hosts interconnected using XCF are said to be tightly coupled hosts. In zOS group of tightly coupled hosts are referred to as sysplex. Note that the maximum speed is 10 Gbps data rate. In order to support such large potential for data movement as mentioned the OSAExpress and OSAExpress2 cards support mode of operation called Queued Direct IO or QDIO. There are several different channel types supported by an OSAExpress2 Network Control Program under Communication Contoller for Only the OSAExpress2 card supports the OSC and OSN channel types. The following text uses the term OSAExpress to denote function that both OSAExpress and OSAExpress2 can support. QDIO mode is referred to as OSD because the CHPID type coded in the IOCDS mode compared with nonQDIO mode . Consequently IO interruptions and IO pathlengths are minimized. 20 improvement in performance versus nonQDIO mode The reduction of system assist processor utilization How is this all accomplished Instead of attaching an OSAExpress card using channel and hence utilizing channel or IO program combined with CCW operations the OSAExpress card attaches using an STI bus. STI stands for SelfTimed Interface. The OSAExpress card is still connected within the IO cage but the STI bus is directly connected to the memory bus of the CPC. What is used instead of CCW operation to signify that IO needs to be completed With an OSAExpress card running in QDIO mode IO operations are effected using signal adapter instruction or SIGA. The SIGA is still processed by the SAP similar to the way CCW is processed by the SAP. However the SIGA effectively passes pointer to the data because the data already occupies internal storage. This bus itself has data speed of up to 2.7 GBps which is plenty of bandwidth to handle 10 Gbps Ethernet LAN speed. QDIO is highly efficient data transfer architecture which dramatically improves data transfer speed and efficiency for TCPIP traffic. Access to an OSAExpress port can be shared among the system images that are running in the LPARs to which the channel path is defined to be shared. Also access to port can be shared concurrently among TCPIP stacks in the same LPAR in different LPARs or in different logical channel subsystems. When port sharing an OSAExpress port operating in QDIO mode has the ability to send and receive IP traffic between LPARs without sending the IP packets out to the LAN and then back to the destination LPAR. DMA allows data to move directly from the OSAExpress microprocessor to the host memory. This bypasses three layers of processing that are required when using ESCON and OSA2 features dramatically improving throughput. Priority queuing sorts outgoing IP message traffic field. This priority is used to reflect the business priorities assigned to the application user ID time of day and other characteristics. Enhanced IP network availability is service of the QDIO architecture. When TCPIP is started in QDIO mode it downloads all the home IP addresses in the stack and stores them in the OSAExpress feature. The OSAExpress feature port then responds to ARP requests for its own IP address as well as for other IP addresses active in the TCPIP stack . The Address Resolution Protocol Takeover provides the capability of switching OSAExpress port operations from one OSAExpress to another OSAExpress running in the same mode. When TCPIP is started in QDIO mode it downloads all the home IP addresses in the stack and stores them in each OSAExpress feature to which it has connection. This is service of QDIO architecture and occurs automatically only for OSD channels. If an OSAExpress feature fails while there is backup OSAExpress available on the same network ID TCPIP informs the backup OSA of which IP addresses to take over and the network connection is maintained. The takeover is effected by something called gratuitous ARP. gratuitous ARP is an unsolicited ARP response. All hosts on the LAN Reminder VLAN frame looks almost the same as an Ethernet frame. The difference is that VLAN frame has extra field containing number that identifies the VLAN. This number is called VLAN tag. new MAC address for the backup OSA. When the CHPID type is set to OSE the OSAExpress card is functioning in nonQDIO mode. An OSE channel type does not support the many of the features of an OSAExpress running QDIO mode. For example direct memory access and enhanced IP availability are only available with channel type of So why run nonQDIO mode at all In nonQDIO mode an OSAExpress card can support SNA and APPN traffic . In addition in OSE mode an OSAExpress card can run IP and SNAAPPN traffic concurrently. Some manual configuration is required using program called the OSA Support Facility or OSASF. The OSC CHPID type is available on newer mainframes running an OSAExpress2 card or an OSAExpress card with the Gigabit Ethernet feature. The OSC is special channel type that eliminates the need for an external console controller. The end effect is that access to the HMC and to the zOS system console is made easier. The OSC CHPID can also be used to connect TN3270 sessions . Because the OSC mode is not general usage mode of operation no further discussion of it is undertaken here. The OSN type is only available with OSAExpress2 and requires z9 mainframe or later model. The primary intention of this type is to free organizations from the constraints of obsolete hardware device types 3745 and 3746. The 374x device types as they are called are no longer manufactured or sold by IBM. 374x host is required to run the Network Control Program . NCP is significant functional component of subarea type SNA networks . CDLC cannot be used over an OSD or OSE channel type and even with channel type OSN it can only communicate to other LPARs within the CPC. Historically 374x devices were often connected to parallel or ESCON channels which support CDLC. Where then will the NCP run On software program called Communications Controller for Linux . And as mentioned both LPARs must be within the same CPC since the data flows do not enter the network. In many cases CCL provides the easiest way to migrate from older SNAbased network controllers to modern network devices. The CCL functional capabilities provide alternatives where either no viable solution existed before or where prior alternatives posed significant implementation challenges. type of OSE is used to communicate with CCL using LLC 802.2. With the OSN channel type the communication would not flow out to the switch. Instead CDLC would flow from LPAR to LPAR within the OSAExpress2 card itself. Clarification XRF is the Extended Recover Facility. XRF is feature of SNAAPPN environments that greatly improves the recoverability of an application or host failure. SNI stands for SNA Network Interconnect. SNI is used when connecting two separate subarea SNA networks to each other. Mainframe HiperSockets is technology that provides highspeed TCPIP connectivity within central processor complex. It eliminates the need for any physical cabling or external networking connection between servers running in different LPARs. The communication is through the system memory of the processor so servers The HiperSockets implementation is based on the OSAExpress Queued Direct IO protocol hence HiperSockets is also called internal QDIO or IQDIO. The microcode emulates the link control layer of an OSAExpress QDIO zOS is not the only operating system running on mainframe host that can take advantage of HiperSockets. Other operating systems include zVM and Linux. This HiperSockets channel path exclusively serves three Linux systems Note zVM is capable of functioning very similar to an LPAR. Instead of doing the work at the hardware level zVM creates separate virtual environments using software. It is highly efficient and it is sometimes used to run large numbers of separate and independent Linux hosts all within single LPAR. Connected to this HiperSockets channel path are all servers in the mainframe The multiple Linux servers running under zVM in LPAR1 All zOS servers in sysplex for nonsysplex traffic All zOS servers in sysplex for nonsysplex traffic This is the connection used by sysplex to transport TCPIP This is the connection used by sysplex to transport TCPIP userdata traffic among the three sysplex LPARs. The CHPID type used for HiperSockets connection is called IQD. The connections to the central processor complex are made in physical area of the processor frame called an IO cage. Within the cage OSA cards and memory modules are physically attached to the central processor complex. Parallel FICON and ESCON connections are all made within the cage as well using an adapter card. numerous small black fiber channel connectors are ESCON cards. The next card to the right is an OSA card with two RJ45 connectors. The cards connected to the large black cables are intersystem coupling cards for coupling links. To Note So how fast is IQDIO How about transferring data between zOS and Linux using FTP First multiple FTP connections must be run in order to get close to utilizing the bandwidth of the interface. When transferring data between zOS LPAR and Linux LPAR over IQDIO 50 FTP connections produced total throughput of 600 MBps. The limitation here however is not zOS. In order to increase the throughput the Linux limitation needs to be overcome. The test was expanded to FTP connections between single zOS LPAR and 4 separate Linux LPARs. With 120 total FTP connections the IQDIO throughput Introduction to the New Mainframe Networking the far right of the image is another ESCON card with several fiber optic cables connected to it. Network connections can be made in several different fashions. The mainframe originally relied upon the channel subsystem to offload IO processing to channel programs. DASD is still accessed using ESCON channels but for networking connectivity OSAExpress cards offer better performance and availability. The OSAExpress and OSAExpress2 cards provide redundancy capability as well as throughput improvements when running in QDIO mode. QDIO mode allows direct access to central memory. QDIO mode can be emulated within CPC by allowing memory to memory data transfer among LPARs running zVM 1. What is the primary advantage of the channel subsystem 2. Which modes can you use for an OSAExpress2 3. Explain how to associate protocols with the OSAExpress. 4. What is logical channel subsystem 5. What is gratuitous ARP What is the effect of such an ARP Introduction to the New Mainframe Networking 2. When an OSAExpress card is running in QDIO mode it supports priority queueing via the Type of Service field. Where would the Type of Service field be defined or documented 3. What are some of the implications of using DMA for QDIO or IQDIO devices 1. Try issuing TSO NETSTAT DEV command. What type of physical links are MDEV where nnnn is device number provided by the instructor. 3. Try executing an FTP from TSO to the host provided by the instructor. Get the file also provided by the instructor. What sort of throughput did you experience . 4. Ask the instructor to take you for tour of facility with some real mainframe Sample configuration used to demonstrate the various course concepts. Components of this mainframe networking components and environment that many organizations List the components of typical mainframe networking infrastructure. Explain why organizations duplicate key components in the network. Introduction to the New Mainframe Networking require their networks to be reliable always available and fast. They invest great amount of time and money creating an IT infrastructure that supports these goals. The extent to which an organization implements solution depends very much on availability and performance goals balanced against the cost of the especially for availability. center. Processing is divided up physically by central processor complexes and logically by logical partitions as described here central processor complex is physical collection of hardware that consists of main storage one or more central processors timers and logical partition is subset of single physical system that contains resources and which operates as an independent system. In reality there would be more CPCs and LPARs. Most large organizations recovery or site swap in which processing is moved to this duplicate site. The disks would be also mirrored between sites. are required that might include resetting the server . There are also external influences that can cause an outage such as changes to the data center infrastructure or router changes. In our sample configuration in order to allow business processes to continue during downtime the ZOS Company has two mainframe CPCs. When one CPC is down the second CPC can continue to run the business. For the purposes of this example only three LPARs per CPC are shown. In reality there may be many more. minimum configuration might include three LPARs per CPC . The primary Production1 LPAR would normally run on different CPC than that of the secondary or backup Production2 LPAR again to allow for flexibility or outages. The Primary Production1 LPAR might be the normal network owner but the Production2 LPAR should also be able to take over this function along The DevelopmentQA1 LPAR might also need backup this is insurance in case application programmers run tests on new applications that affect the LPAR. The development LPAR is used to develop new software. Most large organizations have inhouse programmers responsible for creating and maintaining applications that are specific to the organizations needs. These applications are created tested and maintained on the development LPAR. test LPAR is sometimes also referred to as system programming LPAR. test LPAR generally provides the basis for software delivery and early testing of changes and new functions. It is also where maintenance would be applied and tested prior to the fixes being implemented in production. Ideally the test LPAR should be as similar to the production LPAR as possible. This might then include second systems programming LPAR on each CPC providing an extra level of confidence when migrating changes through the system . The LPARs would also have network connections to each other by way of interCPC and intraCPC hardware and software features. Production LPARs are critical to maintaining an organizations viability. normally isolated logically from the test and development LPARs. This is achieved by using Parallel Sysplex usually just called sysplex. sysplex is clustering technique involving software and physical components. This technique helps with availability and workload balancing and protects environments from each other. There would be production sysplex for the production LPARs and test or development sysplex for the remaining systems. Some organizations may have many sysplex systems. With the ability to define multiple independent sysplexes even LPARs within single CPC can be isolated logically by participating in separate sysplexes. components are in place to allow for scheduled and unscheduled outages and provide the availability that zOS customers expect. under the zOS operating system. It is normal to run only one instance of TCPIP per LPAR but some situations may dictate additional TCPIP stacks on an LPAR . Organizations buy mainframes for many reasons but they generally fall into one Customer Information Control System. Provides transaction management functions and connectivity to application programs. Runs as an address Coupling Facility. Enables sharing of data between multiple LPARs using Central processor complex. Physical collection of hardware and main storage and one or more central processors. Database 2. Relational Database product used on most mainframe customer sites. Runs as an address space in zOS. Open Systems Adapter. High speed integrated cards used for network Open Shortest Path First. Routing protocol used to communicate between router and mainframe TCPIP OMPROUTE application. Time Sharing Option. An element of zOS that enables users to create an interactive session with the zOS system. TSO provides singleuser logon capability and basic command prompt interface to zOS. Similar to PC command prompt window. Virtual Telecommunications Access Method. The original SNA networking protocol for mainframes. Provides services to TCPIP as well. Runs as an Key aspects that you as an mainframe network administrator should look for in High quality hardware and software components. Software put through rigorous compatibility testing. takeover and recovery. VIPA provide movability and availability Network resource access control. Cryptographic processors network security protocol Dynamic nondisruptive expansion. An example of an IBM internal test to show scalability Compatibility of older applications device types and Each component should be analyzed for what would happen if this component failed and it does not have Is there more than one network path that provides an alternative route to target should component fail Can the alternative component handle the load and performance on its own should failure occur on the How transparent would component failure appear to client of zOS The aim should be that any failure results in nondisruptive dynamic change that has minimal impact on the client. This is not always possible but should be strived for. What are the clients requirements for securing network This requirement might be within the zOS security teams purview but you might be required to implement TCPIP or VTAM features to meet the security policy. There are many more criteria that could be applied for example is the solution scalable and manageable and will it meet the service level agreements The extent to which organizations will go in order to ensure high availability zOS networking environment will vary. Our sample network is designed with focus on maximum availability. If component fails then another component should be able to continue in its place. There are hardware and software components that contribute to availability as Reminder An SLA is formal document between service provider such as the organization running the mainframe and its customer the recipient of the service. Customers of an SLA may be internal to the organization. Switches or routers Two network switches sometimes referred to as the core network switches with multiple paths to each other and to the four OSA cards. Each switch would be able to handle the inbound and outbound data on its own to handle scheduled and unscheduled maintenance. The routers will be capable of running the OSPF routing protocol. Two OSA cards per CPC. These cards are defined and shared by both CPC LPARs. In most organizations there OSAE1 and OSAE2 on CPCA. Each LPAR will have unique IP interface address defined within the TCPIP stack for OSAE1 and OSAE2 cards. The ability for the OSA cards to be shared provides flexibility should card or switch fail. OSPF also plays role here as it can detect link state changes and switch to an alternate path poweron reset. Some changes such as new zOS upgrade by implication might involve swapping CPCs The Coupling Facility is also normally duplicated but The environment is running the OSPF routing protocol. Some organizations may use other routing protocols such as RIP. OSPF is link state routing protocol it runs on routers. Under zOS OSPF is implemented by running an application called OMPROUTE. If OSAE1 became unavailable on LPAR1 OSPF detects this and would send all LPAR1 traffic through OSAE2 until OSAE1 is OSPF within mainframe data center environment is normally which only needs to know about adjacent routes. Each LPAR TCPIP stack normally has allocated what is known as virtual IP address . VIPA address is not tied to any the VIPA addresses. example LPAR2 could take over VIPA address on LPAR1 and users would not lose their connections and would not be aware of for organizations. There are many functions within zOS VTAM environment that organizations would have. Organizations run many of their missioncritical applications on the mainframe and system availability is key factor in maintaining an organizations business. To meet this goal organizations duplicate hardware and software components. There is no single solution that fits all but the general principal of hardware and software failover and dynamic takeover is very attractive to organizations. Most organizations will have two or more central processor complexes to allow for scheduled and unscheduled outages. Most organizations will have geographically isolated site to allow for TCPIP VIPAs are not associated with physical interface and they assist in maintaining availability for applications and users. 1. Why would an organization need duplicate components 2. Name three hardwarebased components that should be duplicated to ensure 3. What does the term RAS refer to 4. Name dynamic routing protocol that can contribute to availability. 5. What are the advantages of TCPIP VIPA address 1. Is pair of OSAExpress cards capable of backup going to be as effective as VIPA address What are some advantages and 2. Could development LPAR be used as backup for production LPAR 3. What circumstances might allow an end user to recognized that the IP address they are targeting is VIPA address Access Method 1. Try searching for Parallel Sysplex on the Web site. Can you locate any books of interest 2. Use your favorite Internet search engine to search for SYSPLEX BANK. Browse some of the hits for an idea of what banks are using sysplex and to Explain the usage of the TCPIP profile configuration file. Explain the basics of the FTP and the telnetd server configurations. Discuss the options for the resolver configuration. The single entity that handles and is required for all IPbased communications in zOS environment is the TCPIP daemon itself. The TCPIP daemon implements the IP protocol stack and runs huge number of IP applications to the same specifications as any other operating system might do. Thats the beauty of TCPIP. For example you can run FTP telnet SNMP sendmail NFS HTTP servers rlogin SSH BIND DNS . All these are standard implementations and many of these applications are ported from the industry standard source code. Any of these applications interact with the same applications running on essentially any other operating system. Though TCPIP sounds familiar on zOS there are differences. First of all TCPIP does not really run as daemon. That term was used in the first paragraph because from casual viewpoint the TCPIP started task is the same as daemon. It starts up and stays running for as long as the operator wants it to. It handles service requests of various kinds from within the operating system environment and from the network. So why you may ask cant these Calling the TCPIP started task daemon is little like calling professional quality sound system radio. Its question of configurability and control. daemon is started by simple command line in script file. There are few environment variables that can be specified. By contrast the TCPIP started task is started as program using Job Control Language . The capability of specifying parameters in JCL opens the door to level of environmental control and configurability that cannot be matched in and how it can affect the way TCPIP runs. As an example the zOS environment can support more than one instance of TCPIP started task. The tasks are completely isolated from one another. In fact you need to define links either externally or internally if you want the two TCPIP tasks to have IP connectivity to each other. Without the JCL and the inherent zOS underpinnings such scenario would be impossible. The other reason why we call TCPIP started task is because the zOS environment really does run daemons. In fact we discuss the zOS UNIX System But were not through with the TCPIP started task just yet. TCPIP is layered protocol and networking professionals want to keep that in mind. Consequently the most commonly used term for the TCPIP started task is the TCPIP stack. As of the time of writing IPv6 continues to be the coming standard for IP addresses. However it has not been widely implemented yet. Therefore although this edition of the textbook uses examples and scenarios in an IPv4 context only you should be aware that IPv6 is fully supported on zOS. Having extolled the versatility of JCL it would be unfair not to provide sample for starting the TCPIP stack so refer to Example 51. This JCL sample is simplified and would not execute if submitted to the job entry subsystem but it contains the basic elements pertinent to TCPIP. Most of the missing statements would be parameters unique to an organizations specific The first statement identifies our JCL as started task procedure. The second statement identifies the program to be executed which in this case is Tip The zOS environment includes convention of program and message prefixes that are generally standardized across each application environment. For the TCPIP environment the message and module prefix is always EZn. Hence many messages begin with EZA or EZB and as we can see the module that starts TCPIP itself begins with EZB. Because zOS console can be very busy place being able to recognize such prefix can immediately provide system operator with context for The third statement is DD statement that assigns pointer to the data set member PROFILE found within data set SYS1. PARMLIB. The TCPIP started task automatically searches for file allocated to by DD statement called PROFILE at startup. There are other methods of allowing the TCPIP program to search other the PROFILE DD statement. Other methods can be used if an organization has The TCPIP profile is read by TCPIP when it is started. If change needs to be made to the TCPIP configuration after it has been started TCPIP can be made to reread the profile dynamically . The primary configuration point for the TCPIP stack on zOS is the TCPIP profile data set. It defines operating characteristics of everything that is under the umbrella of the TCPIP stack. Some of the more significant statements found in IP link and address configuration Static routing information It is impractical to discuss all statements found within the TCPIP profile. Further it is not even practical to discuss all parameters within each of the chosen statements. Keep in mind that even the statements discussed here have other parameters available that have been omitted in order to keep the discussion to reasonable scope. All TCPIP profile statements have default values that are TCPIP supports more than dozen different types of device attachments to the network. The most significant one is the OSAExpress adapter discussed in statements involved in configuring any adapter for TCPIP on zOS host DEVICE statement and LINK statement. In Example 52 there are three Because zOS supports so many different hardware devices the device and link statements are unique depending upon the hardware they are defining. However statements consistently follow pattern in which the first parameter is the device or link name and the second parameter provides information with respect to that hardware being defined. For example the first DEVICE statement has been assigned OSAEDEV1 for device name and the device itself is going to be an MPCIPA type device. MPCIPA is the device type used by TCPIP when the hardware is capable of QDIOtype communication. The subsequent LINK statement assigned name of OSAELNK1 further modifies the QDIO definition as IPAQENET. IPAQENET indicates that the link is real OSA device. By real this is to differentiate it from an IQDIO device which would use type of There are few other options here that warrant special notice. The PRIROUTER parameter is necessary if this link is expected to handle packets destined for other networks or subnetworks. With PRIROUTER coded the adapter not only accepts datagrams for IP addresses that are defined to the TCPIP stack but this parameter also causes the adapter to accept datagrams sent to it that are destined for any IP address. It is up to the TCPIP stack itself to then decide what datagram forwarding behavior. The next statement of interest in this sample is the HOME statement. The HOME statements parameters consist of simple IP address and link name pairs. The HOME statement must include an IP address and link name pair for every Note The TCPIP configuration of an OSAExpress device depends upon the prior appropriate hardware configuration through IOCP and VTAM. Introduction to the New Mainframe Networking hardcoded link active in the stack. For example many zOS hosts would have second OSAExpress card defined and its home address would be included in addresses found in the HOME statement. This brings us to the VIRTUAL DEVICE and LINK statements. TCPIP on zOS supports what is known as virtual IP Address or VIPA. Conceptually VIPA is very simple VIPA address is functionally identical to any other IP address defined to the TCPIP stack except that it is not dependent upon any physical networking hardware. VIPA link is defined in software only there is no physical hardware associated with it at all. Why bother Well VIPA address cannot suffer hardware or LAN or link failure. It is always available. In our sample above the user community would not be told to connect to the real links. Instead clients would connect to 201.2.11.9 the VIPA address. If either of the two OSA links failed the VIPA address would continue to be available using the adapter that still remained active. address can be placed within network topology. In LPAR 1 the VIPA link uses the same subnetwork as the OSAExpress links. From network topology perspective the VIPA address functions as an alias of either OSA card. In LPAR 2 the VIPA address is on its own unique subnetwork. router adjacent to this LPAR would consider the VIPA address to be two hops away. The next hop for such router would be to send the packet to address to the VIPAs address is completed internally in the zOS IP stack. The scenario used for LPAR 2 is the preferred scenario. If the VIPA address is placed in the same network as the physical adapters then it must always remain part of TCPIP stack that has an adapter on that physical network. In other words the VIPA becomes confined to remaining adjacent to the subnetwork of the physical network that it belongs to. When the VIPA is in its own unique subnet it can be moved to any TCPIP stack in the network associated with it. Quite often it has at least one OSAExpress card connecting it to the network with one or more specialized links connecting it to other hosts. If the links are redundant with IP address 201.2.11.100. Some redundancy is built into this scenario either adapter can be used to reach the directly attached network and either adapter can be used to reach the default router. It sounds like static routes have some resiliency right Well the difficulty is in the fact that the second adapter is only used if TCPIP on zOS detects failure with the link. For example if OSAELNK1 were to be stopped manually TCPIP would immediately switch to using OSAELNK2. However if problem occurred with one of the adapters and the problem was not detected by the OSAExpress card or by TCPIP itself then the adapter would continue to be used. Its better than nothing of course but routing protocol like OSPF would automatically sense and make accommodations for The other difficulty with static routes occurs with VIPA when it is in different subnetwork from the physical interface. How does an adjacent router . The AUTOLOG statement contains list of started task names that should be started and remain functional while TCPIP itself is running. But how does TCPIP know whether an application is functional It checks periodically to see if the application has an active listen on the appropriate port. If no listen is detected TCPIP stops the started task and then restarts it. By default TCPIP checks every 5 minutes for an active listen. For example if an organization wanted to have the FTP server automatically started by the TCPIP task and monitored every 5 minutes by the TCPIP task the definitions in Example 54 would accomplish this. Autologging has some limitations. For example in an environment running more than one TCPIP stack at the same time an application like FTPD makes its services available to all active TCPIP stacks in the LPAR. In such context FTPD is referred to as generic server. If FTPD is running with multiple TCPIP stacks then autologging could result in some confusion as each TCPIP stack attempts to stop and restart the FTPD server. It is possible to override generic behavior and have generic server associated with specific TCPIP stack only zOS Communications Server IP Configuration Guide describes which servers are generic and which ones always choose specific TCPIP stack for its services. FTP like some other IP applications is actually zOS UNIX System Services application. It can be started within an MVS environment but it does not remain there very long. It immediately forks itself into the zOS UNIX environment and tells the parent task to kill itself. The FTPD task could very well be executed using usrsbinftpd and few organizations probably do just that. However if you recall Automated IP note on MVS versus zOS UNIX The zOS operating system has existed in one form or another for decades and has been known by many other names since it was introduced in 1964 as OS360. The most common of these older names is Multiple Virtual Storage or MVS. Even today you will often hear zOS system programmers use the term MVS to mean zOS. It is not surprising that MVS is so deeply ingrained because the MVS era was major phase in zOS history and gave us innovations like multiple address spaces and virtual storage among many others. The zOS operating system still provides the interfaces and system services of the original MVS operating system but adds many other functions including extensive support for UNIX interfaces and system services. zOS essentially adds UNIX environment through system component called zOS UNIX System Services. For the purposes of this textbook we still need to keep the MVS term around. We use the term MVS services in discussions of UNIX System Services whenever we need to refer to zOS functions that operate aside from the UNIX environment in zOS. The term MVS is important because it can be used to differentiate what would be kernel services in some other operating systems from other services . We use the term MVS services to include for example the zOS system services provided by the zOS base control program and we refer to system error messages as MVS Because UNIX is an operating system on other platforms you might wonder which of the two MVS or zOS UNIX System Services is the real operating system. The answer is both actually. MVS services and zOS UNIX services are two sets of services available in zOS and there are many others . Most system functions of zOS fall under the MVS heading but the zOS UNIX environment forms significant subset. Understanding this dual nature of zOS is important to your ability to master the operating system. In this textbook some applications are MVS applications in the sense that they do not use the zOS UNIX environment. Other applications are zOS UNIX applications because they must run as UNIX application in the zOS UNIX environment. Additionally all socket communications are handled by zOS UNIX services not MVS services. Thus you often see IP applications like FTP starting in the MVS environment and TCPIP started task if JCL is used. So the FTP daemon is best started using Of interest to this discussion is the SYSFTPD DD statement. This DD points to the FTPD configuration file usually referred to as the FTP. DATA data set. Like the FTP. DATA configuration data set if SYSFTPD is not defined. However in most cases using SYSFTPD is the preferred method. What type of information is defined in the FTP. DATA data set hallmark of the zOS operating system is configurability and the FTP server is no exception. Here is partial list and brief description of some of the more significant items FTP user is given unique root directory and further restrictions can be placed on Data set attributes can be specified. An FTP client can override these attributes while in an FTP session. Tracing and logging Logging of users and detailed debug information can be activated. Output goes to syslogd . If syslogd is not running then messages are automatically redirected to the zOS system console log. When an FTP client logs on the user can automatically be placed into either the zOS UNIX hierarchical file An FTP client can be used to interact with JES or DB2 on configuration statements not defined are assigned default values. As mentioned FTP server messages destined for syslogd are redirected to the zOS system log when syslogd is not running. Tracing and logging of the FTP server can produce significant amount of output. If the syslogd daemon is stopped such FTP messages while not harmful could be an inconvenience. From an enduser standpoint FTPing into the zOS UNIX HFS and FTPing into say Linux appears identical. The FTP subcommands are the same and the directory structures are displayed in the same fashion. However there is one fundamental difference between zOS UNIX and all other UNIX style operating systems zOS UNIX does not use the ASCII character set. Instead zOS UNIX is implemented using the EBCDIC character set. messages and commands are automatically translated going in either direction. An end user of an ASCII FTP client would see no difference of appearance between zOS FTP server and any other FTP server. For file transfers the translation is automatic when using ASCII mode . If an FTP client sends an ASCII encoded file to zOS the file is translated into Reminder Syslogd on zOS UNIX is the same daemon you find running on any other UNIXstyle platform. It is central repository to which applications on the host or network can direct messages. Messages can be classified by EBCDIC prior to being written to disk. If an FTP client retrieves such file it is automatically translated back into ASCII after being read from disk but prior to transmission. An FTP client would never know the other endpoint was storing the With binary transfer no translation occurs. Consequently if an FTP client sends an ASCII file in binary mode to the zOS host this file does not display correctly when viewed on the zOS host. If the FTP client retrieves an EBCDIC file using binary transfer type then the file does not display correctly when it arrives at the host of the ASCII FTP client. As zOS continues to evolve some limited localized support of native ASCII is available. One example is referred to as file tagging which can be used to identify files character set. There are two telnet servers available in the zOS operating environment. The TN3270 server supports line mode telnet but it is seldom used for just that. Instead it is primarily used to support the TN3270 Enhanced protocol refer to telnet line mode server referred to as the zOS UNIX Telnet server or otelnetd. The otelnetd server is functional equivalent to the telnetd daemons found on other UNIX style servers. Since it is zOS UNIX daemon it is run within the zOS UNIX environment and provides the user with shell only . All of the functionality is there that would be expected when connecting to UNIX telnetd server the vi editor environment variables profile processing and more. Additionally like telnetd on other UNIX platforms otelnetd requires inetd to handle its listen. Reminder What is inetd anyway The inetd server is generic listening application like telnetd server. When connect request arrives at that port the inetd server starts an instance of the application and gives the connection to the newly started application. On zOS inetd also handles listens for industry standard implementations of rlogin rshd and rexecd. The bad news is that inetd does not lend itself readily to the AUTOLOG capability of the TCPIP started task. The good news is that otelnetd is not typically used for missioncritical type communication anyway. If you are familiar with configuring inetd for telnet on UNIX style server then you can certainly be comfortable configuring it on zOS. sample of When connection is made to port 23 inetd executes usrsbinotelnetd and passes the parameter to it. The parameter is included for efficiency it reduces the number of address spaces required to run the otelnetd But wait minute How did port 23 suddenly get dropped into the equation The otelnetd server on zOS functions exactly the same as telnetd on any other UNIX platform. It requires an entry in etcservices to inform it of which port number to listen on. In this case etcservices would require line such as otelnet 23tcp. The otelnetd daemon supports the standard line mode terminal definitions such as vt terminals Wyse terminals PC850 and many others. characters sent to client emulator are transmitted in ASCII format. For readers familiar with TCPIP implementation on UNIXstyle platforms you might have experience with file called etcresolv.conf. The resolv.conf file is referred to as resolver configuration file or the TCPIP. DATA file. Reminder The etcservices file is used in UNIX environment by many IPbased applications. When UNIX application is started it must determine what port number it should be listening on and it checks etcservices. So why are there two names It harkens back to our discussion of MVS and its UNIX subset it all depends on whether the application is zOS UNIX application or native MVS application. First however we need to describe how resolver configuration file is used. The TCPIP profile data set defines the operating characteristics of the TCPIP stack. However that is not quite enough. There are few operating characteristics associated with IP applications that must also be defined. The resolver configuration file defines the operating characteristics of IP applications. sample resolver configuration file can be seen in Example 58. Most of these statements should be recognizable to those familiar with IP on UNIXstyle systems. This statement identifies the domain name system domain origin for this host. The host name of the fully qualified host name for this host. When concatenated together the host name in Example 58 is MAINFRAME. XYZ. COM. If an IP application needs to resolve host name to an IP address it uses one of these IP addresses to contact name server. Often on zOS the This statement identifies the started task name of the TCPIP stack that an application can be associated with. It might not be used but should always So how does an application like our FTP server or the otelnetd daemon find its resolver configuration information This topic covers some simple options that work well assuming that the system uses single TCPIP stack environment . Where the information comes from depends on for all applications is the GLOBALTCPIPDATA. The GLOBALTCPIPDATA data set is discussed in Resolver address space. The type of application . This is discussed in Type of The search order for dynamically locating files. This is discussed in The The statements found in resolver configuration file can be globally overridden at the zOS level by the resolver address space. When an application needs to access services identified within the resolver configuration file it is accomplished using the resolver started task . The resolver address space is normally started when zOS is started. The significance of this resolver address space is that the address space itself These statements can be used to provide resolver configuration file settings for both zOS UNIX applications and MVS applications. sample of the JCL for starting the resolver configuration file is shown in Example 59. Again this JCL is only portion of the JCL required. Example 59 Sample JCL for starting resolver address space Note What is meant by stating that the TCPIPJOBNAME statement might be used In an MVS environment the TCPIPJOBNAME statement is used only if the application has not explicitly overridden it . In zOS UNIX environment TCPIPJOBNAME is never used. An application must explicitly choose stack name or else it is associated with all active TCPIP stacks. In other words it is generic server. The statement of interest in Example 59 is the SETUP DD statement. This DD points to file containing statements controlling the behavior of resolver configuration file searches. There are several statements available within This statement is used to identify specific resolver configuration file that contains the resolver configuration statements that are to be applied globally to all IP applications. This statement is used to define default resolver configuration Type of application complicated by the dual nature of applications on zOS host. Where an application searches for resolver configuration file depends on whether the application is an MVS application or zOS UNIX application. For zOS UNIX applications the resolver configuration parameters can be placed that zOS UNIX application searches. It is possible to identify the resolver configuration file in the environment variable RESOLVER_CONFIG but this variable would need to be defined to every zOS UNIX IP application. Obviously etcresolv.conf is simpler to use yet some organizations may prefer to use or require the RESOLVER_CONFIG environment variable. Note Hopefully that SETUP DD statement in Example 59 has not confused you. DD statement can point to zOS UNIX file in the HFS as well as to In this example there is no particular reason why an HFS file was chosen it could just as easily have been zOS data set or data set member. To summarize GLOBALTCPIPDATA is used for both zOS UNIX and MVS IP applications. Then zOS UNIX applications look for RESOLVER_CONFIG application might search for resolver configuration information is the same for both zOS UNIX and MVS IP applications. Here they are in the order in which they are searched for 1. SYSTCPD DD card. The data set allocated to the DDname SYSTCPD is used. In the zOS UNIX environment child process does not have access to from the parent process over the fork or exec function calls. 2. userid. TCPIP. DATA. userid is the user ID that is associated with the current security environment . An MVS environment application could theoretically run without an associated user ID. If so the job Only the first file located is used. For example if the first file found is SYS1. TCPPARMS then that is the only file used even if DEFAULTTCPIPDATA and TCPIP. TCPIP. DATA exist. Not all statements within resolver configuration files are treated equally. Some statements not globally defined can be located dynamically in files later in the search order. For example if TCPIPJOBNAME is not located in the global file it can be read in from the first of the subsequent files in the search order. This makes sense because if more than one TCPIP task is running defining TCPIPJOBNAME in GLOBALTCPIPDATA would only allow one TCPIP task to Note There is benefit to having RESOLVER_CONFIG ahead of etcresolv.config in the search order. For example imagine if you wanted to run zOS UNIX application that used different name server other than what is coded in etcresolv.conf. This can readily be accomplished by specifying the RESOLVER_CONFIG environment variable and pointing it to copy of etcresolv.conf containing different value Introduction to the New Mainframe Networking ever be accessed. By using something like RESOLVER_CONFIG or SYSTCPD different job names could be specified for different environments or applications. But not all of the statements get this second chance. Some statements if not defined explicitly in global file are forced into default values. For example the NAMESERVER statement if not present in the GLOBALTCPIPDATA defaults to blank disabling all name server function. When configuration time comes each statement that is needed for an application should be reviewed to determine where it would best be located. Why does something as simple as searching for an applications IP environment information need to be so complex The answer lies only partially in the dual nature of the zOS operating system. The other reason is that some organizations have need to run more than one TCPIP stack within single LPAR. In such context there are some applications that should be associated with one TCPIP stack and not the other TCPIP stack. There are many different ways that this can be controlled and some of it depends upon the nature of the application itself. But for some applications it is controlled by the TCPIPJOBNAME statement found in the resolver configuration file. This implies that an organization would need to have more than one resolver configuration file one for application which uses TCPIP stack and another resolver configuration file for application which uses stack . The other reason for the complexityand this applies to essentially any aspect of zOSis the configurability requirements. The zOS operating system is designed for the largest organizations in the world. Such organizations are often also less flexible more processoriented and securityaware environments. Consequently zOS must fit into the organizations requirements and not the other way around. What does this really mean Well if XYZ Corporation wants to use certain naming convention for certain data sets because thousands of users security profiles are written with this in mind then zOS TCPIP had better be able to So far we have mentioned only two server applications FTP and telnet. We noted that zOS supports all the wellknown servers. In addition zOS supports all The FTP client can be run from either TSO environment or from the zOS UNIX environment. The FTP client is heavily used on zOS because it runs very well as batch job. Large file transfers are scheduled for running on weekends or in the evenings and the JCL is submitted and often evaluated for success automatically. The line mode telnet client is TSO application only. This application does not get tremendous amount of use because zOS is not platform that lends itself to being client . No IP implementation is complete without the ability to perform rudimentary connectivity tests. The ping command can be run from On the MVS side of zOS there is limit of eight characters for the length of command. So traceroute had to be shortened to tracerte when executed from TSO environment. However within the zOS UNIX environment the command is traceroute as would be expected. The traceroute command performs the UDP expired datagram method of testing the reachability of every hop in If an end user wants to perform rudimentary Simple Network Management Protocol queries the zOS UNIX snmp command can be used. It can also function as trap destination but on zOS there are far better SNMP management environments available. The netstat command is probably the most essential IP command used by network administrators. Technically it is not client at all but it is such wellknown and wellused tool that it is included in this list. It runs as zOS UNIX command TSO command and it as all TCPIP clients do the translation from ASCII to EBCDIC and from EBCDIC Introduction to the New Mainframe Networking to ASCII automatically and transparently to both the end user and the remote application with which it communicates. Of course some of these applications like ping traceroute and netstat do not require any translation at all. The TCPIP started task is the engine that drives all IPbased activity on zOS. The TCPIP profile data set controls the configuration of the TCPIP environment. The FTP server implements the FTP standard and can communicate with any FTP clients on the network. The telnet server implements standard line mode Even though zOS is an EBCDIC host communication with ASCIIbased IP applications is seamless. IP applications running on zOS use resolver configuration file for environmental values. Locating resolver configuration file is somewhat complicated by the dual operating system nature of zOS . TCPIP on zOS supports all of the wellknown server and client applications. 1. How does the TCPIP stack normally locate parameters that define the 2. Which is easier to plan and set up static or dynamic routes 3. What does TCPIP do to an autologged application if it does not have listen Send buffer size 4. What is the most likely meaning of ANONYMOUSFILEACCESS HFS as Server IP Configuration Reference found in the IBM online library if 5. How does an IP application on zOS know what name server to access when DNS services are required 1. What happens if the receive window size that is received from the remote host is larger than the local hosts send window size 2. What would be the reason for NOAUTOLOG parameter on PORT 20 for an 3. Syslogd is the standard message repository for UNIX environments. What is the equivalent repository in an MVS environment 4. What other parameters within the resolver configuration file would need to be unique if more than one TCPIP stack is active in an LPAR 1. Try issuing one of the two commands listed below. What types of devices do DISPLAY TCPIPtcpprocnameNETSTATDEV 2. Why does the AUTOLOG statement for the FTPD server look like this PORT 21 TCP FTPD1. In other words why does the procname have 1 appended 3. Referring to zOS Communications Server IP Configuration Reference found on the IBM online library what is the significance of the JOBNAME 4. What is the purpose of the resolver configuration file 5. If syslogd is not running an FTP message directed to syslogd is rerouted back to the zOS system log instead. The message identifier is BPXF024I. What component of zOS is most likely responsible for the BPX message Explain how dynamic XCF functions in sysplex. Explain how sysplex distributor functions in sysplex. Describe how the sysplex performs problem detection and recovery. cluster of computers refers to group of interconnected computers that are working together as one unit. It is relatively simple concept and it has been around arguably for very long time. Within cluster individual computers can be coupled either loosely or tightly. loosely coupled cluster consists of computers that are running with minimum of communication and cooperation among them. This results in efficient usage of each individual computer but limits the amount of coordination and sharing of workloads. tightlycoupled cluster consists of group of computers that are cooperating to great degree among each other coordinating and sharing sysplex refers to tightlycoupled cluster of independent instances of the zOS operating system. It is beyond of the scope of this text to go into details on how of sysplex as it applies to TCPIP. sysplex can be either basic or parallel. basic sysplex can communicate using channel to channel connections between LPARs. Parallel Sysplex uses something called Coupling Facility . Logically speaking sysplex is group of distinct instances of the zOS operating system. distinct instance is often called an image. The zOS images could be running in separate CPCs or they could be running in separate LPARs within single CPC or it could be combination of both few LPARs in one CPC and few LPARs in another CPC. What makes group of such zOS images into sysplex is the interimage communication. This interimage communication is normally handled through the crosssystem Coupling Facility or XCF. XCF communications function in either basic sysplex or in Parallel Sysplex. If Coupling Facility exists choice can be made as to whether XCF uses CTCs or the Coupling Facility to communicate. Note The Parallel Sysplex Coupling Facility can run either as separate LPAR or within dedicated hardware device. It is capable of managing data structures on behalf of applications requiring interLPAR communication. At the heart of the sysplex is the XCF. As its name suggests XCF handles communication between logical partitions or CPCs. Communication between these LPARs is effectively instantaneous. Coupling Facility. The information sharing is constant and continuous allowing the independent zOS images to know detailed information about the current TCPIP takes advantage of the communication capabilities of the XCF in sysplex in three different ways 2. It determines workload levels within each LPAR in the sysplex through 3. It can send IP traffic among the LPARs. In order for TCPIP to be able to utilize the services of the Coupling Facility accomplished by starting VTAM with XCFINITYES specified in ATCSTRxx. As an implementation note the Coupling Facility if used represents single point of failure within sysplex. Hence an organization always implements backup Coupling Facility which is ready in the event of failure. Now lets take look at zOS Workload Manager . WLM might be described as performance expectation facility. It can be used to define performance goals for different applications and different types of work. In addition WLM can be used to provide information about the overall workload levels on each image within sysplex. For more information on sysplex and WLM see Introduction to the New Mainframe zOS Basics. direct traffic to the LPAR with the lightest workload. The mechanism is relatively simple from the perspective of the TCPIP application TCPIP asks WLM to provide information on workload levels within each LPAR in the sysplex. WLM Introduction to the New Mainframe Networking responds with list containing the LPAR name and number for reach LPAR. Each number indicates the number of connections that should be sent to each LPAR for this time interval. prioritize workload. In turn TCPIP distributes connections to the host with the upon specific service class goals of the target applications address space. This is referred to as serverspecific WLM recommendation. At this point the sysplex has been identified as tightlycoupled cluster of computers with some workload balancing capabilities added for good measure. But how does the IP network take advantage of all this technology The first part of the answer is the dynamic virtual IP address or DVIPA. defined through DEVICE and LINK statement pair and remains unchanged unless explicitly removed by changing the active configuration statements. By contrast dynamic VIPA would normally be activated in one of two different An application explicitly issuing bind function call to the IP address. This is called unique applicationinstance DVIPA. TCPIP stack dynamically activating the address. This is called multiple an XCF group called EZBTCPCS. When an IP application intends to listen or if an application intends to connect For our purposes we focus only on how bind can be used by an application to associate socket with specific IP address. An outline of this process is The process begins with an application requesting the usage of specific IP address. For example one of the HTTP servers that runs on zOS has parameter in its etchttpd.conf file to force the server to use specific IP address. When the HTTP server requests this IP address the TCPIP stack first checks to see if the address is already active. This address might already be active because of one of the following reasons It has been defined by static definition. It has been dynamically created by another application. It has been dynamically created by the TCPIP stack and ending statement. The VIPARANGE statement is exactly what its name implies it defines range of IP addresses that are eligible to be dynamically activated. In Example 61 the range is for IP addresses within the 201.2.10.192 network ID using mask of Since this IP address is within our VIPARANGE defined subnetwork the bind request results in new IP address for DVIPA being activated. The VIPARANGE statement has further options to modify its behavior in context where other applications within sysplex are also issuing bind function with the BIND ip_address keyword to override such bind. TCPIP substitutes the specified ip_address and the application instance DVIPA processing as calls. In our above example how should this IP address behave if another TCPIP stack in the same sysplex be available to start the new application on the alternate LPAR when the original LPAR TCPIP stack or application fails. This is not the case with multiple applicationinstance DVIPA. With multiple applicationinstance DVIPA if the LPAR or TCPIP stack fails the IP address is automatically started on one of the backup LPARs. The application could already have been running on both the primary and backup systems. Presumably the application has not used bind to specific address. When TCPIP automatically activates the multiple applicationinstance DVIPA on the backup host the application is automatically available at this newly activated Having explained the details of DVIPA lets take moment to explain why it DVIPAs allow servers to be made available independently of hardware or software failures. This can be done dynamically by TCPIP or even by system automation product. DVIPA allows multiple LPARs to appear to be single highly available network host. Because DVIPA movement is automatic end users and clients might never know DVIPA With DVIPA applications can be seamlessly moved from one LPAR to another allowing virtualization of the application itself. the XCF links can carry IP traffic. With DVIPA XCF was used to communicate signalling information such as when new instance of an IP address has been dynamically activated on TCPIP stack within the sysplex. The next step in the progression of utilizing sysplexs capabilities is to use XCF to carry IP traffic. In order for TCPIP to take advantage of interLPAR communication VTAM must already have crosssystem Coupling Facility communications active. VTAM uses XCF to establish common membership group ISTXCF that TCPIP uses when communicating over dynamic XCF connections. When dynamic XCF is functional within sysplex pointtomultipoint network is established among all participating LPARs. Each host in the sysplex has direct connection to any other host in the same sysplex. Within the TCPIP profile data set there is only one configuration option required DYNAMICXCF. This option falls within the IPCONFIG statement group as The definition in Example 62 would cause the TCPIP stack to create link using the IP address of 192.168.80.1 within the sysplex subnetwork. This IP address would be directly reachable on the sysplex pointtomultipoint network by any other TCPIP stack in the sysplex that also has DYNAMICXCF coded. Each TCPIP stack would code unique IP address within the same subnetwork. When TCPIP stack becomes active in the sysplex and this stack has DYNAMICXCF coded the following sequence of events occurs internally within the TCPIP stack 1. DEVICE statement for this stacks XCF device is automatically generated. 2. corresponding LINK statement is automatically generated. 3. HOME statement entry using the DYNAMICXCF IP address is added to the 4. The device is started. If TCPIP stack does not have DYNAMICXCF coded it does not participate in the dynamic XCF communications. In other words both end points must code DYNAMICXCF in order for link to be established. There are two special instances of dynamic XCF links that deserve further If available TCPIP uses HiperSockets link in preference to Coupling Facility link. The reason is speedHiperSockets is faster. For more information about Dynamic XCF also functions within an LPAR when more than one TCPIP stack is active in the same LPAR. The link generated is referred to as samehost link which corresponds to device type of IUTSAMEH. Consequently the DEVICE and LINK statements generated by the TCPIP stack vary depending upon whether HiperSockets link is available and whether the link is within or between LPARs. dynamic XCF definitions for sysplex. Each TCPIP stack has unique IP Note All dynamic XCF statements must use common network ID. This is because they form pointtomultipoint network. Refer to 2.4.2 Network address to represent itself in the dynamic XCF subnetwork. The dynamic XCF IP addresses of each participating TCPIP stack are all in the same subnetwork. appropriately it automatically joins the sysplex and activate its IP address within the dynamic XCF network. IP address availability in zOS can be viewed as progressive evolution. Static VIPA eliminates the problem of an IP address being associated with single networking hardware point of failure. Dynamic VIPA allows us to move such IP addresses in event of an application TCPIP stack or LPAR failure. Next dynamic XCF provides automatic generation of links to provide an automated IP layer of connectivity within the sysplex. But what about taking advantage of the Sysplex distributor can be viewed as continued evolution of connectivity improvements. It is combination of the high availability features of DVIPA and the workload optimization capabilities of WLM. The implementation has one significant difference instead of all participating hosts being effectively equal LPARs can be given specific roles to play. When combined with WLM the overall effect on availability is exceptional. Another change with sysplex distributor is that the distribution is possible only with TCP connections. Other layer 4 protocols are not supported. With sysplex distributor the LPARs in the sysplex are assigned roles. TCPIP stack in one LPAR is given the role of being frontend host. It receives inbound connection requests and redirects them to specific target backend TCPIP stack. Other LPARs run target TCPIP stacks that ultimately function as the real endpoint of communication. Sysplex distributor requires that choice be made as to whether host is to function as distributor target backup distributor or some combination. Things are about to get little bit more complicated. So lets define some simple The designated contact for the sysplex. It is the normal owner of the IP address that clients out in the network use to connect to the sysplex. host within the sysplex to which distributing host can redirect connection request. The target host must be running an instance of the target application. For example if client wants to connect to FTP then there must be an FTP server running on the target host to which the session is distributed. host that is designated as backup in the event that the distributing host should malfunction. The backup host takes over the IP address of the distributing host when required. There can be more than one backup host. distributing host can also be target host some sessions are distributed to itself. target host can also be Note In the context of sysplex distributor multiple TCPIP stacks in single LPAR can be functionally ignored. Consequently to simplify our discussion the term host is used to denote LPAR containing single TCPIP stack. backup distributing host. The idea here is to not waste an LPAR just because it is assigned certain role. With these options and flexibility there are considerable number of possible scenario. For example the only application shown as being available on all target hosts is FTP. Such sysplex might be functioning as very busy FTP server farm but it presents single IP address to all incoming FTP clients. Many other applications could be added to these hosts as target applications. for connections. The target application of course is the zOS FTP server. Since sysplex distributor requires dynamic XCF the first configuration step is to enable dynamic XCF on each host. All these configuration statements belong in are used here. The dynamic XCF IP addresses are used by the distributing TCPIP host when it redirects packets to target host. In order to balance the connections to the appropriate targets TCPIP is going to need to be talking to the workload manager. By coding SYSPLEXROUTING in consulted for connection distribution advice. If LPAR 1 is to function as distributor the next thing to be defined is the IP address it uses. Again building on our earlier discussion well use the same 10.134.61.190 is the IP address client on the network would use to connect to At this stage you might notice the only new statement discussed is the SYSPLEXROUTING statement in order to involve WLM. But how do we go about getting the TCPIP stack in LPAR 1 to function as distributing host For this we need to add VIPADISTRIBUTE option within the VIPADYNAMIC block of IP address to be distributed. Only connections for port 21 is distributed. And those connections are only distributed to target hosts that have an active listen on port 21 . The destination IP addresses are all active dynamic XCF IP addresses within the The SYSPLEXPORTS parameter on the VIPADISTRIBUTE statement is used to manage the assignment of ephemeral ports among the LPARs. What about the other hosts in this sample Do they need to have any definitions changed For our purposes the answer is no. Returning to Example 63 LPARs 2 and 3 are both backup LPARs for 10.134.61.190. In event of failure in LPAR 1 LPAR 2 would take over the DVIPA. In addition it automatically inherits the distribution capabilities and characteristics that are defined for LPAR 1. If both LPAR 1 and LPAR 2 fail LPAR 3 would become the distributor of 10.134.61.190. When the problem is recovered and LPAR 1 is brought back into the sysplex it would nondisruptively take back the role as the distributor. Walking through distributed connections distributed connections. To simplify the illustration only three LPARs are Both users are using FTP clients. Here is the sequence of events as they 2. The request is received at LPAR 1. 3. WLM indicates that LPAR 2 should handle this connection. 4. The connection request is redirected to LPAR 2 over dynamic 5. LPAR 2s FTP server responds to and accepts the connection request. The source IP address used is 10.134.61.190 within the sysplex so that each FTP server accesses the same environment. If it is retrieveonly FTP environment then the file systems could readily be duplicated within each LPARs environment. Another question you might have depends upon your knowledge of how FTP works. Each time data transfer request is made in FTP separate and new connection must be established. FTP has two ways it can establish the data connection by using PORT command or PASV command . The PORT command method of establishing the data connection requires the FTP server to send connection request back to the client. Such scenario is not complicated by its occurring within distributed sysplex because an outbound connection does not need to use the distributor. The connection request heads outbound over the appropriate link directly to the host where the In contrast an FTP PASV command requires that the data connection is established by the client sending connection request to the server. When this data connection arrives at the distributing host how does it know which FTP server owns the existing control connection The answer is that the distributing host is stateaware in much the same fashion as stateful firewall. The distributing host keeps track of any inbound TCP connections. In addition the distributing host is also made aware of target hosts establishing outbound connections though it needs to keep track only of outbound connections that use the distributed address as source address. servers do not need to return through the distributing host. Instead they can Controlling distribution zOS can function as server for thousands of different large scale applications like CICS IMS and all of the wellknown IP servers. These applications are often missioncritical and experience workloads in the range of tens of thousands concurrent users. An organization has guidelines for the service level that is appropriate for given service. That is the same effect could have resulted The DISTMETHOD BASEWLM option means the distribution method uses WLM. If we change this option to DISTMETHOD ROUNDROBIN sysplex distributor does not consult the Workload Manager at all. Instead it does simple round robin distribution connections are distributed continually to consecutive hosts. Beginning with zOS V1R7 sysplex distributor can control distribution based upon finergrained data from WLM. In particular sysplex distributor can take into account whether specific application is meeting its performance goals as defined to WLM and can keep track of the target servers response time. Faster responding servers receive more connections. Instead of coding DISTMETHOD BASEWLM in the sample above DISTMETHOD SERVERWLM would be used for this. Cisco Systems has load balancing and connection distribution solution called Content Switching Module . CSM provides layer 4 to layer 7 routing and Note full description of the policy agent is beyond the scope of this text but you should know that the policy agent is zOS application that influences Intrusion detection services . failover control based upon message content and the health of target On zOS the health of sysplex is communicated using the Load Balance Advisor LBA. LBA communicates health data to CSM. CSM then makes decisions on which server to forward workload to. The end result is very similar to the way sysplex distributor functions but the implementation is significantly different. In addition CSM can communicate with nonzOS platforms. Instead of the sysplex distributor node being the target for inbound connections client connection pass through the CSM. The CSM handles each incoming session and distributes it to the appropriate target host. Continuing on our progression of continual improvement sysplex distributor provides tightlycoupled cluster of independent hosts capable of dynamically managing availability of single IP address. When everything is working as designed the system is as effective as can be imagined. However everything does not always work as designed. What happens if TCPIP or an entire LPAR is running but not exactly running effectively For example what happens if critical component is unresponsive or unavailable The handling of such situation is often referred to as system autonomics or autonomic computing. Each TCPIP stack in the sysplex automatically monitors its own health storage shortage. CSM is component of VTAM that handles real storage memory usage. For more XCF connectivity. TCPIP stack responsiveness . Connection success rate . This includes monitoring of whether backlog exists and the connectivity between the distributor and target as well as between target and its client. The statement to activate this capability is shown in Example 64. These resources are by default monitored on 60second intervals. If resource is deemed to be unavailable the RECOVERY option causes the TCPIP stack to remove itself from the sysplex. After the problem has been recovered the TCPIP stack can automatically rejoin the sysplex . You might wonder how to keep on top of the routing requirements of these shifting IP addresses and shifting TCPIP stacks. What sort of routing solution can meet the changing needs of DVIPA We can begin by eliminating static routing . Convergence is the rate at which network routing change is recognized and adapted to within network. The value of DVIPA is greatly diminished if it takes the routing layer several minutes to adapt to an IP address The next choice of dynamic routing protocol is Open Shortest Path First or OSPF and it is the protocol recommended to support DVIPAs in sysplex environment. With OSPF when DVIPA is moved to another host convergence is both fast and automatic. Other hosts in the network can rapidly become aware Open Shortest Path First has become an industry standard for dynamic routing within given network. It is standardized most recently within RFC 2328 but OSPF standards seem to dapple the RFC landscape. Usually the term On zOS the OSPF protocol is implemented by the OMPROUTE server. The OMPROUTE server is actually zOS UNIX daemon. At the time of writing as well as almost all of the features from RFC 2328. The important part is that OMPROUTE is completely compatible with adjacent routers that are capable of The actual configuration of OSPF on zOS or any other host is relatively straightforward. The difficult part is the network topology planning. From network design viewpoint OSPF is incredibly flexible which translates to incredibly complex. Designing wellplanned network is challenge. The good news is that once an OSPF network has been designed the actual configuration and execution of OMPROUTE is straightforward task. However there is one specific recommendation that we should discuss. Even though OMPROUTE is fully functional OSPF router the zOS operating system is not designed to be backbone router operating system. So zOS host should be implemented with the intention of having its dynamic routing requirements minimized. In the case of sysplex the ideal is to define the area encompassing the sysplex as stub area. This is The area above the line is area 0.0.0.2 . What exactly is stub area The OSPF protocol defines given routing area as either being transit area or stub area. transit area as its name suggests is an area that can have traffic passing through it. stub area is nothing more than network deadend. Packets can flow into and out of stub area but traffic does not travel through it. Why is this recommended for the sysplex Here are some characteristics of stub area does not have any interarea traffic . Each stub area router communicates only default route and intraarea OSPF backbone or other straddle two areas. These routers are referred to as area border routers or ABRs. forwarded to the stub area. Ultimately an OSPF stub area allows the sysplex to be connected dynamically to sysplex. At the same time the stub area minimizes the routing overhead required It was said before that communication within the sysplex is as far as TCPIP is concerned effectively instantaneous. This means that when TCPIP stack joins sysplex. Yet there is no guarantee that OMPROUTE is started immediately. In fact the TCPIP started task can autostart very large number of TCPIP applications of which OMPROUTE is only one. Obviously OMPROUTE needs to build routing table for TCPIP in order for network communication can occur. until OMPROUTE itself has been started. This reduces the possibility of connection being routed to host that doesnt yet have connectivity to the OSAExpress cards. There is so much capability built into an OSAExpress card cooperation that occurs between TCPIP and OSAExpress and how this can effect availability within sysplex environment. appropriately automatically receives copy of all IP addresses that are active within TCPIP stack. When DVIPA is added to or removed from TCPIP stack the OSAExpress card is immediately informed of the change. have all shown single OSAExpress card attached to each individual LPAR. Each OSAExpress card however has two physical connectors available. And an OSAExpress card can be shared with more than one LPAR . As result most implementations of an OSAExpress card involve sharing the OSAExpress card between two LPARs. Introduction to the New Mainframe Networking from the LPARs in order to show the crossadapter redundancy of sharing OSA adapters. The presumption here is that all adapters are connected to the same LAN segment and the same subnetwork. If we imagine that the OSAE2 card were to fail we can readily see that both LPARs would maintain complete connectivity to the LAN using OSAE3. What is not so apparent is the gratuitous ARP processing. Gratuitous ARP processing is part of the availability features of the OSAExpress card. For more information see the discussion of ARP takeover in 3.7.1 Queued Direct IO on The end result is that even though IP addresses 201.2.11.1 and 201.2.11.4 appear to be lost when OSAE2 is lost they are immediately revived in the OSAE3 card by gratuitous ARP updating the MAC addresses of the lost IP addresses. In other words network connectivity is reestablished by making changes at layer 2 instead of layer 3 . The routers in the network would effectively not be involved and routing tables remain unchanged. sysplex is cluster of tightlycoupled independent instances of the zOS operating system. The internal communications within sysplex are facilitated by the Coupling Facility. TCPIP takes advantage of the Coupling Facility and Workload Manager to optimize availability and load balancing in sysplex. Availability is enhanced by the ability to dynamically move IP addresses using dynamic VIPA. Sysplex distributor combines dynamic VIPA Workload Manager and autonomic computing to create the highest possible availability of an IP host. Within sysplex environment OSPF is used to handle routing changes 1. What is the role of Coupling Facility 2. How many different types of links could be utilized within dynamic XCF 5. Can distributing node also be target node 1. How might DVIPA 3. What are the relative advantages and disadvantages of the two types of 4. What are the benefits of using tightlycoupled sysplex environment to run Workload Manager 1. Try issuing the following command in an SDSF session What does the output tell you about the nature of the sysplex you are logged onto Is it sysplex at all 2. To display the active XCF groups in the sysplex try issuing the following Do you see group called EZBTCPCS listed If the answer is yes try Can you make sense of what the display output is telling you 3. Using the IBM online book library find out what the SERVICEMGR parameter does when coded on VIPADEFINE statement. 4. As mentioned target servers response time is tracked when using distribution method of SERVWLM. This is called target server responsiveness or TSR. Using the IBM online library what TSR value would represent server that is accepting all new connection requests 5. Use the IBM online book library to find out how to ensure that TCPIP does not rejoin the sysplex until OMPROUTE is running. is data communication architecture established by IBM to specify common conventions for communication among the wide array of IBM hardware and software data communication products and other platforms. Among the platforms that implement SNA in addition to mainframes are IBMs Communications Server on Windows AIX and Linux Microsofts Host Integration Server for Windows and many more. The way in which products internally implement these common conventions can differ from one product to another but because the external interface of each implementation is compatible different products can communicate without the need to distinguish among the many possible product implementations. SNA products recognize and recover from loss of data during transmission use flow control procedures to prevent data overrun and avoid network congestion identify failures quickly and recover from many errors with minimal involvement of network users. SNA products also increase network availability through options such as the extended recovery facility backup host alternative routing capability and maintenance and recovery procedures integrated into workstations modems and controllers. In 1974 IBM introduced its Systems Network Architecture which is set of protocols and services enabling communication between host computers and peripheral nodes such as IBMs dedicated hardware boxes the 3174 controller for 3270 type displays and printers controllers for the retail and finance industry and more. The mainframe subsystem that implements SNA was named Virtual Telecommunication Access Method . The robustness of the SNA protocol the IBM hardware and the transaction management infrastructure software supplied by IBM made SNA the dominant protocol in the Fortune 1000 companies. In order to understand the rationale of the many functions and services in SNA you must understand the computing environment at that time. Prior to 1974 data processing was batchbased. Batch means that data was recorded on paper usually on predefined templates and was keyed into media readable by the computer system. The computer department executed various Around 1974 transaction processing was introduced. People used terminals to key in the data directly and receive the output for their inquiry instantaneously. To implement transaction processing networking infrastructure was put in place. The carriers at that time were geared to supply voice services rather than data bits per second. The human ear can tolerate small errors in telephone lines but computers cannot. Even missing bit or an extra bit in data communication line can be catastrophic. Try to imagine what might happen to your bank account if the ATM you use receives garbled message. In the early 1970s computer memory was scarce and expensive resource. Devices with 16 KB of memory were common in the computer industry. These devices were slow compared to the CPU speeds we see today. IBM had to address the limitation imposed by the communication lines and networking hardware and developed robust protocol that would guarantee the integrity of the messages. During the 20year period when SNA was the primary networking method many CICS and IMS application programs were developed and put in place. The application programming interface of these application programs is heavily dependent on the underlying protocol SNA. It is apparent that TCPIP is the dominant networking protocol now and for the foreseeable future. Today new applications use stateoftheart programming techniques like Java and HTTP but it will take many years until all SNA networking application is dependent on the communication protocol it uses. Every protocol provides an application programming interface . TCPIPs API is called socket programming and SNA has its own API. Migrating networking application from one protocol to another requires replacing the calls to the API. Business mangers are reluctant to invest without introducing new functions and improvements. More importantly in the past 30 years businesses have invested tremendous amount of labor and money in developing SNA applications. It is estimated that the investment made in CICS and IMS applications is in the range of 20 trillion US dollars. Considering the investments in SNA applications these programs will be used for many years. To recode these applications as TCP socket applications is often impractical and costprohibitive. Besides alternatives exist. IBM introduced new technologies to help businesses preserve the investment in SNA and use IP as the protocol for connecting SNA computers. The technology Introduction to the New Mainframe Networking is known as SNAIP integration and it is described in detail in remain unchanged thereby preserving the investment in SNA. Because SNA applications will exist for years to come someone has to care for SNA definitions problem determination recovery business continuity procedures and many other tasks. These tasks are the responsibility of the mainframe networking systems programmer who needs to know in depth the architecture and how to implement SNA on various hardware and software adapted to the changes in data communication. Today there are two implementations of SNA subarea networking and Subarea networking was the initial implementation of SNA that defined mainframebased hierarchical networks in which every resource and SNA route had to be predefined. In the initial implementation of SNA adding resources or changing SNA routes necessitated the shutdown of parts of the To address the deficiency of the static nature of subarea SNA IBM introduced an SNAbased peer network with no hierarchical relations and with dynamic definition of resources. At later stage APPN was enhanced with the introduction of High Performance Routing and SNAIP which as its name implies is high performance routing protocol that can be optionally exploited by APPN. Neither subarea networking nor APPN resolved weakness related to the loss of an SNA session when resource along the session route fails. Besides improving routing performance HPR provides nondisruptive rerouting of the SNA session to an available alternate route. HPR also enables the integration of SNA into IPbased backbones. Hierarchical systems are organized in the shape of pyramid with each row of objects linked directly to objects beneath it. SNA subarea besides implementing the model of hierarchical system is centrally managed from the top of the Network resources in SNA are managed from central point of control that is aware of all the activity in the network whether networking and organizational requirements hierarchical network can be divided into subnetworks where every subnetwork has control point with its controlled resources. We can use an airport control tower as an example to explain the centrallymanaged approach. All airplanes in the control tower sphere of control also operates the resources by granting landing and takeoff authorization. In peer network every resource is selfcontained and controls its own resources. Most of the time networking resource in peer network is not aware of its network peers and learns about their existence when it starts to communicate with the peer resources. We can use Windows workstation as an example. We define only the local network of the workstation. The workstation can connect and exchange data with every resource it is authorized to access as long as the physical path is national real estate franchise is good illustration of peer network. Every local real estate office maintains the listing in its area and is not aware of the information stored in other offices. If customer who plans to relocate asks for service from the local office the office will call the office in the city customer had not made this request the local office would not be aware of the remote office and would learn about the remote office only when there was need to access data that was stored remotely. By now you are probably asking yourself why SNA initially followed the hierarchical path and TCPIP which was developed at the same time is peer protocol. Well the answer is that the goals of the protocols were different. TCPIP was developed to provide collaboration between computers and data sharing. SNA was developed for central control. In the 1980s TCPIP was used extensively by scientists who wanted to share research papers and ideas stored on their campus computers with academic staff around the world. IBM designed SNA for business data processing applications. The hierarchical topology of SNA matches the organizational Introduction to the New Mainframe Networking structure of businesses and enterprises. The most common example is bank where the tellers in the branch require access to the banks central database. The same paradigm is also true for the insurance and retail industry. Also businesses that have regional offices connected to corporate site can implement the hierarchical network model. The initial implementation by IBM was the SNA subarea network. This network is hierarchical network implemented by the Virtual Telecommunications Access Method in the mainframe. VTAM is the software that controls communication and data flow in an IBM mainframe SNA network. VTAM resides in the mainframe and supports wide variety of network protocols like SDLC VTAM controls data transfer between channels and OSA LANattached devices and performs SNA routing functions. VTAM provides an application programming interface that enables the development of application programs that communicate using SNA with remote application programs or devices. Currently VTAM is part of Communications Server for zOS and is called SNA data communication network can be described as configuration of nodes and links. Nodes are the network components that send data over and receive data from the network. Node implementations include processors controllers and workstations. Links are the network components that connect adjacent nodes. Nodes and links work together in transferring data through network. An SNA node is set of hardware and associated software components that implement network functions. Nodes differ based on the architectural components and the set of functional capabilities they implement. Nodes with different architectural components represent different node types. T5 node is located only in the mainframe. The software that implements the T5 node is the SNA component of the Communications Server. The SNA component in zOS is also referred to as VTAM and the hardware is the IBM 3745 or 3746 device. The Communication Controller of Linux is software package that replaces the 3745 or 3746. T2.0 is peripheral node that attaches to the communication controller or the mainframe. T2.0 is an alias displays and printers and is connected through communication line to the T4 node or through channel to the T5 node. Additional devices that implement T2.0 node are banking branch controllers and retail store controllers. T2.1 is peeroriented peripheral node that attaches to mainframe communication controller or another peripheral node. T2.1 node is called low entry networking The links connecting two subarea nodes either type 5 to type 5 or type 5 to type 4 or type 4 to type 4 are called transmission groups. For more information about After reading the previous note most likely you asked yourself what is the hardware and software. The T2 node was replaced by workstation that implements software called 3270 emulation and the banking and retail controller by Windows Unix or Linuxbased servers. The 3745 and 3746 hardware is nearing its end of life. The migration to TCPIP in the backbone reduces the number of lines in the 3745 and 3746. The OSA and the routers can implement most of the functions of the 3745 and 3746 at much lower cost. One of the alternatives to the 37453746 hardware is IBMs Communication Controller of Linux software package implemented on the mainframe. connections between the various nodes. Nodes that perform different network functions are said to act in different network roles. It is possible for given node type to act in multiple network roles. T4 node for example can perform an interconnection role between nodes at different levels of the subarea network hierarchy. When T2.0 or T2.1 is connected directly to T4 node the T4 node performs boundary function. When interconnecting nodes in different subarea networks the T4 node performs gateway function. Every T5 node in subarea network contains control point which in general manages the network resources. Management activities include resource control points domain and the range of its capabilities depend on the type of node in which it resides. Regardless of the node type control point performs the following common functions type 5 subarea node contains system services control point . An SSCP activates controls and deactivates network resources in subarea network. In order to control and provide services for its subordinate nodes an SSCP establishes sessions with components in the network. For example using directory of network resources an SSCP can use session to assist an application in locating partner and establishing communications session. An SSCP provides the following functions Manages resources on subarea network level in accordance with the Coordinates the initiation and termination of sessions between applications in separate nodes within its domain or across domains in cooperation with In subarea network every T5 and T4 node is assigned subarea number. The subarea number has to be unique in the SNA network. The SNA network is assigned network identifier referred to as NETID. All the resources in the same subarea network carry the same NETID name. In the same NETID subarea network you can have more than one zOS system that implements the SNA protocol. Every zOS system with VTAM that implements SNA is referred to as domain which is an area of control. Within subarea network domain is that portion of the network managed by the SSCP in T5 subarea node. When subarea network has only one T5 node that node must manage all of the network subarea network that contains only one T5 node is singledomain subarea network. When there are multiple T5 nodes in the network each T5 node may control portion of the network resources. subarea network that contains more than one T5 node is multiple domain subarea network. The SSCP can also set up and take down sessions with other domains through crossdomain network. Before applications in one domain can have crossdomain sessions with resources in another domain CDRM session must be established between the SSCPs of the two domains. For session between SSCPs to exist VTAM must know about all crossdomain resource managers with which it can communicate. You must define to VTAM its own crossdomain resource manager and all other crossdomain resource managers in the network. The crossdomain resource manager that represents the SSCP in your domain is called the host crossdomain resource manager. The crossdomain resource managers that represent the SSCPs in other domains are called external crossdomain resource managers. To establish crossdomain session that is session from resource located in one domain to resource connected in another domain you have to have physical connections in place to define the logical connection between the subarea nodes . transmission group is physical link or group of physical links with similar characteristics connecting adjacent nodes that is viewed as composite unit for routing SNA messages. These multiple parallel links protect against individual Each transmission group is identified by assigning the same number to each link in the group. Links can be assigned to Introduction to the New Mainframe Networking transmission group numbers 1 through 255. Several links between the same two subarea nodes might have the same TG number or the TG number might represent only one link. The physical path between two adjacent subarea nodes is an explicit route. An explicit route is an ordered set of subarea nodes and transmission groups along path between communicating subarea nodes including The transmission group used to connect each subarea pair along the route Forward explicit route Explicit routes originating in T5 or T4 are referred to as forward explicit routes and are numbered 0 through 15. Reverse explicit route Reverse explicit routes that terminate in T5 or T4 must use the same set of subarea nodes and transmission groups as their corresponding forward explicit route. They are also numbered 0 through 15 but they do not have to have the same explicit route number as the corresponding forward explicit route. The logical path between two subarea nodes is virtual route. Virtual route virtual route is bidirectional logical connection between two subarea nodes. At least one end of virtual route must be in subarea node that activates virtual routes. All hosts can activate virtual routes. Eight virtual routes numbered 0 to 7 can be defined between two subarea nodes. One or more virtual routes must be defined for each forwardreverse explicit route pair. virtual route places transmission priority on data traffic using the underlying explicit routes. Transmission priority The transmission priority identifies the priority of message units flowing over an explicit route during session. The three possible levels of transmission priority are 0 1 or 2 . In general highpriority messages are routed before lowpriority messages. Within specific transmission priority messages are routed on firstin firstout Route extension route extension is logical connection between subarea node and peripheral node. peripheral node uses local addresses for routing and requires boundary function assistance from an adjacent subarea node to communicate with nonadjacent subarea node. routes. ER0 is the physical path that connects HOSTA to NCPA using transmission group TG1 and NCPA to NCPB using transmission group TG15. The reverse explicit routes traverses the same physical path that is the same subareas and transmission groups. Virtual route VR0 is the logical connection between the endpoint subareas. VR0 connects HOSTA and NCPB. In our example VR0 maps onto ER0. VTAM routing definitions are static and you have to define the routes prior to add route definitions dynamically. type 5 nodes. The media connecting the two type 5 nodes is an Ethernet LAN. HOSTA has been assigned 1 as its subarea number and HOSTB is defined as To connect the two hosts you have to assign and define the explicit route and the virtual route. ER0 is assigned as the forward and reverse ER. ER0 uses the Ethernet OSA as the physical media. VR0 is the logical connection between HOSTA and HOSTB. The VR is mapped to ER0 on both sides HOSTA and HOSTB. resource in an SNA network is network accessible unit which is either an origin or destination of information transmitted by the transport network . You already read about control points and system services control points which are network accessible units. Other Physical units are components that manage and monitor resources such as attached links and adjacent link stations associated with node. SSCPs indirectly manage these resources through physical units. Physical units exist in subarea and type 2.0 nodes. The PU supports sessions with control points in type 5 nodes and also interacts with the control Receives and acts upon requests from the system services control point such as activating and deactivating links to adjacent nodes Manages links and link stations while accounting for the unique aspects of End users and applications access SNA networks through logical units which are the entry point through which users and applications access the SNA network. Logical units manage the exchange of data between end users to applications and application to application acting as an intermediaries between the two session partners on the two endpoint LUs. Because SNA is connectionoriented protocol prior to transferring data the respective logical units must be connected in session. In SNA hierachical networks logical units require assistance from system services control points which exist in type 5 nodes to activate session with another logical unit. session between logical unit and an SSCP is called SSCPLU session. Control information flows from the SSCP to LU session. session between two logical units either in the same node or in two different nodes is called an LULU session. The session between two LUs is used for application data flows. All node types can contain logical units. In SNA hierarchical networks the logical unit has sessions with only one control point in type 5 nodes and with logical units in other nodes. control point assists in establishing session between its managed LU and an LU it does not manage in different node. HOSTA and is in session with an application in HOSTB that is not the host that The control point assists in establishing the session between the two LUs and does not take part in the data transfer between the two LUs. SNA defines different kinds of logical units called LU types. LU types identify sets of SNA functions that support enduser communication. LULU sessions can can communicate only with another LU type 2 it cannot communicate with an LU The LU types that SNA defines the kind of configuration or application that each type represents and the hardware or software products that typically use each This is for application programs and singledevice or multipledevice data processing workstations communicating in an interactive or batch data transfer. An example of the use of LU type 1 is an application program running under IMSVS and communicating with 3270 printer. This is for application programs and display workstations communicating in an interactive environment using the SNA 3270 data stream. An example of the use of LU type 2 is an application program running under IMSVS and communicating with an IBM 3270 display station at which an end user is creating and sending data to the application program. data stream. An example of the use of LU type 3 is an application This is for transaction programs communicating in clientserver data processing environment. The type 6.2 LU supports multiple concurrent sessions. LU 6.2 can be used for communication between two type 5 nodes type 5 node and type 2.1 node or An application program running under CICS in zOS system communicating with another application program running under CICS in another zOS system. An application program in Microsoft Host Integration Server or AIX Communications Server communicating with CICS in zOS system. LU types 1 2 and 3 are referred to as dependent LUs. An SSCPdependent LU requires assistance from system services control point in order to activate an LULU session therefore it requires an SSCPLU session. All non6.2 LUs are dependent some type6.2 LUs are dependent and some are independent. type 2.0 node supports only dependent LUs. type 2.1 node can support any combination of dependent and independent LUs. LU 6.2 can act either as dependent or independent LU. An SSCPindependent LU is able to activate an LULU session without assistance from an SSCP therefore it does not have an SSCPLU session. Only type6.2 LU can be an independent LU. type 2.1 node supports independentLU protocols to other directlyattached independent LUs in type 2.1 nodes. In an SNA network messages flowing through the network contain either request or response. Requests are message units that contain Enduser data called data requests. Examples of enduser data include payroll data personnel data insurance policy data and inventory data. Network commands called command requests. Network commands initiate and terminate sessions and control communication between network accessible units. Responses are message units that acknowledge the receipt of request. Responses are either positive or negative. Positive responses indicate that request was received and is acceptable. Negative responses indicate that request was received but is unacceptable they also contain error codes that explain why the request is In TCPIP the unit that is transferred through the network is called packet. are used to route the packet and manage the TCP session. In SNA the unit that flows in the network is called path information unit . As with every networking protocol messages are routed to an address in the network. In an IP network the address is the IP address of the host and the address is assigned either dynamically using DHCP or it is static IP address. In SNA the messages are sent to network accessible unit . The addresses are assigned to the network accessible unit by the control point when the physical units or logical units are activated. The path information unit carries three fields that are used by the network different formats by form indicator type. uses the different FID types to route data between different types of nodes. Two FID2 This format is used to route data between subarea boundary node and an adjacent peripheral node or between adjacent APPN or LEN nodes. FID4 This format is used to route data between subarea nodes. and the structure of the addressing field for these two FIDs. In FID4 the network accessible unit is formed from the subarea and element address. In FID2 used by T2.0 nodes the element address is 8 bits long limiting the number of elements to 255. T2.1 also uses FID2 and handles the destination address field and origin address field as one 16bit field. Each request that an NAU sends also contains request unit . request unit is field of variable length that contains either enduser data or an SNA command . Data RUs contain information that is exchanged between end users. Command RUs control the operation of the Introduction to the New Mainframe Networking in the associated response unit. bit called the requestresponse indicator The receiving NAU indicates whether the response being returned to the request sender is positive or negative by setting single bit. response unit contains information about the request. Positive responses to command requests generally contain 13 byte response unit that identifies the command request. Positive responses to data requests contain response Because SNA is connectionoriented protocol before two entities in the network can communicate connection has to be set up between them. End users gain access to an SNA network through logical units and exchange information over LULU sessions. Once network resources are active LULU sessions can be initiated. Either of the participating logical units can initiate an LULU session. network operator can initiate an LULU session. system definition can specify that an LULU session be initiated automatically when certain resources become active. Typically one of the participating logical units initiates an LULU session. The two logical units that communicate with each other over session are called In SNA every LU can initiate session with partner LU. The LU sends session request to its control point. The control point locates the partner LU either within the domain of the control point or in another domain or in another network. In an SNA subarea after the partner LU is located by the SSCP the virtual route is chosen and BIND message is sent by the LU and the session is started. The virtual route is mapped to the explicit route. LULU session initiation generally begins when the session manager in an LU submits sessioninitiation request to the appropriate control point. In subarea network it can be either the system services control point controlling the LUs domain or in the case of type 2.1 peripheral node the LUs local control point . sessioninitiation request specifies the requested session partners network name and mode name. The mode name identifies which set of session parameters that the requesting logical unit chooses for the session. In subarea network the mode name is associated with the parameters through mode table created during system definition. Using the specified set of session parameters the control point builds BIND image. The control point transmits the BIND image in control initiate request to the primary logical unit. The primary logical unit is the LU responsible for activating the session. The PLU activates the session by sending bind session request to the secondary logical unit . The SLU then returns BIND response to the PLU. response unit flows between the session partners and the session is started. The session parameters set up by BIND include the maximum message size that session partner can send or receive the protocol used between the session partners the window size and more. By comparison TCP is based on clientserver relationship. The client initiates the connection with the server. The TCP server cannot initiate connections with the client instead it listens on specific port for connection requests. The client must know the IP address of the host where the server exists. In an SNA network different classes of service can be specified based upon the needs of the end users in the network. SNAs class of service is similar to TCPs Type of Service . The two methods handle the prioritization of messages in the network. class of service designates the characteristics of session. It includes such characteristics as security transmission priority and bandwidth. The process of defining class of service is an activity that must take place before route for During session initiation the class of service for the session is obtained from the sessioninitiation request or derived from mode name specified in the sessioninitiation request. The route then selected for the session depends on the class of service for the session and available routes. VTAM definitions are located in two data sets referenced by two DDNAME statements in the VTAM started task stored in SYS1. PROCLIB One data set stores text definitions of the SNA network and is referenced by the VTAMLST DDNAME. The definitions include the various nodes in the network routing and hardware components such as channeltochannel connectors and OSA cards. The second data set stores load modules and is referenced by the VTAMLIB DDNAME. The files stored are initially coded using VTAMsupplied macros than assembled and linkedited into the VTAMLIB data set. Examples of these binary definitions include LOGMODE tables and class of service tables. Start options provide information about the conditions under which VTAM runs. They also enable you to tailor VTAM to meet your needs each time VTAM is started. Many options can have defaults specified as start options thus reducing the amount of coding required. Many start options can be dynamically modified and also displayed. You should be aware that some start options cannot be dynamically modified and require that VTAM be recycled. complete list of start options is listed in zOS Communications Server SNA Resource Definition To use start option list create data set member named ATCSTRyy and put it in the VTAMLST partitioned data set. The yy value can be any two characters or and therefore different VTAM is started from the zOS console or during zOS system startup with the following command START VTAMLISTyy. When VTAM initializes LISTyy determines which option list to use. For example if you specify LIST01 VTAM uses ATCSTR01. VTAM always first attempts to locate ATCSTR00 regardless If ATCSTR00 does not exist VTAM sends warning message to the operator. To avoid receiving this message create an ATCSTR00 file that contains only comments or start options that are always used for that particular VTAM. The SSCPID start option provides VTAM with unique numeric identifier. The SSCPID value is used by some physical units to identify the VTAM with which it is in session. If you plan to expand or incorporate singledomain network into larger network be sure that the value of SSCPID is unique for each host. The SSCPID value you specify must also be different from the SSCPIDs in other networks that can be in session with this host. The SSCPNAME start option provides unique name for VTAM. This option is required for singledomain network but is primarily used in multipledomain and multiplenetwork environments to identify particular VTAM. The SSCPNAME option must be different from the HOSTPU start option that identifies the physical unit within VTAM. Note The SSCPNAME should match the name that is coded in the crossdomain resource manager major node for this The NETID start option provides VTAM with the network identifier. If you connect your VTAM to another network the HOSTSA specifies the subarea number of this VTAM. The HOSTPU start option is recommended for identifying VTAM to the network. Use the HOSTPU start option to assign userdefined name to the VTAM host configuration list specifies the resources that are to be activated when VTAM is started. The names of the resources that are activated when VTAM starts should be placed into an ATCCONxx member in the VTAMLST partitioned data set where xx is any two alphanumeric characters. The value xx can then be used on the CONFIG operand of the VTAM START command or on the CONFIG start option in your start option list to specify which definitions are to be is ATCCON01 you can specify CONFIG01 on the VTAM START command. Examples of resource definitions that can be included in configuration list are Minor nodes defined in previously listed major nodes In addition to specifying start options and coding configuration lists youll need to identify resources in the network to VTAM. Depending on your network you might need to define combination of the following resources 1. Application programs . Every SNA application program like CICS IMS TSO inhouse VTAM applications and applications developed by other companies is defined to . When the application initializes it connects to VTAM and informs VTAM that it is ready to accept requests and service the LUs in the network. 2. Network control programs and peripheral nodes . The NCP is the software residing in the communication controller. The first task the systems programmer performs is the definition of all the lines the peripheral equipment links that connect to other SNA domains and the explicit routes that traverse the NCP and virtual routes that originate in the NCP. During VTAM initialization VTAM connects to the NCP activates the resources in the NCP and determines if load of the NCP is PUs and LUs are defined either in the NCP or in VTAM. PUs are connected to the mainframe by any of the following means . Directly attached through channel . Switched lines to the NCP . LANconnected either directly to an NCP attached LAN or directly to the PUs connected through dialed lines and LANconnected PUs are defined to VTAM as switched major nodes. The definition includes the PU and the LUs associated with that PU. Because switched connection is casual and unauthorized people can dial the Introduction to the New Mainframe Networking mainframe or connect to LAN jack the switched major node includes some very basic security definitions that can be used to identify the switched major node during the connection phase. 4. If your network is multiple domain network additional definitions are CDRM defines the logical connection to an adjacent mainframe. The CDRM connection is used to send and receive control information CDRSC is an SNA resource that exists in the domain of another host and sometimes sets up or actually maintains session with local resource in the domain of the host where the CDRSC is defined. The ADJSSCP is used to control the search for SNA resources in multidomain SNA network. The ADJSSCP determines the order the search is performed or in other words which mainframe is queried first to determine if it owns the resource and the subsequent hosts if the query was not successful. Any of these resources can be predefined to VTAM using static definition statement. For example applications are predefined using an application major node and application statements switched PUs are predefined using switched major node for the PUs and LUs. In addition many VTAM resources can be dynamically defined as VTAM learns of them. switched PU may be defined when it dials in. Not all resources need be initially defined to subarea nodes. There are several methods by which resources may be dynamically defined to the network. Although static and predefined resources are burdensome many mainframe installations are reluctant to allow dynamic definition of PUs or LUs. The rationale for preventing dynamic definition is security. When every resource is predefined the installation has more control over who acceses the mainframe. Another method for dynamic but controlled definition is called dynamic reconfiguration which allows system programmer to add or delete peripheral nodes dynamically. The resources are added or deleted through VTAM major node configuration statement or by using VTAM commands entered from the console. Following are examples of dynamic definitions of SNA resources and how VTAM determines the credentials and attributes for dynamically defined resources. Dynamic definition of independent LUs allows an independent LU to activate an LULU session with an LU in subarea network without prior definition of the independent LU to the subarea network. The SSCP controlling the independent LU as determined from the bind request sent by the independent LU. Subsequently other LUs in the network can activate LULU sessions with the independent LU because the independent LU is then known to the subarea Dynamic definition of dependent LUs allows dependent LUs to be defined using information that specifies how many dependent LUs can be on switched or nonswitched lines. Dynamic definition of switched resources uses information from the exchange ID message exchange for switched PUs to allow switched PUs and their associated dependent LUs to be dynamically defined. As with dynamicallydefined independent LUs other LUs in the network can activate LULU sessions with the dynamically defined dependent LU because the dependent LU is then known to the network. Dynamic PU definition dynamically defines an adjacent link station . This method like dynamic definition of switched resources relies on information obtained from the XID to create dynamically definition of the resource. After the adjacent link station is defined it may be used for connectivity to its independent The siftdown effect enables you to code an operand on higherlevel node so that you do not need to recode it on each lowerlevel node for which you want the same value. As result the siftdown effect greatly simplifies the coding process. zOS Communications Server SNA Resource Definition Reference identifies and describes the definition statements and operands to which sifting network. The network consists of two mainframe hosts named HOSTA and HOSTB. The two mainframes are connected using Ethernet LAN implemented An SNA network implements the IBM proprietary networking architecture. Subarea networking carries out hierarchical network paradigm. The information about the network resources and the definitions are stored in the mainframe that is located at the top of the hierarchy. The hierarchical structure of the network follows the organizational structure of many enterprises and thats one of the reasons that SNA was accepted and implemented by many enterprises. In an SNA subarea network the routes between subarea nodes and almost all the resources are predefined. In TCPIP the unit that is transferred through the network is called packet. packet and manage the TCP session. In SNA the unit that flows in the network is SNA in general has robust data flow control that permits constant flow of data to and from the network and prevents congestion in network resources. Although today TCPIP is the dominant networking protocol in almost all organizations SNA applications will exist for the foreseeable future due to the switched major node 1. Why was SNA built as hierarchical and robust network 2. Name the node types and their role. 3. What is the major difference in setting up session between SNA and TCP 4. What is the difference between an explicit route and virtual route 1. How is routing performed in an SNA subarea network 2. What is the importance of assigning class of service in SNA and the equivalent priority scheme in TCP 3. What member in SYS1. VTAMLST determines which major nodes will be activated during VTAM startup How does VTAM determine the member name that will be used 4. What parameters in ATCSTRxx determine the host subarea and the SSCP received. You will be using the SDSF feature of TSO to communicate with the system console. It is common practice that the TSO SDSF system log is reached from the main menu by entering . LOG in the command line. forward slash . This is not necessary when you enter commands from the Name the major nodes that have been activated. Note This is snapshot of the currently active major nodes. You do not know whether these nodes were activated at VTAM startup or manually by an to HOSTC you are asked to define two virtual routes. VR1 that maps to ER1 is direct connection from HOSTA to HOSTC. VR0 travels through HOSTB when it is used for data transfer from HOSTA to HOSTC and vice versa. ii. Which explicit routes are used on the path 5. Name another VTAM command that is used to display the routes from one subarea to another. How can you control which explicit route or virtual route is In the mid1980s SNA subarea networking was the dominant networking protocol used for data processing. Its robustness management tools and Explain how to define VTAM APPN networks. Explain how to define combined VTAM and APPN network. Introduction to the New Mainframe Networking predictable response time attracted many organizations to SNA and they used SNA in their missioncritical applications. The major drawback of SNA subarea network was the requirement to provide static definitions for most SNA resources. At the same time intelligent workstations were proliferating. The hierarchical nature of subarea SNA was not suitable for these workstations which required peer connections and dynamic definitions. Another criticism users of SNA subarea networking had addressed session continuity. Although subarea networks use alternate routes failure of hardware or software components along the route causes the sessions along the route to fail. Although sessions can be reestablished over alternate routes the process affects the endusers session availability. IBM developed APPN to reduce to minimum the task of defining SNA resources and routes. The definitions are limited to the local APPN node where one defines the name assigned to the resources the attachment to be used and the node type. local network and searches for resources in the network and adjacent networks. When establishing session APPN selects the best available route between the session partners. Some APPN nodes implement intermediate session routing. Nodes that support intermediate session routing are used along the session path to route session data between the two session endpoints. The initial APPN implementation did not address the session continuity problem and when organizations started to implement APPN they realized that the performance of the intermediate session routing function was poor. IBM went back to the drawing board and developed an extension to APPN called high performance routing . HPR introduced the rapid transport protocol and automatic network routing . These two added functions address session continuity and the switched to an available route without disrupting the session. The end user is not aware that failure took place along the path of the session. APPC also known as LU 6.2 was introduced by IBM in 1982 to address the exchange of data between two peer programs that are located either in the same computer or in two systems connected by the network. APPC is an architecture that defines set of networking protocols and an application programming interface . Because APPC is networking protocol it does not address the individual programming language syntax. The APPC API is described as an abstract presentation of the various API functions called verbs. The verbs define the sequence and order an application program has to follow in order to communicate with peer application programs. The implementation of APPC in the various programming language converts the abstract API to functions or callable subroutines that conform to the syntax of the programming language. To avoid the hierarchical nature of the mainframe SNA and allow small computers like programmable workstations and iSeries to use APPC the type 2.1 node was introduced. type 2.1 node is peeroriented peripheral node that attaches to mainframe communication controller or another peripheral node. For refresher on basic node types see 7.3.1 SNA APPN was designed to support APPC in T2.1 nodes and the exchange of control information between two APPN nodes uses APPC protocol. An Advanced PeertoPeer Networking network is composed of group or groups of connected T2.1 nodes. T2.1 nodes provide sessions between LUs and peerlevel connectivity using the APPC protocol. Unlike hierarchical SNA subarea networks sessions between two APPN or lowentry networking nodes can be established without involving mainframe in the session setup. eliminating the need to predefine resources owned by other nodes Maintain knowledge of APPN network topology and use this information to select the best available path to route sessions between SNA resources thereby eliminating the need for As their names imply APPC or Advanced ProgramtoProgram Communication deals with programs while APPN or Advanced PeertoPeer Networking deals with networks. APPC defines the rules of how programs exchange information. These rules do not deal with the details of network setup and routing. It is APPN that defines how APPC traffic gets from one point to another in network. reasonable comparison between APPC and APPN is the difference between person using the telephone and the services the telephone company offers. For example when you want to call someone you look up the telephone number and then enter it. Both parties identify themselves and the exchange of information begins. When the conversation is finished both parties say good bye and hang up. This protocol although informal is generally accepted and makes it much easier to communicate. APPC provides the same functions and rules only between application programs instead of people. An application program tells APPC with whom it needs conversation. APPC starts conversation between the programs so they can exchange data. When all the data has been exchanged APPC provides way for the programs to end the conversation. APPN provides networking functions similar to those provided by the telephone companies. After dialing telephone number the telephone network routes the call through trunks switches branches and so on. To make the connection the network takes into consideration what it knows understanding the details of the network. person is able to talk on the telephone to another person no matter where they are or no matter how the call APPN provides these functions for APPC applications and their data. It computes routes for APPC communication through the network dynamically calculating which route is best. Like the telephone company APPNs routing is done transparently. APPC applications cannot tell whether the communications partner in the APPN network is located in the same computer one office away or in another country. Similarly if someone moves within the same city and takes their phone number the phone network handles the change with no other user The building block for an APPN node is the T2.1 node. Be aware that T2.1 by itself does not provide any APPN functionality additional software is required to make T2.1 node an APPN node. Depending on the software that implements APPN in T2.1 nodes the node can case of an isolated pair of lowentry networking nodes to large APPN network. Using lowentry networking or APPN protocols any node can control the establishment and termination of sessions. The following node types can be implemented by T2.1 nodes To ease the migration from subarea networking to APPN in the mainframe the following nodes types can be implemented in mainframe T5 and T4 nodes also support lowentry networking and APPN protocols and are fully compatible with T2.1 nodes in these contexts. They also introduce product features of their own related to enhanced subareaAPPN interchanges. When subarea node implements either APPN or lowentry networking protocols it acts as T2.1 node and can still implement depending on VTAMs definitions the subarea T5 and T4 functions. Lowentry networking nodes also referred to as T2.1 nodes were introduced in the mid1980s to address the requirement for peer networking. The lowentry networking node was the first stage of the APPN evolution. The T2.1 node allows peertopeer connection and provides the physical and sessionlevel connectivity required to support logical unit type 6.2 . T2.1 nodes use protocols with reduced system definition requirements. For example link station roles are negotiated as primary or secondary during the connection phase instead of as in the case of subarea networking being predefined. Lowentry networking nodes do not implement control point. With lowentry networking you must predefine every partner resource along with the first hop toward that resource . This predefinition requirement is the primary drawback of lowentry networking nodes. nodes represent networking infrastructure like LAN serial lines or frame relay ATM. The T2.1 nodes are lowentry networking nodes without any APPN established between nodes and and nodes and . Because node does not include control point or any APPN functions it cannot route sessions and therefore node and node cannot establish LU LU sessions. Prior to APPN when only T2.1 nodes were implemented in SNA transferring data from node to node required either direct physical link between node and node or an application program in node that relays messages between APPN end nodes implement subset of the full APPN functions and rely on one of the network nodes they are attached to for accessing the network locating resources and providing routing services. All end nodes provide peer environment for LULU sessions where one LU resides in the local end node. The end node is limited in what it can do on its own. It requires the cooperation from an adjacent network node server with which the end node has established CPCP sessions. The adjacent network node server assists the end node in locating session partners choosing session paths and routing the bind to establish sessions. APPN end nodes also can register their local LUs and local topology to their network node server as well thereby eliminating the need to send these EN transmission group vectors on every APPN locate requestreply. By reducing the number of searches sent to an EN the EN can dedicate more of its resources to more productive work. End nodes do not participate in network topology exchange but they do maintain their local topology. An APPN end node can have links to multiple nodes including multiple network nodes but may have CPCP sessions with only one network node at time . Multiple attachment points between an end node and the APPN network may be desirable for increased throughput and high network availability. Attachment to multiple network nodes allows the end node to switch to different network node server if its original network node server fails or connectivity to it is lost. An APPN end node can attach to any lowentry networking or APPN node. An end node is not required to have the same NETID as its network node server or any adjacent node. An APPN network node implements full APPN functionality and services. collection of connected network nodes comprises the APPN backbone. An APPN network node manages and supports its own resources and that of its served APPN end nodes and low entry networking nodes. the APPN network node. LU5 through LU8 reside in different T2.1 nodes. Only LU5 and LU6 in the APPN end node are serviced by and known to the APPN network node. To make the network node aware of the existence of the LU7 and LU8 these LUs must be defined manually to the network node because they are owned by lowentry networking node. If any LU residing in the lowentry networking node must establish session to any of these LUs LU1 through LU6 must be predefined on the lowentry networking node. An APPN network performs intermediate routing of data on sessions that The diagram illustrates how the session path between LU7 in the lowentry networking node and LU5 in the end node traverses the network node and eventually is routed through the network node. The APPN network node performs intermediate session routing for the data transferred on the LULU session between LU5 and LU7. An APPN network node provides network searches network topology management session route selection and services to its own LUs and end nodes attached it. An APPN network node can be session endpoint or an intermediate node on session path. One deficiency of APPN is that searches for resources can flood the network and eat up too much bandwidth. network node server is one of the functions that can reduce the searches in the network. The term network node server refers to network nodes role in providing network services for specific APPN end nodes APPN was designed for implementation on wide range of hardware platforms and operating systems including programmable workstations desktop computers UNIX and Windows servers and the IBM mainframe. The APPN function and services for the various platforms are different depending on the APPN node role. Therefore APPN architecture defines basic and optional sets. All APPN nodes must adhere to the basic set of functions according to their node type and they can implement one or more option sets. The requirement that an APPN node must implement the basic set defined for its role assures that the node can establish APPN sessions with its peer node. node that implements the basic set can communicate with nodes that implement additional option sets. The two nodes learn the optional capabilities of other nodes in the network when they connect to each other andor when they exchange network topology. If an optional set is implemented in node the complete optional set should be implemented. There is no ability to implement subsets an optional set. The following specialized network node types are examples of optional sets of central directory server is implemented only in network node. It provides more extensive functionality than the directory services in basic When network node receives search request it checks its database for the resource. If it does not find the resource in its database it sends the request to central directory server if one exists in the network. When the central directory server receives search request it checks its database for the resource. If the central directory server that received search request locates the resource in its own database it verifies the information and If it does not find the resource in its database and there are other central directory servers in the network it sends the search to the other central directory servers only. If the central directory server receives positive reply from any of database with the information and notifies the originating network node of the If the central directory server receives negative replies from all the other central directory servers it initiates broadcast search. The search steps performed in an APPN server through the topology database. At any given point in time every network node knows where every reachable central directory server exists in the network. Independent SNA networks might have requirement to be interconnected. For instance mergers and acquisitions might require interconnecting two SNA networks or two business partners might need to exchange information. Two subarea networks can be interconnected through SNA network interconnection . SNI is an SNAdefined architecture that enables independent subarea networks to be interconnected through gateway. An extended border node is network node capable of multiple APPN network connections and it can maintain CPCP connectivity with network networks connected using extended border nodes. APPN topology information does not cross the extended border node connection or APPN subnetwork boundary but search requests can and an LULU session can be set up. An APPN subnetwork boundary is assumed when an extended border node is connected to network node with Branch extender is an extension to the APPN architecture that allows an APPN node to appear as network node to the downstream end nodes and lowentry networking nodes and as an end node to the wide area network see Introduction to the New Mainframe Networking broadcast search flows between the WAN and the branch office. The operations staff of the mainframe is not interested in whether workstation in the branch is booted or powered off. The branch extender isolates the mainframe from the networking equipment in the branch. The topology and directory server of the network node part of the branch extender store the information about the branch networking equipment. The information is not propagated to the mainframe APPN databases. As the name implies the specialized VTAM nodes are implemented only in the mainframe. These nodes enable the mainframe to connect directly to both An interchange node resides on the border of an APPN network and subarea to enable the integration of the two types of networks. Because an interchange node can convert session requests from one protocol to the other and can provide intermediate routing it can establish sessions from one type of network An interchange node combines the function of subarea node and network node. It controls resources and functions as network node in the APPN network and as an SSCP and crossdomain resource manager in the subarea network. All of the characteristics described for network nodes and Uses subarea path definitions to determine routes within the subarea network Uses the topology database to determine routes within APPN networks Uses both SSCPSSCP and CPCP sessions to communicate with other Has subarea number and is defined as network node NODETYPENN Can own and activate network control programs The interchange node communicates network control data by using SSCPSSCP sessions with other subarea nodes and CPCP sessions with other APPN nodes. To enable it to participate in the subarea network it is defined with unique subarea number and requires subarea path definition statements. It can be connected to other APPN nodes lowentry networking nodes and subarea Many IBM mainframe installations implement interchange nodes because both APPN and subarea components must coexist in most mainframe networks. Those installations still have subarea networking but are starting to implement VTAM determines the node type of the mainframe using two parameters in VTAMs start option. If HOSTSA is set to subarea number and NODETYPENN an interchange node is implemented by VTAM. If HOSTSA is not specified and CONNNTYPENN or EN VTAM implements an APPN node. Because Network Control Program does not have control point NCPs cannot function as APPN nodes by themselves. Instead NCPs work with their owning VTAM to present the appearance of single APPN node to other APPN nodes. This collection of VTAM network nodes and its owned NCPs is referred to as composite network node note the The composite network node can have either APPN functions only or both APPN and subarea functions. Existing subarea protocols are used within the composite network node for communication between the T5 node and its T4 nodes. APPN protocols are used to communicate with other APPN network nodes and end nodes. The T4 node provides boundary function services for attaching other APPN nodes. The rationale for composite network nodes is to ease the migration from subarea network to APPN. With composite network nodes an installation can preserve its current hardware while still providing migration path to APPN networking. composite network node configuration provides the functional combination of single T5 node. All the T4 nodes that the composite network node owns appear as one logical APPN network node to other lowentry networking and APPN nodes to which it is interconnected. acting as an interchange node. The interchange node supports SSCPSSCP sessions with other VTAM nodes as well as CPCP sessions with adjacent APPN network nodes and end nodes. This enables the interchange node to use both APPN and subarea data flows to locate LUs. From the APPN nodes viewpoint LUs owned by subarea VTAMs combines the function and roles of an APPN end node and subarea node and resides on the periphery of combined APPN Does not perform intermediate session routing or interchange node functions in combined APPNsubarea network. Uses CPCP and SSCPSSCP sessions to communicate with other nodes Can attach to NCPs over APPN or subarea links but cannot activate NCPs Has subarea number defined on the HOSTSA start option Like data host in subarea network migration data host is dedicated to processing application programs and does not control network resources. It also participates as crossdomain resource manager in the subarea network. The migration data host also functions as an end node in the APPN network. All of the characteristics previously described for end nodes apply to migration data hosts. To enable the migration data host to participate in the subarea network it is defined with unique subarea number and supports subarea path definition The migration data host communicates network control data by using SSCPSSCP sessions with other subarea nodes and CPCP sessions with its network node server. It can be connected to other APPN nodes lowentry networking nodes and subarea nodes. To perform directory services and topology and route selection services adjacent APPN nodes throughout the APPN network use the pair of CPCP sessions to exchange network and control information. CPCP sessions are always logical unit type 6.2 sessions. Using this session type contention situation could arise if both session partners attempted to allocate conversation and exchange data at the same time. This situation is resolved by defining one of the sessions the contentionwinner session and the other the contentionloser session. The primary session partner refers to its session as the contentionwinner session and the secondary session partner refers to that same session as the contentionloser session. The contention winner side of the session is the one that initiates the BIND. CPCP sessions are only established between adjacent APPN nodes and always use the CPSVCMG logon mode name and APPN class of service. An end node establishes CPCP sessions with an adjacent network node. The network node that has CPCP session with the end node is referred as the network node server . End nodes can have active links to many network node or end nodes at the same time but can establish CPCP session pair with only one network node server at time. If CPCP session fails an end node can immediately choose another adjacent network node to act is its network node server. Note the End nodes never establish CPCP sessions with other end nodes. The end node is the node that always initiates the activation of the CPCP End node can register their local topology with their network node server. To perform directory services topology and route selection services adjacent APPN nodes throughout the APPN network use the set of two CPCP sessions to exchange network and control information. network node or composite network node can establish CPCP sessions with any network node or composite network node to which it has an APPN direct link that supports CPCP sessions. CPCP sessions between two network nodes are used to perform searches for resources exchange topology information and can be used to register resources with central directory server. After an APPN connection has been established identification information is exchanged between the nodes and CPCP sessions can be established between the control points in the directly attached nodes. After the CPCP sessions are established the two nodes exchange CP capabilities which indicate the level of network services provided by the control point. Each APPN node has one network ID and one CP name assigned. The network ID identifies the network to which the node belongs and the CP name is unique within that network. The network ID and CP name are defined in the APPN node at the time of system definition. Within an APPN network all interconnected network nodes share common network ID. CPCP sessions are allowed only between network nodes that have the same NETID unless one or both network nodes are defined as extended border nodes An end node can use the same network ID as its network node server or it can use different network ID. APPN provides automatic network topology and directory support within APPN networks that simplifies network definition and permits dynamic selection of network routes. Some of the functions of APPN are topology and route selection services and distributed directory searches as described here Topology and route selection services selects the best route to access remote LU based on set of userspecified criteria. Using the properties of the nodes and links in the network that are maintained in network node topology database network node server calculates the best route from the local control point of the primary LU to the control point of the secondary LU according to the class of service selected by the LU initiating the session. Topology and routing services is responsible for three functions 1. Maintaining the topology database 2. Maintaining the LOGMODEtoAPPNCOS mapping table Distributed directory searches determine the current LU with which local LU can establish session. The information collected during the directory search is stored in the network nodes directory database. You can compare the directory services database to telephone book or address book in which you look up name and determine its An SNA session is the logical connection between two LUs. The LU that originates the session is named the originating LU and its session partner is the destination LU . As its name implies the originating LU initiates the session by sending BIND to the destination LU. The topology database is like map in which the APPN network nodes furnish destination logical unit DLU and decide what the best route is between them based on your driving requirements . That is do you want to take the high speed route the scenic route or the most direct route The BIND sent by the originating logical unit uses the best route that is available at the time the session is setup. The topology database consists of local topology database unique to node and network topology database whose entries are replicated across all network nodes in the same topology subnetwork. The topology database stores and maintains the nodes and the links in the networks and their characteristics. component called the topology database manager creates and maintains the topology database. An APPN network node provides route selection services to itself and to its client end nodes. It maintains network topology database that has complete and current information about the topology subnetwork or NETID subnetwork in which it resides. This information includes all the network nodes in the subnetwork and their node characteristics and all the links interconnecting these network nodes and their link characteristics thus creating connectivity map showing the arrangement of nodes and links. network node uses the network topology database to compute routes for sessions that originate at LUs in its domain . Each route that network node computes is the current leastweighted route from the node containing the origin LU to the node containing the destination LU for the requested class of service. To determine an appropriate path through the network the route selection algorithm first assigns weights to transmission groups and nodes. These weights are scalar values for each node and transmission group based on the relative significance of the characteristics for the requested class of service. Note The primary logical unit is the LU that sends the BIND. An end node maintains only local topology database while network node maintains both local topology database and the network topology database. The network topology describes the network nodes and transmission groups between network nodes . All network nodes have an identical copy of this data and to inform the NNS of its links to other end nodes and network nodes. An APPN network node provides directory services to its locally resident LUs and to the LUs in its client end nodes. It also assists other network nodes in the directory database. The network node and the collection of resources it serves are called network node domain. For an LU located in network node the local directory maps an LU name to the CP name of the network node where that LU is located. For an LU located in an end node the directory includes the CP name of the owning end node and the end nodes network node server. Because APPN end nodes do not maintain directory database an end node that does not currently have CPCP sessions with network node server cannot establish sessions with resources located on other nodes unless these resources are predefined in manner similar to lowentry networking partner LU definitions. lowentry networking node or APPN end node maintains local directory containing entries for locally resident LUs. An APPN end node that does not currently have CPCP sessions with network node server also maintains entries for those resources residing in an adjacent node connected as peer Entries in lowentry networking node are defined manually. Because it does not support CPCP sessions lowentry networking node sends an LULU session activation request over the link associated with the predefined session partner. If the destination LU is located in node that is not adjacent to the lowentry networking node it must be connected to network node. This network node can locate the resource and select the appropriate route. An APPN end node has an alternative to the lowentry networkings complete directory of all LUs with which it initiates sessions it can initiate locate search into the APPN network to find desired LU by invoking the services of its network node server. Because the network node server identifies the route to be used for the requested session in its search reply the resulting LULU session is not required to traverse the network node server. Each network node starts with topology database containing only itself. When it joins the network network node receives copy of the current network topology database through topology exchange with another network node in the network. As long as it remains connected to the network its local copy of the routing resources within the network to maintain consistency with the topology databases of all other connected network nodes. NN3 and NN4. NN1 is about to join the APPN Network. Prior to connecting NN1 to the APPN network every network node has an identical representation of the network in its topology database. NN1 starts with topology database containing transmission group state or characteristic changes or when CPCP sessions are TDU is originated by NN1 and is sent from NN1 to NN2. NN2 propagates TDU to all its adjacent network nodes . The adjacent nodes that received the TDU propagated TDU to their adjacent network nodes. NN2 sends TDU that describes the entire network topology including the transmission group from NN2 to NN1. As you can see NN3 receives two identical TDUs one from NN4 and one receives an identical TDU NN3 uses the resource sequence number as resource sequence number is associated with each transmission group and topology database . If the resource sequence numbers are equal or the resource sequence number Only new information needs to be forwarded. There is also the flow reduction sequence number as explained here. Network nodes assign flow reduction sequence number values to topology resource records when the records are modified or newly created. The resource records allows network node to track the most recent time the local partners as well as the most recent time each resource record in its topology The FRSN identifies how much of the topology database must be exchanged when the network node rejoins the network. database that exists in all network nodes once topology database has reached Among the functions that directory services in an APPN network node provide is following steps until one succeeds known directed search is sent to the suspected destination. directory database perform broadcast search of served end nodes. If the destination resource still has not been located this network node performs either central directory server search or broadcast search of the native network. If this network node is not central directory server then the topology database is used to determine if central directory server exists in the network. If central directory server exists then directed search is sent to central directory server is then completely responsible for locating the resource in the native network and any attached subarea networks. This is done by first performing broadcast search of the native network and if necessary serial search of all interchange nodes in the network to allow these ICNs to search their attached subarea network. If this network node is central directory server or if no central directory server exists in the network or if the directed search to central directory server fails to reach the central directory server then this network node is responsible for locating the resource in the native network and any attached subarea networks. This is done by first performing broadcast search of the native network and if necessary serial search of all interchange nodes in the network to allow these ICNs to search their attached subarea If the destination resource still has not been located and this node is border node perform serial search of adjacent APPN networks by sending directed Broadcast search database information has failed or there is no database information for the requested resource. broadcast search does not use database information broadcast search is sent to every adjacent network node at the same time. Each of the adjacent network nodes then forwards the broadcast search to all other adjacent network nodes and so forth. After propagating broadcast search to all adjacent network nodes each network node also searches all its client end nodes to determine if the target LU resides within the domain of the network This process allows the entire network to be searched. When the search reaches the network node serving the destination resource that node sends back positive reply to the first search request it receives. ENA sends directed search to its network node server . Because the directory database of NN1 has no information about LUB NN1 sends network broadcast search to all end nodes served by NN1. If LUB is not located in the NN1 local domain the broadcast search is propagated at the same time to all adjacent network nodes. After propagating the broadcast search network nodes search local node and Since LUB is located in ENB ENB sends positive reply to the broadcast search request. The reply is returned along the same path as the request. The directory both LUB and LUA. directed search is sent by directory services to the node recorded as the owner of the requested LU to verify the information. directed search can be sent to network node server from an end node and to an end node from network node server. directed search can also be sent from network node to central directory server when the network node does not have information on the Directed searches are always sent to the network node server of the destination resource. This is because the topology database does not allow network node to compute Locate path all the way to an end node in the domain of another network node . directed Locate search request is request that is sent along predefined path from one network node to another network node. The origin network node calculates path of CPCP session hops to the target network node and appends the routing information to the search. Each network node along the path relies on that routing information for choosing the next hop and ensuring that the search travels directly to the destination network node. ENB which sends locate request to its network node server. The sends directed search. Because LUA is located in an end node the directed search is addressed to ENAs network node server . The search contains the path over which the directed search should be sent. Each network node along the path forwards the directed search to the next network node. The last network node on the path forwards the search to The function and services of the central directory server are discussed in another search that illustrates the role of central directory server. the APPN network. LU2 which resides in NN2 wants to start session with they just forwarded the search. For the broadcast case NN2 and NN3 must interpret the search in order to determine if the target LU is in their domain. The reason these intermediate network nodes do not cache this information is because they may never need it Because LUB is not stored in the directory database the only way to locate LUB is to initiate another broadcast search. Instead of initiating broadcast search NN2 sends directed search to the central directory server. The central directory server is now completely responsible for locating LUB in the native network. Sending directed search queries to other central directory servers in the If broadcast was necessary then the central directory server will cache the central directory server builds up knowledge of all of the resources that existing the network for this search Here there is one more step. Since the positive reply was sent to the central directory server the central directory server has to inform NN2 which initiated During session establishment the network node server of the origin LU refers to the topology database to calculate and select the current best route through the APPN network from the primary LU to the secondary LU for the requested class of service. network node calculates routes for sessions that originate at the LUs in it and at the end nodes it serves. When route is calculated it is stored and can be reused. Route selection is based on how the actual characteristics of each node and transmission group along the possible paths match the characteristics required by the requested class of service. The route that network node selects is the current leastweight route from the node containing the origin LU to the node containing the destination LU. Because remote end node links are not stored in the topology database route calculation for sessions between LUs that reside in end nodes uses transmission group vectors . The end node sends transmission group vector for each link that it has to other network nodes or end nodes on locate request or reply. These end node transmission group vectors are temporarily added to the topology database when computing routes to or from end nodes. For each APPN transmission group transmission group characteristics are defined. To distinguish one transmission group from another transmission group characteristics can be specified in the following ways As transmission group profiles which are groups of transmission group characteristics that can be applied to several PUs The characteristics of transmission groups owned by other nodes are learned Eight standard transmission group characteristics are defined by the APPN architecture. These transmission groups must be used by all APPN products. The transmission groups are COSTTIME COSTBYTE PDELAY CAPACITY SECURITY and the three User Parm values try to assume reasonable default values based on the type of the link. An APPN class of service defines the required or requested characteristics of route for session. class of service consists of set of ranges of acceptable values for the characteristics of links and nodes to be used for session specifying that particular class of service. APPN classes of service are defined in VTAMLST definition list. Unlike the class of service for the subarea network where the class of service is actually list of VRs that are acceptable for particular class of service APPN class of service specifies the types of routes that are acceptable for class of service. Each APPN class of service has table of definitions. Each column represents transmission group or node characteristic. Each row is defined by range for each characteristic. Each row defines weight for transmission groups or nodes that fit the range. Rows are typically defined from most to least restrictive represent ranges with the top value in LINEROW representing the minimum value and the bottom value in LINEROW representing the maximum value. These values are compared the total path weight . High performance routing is an addition to APPN that improves reliability increases network performance and was designed to exploit higher link speed Intermediate session routing requires significant processing for error control flow control and segmentation at each intermediate node. The significant processing causes significant latency in each node. As higher speed connections evolved the APPN architecture was required to introduce some changes and enhancements to allow switching in intermediate nodes to be done at higher speeds thereby improving the throughput of data. HPR addresses this by routing at layer 2 and 3 and changing the existing intermediate session routing which is done in basic APPN at layer 5. HPR the higher layer requires more resources and that affects the performance. As HPR is done in lower layers than ISR the delay in each node along the path is HPR has also shifted the error recovery to the end points instead of individual lines. The two endpoints are the APPN nodes end node or network node that provide for the LU LU session. With basic APPN every network node was responsible for recovering from errors on the two links that were used to deliver the data to and from the network node. The error recovery consumed resources and affected performance. With high speed networking the reliability of the communication lines improved dramatically. Today the ratio of errorstotraffic is in the range of 109. The probability for error is very low and moving the responsibility for error recovery to Introduction to the New Mainframe Networking the end points improves performance and does not affect the integrity of the HPR has also been designed to provide nondisruptive path switch to route around failures. In simple words nondisruptive path switching addressed one of the major deficiencies of SNA compared to other protocols. With nondisruptive path switching session is switched to another available path without affecting session availability to the end user. transport protocol and automatic network routing . RTP is connectionoriented fullduplex protocol designed to support data in highspeed networks. RTP connections are established within an HPR subnet and are used to carry session traffic. These connections can be thought of as transport pipes over which sessions are carried. RTP connections can carry data at very high speeds by using lowlevel intermediate routing and minimizing the number of flows over the links for error An RTP connections physical path can be switched automatically to reroute sessions around failure in the network. The RTP connection is reestablished over new physical path that bypasses the failing link or node and the sessions traffic flow is resumed on the RTP connection nondisruptively. Any data that was in the network at the time of the failure is recovered automatically using RTPs endtoend error recovery. In basic APPN error recovery is done on every link in network. To address the emerging highspeed lines with much lower bit error rates HPR removed the requirement to do linklevel error recovery and instead does error recovery on an endtoend basis. This improves performance by reducing the number of flows required to do the linklevel error recovery on every link. RTP also supports selective retransmission where only missing or corrupted packets are resent and not all packets since the failure occurred. major observable fact in multilink transmission group is that packets may arrive at the endpoint out of sequence. The RTP endpoints Flow control is the mechanism that controls the pace at which data is sent into the network to prevent flooding the resources along the route and to prevent the endpoint from being congested. In an APPN network flow control is done on each stage of the session by using adaptive sessionlevel pacing. This method provided excellent performance for networks with low speed lines and poor quality. For highspeed networks adaptive sessionlevel pacing was found inadequate due to the amount of processing required in each node. HPR introduced protocol that is suited for highspeed routing called adaptive rate based flowcongestion control. It regulates the flow of traffic over an RTP connection by adaptively changing the senders rate based on feedback from the receiver. This protocol allows for high link utilization and prevents congestion before it occurs rather than recovering from congestion after it occurs. from many sessions requesting the same class of service can be routed over the same RTP connection. Automatic network routing is lowlevel routing mechanism that minimizes cycles and storage requirements for routing packets through intermediate nodes. An ANR node is an intermediate network node on the path of an RTP connection. ANR nodes are not aware of SNA sessions or RTP connections passing through the node. All an ANR node must do is read the the path. The ANR information is learned by the RTP endpoints during establishment of the RTP connection by sending Route Setup message which flows through all nodes on the prospective HPR path. ANR takes place at lower layer than APPN intermediate session routing and significantly improves performance in the intermediate nodes. The ANR node routes the HPR packet and does not provide functions such as linklevel error recovery segmentation flow control and congestion control. These functions Intermediate nodes that implement ANR are not aware of the SNA sessions or the RTP connections that are established across the nodes. This means that there is no requirement to keep the routing tables for session connectors that are kept in basic APPN. Source routing is technique whereby the sender of the data specifies the route that the data should take through the network. In an IP network andor the recovery. The same mechanism takes place on the reverse route. connection has specific format. It consists of three components the network nodes. It provides addressing for the packet as it transverses the HPR network. labels. The NHDR consists of some indicators that RTP can switch automatically and reroute data around failed node or link without disrupting the LULU sessions. This is called nondisruptive path switching because LULU sessions survive link failures. Nondisruptive path switching within the HPR portion of the network automatically occurs in an HPR subnet to bypass link and node failures if an acceptable alternate path is available. Link failure is detected by the RTP end points when the first link on the RTP pipe fails or data packets that were sent are not acknowledged. One or both RTP endpoints detect the failure and redrive the RTP setup. If necessary another APPN search is initiated to locate the best path available to the partner LU. connection network is representation of shared access transport facility enabling dynamic direct connectivity between any pair of link stations attaching to the facility. An example of shared access transport facility is local area network where two nodes can communicate directly with each other without the need to use router. you do not implement connection network you have two options. The first option is to define links only from each end node to the network node link definitions for network consisting of nodes but sessions between any two end nodes must traverse the network node which of nodes on the LAN. The second option requires more link definitions definitions for network of nodes but sessions between any two nodes always use direct link between the session Also notice that as more nodes are added to the shared access transport facility the number of link definitions required grows exponentially with option 2 while the number of link definitions grows linearly with option 1. Another problem with option 2 is that as more nodes are added to the LAN new links must be defined on every other node on the LAN in order to exploit direct connectivity to the nodes being added. The administrative task of defining new links is considerably reduced by using connection network to represent the shared access transport facility. Using connection network allows nodes attached to the shared access transport facility to exploit direct connectivity over the LAN without defining direct link to every other node on the LAN. connection network is implemented by defining virtual routing node to on the LAN defines link to the real network node and link to the VRN which identifies the end nodes connectivity to the shared access The links to the VRN provide the appearance of second path between any two end nodes on the same shared access transport facilitybut this second path is preferred over the path through the network node because the VRN is virtual network node rather than real intermediate network node. The VRN link definitions are used during session establishment to dynamically activate new link directly between the session endpoints. When the last session traversing dynamic VRN link ends the dynamic link is inactivated until it is needed again for another session. Because each end node attached to the connection network defines only 2 links the number of link definitions required for network with nodes is 2n. This is very comparable to option 1 above but avoids the overhead of routing sessions through real intermediate network nodes. Furthermore when new end nodes are added to the connection network new link definitions may be required on the network node but no additional link definitions are required on any of the existing end nodes in order to exploit direct connectivity to the new end nodes. Traditional SNA networks consist of peripheral nodes called physical units containing several types of logical units . The physical units are typically type PU2.0 or type PU2.1 . The LUs can be type LU0 LU1 LU2 or LU3. Types LU0 LU1 LU2 and LU3 are called dependent LUs because they require VTAM services to establish LULU sessions. Another LU type is LU6.2. An LU6.2 can act as either an dependent or independent LU. LU6.2 acts as dependent LU when it resides in type PU2.0 node and as an independent LU in type PU2.1 nodes. As an independent LU an LU6.2 can initiate sessions on its own without services from VTAM. Historically type PU2.0 and PU2.1 nodes have been attached to subarea boundary function typically to an NCP. As an alternative many customers have used Open Systems Adapters channelattached routers or 3172s to attach these devices to VTAMs boundary function rather than NCPs. The dependent LU server is implemented only in Type 5 network nodes. The DLUS function enables VTAM to provide SSCP services for dependent LUs located in remote APPN end nodes or network nodes which act as the dependent LU requester . Two LU 6.2 sessions are established between DLUS and dependent LU requester and these LU 6.2 sessions are collectively known as the CPSVRMGR pipe. SSCPPU and SSCPLU session flows use the CPSVRMGR pipe. An SSCPPU session is established between VTAM network node and the PU that owns the dependent LU and an SSCPLU session is established between VTAM and the dependent LU. Session initiation flows for the dependent LU are sent over the SSCPLU session and VTAM can use subarea or APPN flows to initiate session with the PLU. BIND and session data are then routed directly between the PLU and the The DLUR can be located in remote sites. Using DLUR also eliminates the need for dependent LUs to be physically adjacent to VTAM or NCP subarea Dependent LUs attached to DLUR can also exploit HPR allowing their sessions to be nondisruptively routed around failures in the network. Session paths do not need to include an owning VTAM. The technique to define VTAM APPN network is like the one used to define the ATCSTRxx in VTAMLST and by defining various VTAM major nodes. The first step for implementing APPN on mainframe is to define the datasets Definitions in ATCSTRxx 1. Define the APPN node type. You need to assign the role of VTAM as an APPN node that is either network node or an end node . Do this by coding the NODETYPE parameter. The two possible values are either NN To define VTAM as network node code the following 2. Assign the control point name . The same parameter used for subarea networking is used for APPN. To assign CPNAME1 as the control point in ATCSTRxx code 3. Assign network ID . The same parameter used for subarea networking is used for APPN. To assign NET1 as the network ID in ATCSTRXX code 4. Specify the level of high performance routing support. VTAM can support either automatic network routing or rapid transport protocol . The HPR parameter specifies which functions of HPR either ANR or RTP or both are implemented by VTAM. To indicate that VTAM provide RTPlevel HPR support specify HPRRTP is the default. 5. Specify the default link type. For VTAM type 2.1 node the CONNTYPE parameter determines whether connections to adjacent type 2.1 nodes are established as lowentry networking node or attempted as an APPN connection. To specify that the connection be attempted as an APPN connection code 6. Specify CPCP support. The CPCP parameter specifies whether VTAM supports CPCP sessions with adjacent type 2.1 node. To specify that CPCP sessions be supported on all connections to adjacent The CONNTYPE and CPCP parameters can be overridden on the physical unit definition statement as illustrated in the following example 7. Define VTAM as the central directory server . If you want to define VTAM as central directory server use the CDSERVR start option. You do not need to define anything on the other network nodes. The other network nodes find out about the existence of the central directory server through normal topology exchanges. To define VTAM network node as central directory server code 8. Control security. To reduce the burden of static definitions VTAM permits dynamic definitions of physical units logical units and adjacent APPN nodes. In the mainframe environment many installations are reluctant to allow dynamic definitions of VTAM resources. The DYNADJCP parameter controls whether adjacent control points are allowed to be created dynamically. To disable the dynamic definition of adjacent control points code 9. The BNYES start option is used to define VTAM as an extended border node . VTAM EBNs must also be NNs. 10. IVTAM is defined as an ICN or MDH then SORDER is used to control the Defining VTAM major nodes 1. Define the adjacent control point major node. If DYNADJCP in ATCSTRxx is defined as NO the ADJCP major node defines the adjacent control point that can establish CPCP session with this VTAM. 2. Create network node server list for end node. To create network server for an end node code NETSRVR major node. The major node list the network nodes that are part of the network node server list. In the last entry the nameless NETSRVR entry allows the end node to select any other known adjacent network node that meets the defined criteria as its APPN is the IBM strategic SNA protocol in the mainframe. It is required for sysplex Enterprise Extender implementation and many other technologies. APPN is dynamic in nature and reduces the amount of predefinition required in an SNA subarea network. APPNHPR was introduced in the mid1990s and supports nondisruptive route switching for failure recovery and connectionless intermediate routing of packets. APPNHPR still maintains the class of service and data flow control using more advance pacing mechanism adaptive session pacing. In contrast to subarea networking where special hardware and software are required for intermediate session routing every node that can act as network node can 1. What is the difference between APPC and APPN 2. How does an APPN network node locate logical units in an APPN network 3. Name the special node types that can be implemented by VTAM. 4. What is an extended border node 5. List the major functions of the rapid transport protocol . 1. What are the major differences between SNA subarea and APPNHPR 2. If major application fails what effect can it have on an 3. Name the differences between IP and APPNHPR routing. 4. What is the advantage of implementing connection 1. Code the XCA for an OSA ethernet definition that also connects to connection network and the PU definition that establishes CPCP session with an APPN network node in network NETA with CPNAME of CPNAME1. How many CPCP sessions are active . What is the control point name of this VTAM node . What is the node type of this VTAM node . Does this VTAM node support dynamic adjacent control points . What is the purpose of the command . What are the minimum and maximum round trip times 5. What can you learn when issuing the following commands Explain the background and motivation behind integrating SNA and IP organizations for integrating SNA and IP networks. List the features and benefits of each of these technologies. Describe how zOS network administrator implements these In the early 1990s enterprises began to implement router technology in their backbone networks. At that time enterprise networks resembled the Tower of Babel that is many networking protocols were proprietary and were not able to communicate with each other. Among the dominant proprietary protocols were IBMs SNA Digital Equipment Corporations DECNET Novels IPX and Microsofts NetBIOS. Router vendors were enthusiastic about introducing their products and competition concentrated on whose product supported the greatest number of protocols. This introduction of multiprotocol routers into an enterprise backbone thus reducing the expenses related to communication lines. But not long after implementing routers in their backbone network network managers realized that it is not easy to control maintain and perform problem determination in network that implements multiple protocols. Implementing only one protocol in the backbone network reduces the complexity of the network and the router. So there needed to be single protocol or set of protocols. The decision on which protocol set to use was easy. At that time the Internet usage was growing dramatically among home users and enterprises. Because of the Internet IP TCP and UDP were the protocols of choice. Many computer and operating system vendors added IP and the application protocols to their products and allowed access to their proprietary hardware and software using Some networking protocols including IBM subarea network were nonroutable. decide where to route the packet. Although APPN intermediate session routing and high performance routing are routable the resources required for implementing these protocols is very high and almost impractical. To several protocols that allow using SNA subarea and APPN protocols in routerbased IP backbone. Ideally all applications use the same protocol. Because TCP is the de facto application protocol TCP is always the best solution. Using TCP as the on the IP network. This however is rarely possible within large organization that has huge investment in SNA applications. transactionoriented program is dependent on the underlying protocol it uses. The application programming interface is different if one uses SNA or TCP. Changing transactionoriented program from SNA to TCP requires redesign of the communication part in the program and replacing the code that handles error recovery exception processing and many other tasks. Many computer that fail to introduce new architectures manmachine interfaces and the like. Shops use their budgets to modernize the applications rather than convert SNA applications just for the sake of having common infrastructure. uneconomical and very technical due to the complexity of the applications lack of skills in endpoint hardware while converging onto an IPbased backbone. SNA applications can be divided into two categories 3270based applications and applicationtoapplication as explained here. In this case end users communicate with the mainframe using 3270 display or workstation that has 3270 emulation software installed. Data sent from data communication program is displayed on the 3270 screen. The screen format is sent from the data communication transaction program and is not manipulated on its way to In this case the remote end has programmable endpoint controller or server that does some local processing sends the data using SNA to the data communication transaction program and upon receiving the In the early days of SNA applicationtoapplication communication used the There are several different ways of running mixed SNA and IP protocols over single IP protocol transport networks. The technique used to integrate SNA into the IP backbone depends on the SNA application type used. This integration method is used for 3270based applications. The SNA 3270 data stream is carried over TCP connections to TN3270 server Data Link Switching . SNA traffic is encapsulated in Enterprise Extender . SNAAPPN packets are encapsulated as User Datagram Protocol packets No change in the SNA application programs is required for any of the three integration methods. Although DLSw can be used for other nonroutable protocols this book focuses on DLSw for SNA. protocol for data transfer over the LAN. LLC2 is connectionorientated layer 2 data link control protocol for SNA over LAN. When LANattached station must establish an SNA session with peer LANconnected station it sends an LLC2 broadcast message called TEST. TEST carries the MAC address of the destination station and originating station carried in the TEST message but this topic concentrates on the MAC address. Since the TEST message is broadcast to all LANattached stations every station examines the MAC address field in the message. If there is match for the destination MAC address the destination station replies with RSP to workstation . Note that the TEST is broadcast message and it is sent to all stations on the LANbut for clarity the other stations on the LAN are not shown here. After establishing the LLC2 connection between the two stations the data transfer uses the LLC2 protocol. Data link switching carries LAN traffic over the wide area network by encapsulating the LAN traffic in TCP packets. Data link switching was first developed by IBM to provide SNA support within multiprotocol routers. The DLSw formats and protocol were then made available to the wider community and published as RFC 1434. The RFC was then later enhanced and republished Ciscos implementation of data link switching is known as DLSw and it contains additional enhancements to the original DLSw. The working infrastructures of most organizations include Cisco routers. To illustrate how DLSw carries LAN traffic over the WAN the distance As in the previous example workstation initiates connection to workstation by sending TEST frame broadcast. Router which is attached to the same LAN receives the TEST frame encapsulates it in TCP packet and transmits it to all its peer DLSw routers. The TCP broadcast packet is called CANUREACH. CANUREACH is received by all DLSw routers including Router . Router has already learned the MAC addresses of the workstations hosts and servers that are attached to its LAN. When the CANUREACH packet with the MAC address of workstation reaches router the router converts the CANUREACH packet to TEST LLC2 frame. Because the TEST is broadcast frame all stations on the LAN receive the TEST frame. Workstation which has been assigned the 4200.0000.0002 MAC address responds with RSP. Because ICANREACH is unicast message Router encapsulates the RSP in TCP packet and sends an ICANREACH packet to workstation only. Router decapsulates the ICANREACH packet to an LLC2 RSP frame. stations. Extending the distance between workstation and workstation and using slower media than the rated speed of LAN might cause LLC2 timers to expire and terminate the connection. Serial WAN link speeds are from 256 Kbps to 2 Mbps while LAN speed is 100 Mbps. With DLSw connections are terminated at the DLSw routers which acknowledge packets locally instead of transmitting these across the WAN. This technique is known as spoofing it reduces WAN traffic eliminating potential DLSw routers were installed in remote branches. The DLSw router enabled the connection of TCPIP and SNA LANattached workstations to the mainframe. Many remote branches still have SDLC devices like IBM 3174 controllers banking or retail controllers. Using the DLSw branch router eliminates the need to connect the SDLC device using dedicated communication link. Remote DLSw requires at least two routers connected to each other over an IP network. DLSw performs an Introduction to the New Mainframe Networking encapsulationdecapsulation function wrapping the SNA frames into IP packets for transportation across the IP network. Local DLSw does not use TCPIP. Instead it enables communication between LANattached SNA devices and an SDLC device that is linkattached to the same DLSw router. connected to the remote LAN workstations to communicate with either TCP mainframebased applications using the communication link that connects the two routers or mainframe SNA application using encapsulated LLC2 in TCP packets . The SDLC traffic is converted by the router to LLC2 and is transported using DLSw. Every router has an IP address assigned to its LAN interface. virtual LAN. Virtual LAN is LAN implemented inside the router and is not related to physical LAN interface. The SDLC link is assigned virtual MAC address and this MAC address is carried in the encapsulated LLC2 frames. network. DLSw routers are located at the edge of the network adjacent to the SNA device mainframe and server in our example. The DLSw routers perform the encapsulation and decapsulation of the LLC2 frame that carries the SNA path information unit or LLC2 commands and responses. The following points summarize some of the DLSw attributes DLSw is forwarding mechanism and supports SNA device types PU2 DLSw provides switching at the data link layer and encapsulates SNA data into TCP packets for transport over an IP network. DLSw does not provide full routing capabilities. DLSw like any other TCP connection utilizes the dynamic and rerouting capabilities of the IP network. DLSw is TCP application that uses default ports 2065 and 2066. SNA endpoints remain the same. No change is required to SNA endpoints. Because SNA data is encapsulated in TCP packets DLSw does not interpret implemented in the IP network. Some older models of the one gigabit OSAExpress cards in QDIO mode support the IP protocol only. Using DLSw rules out the use of these one gigabit OSAExpress cards and imposes the use of 100 megabit OSAExpress cards. These cards are defined in the HCD as nonQDIO. DLSw was the first SNA over IP solution that became available. It is mature product and you will find that many organizations have implemented this From zOS perspective the DLSw configuration and definitions reside on the router. As network administrator you normally would not get involved with these definitions because the group responsible for the WAN does them. You need to interact with this group to get MAC addresses of SNA devices and especially the SDLC virtual MAC addresses that the mainframe uses to initiate the LLC2 connection. For LANattached devices the VTAM definitions remain the same. When changing the SDLC connection from NCP to DLSw router configuration the physical connection of the devices changes. The router attaches via the LAN and in most cases will use the OSA card. VTAM definition has to be altered from an NCP definition to switched major node that includes the physical unit and the logical units of the SDLCattached device. Enterprise Extender has provided useful solution to the dilemma of running SNA applications over IP networks. Extending the enterprise is an appropriate description. Enterprise Extender is standard created by the Internet Engineering Task Force and APPN Implementers Workshop . It is documented in RFC The Enterprise Extender architecture carries the SNA high performance routing traffic of any logical unit type over an IP infrastructure without requiring changes to that infrastructure. It treats the IP network as particular type of SNA logical connection. In this manner the SNA protocols act as transport protocols on top of IP as does any other transport protocol such as Transmission Control An important aspect of Enterprise Extender is the ability to view the IP network In this case the benefit comes from the ability to establish dynamically single onehop HPR link to any host to which IP connectivity is enabled provided that the host implements Enterprise Extender. In general this allows the routing function to be handled entirely within IP. IP routers serve as the only routing nodes in the network. an IP network. The SNA network connects SNA devices without encapsulating the data to the mainframe. The IP network connects IP devices like TN3270 clients and TCP clients implementing Web browsers directly to the TCP stack in Devices that implement Enterprise Extender are located on the border of the IP and SNA network. These devices are connected on one side to the SNA network and on the other side to the IP network. The IP network transports the Enterprise Extender traffic over the IP backbone. The routers inside the IP backbone are pure IP routers not requiring any additional software as in the case of DLSw. In the branch side where the SNA clients exist special hardware and software Communication Server for NT or the Cisco SNA Switching special The role of SNASw is explained later. To the SNA HPR network the IP network appears to be logical link to the IP network the SNA traffic appears as UDP datagrams. The UDP datagrams are routed without any changes to the IP network. network cloud. The path that carried the Enterprise Extender IPUDP datagrams through the IP cloud is the logical link that connects the two mainframes. The EE routes for SNA sessions rapid transport protocol which is logical connection uses the IP network In mixed Enterprise Extender and HPR connection single RTP pipe connects the two endpoints. The single RTP pipe is made up of two hops Enterprise Extender has been designed to run over existing IP networks without requiring any change to applications or to IP routers. SNA applications see the same SNA network interfaces as before while IP routers continue to see familiar Session availability in mixed EE and HPR rerouting of SNA sessions takes place by the protocol where the failure is IP has always had the ability to reroute packets around failing components without disrupting the connection by means of the connectionless property of IP. More recently HPR has implemented nondisruptive path switching which provides the same function as an IP network although in different fashion. The HPR extension to SNA is connectionoriented which has always been characteristic of SNA. However when it detects failure it moves an existing connection around failing component. The use of HPR transport over an IP network provides nondisruptive rerouting around failed network components If the failure occurs in the IP network the rerouting is handled by the IP network. If the failure is the HPR portion HPRs nondisruptive path switching reroutes the session to an alternative path. Thus far we described SNA as robust and reliable protocol. So why is it that Enterprise Extender transports SNA over IP packets using UDP which by definition is unreliable and whose transmission is based on best effort The designers of Enterprise Extender had the task of architecting the way in which SNA and IPbased protocols would be layered to transport SNA data over the IP network. They had three choices for encapsulating SNA data units raw IP datagrams UDP packets or TCP connection. Lets take closer look at each Datagrams are completely compatible with the HPR principles because they flow through the network with minimal overhead and provide no error recovery of any sort. However raw IP provides no means of multiplexing particularly with no Internet Engineering Task Force designated protocol value for HPR. Using nondesignated protocol value can lead to inconsistencies with security measures that filter IP packets based on this Additionally although raw IP allows priority and type of service to be These packets provide the multiplexing required because they contain UDP port numbers which allows Enterprise Extender packets to be distinguished from other IP packets. UDP also permits priority scheme to be implemented independent of the type of service bits because many routers can prioritize traffic based on the received port number. UDP also has low overhead because it does not concern itself with error recovery or flow control. TCP connection also provides multiplexing through port numbers but it incurs significantly higher overhead than raw IP or UDP. TCP connection handles error recovery retransmission and flow control. None of these is required for an HPR connection because the RTP endpoints are responsible The implementation of EE in zOS involves data transfer between the VTAM and the TCPIP address spaces. special connection type called IUTSAMEH is used to move data from VTAM to TCPIP and vice versa. This connection type is used to connect two or more Communications Server for zOS IP stacks running on the same MVS image. In addition it is used to connect Communications Server for zOS IP stacks to zOS VTAM for use by Enterprise For Enterprise Extender zOS Communications Server implements separate UDP layer called Fast UDP that is optimized for Enterprise Extender communication. Fast UDP communicates with Enterprise Extender . In SNA the class of service specified for particular session is used to determine both the route taken by the session and the transmission priority allotted to it. With an IP backbone the route is essentially unpredictable because of IPs connectionless property. However IP provides for transmission priority using bits. However in the past they tended to use the TCP or UDP port number as means of assigning priorities to packets. Enterprise Extender supports the use of both precedence bits and port numbers to inform the IP network of the transmission priority. You should use precedence carried inside the IP datagram so encrypted packets have unreadable port numbers and fragmented packets have no port numbers after the first fragment. For such encrypted or fragmented packets intermediate routers cannot Using IP type of service data path forward packets with higher type of service values prior to forwarding lower priority ToS packets. IP type of service is related to the APPN class of service and the UDP port numbers used for Enterprise Extender traffic. The APPN CoS specifies transmission priority which can be one of the There are products and services that support Enterprise Extender on systems The Cisco SNA Switching Services feature supports the Enterprise Extender function. The SNASw function is implemented as branch network node or branch extender node. BrNN appears as an APPN network node to downstream devices and an end node to upstream devices . Host Integration Server 2004 and IBM Communications Server Both of these software products provide Enterprise Extender capability directly Many organizations have these products deployed for SNA device and application support connecting to zOS through DLSw or SNASw routers. New Enterprise Extender support in both products allow the DLSw or SNASw router connections to be eliminated and an IP WAN router connection is the only IBM Personal Communications for Windows AIX and Linux The IBM Personal Communications family has support for Enterprise Extender. Using the IBMEEDLC interface configuration option Enterprise Extender connections can be set up directly from the desktop to Enterprise Extender on zOS for SNAbased applications. As with Host Integration Server and Communications Server an IP router is the only requirement. Enterprise Extender enables remote branches or workstations to be connected to the SNA backbone using the Internet with no application changes required while maintaining SNA connectivity from end to end. Dependent LU sessions can be carried on an Enterprise Extender connection as easily as any others by using the dependent LU requester function. Some of the benefits of Enterprise Extender include network which eliminates parallel networks reduces equipment lowers data circuit cost and simplifies network management. There are no changes required to SNA applications. SNA can exploit the OSA Gigabit Ethernet interface cards. Endtoend failure protection and data prioritization using the IP router network and zOS Communications Server facilities. As already discussed the motivation to migrate an SNA network to Enterprise To connect to business partners using SNA network interconnect Organizations that still use the older SNA technology with communication lines from the branch to an IBM communication controller Installations that converted their SNA network to DLSw and use an IP Migrating remote branches from the older SNAbased To migrate from the older SNA technology an installation must have an SNAbased server in the branch. The SNA software on the server can be one of zOS Communications Server with Enterprise Extender running on You can also use Cisco Routers with the SNA Switching Services feature The first step is to decide which platform to use at the edge of the IP backbone and the branch. The two options to implement Enterprise Extender on the SNAbased server or to use Cisco routers with SNA Switching Services Implementing Enterprise Extender with Cisco routers does not require any additional software or hardware in the branch. If the decision is to implement Enterprise Extender on the server vanilla IP router is installed in the branch and in some cases an upgrade of the SNA server software is required. Introduction to the New Mainframe Networking within HPR. The HPR packet is integrated into UDP packet and travels through the IP WAN routers to the data center through the OSAE card into the TCPIP stack Enterprise Extender VIPA Migrating remote branches from DLSwIPbased backbone to Despite Enterprise Extender being the preferred solution for transporting SNA data over an IP network many organizations still have DLSw deployed in the WAN. Typically you see DLSw used to transport SNA sessions over the WAN with DLSw and SNASw routers deployed in the host data centers to distribute the SNA session to the host LPAR. The DLSw and SNASw feature code sets may be because it takes time and money to migrate from DLSw routers to SNASw. If an application or device is unable to be converted to TCPIP reducing the complexity of the environment by implementing SNASw Enterprise Extender at the branch level is the next best option. The following examples give you an idea of the SNAIP implementation parameters you can expect to see defined within the zOS networking and related components. There are many other environmental and related component parameters that are not shown. For full understanding of what each parameter does consult the relevant technical manual. Enterprise Extender has number of key parameter definitions that are required in order to enable Enterprise Extender on zOS. There are also some optional definitions that might be implemented depending on an organizations There are also definitions required on the SNASw routers and implementing the definitions is normally job performed by the network group responsible for looking after the WAN equipment. Though this task is usually not handled by zOS network administrator as with any of the data center network equipment you need to provide some information in order for the SNASw router to connect prerequisite of implementing Enterprise Extender on zOS is that Assuming that TCPIP address space is up and running prior to the implementation of EE the following tasks must be performed in the TCPIP profile definitions 1. Define static VIPA and assign it an IP address. 2. Define the IUTSAMEH device. If as part of parallel sysplex definitions you define distributed VIPA using DYNAMICXCF then IUTSAMEH is automatically defined. 3. It is recommend that dynamic routing protocol like OMPROUTE be If you are using OMPROUTE there are additional definitions required but these are not shown. Example 91 illustrates an example of TCPIP profile statements that are part of the Enterprise Extender definition process. The TCPIP profile shows device statements for Enterprise Extender the Enterprise Extender UDP ports IP address of Enterprise Extender and start device statement for Enterprise The example uses unique Enterprise Extender VIPA because it provides separation from the IP VIPA and can be monitored independently. Some sites use the IP VIPA as the Enterprise Extender VIPA as well. TCPIP needs definition for the port represented by the VTAM application . This must be active before VTAM can establish any Virtual Device and Link statements There will also be other VIPA device statements for the IP VIPA. There will also be other device statements for the interfaces such as OSA. PORT statements for Enterprise Extender 1200012004 default ports Define the Enterprise Extender IP address and TCP name in the VTAM ATCSTRxx configuration list member see Example 92. Do this on each of the LPARs that are Enterprise Extendercapable. Example 92 VTAM statements for Enterprise Extender SYSP. VTAMLST APPN definitions EE specific definitions External communications adapter VTAM major node defines the IP port Important We recommend that you use unique IP VIPA address for Enterprise Extender on each LPAR. connection to the zOS TCPIP stack that VTAM will use for EE connections AUTOGEN VTAM Switched major node definition member for remote SNA Server node SYSP. VTAMLST VTAM CDRSC definition member for independant LU on PU1 above. SYSP. VTAMLST VTAM Switched major node for CISCO SNASw router Example 93 shows the definitions you might have between two interconnected partners. This configuration would replace an SNA network interconnect connection between two frontend processors . The example includes the type of extended border node implementation definitions you see under VTAM at both organizations. There are most likely firewall definitions and ports 12000 12004 need to be opened. Consult your security administrator for the firewall definitions. Example 93 Extended border node configuration VTAM Start options for EBN in ATCSTR05 member ORGCP1 will have similar ATCSTRxx definitions IPADDR will be 182.20.5.20 VTAM XCA major node required on both NZCP5 and ORGCP1 shown in Example 92 IPADDR10.192.25.10 remote TCPIP address for connection. The existing VTAM application definitions remain the same. The Cisco SNASw router configuration is normally done by the WAN network administrator. However the zOS network administrator needs to provide input to these definitions. The minimum required definition will include the following The IP addresses of the static VIPA that is used for EE Not all the required definitions are shown in Example 94 but this gives you an idea of what type of parameters are set for Enterprise Extender. snasw rtp pathswitchtimers snasw cpname NETX. SNASWR1 network name of router snasw dlus NETX. SSCP1 backup NETX. SSCP2 Primary and backup DLUS server snasw port xxxx hprip xxxxx Lan interfaces and connection types snasw link SSCP1 port EE ipdest 10.134.61.188 nns define uplinkslpars snasw link SSCP2 port EE ipdest 10.134.61.189 nns snasw link xxxxx port EE ipdest 10.134.61.190 snasw mode INTERACT cos INTER code all VTAM LogmodeCOS entry names Consolidating SNA onto IP is not simple task. Each set of SNA components and applications should be evaluated on its own merits and solution should be found that best suits the requirements. In some cases solution is to replace an application or device in other cases it is to use DLSw SNASw BEX Enterprise Extender or combination of these. DLSw is common SNA over IP solution found in the wide area networks of many organizations. The DLSw router is deployed at branch offices and peered to data center DLSw routers. Upstream to the zOS mainframe is normally through SNASw routers and OSAExpress. DLSw provides switching at the data link layer and encapsulates SNA into TCP packets for Enterprise Extender is an extension of SNA high performance routing and provides encapsulation of SNA packets into UDP frames at the edges of an IP Enterprise Extender capable devices and components include zOS Communications Server Cisco routers Microsoft Host Integration Server and IBM Communications Server for Windows and AIX and IBM Personal Communications for Windows. SNASw branch extender support within the above components can be deployed at the branch office and connects directly to Enterprise Extender on zOS through an organizations IP router network. SNA views Enterprise Extender as just another data link control while IP views Enterprise Extender as UDP application. No changes are required to the SNA applications if Enterprise Extender is deployed. Core zOS enablement definitions are contained in VTAM but also requires TCPIP setup. routing protocol such as OSPF although it has no Enterprise Extenderspecific definitions is required to provide high availability IP routing wide area network 1. Why are organizations migrating to an IP network infrastructure as 2. What are some reasons why organizations do not convert their SNA to run on an IP network. 5. In which components is the Enterprise Extender VIPA address defined 6. What type of packet is used to transport Enterprise Extender 1. What are some weaknesses of DLSw when compared to SNASw 2. Letswakeup Corporation has Enterprise Extender installed on zOS and has SNA servers deployed and SNA applications and devices. Management has provided budget for improving the connectivity from the What solution would you put in place Explain why. 1. Try issuing the following command in an SDSF session This command displays the VTAM APPN characteristics defined in the VTAM What is the IPADDR and TCPNAME set to What is BN set to Identify the fundamental characteristics of the 3270 data stream. Understand the purpose behind the TN3270 Enhanced Understand the functional flow of TN3270E session. Understand the basic configuration elements of the TN3270E server. During the last several decades before the Internet Protocols rise in popularity large organizations established their own SNA networks. These SNA networks were used to communicate between remote endusers and the centralized mainframe. The display management protocol used to facilitate this communication within an SNA environment was called the 3270 data stream. At 3270 terminal was nonprogrammable workstation. Stated more simply it was display screen with keyboard terminal displays consisted of 12 rows and 80 columns of text characters no more and no less. Eventually 24 80 screen size became the standard with some alternate sizes available. To give the old 3270 terminal credit it did support selector pen and even magnetic strip reader. The selector pen was lightbased and it was used to select options on the text screen similar to how mouse is usedbut of course the 3270 terminal did not support mouse. The 3270 terminal containing nonprogrammable display and keyboard . The device type for these control units up until the time the Internet Protocol began displacing SNA networks was called 3174 control unit. The 3174 had some programmability which allowed an expansion of the capabilities and connectivity options of 3270 workstation but it still was long way from todays GUI workstations. 3270 display terminals were attached to the 3174 using ports with up to 64 terminals capable of connecting to single 3174. The 3174 control unit had other capabilities as well including printer support. At the time of the 3174s demise it had even expanded to include support for During the most recent decade corporate networks started implementing IP as applications existed they looked at integrating the SNA protocol into their IP backbone. The technology used to move from SNA 3270 applications to TCPIP Prior to explaining the TN3270 implementation you need to know little more about the 3270 data stream. The 3270 data stream operations are designed primarily for transmitting data between an application program and 3270 display with keyboard so end users The 3270 data stream is also designed for transmission of data to 3270 printers. The 3270 data stream is implemented using mapped character buffer in the device. In turn this mapped character buffer forms the display on the screen. Data received from the application program and data to be transmitted to the application program are stored in device buffer and displayed on the screen in when the end user modifies the buffer data and when new data is received from character position on the display. All characters and numbers are represented through Extended Binary Coded The 3270 write control character is often included in the 3270 data stream. It is Write characters to the terminal display. Erase and write characters to the terminal display. Erase All UnprotectedErase all unprotected fields on the terminal display. Read unprotected data fields that have been changed on the terminal display. Attention Identifier Send control information Send control information characters. Alphanumeric or numeric. Unprotected alphanumeric fields are fields into which an end user normally enters data using the shift keys . numeric field can contain only numeric characters with the addition of the period and minus sign . Field attributes can be further modified using extended field attributes. Extended field attributes allow such characteristics as foreground and background colors and highlighting to be controlled. These 3270 data stream fields appear reasonably straightforward but the implementation details can quickly become very complex 3270 Data Stream Programmers Reference provides exhaustive details. Function and attention keys Function keys are the numbered keys prefixed by the letter found across the top of standard keyboard. Function keys are heavily used in the TN3270 environment as shortcuts for interacting with function key is analogous to shortcut such as using the Alt Tab keys to switch applications in Windows environment. The attention key is used to present an external interruption to an executing application. Attention interruptions are used to halt execution. Introduction to the New Mainframe Networking emulated 3270 display terminals. Display terminals are emulated in software called TN3270E clients which can run on standard personal computer or workstation. In the world of genuine 3270 display terminals 128000 end users would require an enormous amount of dedicated limited function keyboard and display devices not to mention 2000 dedicated 3174 control units As network usage exploded the SNA 3270 method of communicating with mainframe became untenable. The solution came in the form of the Internet Protocol . The original basic telnet protocol was defined in RFC 854. This RFC effectively defined all that was needed to support the 3270 data stream since the 3270 data stream is just part of the telnet data payload. In other words the 3270 portion was implemented outside of the telnet protocol. Specific options could be negotiated using the telnet option standard of RFC 855. Option negotiation in turn allowed for device type negotiations to be completed as part of the telnet session setup. Initially there was no formal standard for TN3270 but it was clarified in an early RFC titled TN3270 Current Practices . TN3270 itself began to take shape more formally with RFCs 1646 and 1647. RFC 1647 was significant RFC because it was the first formalization of the TN3270 Enhanced protocol known as TN3270E. TN3270E improved upon the TN3270 protocol to include control of LU name selection as well as full support of Attention Identifier such as SYSREQ and the attention key. Today TN3270E is the standard IPbased method of communicating with mainframe. Often the term TN3270 is used synonymously with TN3270E since Reminder LU stands for logical unit. An LU is the SNA entity that represents an endpoint of communication for session. For example an LU can represent an application endpoint or an enduser endpoint . The enduser endpoint LU is referred to as the terminal LU. In addition if an SNA application wants to send data to printer it will form session between the application and an LU that represents the printer. from an enduser perspective it can be difficult to tell the difference. Most TN3270 clients however run TN3270E even though end users may think they TN3270E is currently defined in RFC 2355. TN3270E client uses the TN3270E protocol to access the resources on TN3270E server. However the TN3270E client cannot complete the connection all the way to the target application because the TN3270E client communicates according to the TN3270E protocol while the target application expects communication to be SNA protocol. application . In effect the TN3270E server is nothing more than protocol converter on one side it maintains an RFC 2355compliant TN3270E session on the other side it emulates 3270 data stream terminal to VTAM. The target application cannot tell the difference between genuine 3270attached terminal and TN3270E serveremulated terminal. Among the many capabilities of TN3270E one is the ability of the TN3270E client specifically to request an LU to be used as the terminal LU. This level of control allows more control from an SNA point of view. The characteristics of the TN3270E connection are negotiated during the start of the connection. For example the client can request an LU name 3270 function can be negotiated and perhaps most significantly the client must choose The TN3270E server has more than just 3270 terminal display capabilities. It can also support the SNA print data stream. Using the same scenario as in some SNA sessioninitiation differences print data stream can be emulated by the TN3270E server. This means that an SNA application can direct print data stream to an SNA printer LU as it always has. If that printer LU is TN3270Eowned printer LU then the TN3270E server accepts the print data stream from the application and forwards it to the TN3270E client running on the workstation. The workstation can then print the data using normal workstation printer facilities. TN3270E allows the terminal session LU to identify implicitly the printer session LU that should be used. This is done using the ASSOCIATE command during the TN3270E printer connection setup. If an ASSOCIATE command is sent then the TN3270E server selects printer LU based upon the terminal LU to which the client is already connected. Obviously the TN3270E server must be set up with onetoone mapping of terminal LUstoprinter LUs. Note The TN3270E server is primarily protocol converter . However the TN3270E server does intercept and alter some 3270 data stream flows. It can also alter the sequence of SNA flows. Consequently it is more than just converter. Device types There was more than one model of 3270 display terminal. IBM 3278 models 1 through 5 were the most common 3270 display terminals. The primary significance of device type selection is the number or rows and columns available in the TN3270E client window. Here are some TN3270related functions that zOS Communications Server Transport Layer Security zOS Communications Server supports TLS . TLS provides secure data transmission between the TN3270E Server and an TLScapable client. In an TLS session any data on secure port is protected using one of several optional cipher suites. Note that since TN3270E protocol ends at the TN3270E server data travelling over the SNA session is not protected. Client authentication and SAF level checks are supported. The TN3270E server can listen on multiple ports. In addition more than one TN3270E server or among separate TN3270E servers listening can be controlled so that it is only active on one specific IP address. Through these functions you can define different security levels or different configuration parameters or both for each port and IP address Mapping an IP address to an LU name This function provides the ability to select both an LU name and an application name for incoming TN3270E sessions. The selection is made on the basis of specific IP address group of IP addresses subnet or the link name used to connect to the zOS host. The function makes the LU name and the application name predictable and controllable. In addition zOS Communications Server supports selections based on an IP host name or group of names as well as an IP address. With the increasing use of dynamic IP this can be beneficial in maintaining control over the mapping. Reminder SAF stands for System Authorization Facility. SAF allows mainframe applications to use an external security manager to control access to resources such as data sets and MVS commands. SAF also forms user ID repository which can be used to authenticate TLS client. Introduction to the New Mainframe Networking the target application is normal SNA protocol and because of this the target application could actually be anywhere within the SNA networkon another mainframe host in the organization or even in another organization. You can also place the TN3270E server on host other than the mainframe. There are TN3270E servers available on the following platforms Other TN3270E software packages are available. Though TN3270E server can run in many places there are reasons to If all your 3270 applications are in the same place then it makes sense to implement TN3270E in zOS Communications Server. Through the judicious use of the workload manager and the DNSbased functions of zOS Communications Server you can create resilient TN3270E server that can handle tens of thousands of connections. If all your 3270 applications are in the same place there is no requirement for LPARs you can still implement TN3270E in the mainframe and use channeltochannel connection between the LPARs to connect the TN3270E server to the target application. You can implement TN3270 in zOS Communications Server either within the TCPIP address space or beginning with zOS V1R6 in its own address space. The definitions of the TN3270E server are identical in both configuration alternatives. However when running the TN3270E server as its own task the definitions for the TN3270E server cannot be placed in the TCPIP profile data The startup JCL has PROFILE DD statement that points to profile data set that contains parameters to control the TN3270E server. Within this profile data set are two fundamental statements blocks used to define TN3270E server discussion of significant statements from within these statement blocks follows. The TELNETPARMS statement block contains the TN3270E protocol and other nonVTAM attributes. Example 101 The following provides information about basic TN3270E server setup. You can find complete details on TELNETPARMS block statements in zOS Communications Server IP Configuration Reference. Use this statement to activate TN3270E session tracing. When EXCEPTION is Many options are allowed on the DEBUG statement. The most commonly used is the DETAIL option. The DETAIL option should be used only when debugging specific problem and under controlled environment because the option can produce large amount of output. The benefit is that the option can quickly point The last parameter on the DEBUG statement controls the destination of where trace records are directed. If trace records are to be written to the JOBLOG instead of the zOS console the CONSOLE parameter can be changed to JOBLOG. If no destination is specified the destination default is JOBLOG. This statement tells the TN3270E server that this block of TELNETPARMS statements applies to connections using PORT 23. This statement also causes the TN3270E server to listen on port 23. The PORT statement can include an IP address to associate the port with specific IP address. The purpose of these statements is to clean up sessions that are simply inactive. If terminal or printer SNA session is inactive for 4 hours then the TN3270E server ends both the TN3270E connection and the SNA session. Note that the activity only applies to communication occurring on the SNA session side of the These two statements work in tandem and apply to the TN3270E protocol side The TIMEMARK statement controls how often the TN3270E server should send out an Are you there packet to TN3270E client that has not had any activity. After 600 seconds of idle time this TN3270E server sends DO TIMEMARK packet to the client. If any kind of response is received the connection is marked as healthy. The SCANINTERVAL statement controls how frequently the TN3270E server seconds the TN3270E server checks to see if any connections have been idle for more than 600 seconds. The TN3270E server sends the clients of such connections DO TIMEMARK packet. If any such connections have not sent out response by the time the next SCANINTERVAL is performed then the SNA and TN3270E portions of the session are both cleaned up. When client user enters the LOGOFF command to end session with VTAM application the LUSESSIONPEND prevents the connection from being dropped. Instead the client is returned to the screen from which the logon originally was This statement enables USS message table to send USS message 7 to the client in the event of logon failure. This statement should normally be coded. Message 7 is the number assigned to the USS message for logon failures. More on USS messages is found in 10.4.2 BEGINVTAM statement block on As mentioned earlier the TN3270E session setup requires that the client identify what type of TN3270 terminal it wants to represent. The TELNETDEVICE statement is used to control the SNA session characteristics hat are to be used for both the TN3270E portion and SNA portion of the connection. The second column indicates the device name. The last two columns indicate the logon mode entry to be used for TN3270 and TN3270E connection respectively. There are many more configuration statements which are not shown here relating to the TELNETPARMS block. The BEGINVTAM statement block is used to define characteristics that are related to the mapping of the VTAM configuration. Note There are three different places that an initial TN3270E connection can be directed to the USS message 10 screen the default application or the telnet solicitor screen. More information on this The capabilities of mapping within BEGINVTAM statement block are complex and extensive. The sample used here has been kept simple to facilitate explanation of the basic concepts. This statement is used to connect this BEGINVTAM block with TELNETPARMS statement for the same port number. In other words connection to port 23 uses these BEGINVTAM statements as well as the TELNETPARMS statements for the same port number. When TN3270E client connects to the TN3270E server it needs to be mapped to an LU that the TN3270E server can use to represent this client on the SNA session. If the client does not specify specific LU and if no other mapping statement directs different LU to be used for this client then an LU from this default mapping is assigned to the connection. In this sample the LU range from TCP00001 to TCP00099 are available as This statement does not do any mapping. Instead it defines group of LUs that can be used for TN3270E terminal sessions. The FFFXXX pattern indicates that the first three characters are fixed while the remaining four characters represent hexadecimal range. This statement does not do any mapping. Instead it defines group of LUs that can be used for TN3270E printer sessions. The FFFXXX pattern indicates that the first three characters are fixed while the remaining four characters represent hexadecimal range. Again this statement does not do any mapping. It defines group of IP addresses that identify TN3270E clients . This statement maps the LUGRP1 group of LUs to the IPGPAY group of clients. In other words connections from the 9.8 network can only use terminal LUs from within the LUGRP1 range. This statement maps the PRTGRP1 to the same network such that TN3270E connection from client on the 9.8 network can associate printer from this This statement specifies that the USS message 10 panel be presented for the initial connection. If LUSESSIONPEND is coded client is returned to this screen after logging off from an application. Note Together these two LUMAP and PRTMAP statements form the onetoone mapping that is necessary for TN3270E printer connection to utilize the ASSOCIATE command. Note Unformatted System Services messages are messages used in an SNA environment to facilitate application access. USS message 10 is the standard logon message presented when session is initially established. USSMSG7 as mentioned is the standard error message presented when command entered at USSMSG10 screen fails Note that this statement is commented out. That is because DEFAULTAPPL and USSTCP statement have the same function they control where user is directed at initial connection time. If USSMSG10 panel is not desired this statement could be used to direct client to specific application at connect time. It is possible for client to negotiate line mode when connecting to the TN3270E server. In such situation this setup connects the client to the TSO application. This statement limits the selection of application for TN3270E client to the TSO application only. This is security statement to control what applications can be selected from the USSMSG10 panel. If the TN3270E server is going to acquire LUs on behalf of TN3270E client The definitions in this member are for an APPL major node. The TN3270E server uses these application LUs to function like terminal LUs. The LU names coded in VTAMLST must match any LU names generated via the mapping statements in the BEGINVTAM statement block. The TN3270E protocol represents the evolution of SNA as it converged into the world of TCPIP. It is the primary method of connecting end users to mainframe computers. It consists of characterbased data stream. TN3270E connectivity is handled on the mainframe by the TN3270E server. The TN3270E server converts TN3270E TCPIP connection to an SNA session. The terminal LU of the SNA session is emulated so that the SNA application functions as though it were connected to nonprogrammable 3270 terminal. 1. Does the TN3270 data stream support graphical user interfaces 2. Why cant certain fields within 3270 screen be overtyped 3. How does an SNA application know that it is ultimately communicating with TN3270E client and not real 3270 display terminal Introduction to the New Mainframe Networking 4. What statement is used to control SNA session inactivity 5. What happens if there is no LU mapped to client identifier when TN3270E connection is established 1. Why might the ability to have workstation client select specific LU be of 2. From an SNA perspective what are some of the considerations for TN3270E 3. Why cant the TN3270E server definitions be placed into the TCPIP profile data set when the server is to be run as its own task 4. What do you think is the purpose of the TCPIPJOBNAME statement shown in would you expect to happen if client requested LU name TCP00100 1. Use RFC 2355 to determine what the effect is of an IAC DO TN3270E. 2. What is the screen size of your TN3270E session used to connect to the lab mainframe Can you change this If yes how 3. Use zOS Communications Server SNA Resource Definition Reference to guess why the IBM32781 device type is not implemented in TN3270E. 4. Use the DISPLAY TCPIPprocnameTELNETCONN command to display all active TN3270E connections. Look for the IP address of your workstation among the results and note the connection identifier. 5. Using the connection identifier found in exercise 4 try issuing the following command DISPLAY TCPIPprocnameTELNETCONNCONNnnnnnn where nnnnnn is the connection ID. Examine the output. administrator would perform. Use zOS network component commands and displays that are commonly used to monitor and control the network. Understand the importance and the breadth of documentation in the role of The role of zOS network administrator can span wide area. In some organizations you might be generalist looking after all zOS networking components printer subsystems and even some of the hardware. However it is more likely that you will specialize in one or two particular areas such as VTAM understanding of the role. Within zOS networking components on the mainframe some common tasks you Provide performance and usage network statistics. include operations the WAN network team zOS systems programmers change control business users and testers. Monitoring means watching and observing normally to see something change. This does not imply that you sit there all day watching screen. There are network management tools installed on zOS to capture alert and monitor messages and events. There are also network operations staff who normally are might design control monitor and manage the use of the zOS networking software and the associated network. In some organizations this role might be subdivided. For example system operator might be responsible for monitoring and controlling simple aspects of the network only while the network administrator handles design and effectively all aspects relating to the continuous running of network in zOS The network subsystems such as VTAM TCPIP and related components are normally started as part of the IPL sequence under zOS. There is normally an automation software product that controls the startup and this has dependency checks or parentchild relationships built into it. For example VTAM would have to start prior to TCPIP and FTP cannot start before TCPIP. Because FTP requires TCPIP and TCPIP requires VTAM such starting order makes sense. However it is not always necessary. For example TCPIP waits for VTAM to start up if it detects VTAM is not available. VTAM and TCPIP are started tasks with JCL procedures like many other zOS components. You should familiarize yourself with the JCL and data sets and members that are in use for these components. The network administrator should work with the automation team to address any network component startup sequences and dependency requirements you have. Many of the network component dependencies and relationships could already Some of the dependencies that are of concern during system startup are The network cannot start before the zOS operating system and JES2 are up Network devices need to be varied online by the operating system. Note In mainframe environment system automation is relied upon heavily. Operational tasks are changed and generally simplified by system automation package. Some commonly used system automation packages include Tivoli NetView for zOS and System Automation for zOS. System automation is used not only to handle monotonous monitoring and zOS console watching but also for error recovery and fault tolerance. Note The zOS network administrator should develop the initial network component runbook for operations. If there are any particular checks or commands that need issuing as part of an network startup or takedown these The network must start up before any network applications. There is no point in starting these before the underlying network is available. Enterprise Extender cannot be enabled until both VTAM and TCPIP are OMPROUTE is started after TCPIP is available. In sysplex ownership of dynamic VIPA addresses is dynamically determined but there might be moment during IPL when the dynamic VIPA is associated with backup host. The same applies to sysplex distributor VTAM. Note that on the START command includes the desired VTAM list data set. The systems programmer might be using zOS symbolics to identify the LPAR. For example VTAMSYMB is not actually the name of the VTAM list data set. Instead it is system symbol variable. In VTAM context each LPAR has unique set of definitions and startup requirements. Using symbolics can allow different variable substitution to occur on each LPAR but the command can be the same. For example on and LPAR numbered 5 VTAMSYB might get The series of IST093I messages occur when VTAM reads the ATCSTRxx start list member and any other specified start option members to build the VTAM list member ATCCONxx which contains list of VTAM resources defined by the zOS network administrator. The list of IST093I messages are comprised of resource names that should eventually become familiar to the network Example 111 Some of the network startup messages for VTAM Note System symbols are defined in the zOS parmlib member SYS1. PARMLIB. Each LPAR can utilize system symbols to provide simple variable substitutions in JCL system commands and even within TCPIP configuration files. One of the most important messages within Example 111 is message ITS020I. This message is an indication that VTAM is now ready to receive and process VTAM network operator commands. The TCPIP procedure might also be started through TCPIP does not take any parameters with respect to its configuration . Instead the TCPIP task locates its parameters from the PROFILE DD statement within the startup JCL itself. The profile is read as outlined by the EZZ0309I and EZZ0316I message pair. Afterwards significant configuration attributes found within the TCPIP profile are identified with series of messages. The EZZ4202I message is of some interest TCPIP not only depends upon VTAM but it also requires that zOS UNIX System Services be active and As with VTAM TCPIP has message indicating that it has completed its startup and is ready to accept operator commands EZAIN11I. Note the messages that continue after EZAIN11I. The TN3270E server can be After TCPIP has started up the TN3270E server configuration statements are then processed. The EZZ4313I messages are of significant interest herea network administrator would want to make certain that all devices come active during the startup. the OMPROUTE and FTPD started tasks. This could also be accomplished through an external automation package. Tip How might external automation work In Example 112 an automation OMPROUTE and FTPD and any other software that depends upon TCPIP. network shutdown would normally be process that occurs as part of scheduled zOS IPL or swapping of the network from one LPAR to another. scheduled IPL occurs during period of little activity on the host and network. Usually this means it occurs in the evening hours of weekend. The reasons for requiring scheduled IPL are becoming fewer and fewer since hotswappable devices and components are becoming prevalent on the mainframe. Some maintenance still requires an IPL however and there are also organizations that do scheduled IPLs as part of regular routine. Regardless if an LPAR is shut down that means the network components must also be shut network shutdown might be independent of an LPAR shutdown. For example in partial network shutdown TCPIP or VTAM might be taken down independently. In such situation there might be VIPA takeover process that It could be dynamic VIPA that moves automatically to running LPAR in the sysplex. It could be static VIPA that is moved by the network administrator during the shutdown. Or it could be an external automation package moving the static VIPA automatically. Client connections might never be aware of the changes occurring. Because TCPIP depends upon VTAM shutdown of VTAM not only ceases all SNA communications it also halts all IP communications. TCPIP automatically detects that VTAM is no longer available and waits for it to be restarted. The order in which the network is shut down is the reverse of startup. Generally it is 1. TCPIP and VTAM applications are stopped. Each application has its own This process is normally performed through automation. Note that when halting network VTAM and TCPIP communications the network administrator must ensure that access to the console is not also halted Network administrator tasks can be varied and are one way or another derived from the needs of the organization within which the network administrator functions. In an environment other than zOS network administrator might have role that encompasses many platforms and hardware areas. However zOS network administrators tend to have more narrow focus. One reason for this is complexity network administration on zOS requires good working knowledge of zOS itself. It also requires good knowledge of networking hardware. When you combine this with the actual VTAM TCPIP LAN and WAN knowledge required the zOS network administrator is unlikely to have the opportunity to include other platforms. So what are some of the tasks zOS network administrator might undertake When something does not function as expected the network administrator is one of the first persons to work towards resolution. VTAM TCPIP and their associated applications must Planning includes network architecture decisions such as what role zOS plays in the network and how zOS should be situated in the network. In zOS network environment all changes are part of planned and controlled process. The zOS network If new feature is to be added to the zOS host or to the network used by the zOS host then an evaluation In the TCPIP world new networking applications and Network capacity requirements are always changing. The following are examples of some of the VTAM commands that you might use in zOS network administration role for controlling VTAM. Refer to zOS Communications Server SNA Operation which contains VTAM operations and commands for detailed information about commands and examples. Like all zOS environments VTAM allows the use of some abbreviations instead of the full command syntax. Some automation software products have taken this step further and provided command executables that eliminate the need to As previously mentioned Enterprise Extender is being used by many customers to carry SNA traffic over IP networks. One of the most commonly used The DISPLAY NETEE command gives the network administrator quick look at the remote IP endpoint of an EE session. In this sample the remote endpoint was 192.168.80.90. The command output provides the line and PU information as well as other node characteristics. You can see some information on the connection beginning at IST2035I. NLP stands for network layer packet. The NLPs retransmitted count can be used to gauge the amount of lost traffic over the session. Example 113 DISPLAY of remote Enterprise Extender endpoint Documentation of the environment changes and procedures are all part of the network administrators APPN RTP link. The administrator would use the DISPLAY NETRTPS command. In Example 114 the RTP PU cnr00004 is being tested. An RTP PU represents the endpoint of an HPR route. The result of this command is display in message IST1792I of the total time taking to traverse the route. Routing delays in SNA can be of significant impact for time sensitive applications. The advantage of this test command is that the network administrator might be able to determine The primary command used for controlling VTAM is the VARY command. Usually vary command results in complete state change from active to inactive or the opposite. The vary command is abbreviated as the single The following command activates VTAM resource that might be in an inactive state or new VTAM resource you have just created The following command inactivates VTAM resource that might be in an active state have problem or needs to be made inactive in order to modify it and The MODIFY command enables you to change VTAM options tables storage and traces. Usually modification command results in change in the Note Ensure you understand the Vary and Modify commands well before using them. Your security profile as zOS network administrator provides you with the ability to fully control the network and its resources. Always verify that the resource you are changing or modifying is the correct oneand Introduction to the New Mainframe Networking characteristics of resource but does not change it from state of active to inactive or the opposite. The modify command is abbreviated as the single character This command allows you to modify VTAM start options with some options there are restrictions and dependencies. If your VTAM was defined as an interchange node containing both subarea and APPN support you might want to change the VTAM search order option. If the current setting was set to SUBAREA you might want to change it to APPNFRST. You would do this with the following command The following command enables you to turn on VTAM buffer for particular The following command stops the VTAM buffer trace Example 117 shows the use of these commands. Example 117 VTAM buffer trace started then stopped on VTAM LU This command is used to stop VTAMwhich is not something you want to test on your production LPAR HALT command without any parameters performs nondisruptive end to VTAM. The HALT command is abbreviated as the single The following command halts VTAM more quickly than normal HALT command. The command causes sessions to be terminated and VTAM to shut down. The following command causes VTAM to abend and is quite nasty way to bring it down. You would normally try the NETQUICK command first. TCPIP like VTAM has number of commands available to monitor and change the environment. TCPIP provides two methods of issuing most commands either through the zOS console or from within TSO or zOS UNIX environment. However the TCPIP commands are documented in zOS IP System Administrators There are many commands for displaying and testing TCPIP network. Commands such as PING and TRACERTE are not covered here. They are effectively standard across all platforms and commonly used. The DISPLAY TCPIPNETSTAT command is often used to monitor TCP connections. On busy host the command would need to be narrowed in focus to only display connections for specific port. For example display of all TN3270 connections can be found in Example 118. In this display network administrator can determine at glance the number of connections and the state of each connection. If problem is noted the IP address of the remote host is readily available. USER ID CONN LOCAL SOCKET FOREIGN SOCKET STATE using the DISPLAY NETSTAT command. network administrator might do the TCPIPtcpprocnameNETSTATDEV which describes whether the device is fully functional or not. In addition various statistics and counts can be found. If connectivity problem is apparent and the device appears to be in good working order the next step would be to ensure that there were no routing routing problem could exist elsewhere in the network. TCPIPtcpprocnameNETSTATROUTE command. network administrator would need to know which network is this table for the appropriate route taken to reach the network. The flags field tells the network administrator information about the route. For The route is gateway route. The route was created from static definition . The route is host route with network mask of 32 bits . Other route flags exist and can be found documented under the NETSTAT command in zOS IP System Administrators Commands. You can alter the TCPIP configuration with the VARY TCPIPOBEYFILE command. The OBEYFILE command is very powerful command since it can change any aspect of the TCPIP configuration. This is because the OBEYFILE command runs process similar to the process that runs during TCPIP startup. An OBEYFILE causes TCPIP to read configuration file in the same fashion as VARY TCPIPtcpprocnameOBEYFILEDSNyour.obey.file In this example your.obey.file might contain new set of DEVICE and LINK statements HOME statement or both. Sometimes TCPIP connections need to be cleared by issuing VARY TCPIPDROP command. Before issuing the command network administrator would have discovered connection that needed to end. Either the connection is an undesired connection entirely or else it is in state needing external intervention. Such connection would be discovered using the DISPLAY Note in this example that the last connection is in state of CLOSWT. CLOSWT connection is often an indication of problem with TCPIP application. The application might have left this connection in this unusable state indefinitely. The DROP command can be used to cleanup such connection. The command to remove the CLOSWT connection above would need to use the connection number to identify it on DROP command as This command would clean up the connection as far as the zOS TCPIP is The VARY TCPIPSTOP and VARY TCPIPSTART commands are used for stopping or starting TCPIP devices. These commands are often the first set of example if device problem was detected and the device was known to be inoperative VARY STOP followed by VARY START would be attempted. Note Unlike VTAM of which only one copy can ever execute within single LPAR up to 8 TCPIP stacks can be running concurrently within an LPAR. To identify which stack should be the target of command tcpprocname should be included in any TCPIP command. If tcpprocname is not coded then the operand defaults to TCPIP. cause TCPIP to attempt to restart automatically device under most This command causes TCPIP to attempt to restart device OSA2380. There are individual commands to control the more significant TCPIP applications. There are MODIFY commands to control the IKE server OMPROUTE the policy agent SNMP and others. In this The FTP server supports the ability dynamically to activate and to deactivate several different debugging options through the MODIFY command. The DEBUG parameter can take several different operands such as There are other operands available as well as the ability to trace specific user ID. For example the command to activate tracing of FTP subcommand and Interpretation of an FTP trace requires indepth knowledge of both zOS and FTP. Often FTP traces are taken at the advice of IBM service personnel who also handle the interpretation of the output. The command suite for controlling the TN3270E server is large for two reasons. First the TN3270E server is the foremost method of communicating with the mainframe. Second the TN3270E server straddles SNA and IP which means it must operate with consideration for both environments. The DISPLAY TCPIPtelenetprocnameTELNET command supports large suite of operands that affords the network administrator either wide or narrow view of the TN3270E server. Operands can produce summaries or detailed displays For example if network administrator wanted complete view of the configuration of an active TN3270E server the following command could be The resulting output from this command has been edited for brevity and can be which controls whether the user is returned to logontype panel after logging off of an application. BASIC connection contains no TLS security. The inactivity timers are important to the TN3270E server in event of network outages or clients that fail to indicate the connection has ended. The total number of records displayed is indicated at the bottom of this display command. This simple command resulted in 85 records. This command provides considerably more detail about the active TN3270E servers profile than what has Example 1111 DISPLAY TELNET PROFILE command The other commonly use DISPLAY command is the following Example 1112 shows the output from this command. Using this command is quick way for network administrator to determine all active connections to the TN3270E server. It is similar to the DISPLAY NETSTAT command with filter on port 23 except that basic TN3270E information is included. The field TSP PTR stands for terminal session protocol pointer. The terminal column describes whether the connect is for terminal or printer . The session column indicates whether the session is active pending or negotiating . The last column protocol is normally for TN3270E but sometimes 3 could show up here indicating TN3270 connection is in use by There are also commands to stop and restart TN3270E services without having to actually stop or restart the TN3270E server. Documentation is something that people like to readbut typically do not like to create Good documentation is worth its weight in gold. It is great tool for learning about your network environmentand it helps to reduce the time it takes to resolve problem. The IBM VTAM and TCPIP network product manuals are very detailed. Such proportionate detail might be necessary in regard to the processes diagrams and setup information relevant to your organization. The information found in this Types of devices protocols in use mainframe network interfaces LPAR names TCPIP addresses network and subnetwork addresses VTAM SSCP names data center switches and routers the mainframe is connected to. This information might not be shown all on one diagram but somewhere it must all Describe the components in the diagram and explain how they are defined within your organization. Include IP network and subnetwork information. Document application names and describe how they connect to the network. Include details about connections to other organizations as well as brief description of what the connections are used for. Also include WAN service provider details the protocol used and the equipment used by the WAN Remember to cover the existence of any virtual private network Document the data set names of source libraries and where to find started task procedures VTAM names of devices applications and resources. TCPIP naming conventions might include information about host names. Document the processes relevant to the networking role common tasks you might have to perform such as problem diagnosis change control call out procedures and describe where to find additional information. Networkrelated products tools exits and automation Document the network controlling and monitoring products as well as exits that might have been implemented and why. Also cover automation dependencies and explain how to start or stop components manually. Keep log of all changes describing when what and why change was and to document their resolution. Document the people and group contacts that you work with either in security policy should never be far from network administrators mind. The network generally represents the most vulnerable aspect of host. Any changes and all processes must be in accordance with an organizations security policies. Consequently any or all of the documentation described here might include information on security classification or usage guidelines. There are many other categories that could be included here. It sometimes seems as though documentation is more work than is worthwhile. However documentation is an integral part of high availability network and host environment. Some say the document should be living and breathing thing the The role of zOS network administrator can span wide area. In some organizations you might be generalist looking after all zOS networking components printer subsystems and even some of the hardware. However it is more likely that you will specialize in one or two particular areas such as VTAM The network administrator is the person who might design control monitor and manage the use of the zOS networking software and the associated network. In some organizations this role might be subdivided. For example system operator might be responsible for monitoring and controlling simple aspects of the network while the network administrator handles design and management. There are also network operations staff who normally are the first effectively all aspects relating to the continuous running of network in zOS Monitoring means watching and observing to see something change. This does not imply that you sit there all day watching screen. There are network management tools installed on zOS to capture alert and monitor messages and events. In mainframe environment system automation is relied upon heavily. Operational tasks are changed and generally simplified by system automation package. System automation is used not only to handle monotonous monitoring and zOS console watching but also for error recovery and fault tolerance. Some commonly used system automation packages include Tivoli NetView for zOS VARY command 3. Who is responsible for initial problem source identification 4. What information is necessary in order to perform DROP of connection 5. What command can be used to display the maximum transmission unit size of an EE connection 2. What two statements should an OBEYFILE containing DEVICE and LINK statement also include 1. Another VTAM command is NETMAJORNODES. Try this command on the test system. How many major node definitions are active and how many of these definitions were activated by VTAM 2. Using the display commands identified above determine the LU name assigned to your TN3270E terminal session. 3. What operand of the DISPLAY NETSTAT command can be used to identify all active home addresses 4. What command could be used to quiesce the TN3270E server 5. According to zOS Communications Server SNA Operation when should Explain what security means in data processing context. Give reasons why security is of importance. Demonstrate the actions that can be taken to improve network security Security is all about how to reduce the impact of either intentional or unintentional damage. Damage is something most of us try to avoid. If youre car lover there are actions you take to prevent your car from being stolen like attaching wheel lock setting the car alarm and locking doors. To prevent unintentional damage to your car you might park it in your garage away from In addition security includes the actions taken once damage has or is believed to have occurred. If your car is stolen you call the police. If your car door gets dinged you have body shop fix it. Planning for security includes asking questions like What if this happens or How do prevent that from happening Security also includes answering the question Do really want to go through all this work in order to be more secure Security does not always cover all possibilities and this can be conscious decision. It is possible to break an encrypted sessions code and decrypt the data illicitly. However that is highly improbable. Is highly improbable good enough It would be nice to answer yes but the truth is it depends. Prior to dealing with the implementation of security significant amount of planning is required. Generally large organization will create security and awareness. However these topics are not covered here. And as mentioned before the choice of implementation depends on an organizations requirements. The security features relating to TCPIP on zOS are extensive. When combined with all the security capabilities of the System Authorization Facility interface the amount of control that can be exercised is phenomenal. However such system could also become unmanageable. There is cost in terms of usability and manageability with every security feature activated. So the fact that security feature exists is great but it certainly does not mean it needs to be put into effect. Remember it all depends. Keep in mind that the security options discussed here do not represent complete list. Some features that are seemingly unrelated to security may inadvertently enhance it. For example zOS clustering represents an availability information about this topic. Availability improvements are very much form of improved security. If successful attack is made against an individual host in cluster of computers the presumption is that one of the other hosts in the cluster can make up for the Some security features have somewhat overlapping effects. IP filtering and network access can both be used to prevent certain packets from reaching their destination. SSL and IPSec both result in improved confidentiality and integrity of Reminder The SAF interface is standardized function call available to all applications running on zOS. The interface call is used to provide quick and controlled authorization authentication and logging services. The SAF call is forwarded to an external security manager such as the Resource Access Control Facility . Note that the term external refers to the fact that the security management is an independent entity outside of the currently executing applications environment. The external security manager manages secure database that is used to verify the security information as it relates the user ID active when the SAF request is made. The term industry standard is used to describe security features of zOS that are widely implemented and either formally or informally standardized. One item discussed Transport Layer Security will be treated in some detail because it is probably the most widely used security protocol on the Internet today. And in order to implement it on zOS it is concept that should be fully understood. Although application layer security is not standardized there is one application layer form of security that approaches an industry standard user ID and password authentication. On zOS the authorizations granted to an end user are all associated with the active user ID. When user logs onto telnetd for example SAF call is made to verify that the password supplied matches that of the user ID. Once verified this user ID becomes associated with security environment that lasts the duration of the session. SAF products such as RACF allow the creation of specific password rules forcing them to be of minimum length to be renewed regularly to be not repeated and to contain variety of character types. Although it is often taken for granted the user ID and password method of Secure Sockets Layer is protocol standard developed by the Netscape Communications Corporation that uses encryption to provide confidentiality and authentication between two TCPIP applications. As SSL gained in popularity the IETF formally standardized SSL made few improvements and changed the name to Transport Layer Security . TLS is defined in Request for Comments 2246. TLS connection begins with handshake. As the name suggests the handshake entails the initial setup of TLS connection. During the TLS handshake an exchange of information occurs that includes the following to operate using TLS. Note TLS authenticates by using certificate. By default TLS requires that server always send certificate to the client. This certificate allows the client to verify that it is really talking to the server it attempted to contact. The idea here is to prevent computer in If the FTP server requires modifications to support TLS what about the FTP client The FTP client also requires similar configuration changes. Of special note however is the certificate requirements. As noted the FTP server sends out certificate to prove its identity. How does certificate that can vouch for the authenticity of the certificate received from the Virtual Private Network is general term used to describe secure tunnel between two endpoints. The term does not describe protocol. The industry standard protocol for VPN is an architecture called IPSec. The IPSec architecture is outlined in RFC 2401 and its implementation encompasses RFCs 2402 2406 and 2407 . This implies that the endpoint of VPN may exist on the same host as the application is running on or the endpoint could be at an adjacent firewall on the network. It all depends upon the organizations needs. The other implication of being at the network layer is that all IP traffic can be directed through VPN. All traffic implies not just traffic from different applications but also traffic from different applications using other protocols like UDP or ICMP. With TLS only the traffic between the two implementing applications is protected. VPN can be further divided into two different types manual VPN and dynamic VPN. Although zOS supports manual VPNs they are not very commonly used. Consequently this text only discusses dynamic VPNs. dynamic VPN requires separate server to support the exchange of the keys that will be used to encrypt data at each end point. In zOS the key exchange is supported by the IKE daemon. IKE stands for Internet Key Exchange which is the standard protocol used to exchange keys for VPN. How is this all accomplished on zOS The characteristics of the dynamic VPN are controlled by the TCPIP stack using information from the policy agent. The policy agent is daemon that runs with the purpose of reading policy definitions from Lightweight Directory Access Protocol server. The policy definitions are in turn read by the TCPIP stack. Heres where you can breath sigh of relief the policy definitions themselves are created using graphical user interface application running on workstation. The application is called the zOS IP Security Configuration Assistant. It makes the creation of the rules surrounding VPN relatively simple task. Application Transparent TLS is unique usage of TLS on the zOS end of the session. In principle it is quite simple Instead of having the application itself be TLScapable and TLSaware the establishment of the TLS connection is pushed down the stack into the TCP layer. Many applications on zOS can run without even being aware that the connection is using TLS. Remote clients cannot distinguish between normal TLS and ATTLS . than the standard TLS. Because TCPIP is layered protocol the changes done at the TCP layer are hidden from the application layer. ATTLS will appear identical to normal TLS to any application connecting to the zOS host. The ATTLS environment is activated by simple option within the TCPCONFIG statement block in the TCPIP profile data set TTLS. When coded the TCPIP stack will use the policy agent to determine how to handle each applications communication. OpenSSH stands for Open Secure SHell and it is sometimes referred to as secure shell. Unlike TLS and IPSec OpenSSH is not formalized standard but it is used widely in the IP community and consequently it was ported to the zOS platform. As of the time of writing the IETF is working on producing RFCs to The sftp client and sftp daemons provide secure FTPlike The scp command is secure alternative for the remote The ssh command functions similarly to the remote login command or remote shell commands. The daemon end is supported by the secure shell daemon Note This is the second time the policy agent has been mentioned as the source of configuration data. So you might wonder why are some definitions coded in policy agent and some in specific application configuration files The primary reason for this is conceptual the configuration data that belongs under policy agent control should be information that is related to the policies and goals of the organization. Remember that security choices should flow from security policy document The policy agent is the service that implements the policies. The other advantage of the policy agent is that it uses LDAP as the source of the policies. The LDAP directory service is networked repository of configuration data available to all hosts in the network. readily apparent advantage is that multiple TCPIP instances take advantage of policy data Like most protocols relating to IP LDAP is defined via RFCs 1777 OpenSSH on zOS is the same as you would expect to find on any UNIXlike There are some security features for which although implemented on most platforms no standardization is necessary. Additionally there are some security There are two fundamental varieties of Intrusion Detection Services . IDS can function within the domain of an individual host or it can function as network IDS with scope including the entire network to which it is attached. On zOS the scope is the former kind only IDS functions within the zOS host only and no efforts are made to function outside of the zOS host. There are specialized platforms designed to perform network IDS and it does not make sense to use zOS in such role. In zOS the IDS capabilities are built into the stack itself. Many of these capabilities are automatically handled by zOS. For example malformed packets are automatically discarded independent of any settings controllable by the system administrator. But the capabilities of IDS can be expanded to include the following types of scan is systematic accessing of network resources over period of time from single IP address. Scan attacks are not detrimental to host. However they are an indication that host on the network is trying to determine what ports are open for attacks are important since the host doing the scan may later be the same host that launches more virulent attack. An attack on host can take many forms. It is impossible to list all of them here but few examples are flood attacks redirection Intrusion detection is not precise science. Scans can come in slowly or quickly depending upon the hacking tool in use. Also flood of connection requests may just be large group of users logging back on to the host. For example if an intermediate router went down briefly and 5000 users were disconnectedwhen the router came back up flood of new connections could be received at the Therefore it is up to an individual organization to determine what sequences of events are to be considered an attack and what sequences are benign. The implementation of the IDS rules is done through the policy agent. What exactly can SAF do in networking context It can collectively do more to restrict enduser capabilities than any organization might ever want to implement. In other words the security features listed here might be used in combinations but it is unlikely that any organization would want to implement more than few of these features. Every organization is unique. Most organizations will find that some of the SAFbased security features listed here have place in the context of their security policy. As mentioned in 5.5.2 The multistack environment on active TCPIP stack. And for the most part if the application has been coded to support it stack can be accessed by changing the TCPIPJOBNAME statement in resolver configuration file. That sounds like security exposure and it is. SAF can be used to restrict which TCPIP stack can be accessed by any individual user ID. One of the standard methods of penetrating network is to use intermediate hosts as access points. An organization might want to limit such possibility. In addition some user IDs on the zOS host might be used restricted on an individual user ID basis. In order for hacker to continue through an intermediate host port is required. SAF can be used to restrict the range of ports that user ID can access. In addition port access can be used to add extra security to the user IDs associated with server or daemon. Since some servers may run with zOS UNIX Introduction to the New Mainframe Networking user ID to only using the port number it requires. The output from the netstat command depending upon the option used can be considered security risk. For example information about attached networks or interface type could be used to the advantage of hacker. Using SAF all or some of the netstat command options can be restricted. TCPIP on zOS supports packet tracing service. The service uses facility of the MVS operating system itself to perform lowimpact packet tracing including filtering options. There is zOS packet trace formatting environment but the data can also be exported in formats compatible with workstation trace analysis unauthorized tracing of data. So how does all this SAF control get put into place Within the SAF environment is class of resources called SERVAUTH. The SERVAUTH class can have resources defined to represent IP features. ID with UID of 0 has near complete control over the UNIX operating environment. On zOS host this is not quite as dangerous as on be regulated accordingly. Reminder The netstat command on zOS is available Windows and UNIX style operating systems. It can be Using the same order as above the resource definitions begin with something After such resource has been defined universal access permission must be set. For example universal access of none might be used. This would implicitly protect the resource from being used by any users. Individual user IDs could then be given access to the resources as needed. Port control is big topic in the world of network security. Within an organization an individual or group responsible for boundary device firewall might have strict rules as to which ports can be used. And even within the zOS host keeping tight control on which ports are available is key to healthy and secure system. With respect to firewall one of the most difficult areas is ephemeral port usage of an FTP server. The FTP server may request an ephemeral port for performing data transfer. By default this port number could be anywhere in the range from 1024 to 65535. However it is not desirable to open up such wide range of ports to firewall. Using the PORTRANGE statement in the FTP. DATA configuration file FTP ephemeral port usage can be limited to low port and high port range. This same range would be reflected in the appropriate port filtering firewall. Access to ports below 1024 should normally be restricted. This is controlled by the RESTRICTLOWPORTS option on the TCPCONFIG and UDPCONFIG statements in the TCPIP profile see Example 122. Reminder An ephemeral port is one that is required to complete connection between endpoints but the actual port number required is unimportant. Ephemeral ports are assigned by the TCPIP stack in sequential pattern. An application that needs an ephemeral port asks for the port and it is provided by the TCPIP stack. Ephemeral port numbers If RESTRICTLOWPORTS has been coded how can an application such as Web server or FTP access their normal ports The answer is in the PORT statement shown in Example 122. PORT statement entry is required for any application that wants to use port below 1024 when RESTRICTLOWPORTS is in effect. The TCPIP stack reserves ports for the started task name listed on the PORT statement. The PORT statement is rudimentary form of control only an application with the assigned task name is allowed to use that port number. The FTP application has the unusual characteristic of using second connection for the data transfer. It also allows user to hop to secondary FTP process called proxy. Consequently the FTP environment has some further restrictions possible. Obviously such controls would be effected using the FTP. DATA configuration data set. For example the FTP subcommand PORT can be disabled in proxy environment by using the PORTCOMMAND statement. Or the IP address of PORT or PASV command can be forced to match the IP address for the remote end of the control session. This is accomplished using the PORTCOMMANDIPADDR and PASSIVEDATACONN statements respectively. Although the zOS operating system is not an ideal operating system to run as routing host it does have the capability of running fully functional firewall filtering rules. Packets can be permitted or denied based upon source destination or port number. Probably the best usage of IP filters on zOS would be as secondary line of defense if boundary firewall has been compromised the IP filters on zOS could form another hurdle to potential hacker. In zOS IP filtering rules are activated using the policy agent. There are many aspects of the implementation of network that can be Multiple TCPIP stacksAlthough having separate IP stacks within single LPAR can be compared to having two separate hosts running TCPIP it is not quite as secure. However if clear delineation between IP endpoints is required within single LPAR multiple TCPIP stacks provides this capability. By using two stacks instead of two IP addresses within single stack there is greater activated to prevent applications from activating certain socket options such as the ability to send The TN3270 environment is unique and complex enough to warrant some special attention. As mentioned the TN3270 server supports TLS. In addition the TN3270 server makes full use of SAFbased authentication. And if desired TLS and SAF can be used together to force TN3270 client to send certificate that is associated with SAF controlled user ID allowing product like RACF further control. sample excerpt of some related TN320 server statements is shown in Take closer look at these statements as they apply to security TN3270 configuration statements can control the LU that given network or IP address can access. In this case IP address 9.29.168.30 will be assigned VTAM LU called TSOLU001. The application selection can be limited based upon network or If workstation TN3270 client sends too much of certain type of data or just too much data at single time the connection may be dropped . The TLS configuration statements are effectively the same as those for the FTP application. SNA can be roughly divided into two types of implementation subarea and APPN. The security considerations are slightly different between them. The networks that contain genuine SNA traffic are generally not publicor at least are considered to be secure networks again reducing the security requirements of SNA traffic. In the event that security measures are considered appropriate for SNA traffic When using an encrypted session LU authentication can be performed to certify that the key used by each endpoint is the same. However if authentication is not requested the mismatch of the session keys prevents An additional code can be sent with all SNA data messages. This code can be used to verify that the confidentiality between sessions. It is reasonable to state that the majority of APPN traffic is now encapsulated when it is on the network using UDPIP . An APPN session can be defined to require that data be encrypted between LUs. Security entails the reduction of the likelihood of damage whether intentional or inadvertent. Within context of zOS features such as TLSSSL IPSec and SSH can be used to improve the security of data on the network. Other features such as ATTLS and SAFbased security can also be used. Intrusion detection Note SNA uses symmetric encryption for LU to LU sessions. This means that the key at each endpoint is Introduction to the New Mainframe Networking services are already active to some degree within the TCPIP stack. The policy agent is the repository for many securityrelated configuration values. Due to the complexity of the SNA architecture and its present limited use in the telecommunications portion of the network security in this context is not as much of concern as it is in an IP context. 1. List at least five different elements of security. 2. ATTLS TLS and VPN are implemented at which layers netstat or netstat route command 4. Are some intrusion detection services automatically part of TCPIP function 1. RFC 2246 is the IETF standard for TLS. What are the goals of this RFC 2. Does an application need to be rewritten to support VPN 3. What are some advantages of using ATTLS instead of TLS 4. Why might vendor of TCPIP application be reluctant to provide details of the intrusion detection capabilities of their product 1. In TLS which end of session is always Virtual Private Network 2. What are some of the functional differences between TLS and IPSec 3. Try issuing netstat ids or netstat command. What can you learn from protects itself from attacks such as malformed packets and TCP SYN floods. Using the Internet or any other source determine what is entailed in either Problem determination occurring in Communications Server for zOS . This List the different types VTAM traces. Use zOS commands to display or modify data and settings. Because businesses depend so heavily upon the availability of data processing systems problem in the network can be catastrophic. Real money is lost when networks fail. When the network is used for connecting transaction processing mainframe to the outside world the losses can be staggering. Keeping the network problemfree and responsive is priority so determining where problem lies and fixing it quickly is imperative. Your first indication of network problem may come from users or operators. To begin determine the general cause of the problem by performing these tasks Read messages in the system log. Each zOS component has unique TSO messages are prefixed by IKT. Look for suffix which stands for error for example Check the appropriate messages manual for an explanation of the error. Also check the system log to see if the system created an internal memory dump at error time. Note Unsure of the appropriate messages manual Try the following Web Find out whether the system has changed for example whether the Check whether there has been recent hardware change to OSAExpress type or definition After you locate the general problem area use the tools and diagnostic aids at your disposal to track down the problem. zOS has number of tools and diagnostic aids for VTAM and TCPIP that you should be aware of. The following tools and diagnostic aids can be available on or are utilized by any zOS application. Consequently they are common to both TCPIP and If an application ends abnormally it can generate memory dump for analysis. The dump known as an abend dump is produced when one of the following The operator enters CANCEL command. job abnormally ends as the result of an invalid operation. The start procedures for VTAM and TCPIP usually have special DD statements that direct the Note So what is an invalid operation divide by zero is an obvious example which could result in an abend 0C9. For more information on abend Introduction to the New Mainframe Networking memory dump to storage device. The resulting dump is written to the data set The standalone dump might also have been named laststand dump. In other words when an entire zOS LPAR is in such state that it is no longer functioning properly program called standalone dump program can be executed. The sole purpose of the standalone dump program is to move raw Examples of states in which standalone dump is taken are when the system is disabled in loop or any other condition that significantly impacts system performance. standalone dump program is effectively its own operating system zOS itself is halted and the standalone dump program runs on its own. After the dump is completed zOS must be reIPLed. Obviously standalone dumps are rare and disruptive events in data center. IBM support will be requested to do an analysis of the system to determine what precipitated the failure. but the exact areas that are dumped depends on system settings. An SVC dump is written to SYS1. DUMPnn data set the SYSMDUMP output data set or the data set specified on the DCB operand of program exception occurs. VTAM or TCPIP may or may not be terminated as part of this process. An operator requests dump with the operating system DUMP command. An operator uses SLIP command with ACTIONSVCD specified and an event occurs that matches the trap indicated in the SLIP. Reminder An SVC is supervisor call which effectively executes system macro. The hexadecimal operation value of an SVC is X0A. The SVC number assigned to an abend is X0D. If program wants to execute an SVC dump the instruction used is X0A0D. System recovery routines produce an SVC dump due to VTAM or TCPIP Dumps although undesirable do happen. When they occur the obvious goal is to capture information that will assist in problem determination and ultimately avoid having the problem recur. The writing of the dump to tape or disk device must be done as efficiently as possible. Consequently all dumps are written unformatted. IPCS is the tool used to format dump of any address space running on zOS. Control blocks can be formatted by IPCS as well summaries and analyses of many aspects of the execution environment. IPCS can also be used to format VTAM and TCPIP traces. In addition to the collected data just mentioned system operators should answer the following questions as part of problem source identification. Presuming that VTAM TCPIP and the network were running well at some point the question then becomes what has changed Obviously initial problem source identification should focus on the area that has been changed. PTF could have problem or the process of applying the PTF might not have been completed correctly. Search the IBM customer support data base using the PTF number. PTF numbers are only useful when the actual failing load module or component is known. Otherwise the approximate maintenance level can be determined by the maintenance level of the entire system usually referred to as PUT level information. To assist in problem determination VTAM hints have been added to many of the architected sense code definitions. See Example 131 for sample of decoding VTAM sense code. switched subarea links are defined. VTAM sense codes are four bytes long as explained here The first two bytes together are referred to as the sense code. Breaking down these two bytes the first defines For additional information on sense codes refer to zOS Communications The VTAM internal trace provides record of the sequence of events within VTAM. It is probably the most commonly used VTAM trace option. These internal events include the scheduling of processes the management of storage and the flow of internal path information units between VTAM Both the TRACE start option and the MODIFY TRACE command have an message buffers. The IO trace shows all IO sent between VTAM and particular network The SMS trace shows information about the use of buffers including how often buffer pool has expanded how many buffers are currently being used and what was the maximum number of buffers used since the last trace record was written. The TGETTPUT trace shows each message as it passes between TSO VTAM DISPLAY commands aspects of VTAM. For example some of the more common commands are Shows VTAM buffer usage existence of routes and whether route is operational Provides information about the TRL major node or about The following are some sample outputs from VTAM display commands. CSALIMIT 120661K CURRENT 6755K MAXIMUM 6787K Reminder TRL stands for transport resource list. TRL is VTAM major node that can be defined on VBUILD statement or it may be dynamically created. The major node consists of TRL elements or TRLEs. TRLE defines the communications characteristics of multipath channel or In this display you can see that the VTAM internal trace is running and active. The VTAM internal trace options active are PSS and SMS . PSS stands for process scheduling services which is the component of VTAM that handles the scheduling and dispatching of VTAM routines. VTAM passes all external trace data to the generalized trace facility . GTF must be active to use VTAM traces. Consequently trace options must be specified at the starting of the GTF procedure. This procedure must reside in SYS1. PROCLIB. The external trace file produced In library SYS1. PARMLIB the member GTFPARM should contain the VTAM trace options. For example trace components are captured under the GTF trace option USR. In other words GTF must be tracing USR in order to capture the VTAM SYS1. PARMLIB the operator receives prompt at GTF startup. At the prompt the specific VTAM components can be entered as desired. You can use the Interactive Problem Control System to analyze traces written to the GTF output data set. In addition to abnormal end dumps TCPIP has various tools and diagnostic aids By utilizing the zOS component trace facility you can store trace data efficiently into data sets. After tracing of failure is complete Note TCPIP utilizes VTAM for both memory services and for communications IO. Consequently when SYSTCPIS for the TCPIP intrusion detection services trace By far the SYSTCPDA packet trace is the most commonly used component trace. The other trace options are usually utilized only as result of instructions from IBM support personnel or by advanced TCPIP system administrators. Consequently they are not discussed further in this text. Component trace writes trace data either to memory or to an external writer program which writes the data to storage. The following command sequence activates SYSTCPDA packet trace and stores the TRACE CTWTRSTARTpktwrt where pktwrt represents the JCL library member name that is used to invoke TCPIPtcpprocPKTTRACECLEAR where tcpproc identifies the TCPIP address space. The IP address of the client nn.nn.nn.nn was used to filter the packet trace entries to be captured. There are large number of filters available for xxWTRpktwrtEND where xx is an identifier from the TRACE prompt that asks for TRACE Note When reading command like the above the upper case portion represents the actual command itself while anything in lower case represents usersupplied parameter. This convention is used throughout 5. Recreate the problem. 6. STOP all traces as soon as the problem occurs so that the trace entries do not wrap . Displays the home IP addresses for the IP stack . Displays performance statistics . TSO NETSTAT VIPADCFGDisplays dynamic VIPA configuration data . TSO NETSTAT SOCKETS Displays sockets based on client name . Displays information for all TCPIP connections . Note How do you take dump from the zOS console For TCPIP try the following sequence of commands. Note that this includes dump of the TCPIP data space which can contain some internal component trace Other commands that are available for network diagnostics are This command can be very useful in determining whether IP connectivity exists. Excellent options are LENGTH and With LENGTH you can try various packet sizes. With COUNT you can determine the number of times to ECHO requests. For this reason ping can fail in some instances when for instance TCP connection might work With this command you can find the last router packet can reach in disrupted network or you can verify if packets flow over planned route. Example 135 illustrates sample of these commands placed into batch job. The batch output can be kept for further analysis. Notice the program executed in this sample IKJEFT01. IKJEFT01 executes in TSO which means this NETSTAT command is really running in batch TSO Following is an explanation of the flags in the NETSTAT output. This flag indicates that the route entry is up and running or ACTIVE. If there is no then the route entry is defined but not active. This This flag indicates that the route entry specifies an indirect route. That means the destination indicated on the route entry is behind router from this zOS system. If there is no then the route entry specifies direct route. That means the destination indicated on the route entry is on the same local network. This flag indicates that the destination field in this route entry specifies host route. That means this route is used only if the destination IP address of datagram exactly matches all 32 bits in the route entry destination field. If there is no then the destination field in this route entry specifies network route. That means this route is used only if the destination IP address of datagram exactly matches all the network bits in the route entry destination field. Indicates the route is static route that cannot be replaced by routing daemon . Example 137 shows the command output from NETSTAT HOME command. This flag indicates the link OSA23A0LNK is the primary interface. The primary interface can be significant for any applications that make request to TCPIP asking for the default IP address. This address is not used for routing purposes. The interfaces beginning with EZAXCF all have the same IP address and represent various dynamic XCF links to other TCPIP stacks running on other Note LOGREC is the log recording data set used by zOS. Any abend that occurs is recorded here. In addition hardware failures are If TCPIP abends as noted in 13.2.1 Common tools and diagnostic symptoms particularly in load balancing and sysplex environment. Within this The data is not moving at the desired or expected rate. This is also called throughput problem and is usually associated with bulk data transfer. The difficult part is knowing the source of such problem. For example is the problem result of the TCPIP address space the target application the target host itself or is it an intermediate host somewhere in between Could it be switching equipment Or could the problem itself begin at the workstation or The following suggestions can help you to narrow down the source of the Dump of the VTAM primary address space including the Output from the VTAM internal trace Collect the following documentation application is the zOS system log. Messages might also appear in the SYSPRINT SYSERR SYSERROR and SYSDEBUG data sets. DD statements check your JCL to be sure. As noted earlier TCPIP messages begin with the EZ prefix. The zOS Communications Server IP Messages manuals stops processing. Or perhaps the application is looping or in slowdown. The following actions would be appropriate in such situation 1. Obtain an SVC dump of TCPIP or the looping TCPIP application by issuing the DUMP command from the zOS system console. If the loop is disabled the zOS system console is not available for input so take standalone available because sometimes these messages contain return or reason code details that are systemrelated rather than applicationrelated. Note The standard TCPIP applications that are included in zOS telnet SMTP and many other applications write EZ messages that can be Introduction to the New Mainframe Networking 3. Obtain the appropriate portion of the zOS system console log. 5. Obtain the LOGREC output. The following basic sequence can assist in diagnosing networkbased problem 1. Test and verify the TCPIP address space configuration using NETSTAT 2. Test connectivity to remote hosts using the PING and TRACERTE 3. Obtain TCPIP packet trace . The packet trace can be especially useful for determining where delays or response failures occur. By examining time stamps you can determine whether delay is at the zOS end of the connection or somewhere else on the network. The component SYSTCPDA trace is one of the starting points for diagnosing the data can subsequently be formatted using IPCS. There are many filtering and Window frequency 0.0001 0.0001 Windowss that was traced. It begins with the basics such as IP addresses and port numbers and then continues on with all other measurable aspects of TCPIP duration and only few bytes were exchanged. Considering that it is connected to port 21 on the mainframe the bytes exchanged would presumably be FTP commands flowing along the control connection of an FTP session. Much of the other information in packet trace requires sophisticated knowledge When the problem appears to be outside the mainframe sniffer trace may be appropriate. sniffer trace may be used to run trace at the remote end of the connection at the same time as packet trace is running on the more accurately pinpointed. For example by comparing timestamps response time problem can be confirmed as being on the mainframe the remote host or on the network in between. There are many tools that perform packet sniffing network monitoring and protocol analyzing. Two of the most common are the SNIFFER Tool from Network General or ETHEREAL which is an Open Source Software released under the GNU General Public License. Communications Storage Manager is component of VTAM that enables host applications to share data with VTAM and other CSM users without having to physically copy the data. CSM reduces CPU utilization and optimizes system performance during the transfer of bulk data by enabling applications to share The code for CSM comes with VTAM but after CSM is started it runs independently of VTAM and can be used by other zOS tasks and subsystems. CSM is started automatically when it is first invoked and continues to run even if VTAM terminates. While CSM is not in use it retains minimum amount of storage. It terminates only when zOS itself terminates. either CSM has consumed too much storage or else an application cannot get any more storage from CSM. The following sequence of instructions provide 2. Activate CSM VTAM traces 3. If the DISPLAY CSM output indicates TCPIP owns the storage system PARMLIB to specify BUFSIZE. This provides 16 CTRACE buffer. TCPIP must be recycled after this change. Note that xx is the reply number for the response message listed on the 4. Obtain console dumps VTAM and TCPIP. 5. If display output indicates that CSM storage growth is in one of the CSM dataspace pools dump the dataspace named in the netCSM output in Note that dddddddd is the name of the dataspace. 6. If the DISPLAY CSM output indicates TCPIP owns the storage also dump the TCPIP dataspace because you turned on CTRACE in step 3 CSM and VTAM buffer usage can be monitored using the following commands. You can use the following display commands for monitoring storage Example 1310 shows the command output of DISPLAY netCSM command. In this example job name TCPIP has 9.244 MB of CSM storage allocated by CSM on its behalf. The vast majority consists of 4 buffers within the TCPIP When situation arises where network performance does not meet expectations the first step is to what happens if no problem is actually identified Such case can happen when the problem is function of network performance. How large can the largest packet be Is such packet How much throughput is available either as function of the adapter speed itself or else residual throughput of an Other factors can obviously come into play that are actually outside of the network. For example how quickly can the endpoint applications respond to packets coming from the network there are fields such as round trip time which provide an accurate estimate of the time it takes packet to travel out and back along the connection. In characteristics of the connection. Using packet trace to analyze network performance however is not the right tool for the job. There are many available network performance analysis tools that can reveal details of networks performance that packet trace could Because businesses depend heavily on the availability of data processing network problem might include error messages unusual system behavior slow response time or no system response. The network administrator should first determine the general cause of the problem by reading error messages checking for system memory dumps checking to see if software or hardware has changed and reading the system log. After determining the general cause of the problem the network Note TCPIPs dispatching priority should be on par with VTAM dispatching priority which effectively means it should be in the highest service class available. The same goes for critical timesensitive servers like OMPROUTE. Introduction to the New Mainframe Networking administrator should use the tools and diagnostic aids at hand to determine the specific cause of the problem. Lastly tuning tasks should be carried out to ensure good network performance. zOS has diagnostic aids that the network administrator can use abend dumps standalone dumps and supervisor call dumps which the Interactive Problem Control System can format for easier reading. Additionally VTAM has specific aids such as First Failure Support Technology CSDUMPs network traces sense codes VTAM traces and commands that display the state of VTAM components and resources. TCPIP has component traces and diagnostic display CSMs use of storage activate CSM VTAM traces and dump CSM 3. Which components messages are prefixed by EZ 4. What are the data set names used to debug TCPIP 5. What is the command to display the VTAM start options 1. What is the decimal number assigned to the SVC for taking an abend 2. What is the cause of an abend 0C4 with reason code of 4 VTAM internal trace 3. How many FTP subcommands were entered over the control connection for 1. What is the meaning of message IST561I . In SNA link station directly connected to given node by link connection over which network traffic can be carried. adjacent node. In SNA node connected to another node by at least one path that connects no other node. In OSI node that is attached to the Advanced PeertoPeer Networking . extension to SNA featuring greater distributed network control that avoids critical hierarchical dependencies thereby isolating the effects of single points of failure dynamic exchange of network topology information to foster ease of connection reconfiguration and adaptive route selection enduser services and supports sessions between its local control point and the CP in an adjacent network node. It uses these sessions to dynamically register its resources with the adjacent CP to send and receive directory To cause the users terminal to give some audible or visual indication that an error or some associated equipment. ASCII uses coded character set consisting of 7bit coded characters. residing in open systems to exchange information and that contains the applicationoriented protocols Routers that attach to more than one area. All area border routers are part of the backbone so they must either attach directly to backbone IP subnet or be connected to another cells. It is asynchronous in the sense that the recurrence of cells containing information from an individual user is not necessarily periodic. ATM is of the identity of user or process and the construction of data structure that contains the privileges that were granted to the user or process. The process of granting user either complete or restricted access to an object The degree to which system or resource is ready when needed to process data the percentage of time system network or component Generally the percentage is derived by dividing actual availability time by scheduled availability time. set of nodes and their interconnecting links providing the primary data path across network. In local area network multiplebridge ring configuration highspeed link Pertaining to system device file or facility that can be used in the event of malfunction host that is designated as backup in the event that the distributing host should malfunction. The backup host takes over the IP address of the distributing host when required. See The difference expressed in hertz between the highest and the lowest frequencies of expressed in terms of peak cell rate sustainable cell rate and maximum burst size method of running program or series of programs in which one or more records are processed with little or no action from attached peripheral nodes such as interconnecting subarea path control and peripheral An extension to the APPN network architecture that appears as network node LANs that use the same logical link control protocol but that can use different medium access control protocols. bridge forwards An area of storage that compensates for the different speeds of data flow or timings of events by temporarily holding block of data that is waiting devices located between two end points with only one device being able to transmit at given moment. media access method that monitors another stations transmissions. If the transmission it stops transmitting sends jam signal then waits for variable time before trying network searches by providing focal point for queries and broadcast searches and by caching the channel subsystem that manages single IO interface between channel subsystem and set of subchannels that directs the flow of information between IO devices and main storage relieves the another system or process to provide it with access to data services An unwanted condition that results from concurrent transmissions on channel causing the controlled by one or more programs stored and executed in the unit. It manages the details of line control and the routing of data through network. IBM software that supports the development and use of application workstations multiple concurrent connections that use wide range of protocols and several application programming interfaces that may be called concurrently and that are designed for clientserver and distributed application programs. Communications Server includes the necessary interfaces for network management and is available Communications Server is not available as standalone product. Rather it is an element of the amounts of data. CSM enables authorized host application programs to put data in buffers that can be addressed and accessed by other authorized host application programs without any need to copy In data transmission functional unit that permits common transmission medium to serve more data sources than there are channels currently available within the transmission messages into single message or extracts individual messages from the data sent in In data communications an association established between functional units for transport protocol such as UDP that does not require connection to connectionoriented protocol. requiring establishment of session prior to data without modification. The capability to attach variety of functional units without modifying them. device that coordinates and controls the operation of one or more inputoutput devices and synchronizes the operation of such devices with the operation of the system as whole. VTAM the function in the system services control point that controls initiation and termination that allows programs to communicate channeltochannel peertopeer across sysplex. DB2. Database 2. data integrity. The condition that exists as long as accidental or intentional destruction alteration or Interconnection reference model the layer that provides services to transfer data between entities in the network layer over communication link. The data link layer detects and possibly corrects errors data link switching . transporting network protocols that use IEEE 802.2 logical link control type 2. SNA and NetBIOS are examples of protocols that use LLC type 2. See The major unit of data storage and retrieval consisting of collection of data in one of several prescribed arrangements and described by control information to which the system has access. shared memory regions in POSIX. data space contains data only which can be shared by multiple address spaces without inadvertently being elements being transmitted or intended for transmission in character or binarydigit form using defined format. All information sent over data link usually in single read or write operation. For example data In packet switching selfcontained packet independent of other packets which carries information sufficient for routing from the originating Pertaining to an attribute value or option that is assumed when none is explicitly specified. node or network node that owns dependent LUs in its local node or in adjacently attached nodes and node station or particular terminal to which information is to be sent. An external logical unit device control block . control block used by access method routines in storing and retrieving identify an individual server company or some other entity and to associate public key with the The designated contact for sysplex. The distributing host is the normal owner of the IP address that clients out in the protocol that adjusts automatically to network topology or traffic changes. that requires separate server to support the exchange of the keys that are used to encrypt data dynamic XCF links. Links using the crosssystem coupling facility hat can be automatically generated technique used by layered protocols by which layer adds control information to the protocol data unit from the layer it supports. In this respect the layer encapsulates the data from the supported layer. In the Internet suite of protocols for example packet would contain control information from the physical layer followed by control information from Performance Routing that provides encapsulation of implementations temporary port number assigned to process for the duration of call. Ephemeral packetbased networking technology for local area networks that allows multiple transmission groups that connect two subarea nodes. An explicit route is identified by an origin subarea address destination subarea address an explicit route number and reverse explicit route group of coded character sets that consists of eightbit coded characters. EBCDIC coded character sets map specified graphic and control characters onto code points each consisting of 8 bits. EBCDIC is an extension of BCD which uses only 7 bits for interconnects APPN networks having different network identifiers or separate partitions of the same APPN network where the partitioning is to allow isolated topology subnetworks . network routing allowing it to support LULU sessions that do not terminate in its native network. File Transfer Protocol . application protocol used for transferring files to and format identification field . indicates the format of the TH that is the presence or absence of certain fields. TH formats differ in The block of information transmitted between two or more stations in the data link layer of network. It includes delimiters control characters the network based on the address field in the frame and for managing characteristics such as different communication protocols different network architecture or different security policies in which case the gateway performs translation role as well as connection environment service program that records significant system events such as supervisor calls and start IO operations for the purpose of problem variation of the Ethernet protocol that is capable of transmitting data at one In data communication pertaining to transmission in only one direction at time. Contrast hardware configuration definition . An interactive interface in zOS that is used to define Hardware Management Console . console used to monitor and control hardware such processor complex. It eliminates the need for any physical cabling or external networking connection transmission group connecting adjacent nodes. To the routing layer the logical distance between and provides an access point to that network. The host can be client server or both client and program that enables computer that uses the Hypertext Transfer Protocol to serve objects by responding to requests from other In network point at which circuits are either connected or switched. For example in star network the hub is the central node in starring physical area of the processor frame where connections to the central processor complex An IBM licensed program that provides access control by identifying users to the system verifying users of the system authorizing access to protected resources logging detected unauthorized attempts to enter the system and logging detected accesses to protected international communications standard for sending voice video and data over digital telephone lines. The worldwide collection of interconnected networks that use the Internet suite Internet Protocol acts as an intermediary between the higher protocol layers and the physical standards and applications with an organizations existing computer networking intrusion detection service. detects attempts or successful attacks on monitored resources that are part of network or host system. For example 9.67.97.103 is an IP address. The address field contains two parts the first part is the network that consists of subnetworks that are connected through the Internet Engineering Task Force standards that define an architecture at the Internet Protocol layer to protect IP traffic by using various security example bus or ring that can operate independently but that is connected to other parts of reachable neighbors of router or network. The protocols topological database is formed from the APPN or subarea node that represents the connection to another APPN or subarea node that is The monitoring and management of the workload on servers. If one server exceeds its workload requests are forwarded to another server local area network . connects several devices in limited area having at least one end node for an endpoint. In an end node the database The means which directory services in node uses to find resource that is not in that node. The Locate search enables directory services to ask the directory services components in logical channel subsystem . subsystem structure that provides channel path and subchannel controls for configuring from one to four channel subsystem images. Each channel channel paths and each logical partition has access network to communicate with another user or application program. An LU can support at least two directly to other nodes using peer protocols and derives network services implicitly from an adjacent APPN network node that is without the direct use of logical connection between two logical units in an SNA network that typically devices in the network use these addresses to locate specific ports in the network and to create and addresses are 6 bytes long and are controlled by the computer usually in computer center with extensive capabilities and resources to media access control . sublayer of the data link control layer that supports mediadependent functions and uses the services of the physical layer to provide services to the logical modem . converts digital data from computer to an analog Transmission of the same data to selected group of destinations. special form of broadcast in which copies of packet are delivered facility that allows multiple logical partitions to share ESCON channels and optionally to share any of the zOS operating system. In current usage the term MVS refers to those services and functions of physical unit control point or system services control point . It is the origin or the destination of information transmitted by the network hardware technician. specific skills and tools for supporting the physical printed circuit board that plugs into personal computer server or workstation. It controls the exchange of data over network and provides the electronic functions for the data link protocol or access method such as token In Open Systems Interconnection architecture the layer that is responsible for routing switching and linklayer access across the An APPN network node that provides network services for its local LUs and Interconnect network architecture such as the nodes within an APPN network. It includes entries for all network nodes and the transmission groups interconnecting them and entries for all suite of protocols function that provides intradomain information transfer. An alternative to the Routing Information Protocol OSPF allows the lowestcost routing and handles routing in mainframe hardware feature that combines the functions of an IO channel with the functions of network port to provide direct connectivity between Open Systems Interconnection . interconnection of open systems in accordance with In data communication sequence of binary digits including data and control signals that The process of routing and transferring data by means of addressed packets so that channel is occupied only during transmission of packet. On completion of the transmission the reference to an older IBM standard for computer peripheral interface which defines the IBM S360 and S370 channel to control unit interface. This interface uses ESA390 logical function command or program that serves as input or controls actions. The value is supplied by All the members in the SYS1. PARMLIB partitioned data set that contain parameters setting partitioned data set . directaccess storage that is divided into partitions which are called members. Each partition can contain program part of program or data. The path information unit . TH followed by basic information unit or LULU sessions that have one partner LU in its native network. Contrast with extended border node. node that uses local addresses for routing and therefore is not affected by changes in network addresses. peripheral node Interconnection reference model the layer that provides the mechanical electrical functional and procedural means to establish maintain and release physical connections over the transmission In SNA one of three types of network addressable units. physical unit exists in monitor the resources of node as requested by system services control point logical unit system that has been identified as contact point for another subnetwork for the purposes of collecting topology information. attached. The representation of physical connection to the link hardware. port is sometimes more than one port on an adapter. There may be one or more ports controlled by single data link control process. In the Internet suite of protocols or the User Datagram Protocol and higherlevel protocol or application. Some protocols An abstraction used by transport protocols to distinguish among multiple destinations within host key sequence that restarts the operating system without turning system where application programs that are already developed and tested run fix that is made available to all customers. program temporary fix architecture for direct data exchange with IO devices where both the IO device and the program The extent to which the system can be restored to an operational condition after The use of several identical functional units such as several disk drives or power supply systems within one computer system in order to provide data security and certain degree of the Internet suite of protocols and related experiments. All Internet standards are documented In TCPIP program or subroutine that obtains information from domain name server or message unit that acknowledges request unit. It may contain prefix information received in request unit. If positive the Explicit routes that terminate in the host and must use the same set of route selection services . of the topology and routing services component that determines the preferred route between specified computer that determines the path of network traffic flow. The path selection is made from several paths based on information obtained from specific protocols algorithms that attempt to identify the shortest or best path and other criteria such as metrics or protocolspecific destination addresses. segments which use similar or different architectures at the reference model network layer. This protocol determines optimum routes on the basis of route metrics not link transmission speed. datagram forwarding or to establish connection. The information is passed among routers to identify An attack in which host on the network is trying to determine what ports are open on the target host. The host doing the scan may later be the same host that does more virulent attack. can be invoked by authorized programs to take fast unformatted dump of virtual storage to data that provides communication privacy. With SSL clientserver applications can communicate in way services to workstations over network for example file server print server mail server. In network data station that provides facilities to other stations for example file server print server mail server. In the AIX operating system an application program that usually runs in which the services of layer are provided by an entity of that layer to an entity of the next higher adapter where information can be received and transmitted. single service access point can have many links terminating in it. logical address that Internet Protocol uses the services of tokenring adapter. The service access point in this network accessible units that can be activated tailored to provide various protocols and deactivated as requested. Each session is uniquely between logical units in those networks. The individual SNA networks retain their independence. across nonSNA wide area network unpacked by destination. benefit of spoofing is the prevention of In SNA session between system services control point and logical unit . The session enables the LU to request the topology in which every node on the network is connected to central node or hub through which IBMsupplied option that determines certain conditions that are to exist during the time VTAM system is operating. Start options can be predefined OSPF protocol in which interfaces and neighboring routers are always classified as being in particular state. Events on the network causes these states to change in predetermined way providing predictability and control to the OSPF routers on the hosts networks or both by manually entering routes into the routing table. Static routes are not associated resources. Within subarea node all network accessible units links and adjacent network ID. distinct partitioned piece of an internet network represented by two or more sets of addresses that are subsets of the networks range of request that serves as the interface into operating system functions such as allocating storage. The SVC protects the operating system from inappropriate user entry. All An internal control element of processor that assists in many of the that provides communications monitoring and diagnostic functions to central processor complex software function in zOS that increases availability through combination of System Authorization Facility . interface defined by zOS that enables programs to use system authorization services in order to protect system services control point . point in an SNA network for managing configuration peers can divide the network into domains of control with each SSCP controlling the physical and networks. The layered structure of SNA allows the ultimate origins and destinations of information that is the users to be independent of and unaffected by the specific SNA network services and facilities used programmer who plans generates maintains extends and controls the use In the Internet suite of protocols protocol that provides remote terminal connection service. It allows users of one host to log on to remote host and interact as directly attached terminal users of performance. It is generally measured in bits per second kilobits per second megabits mediaattached stations. FDDI or IEEE 802.5 network with ring topology that passes tokens from one attaching ring station to another. See unit of processing consisting of one or more application programs affecting one or more objects that is initiated by single request often Transmission Control Protocol . communications protocol used in the Internet and in hosts in packetswitched communications networks and in interconnected systems of such networks. It transmission group number. In subarea network single link or group of links between adjacent nodes. When transmission group consists of group of links the links are viewed as single logical link and the transmission group is Control information optionally followed by basic information unit or BIU segment that is created and used by path control to route message units and to control their being selected by the path control component in each node along route for forwarding to the next network. The TCP and ISO TP4 transport protocols provide fullduplex virtual circuits on which delivery is reliable error free sequenced and duplicate free. general term to describe secure tunnel between two The industry standard protocol for VPN is an architecture called IP Security Architecture . connection between two subarea nodes that is physically realized as particular explicit route or logical connection that is contained wholly within subarea node for intranode sessions. virtual route between distinct subarea nodes imposes transmission priority on the underlying explicit route provides flow control through virtual route pacing In Internet communications one of set of preassigned protocol port numbers that address specific functions used by transport level The commands control codes orders attributes and data or structured fields for application program and terminal. Data being transferred from or to an allocated primary or tertiary device or to the host system as continuous stream of data and 3270 Information Display System control elements in character form. more detailed discussion of the topics covered in this redbook. IBM provides access to zOS manuals on the Internet. To view search and print Introduction to the New Mainframe zOS Basics SG246366 13.12 zOS Communications Server zOS Communications Server IP Configuration Guide SC318775 zOS Communications Server SNA Customization SC316854 ABCs of zOS System Programming Volume 4 Communication Server These Web sites and URLs are also relevant as further information sources Sysprog Net independent resource for the zOS system programmer You can search for view or download Redbooks Redpapers Hints and Tips This IBM Redbook is designed for onesemester course intended to provide information systems students with the background knowledge and skills necessary to begin using the basic communications facilities of mainframebased system. It provides broad understanding of networking principles and the hardware and software components necessary to allow the mainframe to participate in high volume data communications network. It is part of planned series of textbooks designed to introduce students to mainframe concepts and help prepare them for career in large systems computing. While many of the networking concepts covered are operating systemindependent the main emphasis is on the zOS operating system. You are assumed to have successfully completed introductory courses in computer system concepts including courses in computer organization and architecture operating systems data management and data communications and systems design and analysis. basic understanding of zOS job control library structure and system libraries is assumed. It is strongly recommended that you have already completed an introductory course on zOS such as one that uses Introduction to the New Mainframe zOS Basics or comparable text. In some instances the zOS Basics course and the course associated
Consultative Committee for Space Data Systems and represents the consensus review and authorization of CCSDS documents is detailed in Organization and Processes for the Consultative Committee for Space Data Systems and the record of Agency participation in the authorization of this document can be obtained from the CCSDS The Consultative Committee for Space Data Systems is an organization officially established by the management of its members. The Committee meets periodically to address voluntary the results of Committee actions are termed Recommended Standards and are not considered binding on any Agency. members. Endorsement of this Recommendation is entirely voluntary. Endorsement however indicates the following understandings Whenever member establishes CCSDSrelated standard this standard will be in accord with the relevant Recommended Standard. Establishing such standard does not preclude other provisions which member may develop. Whenever member establishes CCSDSrelated standard that member will provide other CCSDS members with the following information The standard itself. The anticipated duration of operational service. Specific service arrangements shall be made via memoranda of agreement. Neither this Recommended Standard nor any ensuing standard is substitute for memorandum of agreement. reviewed by the CCSDS to determine whether it should remain in effect without change be changed to reflect the impact of new technologies new requirements or new directions or be retired or canceled. CCSDSrelated member standards and implementations are not negated or deemed to be nonCCSDS compatible. It is the responsibility of each member to determine when such standards or implementations are to be modified. Each member is however strongly encouraged to direct planning for its new standards and implementations towards the later This document is CCSDS Recommended Standard for the Physical Layer of signals to be used in optical communications systems of space missions. It was contributed to CCSDS by NASA. The Physical Layer concepts described herein are intended for missions that are cross supported between Agencies of the CCSDS. Attention is drawn to the possibility that some of the elements of this document may be the from the patent holder agreement that all licensing policies are reasonable and non discriminatory. However CCSDS does not have patent law staff and CCSDS shall not be held responsible for identifying any or all such patent rights. Through the process of normal evolution it is expected that expansion deletion or modification of this document may occur. This Recommended Standard is therefore subject to CCSDS document management and change control procedures which are defined in Organization and Processes for the Consultative Committee for Space Data Systems Canadian Space Agency Canada. Centre National dEtudes Spatiales France. China National Space Administration Peoples Republic of China. Deutsches Zentrum für Luft und Raumfahrt Germany. Federal Space Agency Russian Federation. Instituto Nacional de Pesquisas Espaciais Brazil. Japan Aerospace Exploration Agency Japan. National Aeronautics and Space Administration USA. Austrian Space Agency Austria. Belgian Federal Science Policy Office Belgium. Central Research Institute of Machine Building Russian Federation. China Satellite Launch and Tracking Control General Beijing Institute of Tracking and China Academy of Space Technology China. Commonwealth Scientific and Industrial Research Organization Australia. Danish National Space Center Denmark. Departamento de Ciência Tecnologia Aeroespacial Brazil. Electronics and Telecommunications Research Institute Korea. European Organization for the Exploitation of Meteorological Satellites Europe. European Telecommunications Satellite Organization Europe. GeoInformatics and Space Technology Development Agency Thailand. Indian Space Research Organization India. Institute of Space Research Russian Federation. Ministry of Communications Israel. Mohammed Bin Rashid Space Centre United Arab Emirates. National Institute of Information and Communications Technology Japan. National Oceanic and Atmospheric Administration USA. National Space Agency of the Republic of Kazakhstan Kazakhstan. National Space Organization Chinese Taipei. Naval Center for Space Technology USA. Research Institute for Particle Nuclear Physics Hungary. Scientific and Technological Research Council of Turkey Turkey. South African National Space Agency Republic of South Africa. Space and Upper Atmosphere Research Commission Pakistan. The purpose of this Recommended Standard is to specify the Physical Layer characteristics of freespace optical communications systems used by space missions. The primary application photon starved links through an atmospheric channel use of the Recommended Standard for other applications or operating conditions is not precluded. When provided with sequence of pulsed and nonpulsed slots produced by the Coding and Synchronization sublayer this specification describes the required Physical Layer characteristics of the telemetry beacon and optional data transmission accompanying beacon. This Recommended Standard defines Physical Layer schemes in terms of the signal characteristics and procedures involved in the physical transmission of the optical signals. It individual implementations or products the methods or technologies required to perform the procedures or optical communications in which the photonefficiency of the link is of primary This Recommended Standard applies to the creation of Agency standards and to the future data communications over optical space links between CCSDS Agencies in crosssupport situations. It includes comprehensive specifications of the data formats and procedures for interAgency cross support. It is neither specification of nor design for real systems that may be implemented for existing or future missions. The Recommended Standard specified in this document is to be invoked through the normal standards program of each CCSDS Agency and is applicable to those missions for which cross support based on capabilities described in this Recommended Standard is anticipated. Standard they must be implemented when this document is used as basis for cross support. Where options are allowed or implied implementation of these options is subject to specific bilateral cross support agreements between the Agencies involved. The rationale for producing this Recommended Standard is that it facilitates cross support at Such cross support requires specification of set of allowable center frequencies of transmission along with other physical layer characteristics of the signal. The CCSDS believes it is important to document the rationale underlying the recommendations chosen so that future evaluations of proposed changes or improvements will not lose sight of previous decisions. The rationale for the specifications making up this Recommended Standard is expected to be documented in forthcoming CCSDS characteristics annex is Protocol Implementation Conformance Statement Proforma annex lists acronyms used within this document The following conventions apply throughout this Specification the words shall and must imply binding and verifiable specification RECOMMENDED STANDARD FOR OPTICAL COMMUNICATIONS PHYSICAL LAYER the word should implies an optional but desirable specification the word may implies an optional specification the words is are and will imply statements of fact. In this document the following convention is used to identify each bit in an Nbit field. The defined to be Bit 0 the following bit is defined to be Bit 1 and so on up to Bit N1. When the field is used to express binary value the Most Significant Bit In accordance with standard datacommunications practice data fields are often grouped into 8 bit words which conform to the above convention. Throughout this specification such an 8 bit word is called an octet. The numbering for octets within data structure starts with 0. bit is binary digit transferred between the Data Link Protocol sublayer and the Coding and Synchronization sublayer. Other symbols whether binary or nonbinary will be referred to by other names such as binary digits. It should be understood that the ordering conventions described above apply equally to other types of symbols. RECOMMENDED STANDARD FOR OPTICAL COMMUNICATIONS PHYSICAL LAYER encouraged to investigate the possibility of applying the most recent editions of the Space Data System Standards CCSDS 142.0B1. Washington . Interconnection reference model . Two sublayers of the Data Link Layer are defined for CCSDS space link protocols. The Data Link Protocol sublayer provides functions for producing Transfer Frames possible Space Data Link Protocols using optical communications are the TM Space Data Link Protocol the AOS Space Data Link Protocol and the Unified Space Data Link Protocol . The Optical Coding and Synchronization protocol provides the functions of the Coding and Synchronization sublayer of the Data Link Layer for transferring Transfer Frames over an optical space link. The Optical Communications Physical Layer specified in this Recommended Standard provides the required characteristics of the Physical Layer transmission from space to ground and from ground to space. The Optical Communications Physical Layer specifies the physical characteristics of the telemetry signal and separately the physical characteristics of the beacon and optional telecommand signal. In typical application spacecraft transmits telemetry to ground station and ground station transmits beacon and optional transfer frames to the For each of the telemetry and beacon and optional data transmission specifications this Recommended Standard defines the transmission lasers required center frequency tuning range linewidth inband and spillover emissions polarization modulation pulse shape timing jitter and supported slot widths. Throughout the communications session the optical Terminal transmits beacon together with optional AOS or USLP transfer frames. The Terminal receiver locks onto the beacon and uses it to assist in accurately pointing its optical transmitter. Any AOSUSLP transfer frames are also decoded onboard. Telemetry is transmitted from terminal and received by Terminal . This Recommended Standard specifies the physical characteristics of the Terminal and Terminal transmitters. In typical application Terminal on the ground transmits an uplink beacon and optional AOSUSLP transfer frames to Terminal in space and Terminal transmits downlink telemetry signal to Terminal . At the sending end binary vector is received from the Coding and Synchronization sublayer as defined in reference indicating sequence of slots in which light pulses are to be present or absent . The physical characteristics of these pulses at the sending end are described below. At the receiving end the Physical Layer delivers slot measurements to the Coding and Synchronization sublayer. The center frequency shall be 193.1 n0.1 THz where is an integer ranging from 18 to NOTE These center frequencies in the optical Cband are subset of those defined in the The transmitter center frequency shall be accurate to within tolerance of 10 GHz. The modulated laser linewidth shall be less than 6.25 GHz measured at full width 1e2 of maximum over time scale of 100 ms. The laser shall transmit 95 percent of its energy within 10 GHz of its center frequency. Polarized laser emission is optional. When polarized the laser emission exiting the terminal aperture shall be RightHand Circularly Polarized as defined in reference . When polarized emission is used the polarization extinction ratio shall be greater than 10 dB. RECOMMENDED STANDARD FOR OPTICAL COMMUNICATIONS PHYSICAL LAYER The binary vector received from the Coding and Synchronization sublayer defined in reference shall be used to modulate the intensity of emitted light within each slot using OnOff Keying . NOTE modulation of Pulse Position Modulation at the Coding and Synchronization sublayer gives rise to OOK at the Physical Layer in the sense that light pulse is present or absent in each slot. The Root Mean Square pulse timing jitter shall be less than 10 percent of the slot The laser shall support range of Pulse Repetition Rates corresponding to the slot width and PPM order used by the communications link. NOTE For example in system using 1 ns slots with 16PPM and 4 slots of guard time pulses could be as close as 5 ns and as far apart as 35 ns center to center At the sending end binary vector is received from the Coding and Synchronization sublayer as defined in reference indicating sequence of slots in which light pulses are to be present or absent . The physical characteristics of these pulses at the sending end are described below. At the receiving end the Physical Layer delivers slot measurements to the Coding and Synchronization sublayer. The center frequency shall be tunable to any frequency within 26 GHz of 280.18 THz NOTE These center frequencies correspond to wavelengths in vacuum of 1070 nm 1064.15 nm and 1030 nm respectively. The transmitter center frequency shall be accurate to within tolerance of 26.5 GHz. The laser linewidth shall not exceed 53 GHz measured at full width 1e2 of maximum over time scale of 100 ms. The laser shall transmit 95 percent of its energy within 50 GHz of its center frequency. Data transmission is optional. When data transmission is not used the beacon transmission shall be 3.8145 kHz square wave. The period of the square wave is 262144 ns that is an alternating sequence of pulsed When data transmission is used the binary vector received from the Coding and Synchronization sublayer defined in reference shall be used to modulate the intensity of emitted light within each slot using OOK. modulation of PPM at the Coding and Synchronization sublayer gives rise to OOK at the Physical Layer in the sense that light pulse is present or absent in each slot. The slot width shall be 65536 ns. NOTE This Recommended Standard can be compatible with systems using substantially narrower pulse shapes that convey additional higherrate telecommand data than is specified in this Recommended Standard. Such nested outer modulations are not specified by this Recommended Standard nor are they necessarily precluded by this Recommended Standard. The RMS pulse timing jitter shall be less than 10 percent of the slot width. This annex provides the Protocol Implementation Conformance Statement Requirements List for an implementation of Optical Communications Physical Layer . The ICS for an implementation is generated by completing the RL in accordance with the instructions below. An implementation claiming conformance must satisfy the mandatory requirements referenced in the RL. The RL support column in this annex is blank. An implementations completed RL is called the ICS. The ICS states which capabilities and options have been implemented. The following can use the ICS the implementer as checklist to reduce the risk of failure to conform to the standard through oversight supplier or potential acquirer of the implementation as detailed indication of the capabilities of the implementation stated relative to the common basis for understanding provided by the standard ICS proforma user or potential user of the implementation as basis for initially checking the possibility of interworking with another implementation tester as the basis for selecting appropriate tests against which to assess the claim abbreviations and conventions described below. The item column contains sequential numbers for items in the table. The description column contains brief description of the item. It implicitly means Is this qualified optional for group of related optional items labeled by the same numeral it is mandatory to support at least one of the items. the parent item is supported and is not applicable otherwise. One or more of the allowed values must be supported. The support column is to be used by the implementer to state whether feature is supported NA Not applicable. The support column should also be used when appropriate to enter values supported for An implementer shows the extent of compliance to the Recommended Standard by completing the RL that is the state of compliance with all mandatory requirements and the options supported are shown. The resulting completed RL is called an ICS. The implementer shall complete the RL by entering appropriate responses in the support or values supported RECOMMENDED STANDARD FOR OPTICAL COMMUNICATIONS PHYSICAL LAYER column using the notation described in A1.2. If conditional requirement is inapplicable NA should be used. If mandatory requirement is not satisfied exception information must be supplied by entering reference Xi where is unique identifier to an accompanying A2.1.2 Identification of Implementation Under Test Other information necessary for full identification Recommended Standard. Nonsupported mandatory capabilities are to be identified in the HPE beacon and optional accompanying data transmission signaling It is assumed that security is provided by encryption authentication methods and access control to be performed at layer above the physical layer and coding and synchronization sublayer. Mission and service providers are expected to select from recommended security methods suitable to the specific application profile. Specification of these security methods and other security provisions is outside the scope of this Recommended Standard. The Physical Layer has the objective of delivering data with the minimum possible amount of residual errors. The associated channel coding as described in reference must be used to ensure that residual errors are detected and the frame flagged. There is an extremely low probability of additional undetected errors that may escape this scrutiny. These errors may affect the encryption process in unpredictable ways possibly affecting the decryption stage and producing data loss but will not compromise the security of the data. Security concerns in the areas of data privacy authentication access control availability of resources and auditing are to be addressed in higher layers and are outside the scope of this There are no specific security measures prescribed for the Physical Layer. Therefore consequences of not applying security are only imputable to the lack of proper security measures in other layers. Residual undetected errors may produce additional data loss when the link carries encrypted data. The recommendations of this document do not require any action from SANA. No patents are known to relate to this Recommended Standard. RECOMMENDED STANDARD FOR OPTICAL COMMUNICATIONS PHYSICAL LAYER This annex lists key abbreviations and terms that are used throughout this Recommended Unified Space Data Link Protocol center frequency The central frequency of laser beam occupying range of frequencies. laser linewidth The spectral linewidth of laser beam. RECOMMENDED STANDARD FOR OPTICAL COMMUNICATIONS PHYSICAL LAYER optical pulse An emission of photons often constrained with respect to its amplitude shape and duration. polarization extinction ratio The ratio of optical powers of perpendicular polarization. pulse repetition rate PRR The number of emitted pulses per second or the inverse temporal pulse spacing. righthand circular polarization RHCP circularly polarized wave in which the electric field vector rotates in righthand sense with respect to the direction of propagation. spillover emissions The energy of an emission that is outside of defined spectral band. RECOMMENDED STANDARD FOR OPTICAL COMMUNICATIONS PHYSICAL LAYER Information TechnologyOpen Systems InterconnectionBasic Reference Model The Basic Model. 2nd ed. International Standard ISOIEC 749811994. Geneva ISO Recommendation .694.1. Geneva ITU 2012. Moision and . Hamkins. Coded Modulation for the DeepSpace Optical Channel Robert . Gagliardi and Sherman Karp. Optical Communications. 2nd ed. Hoboken New Jersey Wiley March 1995. Hamid Hemmati ed. Deep Space Optical Communications. Hoboken New Jersey

Consultative Committee for Space Data Systems and represents the consensus review and authorization of CCSDS documents is detailed in Organization and Processes for the Consultative Committee for Space Data Systems and the record of Agency participation in the authorization of this document can be obtained from the CCSDS The Consultative Committee for Space Data Systems is an organization officially established by the management of its members. The Committee meets periodically to address voluntary the results of Committee actions are termed Recommended Standards and are not considered binding on any Agency. members. Endorsement of this Recommendation is entirely voluntary. Endorsement however indicates the following understandings Whenever member establishes CCSDSrelated standard this standard will be in accord with the relevant Recommended Standard. Establishing such standard does not preclude other provisions which member may develop. Whenever member establishes CCSDSrelated standard that member will provide other CCSDS members with the following information The standard itself. The anticipated duration of operational service. Specific service arrangements shall be made via memoranda of agreement. Neither this Recommended Standard nor any ensuing standard is substitute for memorandum of agreement. reviewed by the CCSDS to determine whether it should remain in effect without change be changed to reflect the impact of new technologies new requirements or new directions or be retired or canceled. CCSDSrelated member standards and implementations are not negated or deemed to be nonCCSDS compatible. It is the responsibility of each member to determine when such standards or implementations are to be modified. Each member is however strongly encouraged to direct planning for its new standards and implementations towards the later Attention is drawn to the possibility that some of the elements of this document may be the subject of patent rights. CCSDS shall not be held responsible for identifying any or all such Through the process of normal evolution it is expected that expansion deletion or modification of this document may occur. This Recommended Standard is therefore subject to CCSDS document management and change control procedures which are defined in Organization and Processes for the Consultative Committee for Space Data Systems Canadian Space Agency Canada. Centre National dEtudes Spatiales France. China National Space Administration Peoples Republic of China. Deutsches Zentrum für Luft und Raumfahrt Germany. Federal Space Agency Russian Federation. Instituto Nacional de Pesquisas Espaciais Brazil. Japan Aerospace Exploration Agency Japan. National Aeronautics and Space Administration USA. Austrian Space Agency Austria. Belgian Federal Science Policy Office Belgium. Central Research Institute of Machine Building Russian Federation. China Satellite Launch and Tracking Control General Beijing Institute of Tracking and Telecommunications Technology China. Chinese Academy of Sciences China. Chinese Academy of Space Technology China. Commonwealth Scientific and Industrial Research Organization Australia. Danish National Space Center Denmark. Departamento de Ciência Tecnologia Aeroespacial Brazil. European Organization for the Exploitation of Meteorological Satellites Europe. European Telecommunications Satellite Organization Europe. GeoInformatics and Space Technology Development Agency Thailand. Hellenic National Space Committee Greece. Indian Space Research Organization India. Institute of Space Research Russian Federation. KFKI Research Institute for Particle Nuclear Physics Hungary. Korea Aerospace Research Institute Korea. Ministry of Communications Israel. National Institute of Information and Communications Technology Japan. National Oceanic and Atmospheric Administration USA. National Space Agency of the Republic of Kazakhstan Kazakhstan. National Space Organization Chinese Taipei. Naval Center for Space Technology USA. Scientific and Technological Research Council of Turkey Turkey. South African National Space Agency Republic of South Africa. Space and Upper Atmosphere Research Commission Pakistan. CCSDS RECOMMENDED STANDARD FOR PROXIMITY1 SPACE LINK PROTOCOL 31 Categories of Radio Equipment Contained on Proximity1 Link Elements . 31 The purpose of this Recommended Standard is to specify Physical Layer procedures used defined to be shortrange bidirectional fixed or mobile radio links generally used to communicate among probes landers rovers orbiting constellations and orbiting relays. These links are characterized by short time delays moderate signals and short independent sessions. This Recommended Standard defines the Proximity1 Space Link Protocol Physical Layer. The specification for the channel connection process provision for frequency bands and assignments hailing channel polarization modulation data rates and performance requirements are defined in this document. Currently the Physical Layer only defines operations at UHF frequencies for the Mars The Data Link Layer is defined in the two separate CCSDS Recommended Standards entitled Proximity1 Space Link ProtocolCoding and Synchronization Sublayer and Proximity1 Space Link ProtocolData Link Layer . individual implementations or products implementation of service interfaces within real systems the methods or technologies required to perform the procedures or This Recommended Standard applies to the creation of Agency standards and to future data communications over space links between CCSDS Agencies in crosssupport situations. It applies also to internal Agency links where no crosssupport is required. It includes specification of the services and protocols for interAgency cross support. It is neither specification of nor design for systems that may be implemented for existing or future The Recommended Standard specified in this document is to be invoked through the normal standards programs of each CCSDS Agency and is applicable to those missions for which cross support based on capabilities described in this Recommended Standard is anticipated. Standard they must be implemented when this document is used as basis for cross support. Where options are allowed or implied implementation of these options is subject to specific bilateral cross support agreements between the Agencies involved. The CCSDS believes it is important to document the rationale underlying the recommendations chosen so that future evaluations of proposed changes or improvements will not lose sight of previous decisions. Concept and rationale behind the decisions that formed the basis for Proximity1 are documented in the CCSDS Proximity1 Space Link Protocol Green Book reference . This Recommended Standard makes use of number of terms defined in reference . In this Recommended Standard those terms are used in generic sense . in the sense that those terms are generally applicable to any of variety of technologies that provide for the exchange of information between real systems. Those terms are as follows service data unit. For the purposes of this Recommended Standard the following definitions also apply. Many caller and responder Initiator and receiver respectively in Proximity space link session. NOTE caller transceiver is the initiator of the link establishment process and manager of negotiation of the session. responder transceiver typically receives link establishment parameters from the caller. The caller initiates communication between itself and responder on prearranged communications channel with predefined controlling parameters. As necessary the caller and responder may negotiate the controlling parameters for the session . forward link That portion of Proximity space link in which the caller transmits and the responder receives . The term forward is used in association with any parameters referring to the forward link. hailing The persistent activity used to establish Proximity link by caller to responder in either full or half duplex. It does not apply to simplex operations. hailing channel The forward and return frequency pairs that caller and responder use to establish physical link communications. physical channel The RF channel upon which the stream of channel symbols is transferred over space link in single direction. PLTU Proximity Link Transmission Unit the data unit composed of the Attached Redundancy Check 32. Proximity link fullduplex halfduplex or simplex link for the transfer of data between Proximity1 entities in session. return link That portion of Proximity space link in which the responder transmits and the caller receives . The term return is used in association with any parameters referring to the return link. session dialog between two or more communicating Proximity link transceivers. NOTE session consists of three distinct operational phases session establishment data services and session termination. Session termination can be coordinated or if communication is lost the transceivers will eventually independently conclude the dialog is over. space link communications link between transmitting and receiving entities at least one of which is in space. working channel forward and return frequency pair used for transferring User datainformation frames and Protocolsupervisory frames during the data service and session termination phases. The following conventions apply for the normative specifications in this Recommended the words shall and must imply binding and verifiable specification the word should implies an optional but desirable specification the word may implies an optional specification the words is are and will imply statements of fact. NOTE These conventions do not imply constraints on diction in text that is clearly informative in nature. Throughout this Recommended Standard directive parameter variable and signal names are presented with all uppercase characters datafield and MIBparameter names are presented with initial capitalization values and state names are presented with predominantly lowercase italic characters. In Proximity1 data rate coded symbol rate and channel symbol rate are used to denote respectively the data rate of the bitstream composed by PLTUs and Idle data measured at the the coded data rate measured at the interface between the Coding and Synchronization Sublayer and the Physical Layer and the rate measured at the output of the transmitter. for the purpose of the present standard encouraged to investigate the possibility of applying the most recent editions of the Information TechnologyOpen Systems InterconnectionBasic Reference Model The Basic Model. International Standard ISOIEC 749811994. 2nd ed. Geneva ISO Recommendation for Space Data System Standards CCSDS 211.2B2. Washington . CCSDS December 2013. Space Data System Standards CCSDS 211.0B5. Washington . Proximity1 is bidirectional Space Link Layer protocol for use by space missions. It and . This protocol has been designed to meet the requirements of space missions for efficient transfer of space data over various types and characteristics of Proximity space Proximity1 activities are divided between send side and receive side. The send side is concerned with the transmitted physical channel and also with the acquisition of the received physical channel in order to establish Proximity1 link. The operation of the transmitter is statedriven. The receive side is concerned with the reception of data on the received physical channel the input symbols stream and the protocol data units it contains. Once the receiver is turned on its operation is modeless. It accepts and processes all valid local and remote directives and received service data units. On the send side the Physical Layer accepts control variables from the MAC Sublayer of the Data Link Layer for control of the transceiver accepts coded symbols stream from the Coding Synchronization Sublayer of the Data Link Layer for modulation onto the radiated carrier. to the MAC Sublayer of the Data Link Layer. features relevant to the Physical Layer. For fuller description of the overall Proximity1 system of the Data Link Layer and of its sublayers . On the send side the Data Link Layer is responsible for providing the coded symbols to be transmitted by the Physical Layer. On the receive side the Data Link Layer accepts the serial coded symbols stream output from the receiver in the Physical Layer and processes the Protocol Data Units contained in it. Within the Data Link Layer the Medium Access Control Sublayer and the Coding and Synchronization Sublayer have interfaces to the The Medium Access Control Sublayer controls the establishment maintenance and termination of communications sessions for pointtopoint communications between Proximity entities. It controls the operational state of the Data Link and Physical Layers using control variables. It accepts Proximity1 directives both from the local vehicle controller and across the Proximity link to control its operations. The MAC Sublayer is also responsible for the storage and distribution of the Management Information Base On the send side the CS Sublayer generates the output coded symbols stream containing Proximity Link Transmission Units and Idle data which is delivered to the Physical Layer for modulation onto the radiated carrier. On the receive side the CS Sublayer accepts the incoming serial coded symbols stream from the Physical Layer and delimits each PLTU contained in the symbol stream. Layer. The Proximity1 Link system supports the communication and navigation needs between variety of network elements . orbiters landers rovers microprobes balloons aerobots gliders. The categories of radio equipment in network elements are listed in needed shall have transmitreceive frequency coherency capability. from the MAC Sublayer of the Data Link Layer for control of the transceiver. Reference contains the specification of the actions to establish maintain and terminate Proximity1 communications session the actions are specified in state tables and the control variables are defined. Requirements in 3.2.2 and 3.2.3 below complement the The establishment of the communications channel depends on the configuration of the following Physical Layer parameters frequency polarization modulation acquisition idle sequence and coded symbol rates such that common operating characteristics exist in both communicating entities. The MAC Sublayer sets the local transceiver to the desired physical configuration under the control of the directives SET TRANSMITTER PARAMETERS and SET RECEIVER PARAMETERS. SET PL EXTENSIONS directive is the mechanism by which additional Physical Layer parameters defined outside of the Proximity1 Physical Layer can be enabled or disabled. The format and content of these and other Proximity1 directives are specified The operational state of the transmitter shall depend on the state control variables MODE Unless MODE is inactive change in the value of TRANSMIT signals the Physical Layer to transition the transmitter to on or off. An MIB parameter Carrier_Only_Duration is used in the Data Link Layer to control the duration of the carrieronly transmission . The content of the output coded symbols stream is specified in reference which defines an output coded symbols stream FIFO. The data include PLTUs and Idle data for example in the Acquisition sequence that is sent when transmission commences so that the receiving unit can acquire the signal. The format of Idle data is specified in reference . The operational state of the receiver shall depend on the state control variables MODE Physical Layer delivers the received coded symbol streams to the Data Link Layer . The receiver shall sweep the frequency channel to which it is assigned in order to acquire carrier lock at an assigned frequency channel. NOTE During this process the receiver first attempts to lock to the carrier. has acquired carrier signal. to the received RF signal and false when not in lock. symbol synchronization has been acquired and the received serial symbol stream is being provided to the Data Link Layer. symbol lock and false when the receiver is not in symbol lock. NOTE The receiver is considered to be in symbol lock when it is confident that its symbol detection processes are synchronized to the modulated symbol stream and the symbols output are of an acceptable quality for processing by the Data symbol stream to the CS Sublayer. NOTE Soft symbol decisions with at least three bits quantization are recommended whenever constraints permit. This Recommended Standard is designed primarily for use in Proximity link space environment far from Earth. The radio frequencies selected in this Recommended Standard are designed not to cause interference to radio communication services allocated by the Radio Regulations of the International Telecommunication Union . It should be noted that particular precautions have to be taken to protect frequency bands allocated to Near Earth Space Research Deep Space and Space Research passive. The frequencies specified near 430 MHz cannot be used for this purpose in the vicinity of the Earth and particular precautions have to be taken for equipment testing on Earth. However by layering appropriately provision is made to change only the Physical Layer by adding other frequencies to enable the same protocol to be used in near Earth applications in the The frequency range for the UHF Proximity1 links consists of 60 MHz between 390 MHz to Annex of reference defines the SET TRANSMITTER PARAMETERS and SET assignment for the remote vehicles transmitter and receiver for Channels 0 through 7. Annex of reference also defines the SET PL EXTENSIONS directive for Channels 8 through 15. The selection of the frequencies is subject to Space Frequency Coordination Group turnaround ratio. NOTE Enterprisespecific hailing channel frequencies can be defined in the default configuration of the Physical Layer parameters. forward and return frequency pair then the hailing channel shall be the same as the working working channel should be done as soon as possible. Hailing is an activity used to establish Proximity link with remote vehicle. Hailing requires the use of hailing frequency pair. Hailing is bidirectional . either element can initiate hailing. Hailing is done at low data rate and therefore is low bandwidth activity. Channel 1 has been selected to minimize the use of UHF bandwidth. nominally performed on the hailing channel. However if transceivers are compatibly transceivers are fixed frequency and use Channel 0. the link establishment process. There are various parameters associated with the hailing activity that are defined in the MIB. Annex of reference defines these NOTE Forward and return link frequencies may be coherently related or noncoherent. frequency pairs are defined for Proximity1 operations Channel 0. In the case where the system requires only one return frequency associated with the forward 437.1 MHz frequency the return frequency shall be Channel 2. In the case where the system requires only one return frequency associated with the forward 439.2 MHz frequency the return frequency shall be Channel 3. In the case where the system requires only one return frequency associated with the forward 444.6 MHz frequency the return frequency shall be NOTE Channels 8 through 15 are defined in the SET PL EXTENSIONS directive . The assignment of specific frequencies to these In the case where there is need for one or multiple return frequencies paired with one or MHz band in 20 kHz steps and the return frequencies shall be selected from 390 to 405 MHz in 20 kHz steps. These frequency pairs shall be distinct from the frequency pairs defined in Channels 0 through 7. The forward and return frequency components of Channels 8 through 15 are reserved for this purpose. NOTE Forward and return link frequencies may be coherently related or noncoherent. Other frequency bands are intentionally left unspecified until user need for them is NOTE If such need arises users are requested to contact the CCSDS Secretariat at Both forward and return links shall operate with Right Hand Circular Polarization . ratio is between 0.98 and 1.02. frequency carrier. For directly modulated BiphaseL waveform symbol 1 shall result in an advance of the phase of the radio frequency carrier at symbol 0 shall result in delay. The Proximity1 link shall support one or more of the following 13 discrete forward and return values for the coded symbol rate Rcs shown in symbols per second 1000 2000 4000 NOTE The correspondence between Rcs and Rd can be found in annex of reference . SET RECEIVER PARAMETERS and SET PL EXTENSIONS directives defined in annex of reference the coded symbol rate Rcs is set according to the set value of Rd and to the selected coding option. Each channel symbol period as measured at the output of the transmitter shall differ by no more than 1 from the channel symbol period corresponding to the Proximity1 channel symbol rate in use. Generated channel symbol rate measured over an interval greater than 10000 symbol periods shall differ less than 0.1 from the defined Proximity1 channel symbol rates as measured at the output of the transmitter. and over all operating conditions shall be 10 ppm. Residual amplitude modulation of the phase modulated RF signal shall be less than 2 RMS. In noncoherent mode the minimum specification for the oscillator phase noise at 437.1 MHz relative to the carrier power vs. frequency offset from the carrier in Hz. The discrete spurious spectral lines of the transmit RF signal shall be limited by the template . The factor of 2 is due to the use of Bi phaseL waveforms. For the UHF frequencies specified in this Recommended Standard the applicable Doppler 2 200 Hzs . The Doppler frequency rate does not include the Doppler rate required for tracking canister or worstcase spacecrafttospacecraft cases. The Doppler acquisition and tracking requirements imposed on any of the network elements are specified according to radio frequencies employed on the link. resides will determine the applicability of capturing Doppler The requirement applies to the RF interface between all E1 and E2 elements. In the case of the coherent RF interface between E2c elements the effect of the coherent turnaround ratio of the responding element has to be considered. Other frequency bands requirements are intentionally left unspecified until user need for them is identified. NOTE If such need arises users are requested to contact the CCSDS Secretariat at This annex provides the Protocol Implementation Conformance Statement Requirements List for an implementation of Proximity1 Space Link Protocol Physical Layer . The PICS for an implementation is generated by completing the RL in accordance with the instructions below. An implementation claiming conformance must satisfy the mandatory requirements referenced in the RL. The RL support column in this annex is blank. An implementations completed RL is called the PICS. The PICS states which capabilities and options have been implemented. The following can use the PICS the implementer as checklist to reduce the risk of failure to conform to the standard through oversight supplier or potential acquirer of the implementation as detailed indication of the capabilities of the implementation stated relative to the common basis for understanding provided by the standard PICS proforma user or potential user of the implementation as basis for initially checking the possibility of interworking with another implementation tester as the basis for selecting appropriate tests against which to assess the claim abbreviations and conventions described below. The item column contains sequential numbers for items in the table. The feature column contains brief descriptive name for feature. It implicitly means Is this feature supported by the implementation optional but support of at least one of the group of options labeled by The support column is to be used by the implementer to state whether feature is supported No not supported by the implementation. The support column should also be used when appropriate to enter values supported for An implementer shows the extent of compliance to the Recommended Standard by completing the RL that is the state of compliance with all mandatory requirements and the options supported are shown. The resulting completed RL is called PICS. The implementer shall complete the RL by entering appropriate responses in the support or values supported column using the notation described in A1.2. If conditional requirement is inapplicable NA should be used. If mandatory requirement is not satisfied exception information must be supplied by entering reference Xi where is unique identifier to an accompanying A2.1.2 Identification of Implementation Under Test YES answer means that the implementation does not conform to the Recommended Standard. Nonsupported mandatory capabilities are to be identified in the PICS with an explanation of why the implementation is non .1 Support for one of these categories must be indicated. C1 IF THEN ELSE . C2 IF THEN ELSE NA. rate measurements are needed. Channel 1 is recommended Channel 0 is used by legacy systems Channel is to be used by radios with only one channel or if agreed to. The working channel has to be the same as the hailing channel for radios with only Support for at least one of the indicated 13 discrete values for the coded symbol rate The security concern involves radio frequency jamming of the forward andor return link signal. Jamming of the signal could lead to the total loss of data and potential navigation errors if Doppler tracking is disrupted. SECURITY CONCERNS WITH RESPECT TO THE CCSDS DOCUMENT The forward and return link signals are vulnerable to jamming although there are several mitigating factors. The forward signal cannot be transmitted from Earth since the currently specified channels are reserved by ITU to other services on the Earth surface. deliberate attempt at jamming the forward signal in violation of the ITU regulations would disrupt very large number of terrestrial links in light of the difference in distances involved between terrestrial user and Proximity1 user on Mars. Concerning the return signal there is limited availability of equipment capable of generating enough uplink power to effectively jam the spacecraft receiver at interplanetary Jamming of the signal could result in the loss of data or of Doppler measurements. During critical maneuver jamming could cause uncertainty in the Jamming denies all communications and protection must be accomplished by Physical Layer techniques such as spread spectrum andor frequency hopping. This problem is somewhat mitigated by the amount of power and the size of antennas needed to communicate with the spacecraft or by the need of having jamming source in Mars orbit. No patents are known to apply to this Recommended Standard. CCSDS RECOMMENDED STANDARD FOR PROXIMITY1 SPACE LINK PROTOCOL Washington . CCSDS December 2013. System Standards CCSDS 131.0B2. Washington . CCSDS August Radio Frequency and Modulation SystemsPart 1 Earth Stations and Spacecraft.

IEEE International Conference on Consumer Electronics Sr. Director of Marketing of Parade Technologies Inc. VESA Task Group Chair Marketing Notebook and 3D Task Groups Next Generation Display Interface for Personal Computer Products The PC industry plans to phase out VGA and DVI over the next few years DisplayPort will serve as the new interface for PC DisplayPort will serve as the new interface for PC Now integrated into all mainstream GPUs and integrated GPU chip sets DP receptacles appearing on new PCs and Embedded DisplayPort is the new interface for internal DisplayPort is being enabled in handheld applications The scalable electrical interface serves small and large DisplayPort is included in the PDMI standard Color Depth Color Accuracy Multiple display support Power included at connector protocol support included Auxiliary channel can be used for other data traffic Expandable packetbased protocol and link operation rates Provides addition data services and display control options Scalable for large and small devices displays and cables Singlelane can support 1680 1050 at 18 bpp Easier chip integration simpler physical interface Leads to lower system cost lower power sleeker designs Adaptable to other data interfaces types Isosynchronous packet stream and control protocols can be Isosynchronous packet stream and control protocols can be Used directly as display interface or as baseband signal for Used directly as display interface or as baseband signal for Consists of single analog waveform that includes display synchronization and pixel content Physical interface includes AV stream data and timing Use dedicated pixel clock signal Use Hsync and Vync symbols embedded in digital video stream Unlike other uncompressed data display interfaces data packet utilization is similar to communication standards such Ethernet PCI utilization is similar to communication standards such Ethernet PCI Scalable interface fits variety of system and display applications Future extensible to address new applications and system topologies Designed for DisplayPort transport and physical interface but can be extended through other transport standards DisplayPort uses layered protocol for Isochronous AV DisplayPort uses layered protocol for Isochronous AV AV Streams are received by the Source and regenerated by the Sink The Stream Policy Maker manages the transport of the stream The Link Policy Maker is responsible for establishing the data path and keeping the link synchronized. The Transport Layer is the SourcetoSink data interface including AV data packetization and inclusion of other data The Physical Layer involves the electrical interface The Physical Layer involves the electrical interface The layered architecture of DisplayPort allows it to be extensible to other transport The Isochronous AV Stream can sent be within dedicated or shared transport VESA and the WiGig Alliance are currently working on the protocol adapter layer One useful MST application is multiple display support from This is particularly suited for portable devices that have limited Here we will review the DisplayPort Cable signals Signal amplitude andor preemphasis can be increased as result of link training Link training occurs during initial operation or can be re initiated after data errors detected. Link training compensates for various connector cable Each main link lane uses 8B10B encoding which provides Spreadspectrum clocking can be enabled for further EMI All DP Source devices are designed to accept SSC 1 2 or 4 lanes can be enabled depending on AV stream Application Bandwidth New speed option Enabled by DisplayPort 1.2 Specification Signal provided by the Sink to the Source Low signal 2 msec indicates Sink absence Low signal of 0.5 to 1ms indicates interrupt from Sink DisplayPort Source and Sink receptacle includes power pin Standard high bandwidth cables serve existing DP 1.1a and future DP 1.2 systems reduced bandwidth passive cables are available in greater lengths to serve projector and digital Higher bandwidth active cables and hybrid cables also The DisplayPort transport layer is operated at data rate above the stream data rate Stuffing symbols are used between valid data symbols When sending video display data the transfer units are stuffed in means to distribute the video packets evenly over display line This means of data system distribution minimizes data buffering in the display The Vertical and Horizontal Blanking periods are used to send other packet types DisplayPort 1.1a defined the use of single main content stream normally used DisplayPort 1.1a defined the use of single main content stream normally used DisplayPort 1.2 adds the option for multiple data stream within the Transportformatforsendingasinglestreamofvideo oraudio provides very precise time control of audio channel timing. Each audio channel can have an independent time delay adjustment between 0 and 4.3 seconds relative to given Source in 100 nanosecond resolution. Used both for lip sync and MSA Data Packets are sent once per video frame during the vertical The MSA describes the format of the video with given stream The MSA describes the format of the video with given stream Usedforvideostreamclockregenerationinthedisplay Usedforvideostreamclockregenerationinthedisplay Indentifies pixelcolorcodingformatnumberofbitsperpixelcolorgamutand Framing Symbols are used to identify the BEGINNING and END of Vertical Blanking 1Mbps Burst transfer 16 data bytes max Capable of establishing 200Kbps fullduplex link Fast AUX transport format 720Mbps Burst transfer 641024 data bytes max Capable of establishing 200Mbps fullduplex link AUX is first used by the Source to Discover Sink Capabilities display EDID The support of video content protection through HDCP key exchanges Determines DisplayPort link transport capabilities by reading DPCD AUX is also used to discover interface topology If MST is supported and what topology routing will be present The stream and link policy makers use this information to AUX Channel Functions During Normal Link Operation Sink can notify Source that main link data corruption has occurred Data and symbol lock and optional ECC can be used Source can reinitiate link training to reestablish link AUX can be used to transport auxiliary data such as Camera and Microphone AV data from Sink to Source for teleconferencing Fast AUX mode can be used for USB 2.0 data to support USB hub in Display AUX can be used to control display setting and operation Can directly support MCCS using I2CoverAUX protocol Can also support dedicated display control DPCD registers as now used in Example System Application Utilizing AUX Data Transport Many DP 1.1a devices are available from the top PC OEMs Multistream capable Source devices hubs and monitors Protocol layer for USB over Fast AUX in development
